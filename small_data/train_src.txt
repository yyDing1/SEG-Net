virtually enhancing the perception of user actions . <eos> this paper proposes using virtual reality to enhance the perception of actions by distant users on a shared application . here , distance may refer either to space ( e.g. in a remote synchronous collaboration ) or time ( e.g. during playback of recorded actions ) . our approach consists in immersing the application in a virtual inhabited 3d space and mimicking user actions by animating avatars . we illustrate this approach with two applications , the one for remote collaboration on a shared application and the other to playback recorded sequences of user actions . we suggest this could be a low cost enhancement for telepresence .
dynamic range improvement of multistage multibit sigma delta modulator for low oversampling ratios . <eos> this paper presents an improved architecture of the multistage multibit sigma delta modulators ( eams ) for wide band applications . our approach is based on two resonator topologies , high q cascade of resonator with feedforward ( hqcrff ) and low q cascade of integrator with feedforward ( lqceff ) . because of in band zeros introduced by internal loop filters , the proposed architecture enhances the suppression of the in band quantization noise at a low osr . the hqcrff based modulator with single bit quantizer has two modes of operation , modulation and oscillation . when the hqcrff based modulator is operating in oscillation mode , the feedback path from the quantizer output to the input summing node is disabled and hence the modulator output is free of the quantization noise terms . although operating in oscillation mode is not allowed for single stage sigmadeltam , the oscillation of hqcrff based modulator can improve dynamic range ( dr ) of the multistage ( mash ) sigmadeltam . the key to improving dr is to use hqcrff based modulator in the first stage and have the first stage oscillated . when the first stage oscillates , the coarse quantization noise vanishes and hence circuit nonidealities , such as finite op amp gain and capacitor mismatching , do not cause leakage quantization noise problem . according to theoretical and numerical analysis , the proposed mash architecture can inherently have wide dr without using additional calibration techniques .
an ontology modelling perspective on business reporting . <eos> in this paper , we discuss the motivation and the fundamentals of an ontology representation of business reporting data and metadata structures as defined in the extensible business reporting language ( xbrl ) standard . the core motivation for an ontology representation is the enhanced potential for integrated analytic applications that build on quantitative reporting data combined with structured and unstructured data from additional sources . applications of this kind will enable significant enhancements in regulatory compliance management , as they enable business analytics combined with inference engines for statistical , but also for logical inferences . in order to define a suitable ontology representation of business reporting language structures , an analysis of the logical principles of the reporting metadata taxonomies and further classification systems is presented . based on this analysis , a representation of the generally accepted accounting principles taxonomies in xbrl by an ontology provided in the web ontology language ( owl ) is proposed . an additional advantage of this representation is its compliance with the recent ontology definition metamodel ( odm ) standard issued by omg .
the self organizing map . <eos> an overview of the self organizing map algorithm , on which the papers in this issue are based , is presented in this article .
the amygdala and development of the social brain . <eos> the amygdala comprises part of an extended network of neural circuits that are critically involved in the processing of socially salient stimuli . such stimuli may be explicitly social , such as facial expressions , or they may be only tangentially social , such as abstract shapes moving with apparent intention relative to one another . the coordinated interplay between neural activity in the amygdala and other brain regions , especially the medial prefrontal cortex , the occipitofrontal cortex , the fusiform gyrus , and the superior temporal sulcus , allows us to develop social responses and to engage in social behaviors appropriate to our species . the harmonious functioning of this integrated social cognitive network may be disrupted by congenital or acquired lesions , by genetic anomalies , and by exceptional early experiences . each form of disruption is associated with a slightly different outcome , dependent on the timing of the experience , the location of the lesion , or the nature of the genetic anomaly . studies in both humans and primates concur the dysregulation of basic emotions , especially the processing of fear and anger , is an almost invariable consequence of such disruption . these , in turn , have direct or indirect consequences for social behavior .
modeling and cost analysis of an improved movement based location update scheme in wireless communication networks . <eos> a movement based location update ( mblu ) scheme is an lu scheme , under which a user equipment ( ue ) performs an lu when the number of cells crossed by the ue reaches a movement threshold . the mblu scheme suffers from ping pong lu effect . the ping pong lu effect arises when the ue that moves repetitively between two adjacent cells performs lus in the same way as in the case of straight movement . to tackle the ping pong lu effect encountered by the original mblu ( omblu ) scheme , an improved mblu ( imblu ) scheme was proposed in the literature . under the imblu scheme , the ue performs an lu when the number of different cells , rather than the number of cells , visited by the ue reaches the movement threshold . in this paper we develop an embedded markov chain model to calculate the signaling cost of the imblu scheme . we derive analytical formulas for the signaling cost , whose accuracy is tested through simulation . it is observed from a numerical study based on these formulas that <digit> ) the signaling cost is a downward convex function of the movement threshold , implying the existence of an optimal movement threshold that can minimize the signaling cost , and that <digit> ) the reduction in the signaling cost achieved by the imblu scheme relative to the omblu scheme is more prominent in the case of stronger repetitiveness in the ue movement . the model developed and the formulas derived in this paper can guide the implementation of the imblu scheme in wireless communication networks .
a modified offset roll printing for thin film transistor applications . <eos> in order to realize a high resolution and high throughput printing method for thin film transistor application , a modified offset roll printing was studied . this roll printing chiefly consists of a blanket with low surface energy and a printing plate ( clich ) with high surface energy . in this study , a finite element analysis was done to predict the blanket deformation and to find the optimal angle of clichs sidewall . various etching methods were investigated to obtain a high resolution clich and the surface energy of the blanket and clich was analyzed for ink transfer . a high resolution clich with the sidewall angle of <digit> and the intaglio depth of <digit> m was fabricated by the deep reactive ion etching method . based on the surface energy analysis , we extracted the most favorable condition to transfer inks from a blanket to a clich , and thus thin films were deposited on a si clich to increase the surface energy . through controlling roll speed and pressure , two inks , etch resist and silver paste , were printed on a rigid substrate , and the fine patterns of <digit> m width and <digit> m line spacing were achieved . by using this printing process , the top gate amorphous indiumgalliumzinc oxide tfts with channel width length of <digit> <digit> m were successfully fabricated by printing etch resists .
on the complement graph and defensive k k alliances . <eos> in this paper , we obtain several tight bounds of the defensive k k alliance number in the complement graph from other parameters of the graph . in particular , we investigate the relationship between the alliance numbers of the complement graph and the minimum and maximum degree , the domination number and the isoperimetric number of the graph . moreover , we prove the np completeness of the decision problem underlying the defensive k k alliance number .
hyperspectral image segmentation through evolved cellular automata . <eos> efficient segmentation of hyperspectral images through the use of cellular automata . the rule set for the ca is automatically obtained using an evolutionary algorithm . synthetic images of much lower dimensionality are used to evolve the automata . the ca works with spectral information but do not project it onto a lower dimension . the ca based classification outperforms reference techniques .
analytical and empirical evaluation of the impact of gaussian noise on the modulations employed by bluetooth enhanced data rates . <eos> bluetooth ( bt ) is a leading technology for the deployment of wireless personal area networks and body area networks . versions 2.0 and 2.1 of the standard , which are massively implemented in commercial devices , improve the throughput of the bt technology by enabling the so called enhanced data rates ( edr ) . edrs are achieved by utilizing new modulation techniques ( <digit> dqpsk and <digit> dpsk ) , apart from the typical gaussian frequency shift keying modulation supported by previous versions of bt . this manuscript presents and validates a model to characterize the impact of white noise on the performance of these modulations . the validation is systematically accomplished in a testbed with actual bt interfaces and a calibrated white noise generator .
spectral analysis of irregularly sampled data paralleling the regularly sampled data approaches . <eos> the spectral analysis of regularly sampled ( rs ) data is a well established topic , and many useful methods are available for performing it under different sets of conditions . the same can not be said about the spectral analysis of irregularly sampled ( is ) data despite a plethora of published works on this topic , the choice of a spectral analysis method for is data is essentially limited , on either technical or computational grounds , to the periodogram and its variations . in our opinion this situation is far from satisfactory , given the importance of the spectral analysis of is data for a considerable number of applications in such diverse fields as engineering , biomedicine , economics , astronomy , seismology , and physics , to name a few . in this paper we introduce a number of is data approaches that parallel the methods most commonly used for spectral analysis of rs data the periodogram ( per ) , the capon method ( cap ) , the multiple signal characterization method ( music ) , and the estimation of signal parameters via rotational invariance technique ( esprit ) . the proposed is methods are as simple as their rs counterparts , both conceptually and computationally . in particular , the fast algorithms derived for the implementation of the rs data methods can be used mutatis mutandis to implement the proposed parallel is methods . moreover , the expected performance based ranking of the is methods is the same as that of the parallel rs methods all of them perform similarly on data consisting of well separated sinusoids in noise , music and esprit outperform the other methods in the case of closely spaced sinusoids in white noise , and cap outperforms per for data whose spectrum has a small to medium dynamic range ( music and esprit should not be used in the latter case ) .
time series data mining . <eos> in almost every scientific field , measurements are performed over time . these observations lead to a collection of organized data called time series . the purpose of time series data mining is to try to extract all meaningful knowledge from the shape of data . even if humans have a natural capacity to perform these tasks , it remains a complex problem for computers . in this article we intend to provide a survey of the techniques applied for time series data mining . the first part is devoted to an overview of the tasks that have captured most of the interest of researchers . considering that in most cases , time series task relies on the same components for implementation , we divide the literature depending on these common aspects , namely representation techniques , distance measures , and indexing methods . the study of the relevant literature has been categorized for each individual aspects . four types of robustness could then be formalized and any kind of distance could then be classified . finally , the study submits various research trends and avenues that can be explored in the near future . we hope that this article can provide a broad and deep understanding of the time series data mining research field .
a small hybrid jit for embedded systems . <eos> just in time ( jit ) compilation is a technology used to improve speed of virtual machines that support dynamic loading of applications . it is better known as a technique that accelerates java programs . current jit compilers are either huge in size or compile complete methods of the application requiring large amounts of memory for their functioning . this has made java virtual machines for embedded systems devoid of jit compilers . this paper explains a simple technique of combining interpretation with compilation to get a hybrid interpretation strategy . it also describes a new code generation technique that works using its self code . the combination gives a jit compiler that is very small ( 10k ) and suitable for java virtual machines for embedded systems .
rationality of induced ordered weighted operators based on the reliability of the source of information in group decision making . <eos> the aggregation of preference relations in group decision making ( gdm ) problems can be carried out based on either the reliability of the preference values to be aggregated , as is the case with ordered weighted averaging operators , or on the reliability of the source of information that provided the preferences , as is the case with weighted mean operators . in this paper , we address the problem of aggregation based on the reliability of the source of information , with a double aim a ) to provide a general framework for induced ordered weighted operators based upon the source of information , and b ) to provide a study of their rationality . we study the conditions which need to be verified by an aggregation operator in order to maintain the rationality assumptions on the individual preferences in the aggregation phase of the selection process of alternatives . in particular , we show that any aggregation operator based on the reliability of the source of information does verify these conditions .
digital preservation of knowledge in the public sector a pre ingest tool . <eos> this paper describes the need for coordinating pre ingest activities in digital preservation of archival records . as a result of the wide use of electronic records management systems ( erms ) in agencies , the focus is on several issues relating to the interaction of the agencys erms and public repositories . this paper indicates the importance of using digital recordkeeping metadata to meet more precisely and at the same time semi automatically the criteria set by memory institutions . the paper provides an overview of one prospective solution and describes the estonian national archives universal archiving module ( uam ) . a case study reports the use of the uam in preserving the digital records of the estonian minister for population and ethnic affairs . in this project , the preparation and transfer of archival records was divided into ten phases , starting from the description of the archival creator and ending with controlled transfer . the case study raises questions about how much recordkeeping metadata can be used in archival description and how the interaction of the agencys erms and ingest by the archives could be more automated . the main issues ( e.g. classification , metadata elements variations , mapping , and computer files conversions ) encountered during that project are discussed . findings show that the open archival information system functional models ingest part should be reconceptualised to take into account preparatory work . adding detailed metadata about the structure , context and relationships in the right place at the right time could get one step closer to digital codified knowledge archiving by creating synergies with various other digital repositories .
the regulation of serca type pumps by phospholamban and sarcolipin . <eos> both sarcolipin ( sln ) and phospholamban ( pln ) lower the apparent affinity of either serca1a or serca2a for ca2 . since sln and pln are coexpressed in the heart , interactions among these three proteins were investigated . when serca1a or serca2a were coexpressed in hek <digit> cells with both sln and pln , superinhibition resulted . the ability of sln to elevate the content of pln monomers accounts , at least in part , for the superinhibitory effects of sln in the presence of pln . to evaluate the role of sln in skeletal muscle , sln cdna was injected directly into rat soleus muscle and force characteristics were analyzed . overexpression of sln resulted in significant reductions in both twitch and tetanic peak force amplitude and maximal rates of contraction and relaxation and increased fatigability with repeated electrical stimulation . ca2 uptake in muscle homogenates was impaired , suggesting that overexpression of sln may reduce the sarcoplasmic reticulum ca2 store . sln and pln appear to bind to the same regulatory site in serca . however , in a ternary complex , pln occupies the regulatory site and sln binds to the exposed side of pln and to serca .
bootstrap confidence intervals for principal response curves . <eos> the principal response curve ( prc ) model is of use to analyse multivariate data resulting from experiments involving repeated sampling in time . the time dependent treatment effects are represented by prcs , which are functional in nature . the sample prcs can be estimated using a raw approach , or the newly proposed smooth approach . the generalisability of the sample prcs can be judged using confidence bands . the quality of various bootstrap strategies to estimate such confidence bands for prcs is evaluated . the best coverage was obtained with bca bc a intervals using a non parametric bootstrap . the coverage appeared to be generally good , except for the case of exactly zero population prcs for all conditions . then , the behaviour is irregular , which is caused by the sign indeterminacy of the prcs . the insights obtained into the optimal bootstrap strategy are useful to apply in the prc model , and more generally for estimating confidence intervals in singular value decomposition based methods .
a concurrent specification of brzozowski 's dfa construction algorithm . <eos> in this paper two concurrent versions of brzozowski 's deterministic finite automaton ( dfa ) construction algorithm are developed from first principles , the one being a slight refinement of the other . we rely on hoare 's csp as our notation . the specifications that are proposed of the brzozowski algorithm are in terms of the concurrent composition of a number of top level processes , each participating process itself composed of several other concurrent processes . after considering a number of alternatives , this particular overall architectural structure seemed like a natural and elegant mapping from the sequential algorithm 's structure . while we have carefully argued the reasons for constructing the concurrent versions as proposed in the paper , there are of course , a large range of alternative design choices that could be made . there might also be scope for a more fine grained approach to updating sets or checking for similarity of regular expressions . at this stage , we have chosen to abstract away from these considerations , and leave their exploration for a subsequent step in our research .
collaborative multimedia learning environments . <eos> i use the term collaborative , to identify a way that enables conversation to occur in , about , and around the digital medium , therefore making the digital artifacts contributed by all individuals a key element of a conversation as opposed to consecutive , linear presentations used by most faculty at the design school.installations of collaborative multimedia in classrooms at the harvard university graduate school of design show an enhancement of the learning process via shared access to media resources and enhanced spatial conditions within which these resources are engaged . through observation and controlled experiments i am investigating how the use of shared , collaborative interfaces for interaction with multiple displays in a co local environment enhances the learning process . the multiple spatial configurations and formats of learning mandate that with more effective interfaces and spaces for sharing digital media with fellow participants , the classroom can be used much more effectively and thus , learning and interaction with multimedia can be improved .
clustering multi way data via adaptive subspace iteration . <eos> clustering multi way data is a very important research topic due to the intrinsic rich structures in real world datasets . in this paper , we propose the subspace clustering algorithm on multi way data , called asi t ( adaptive subspace iteration on tensor ) . asi t is a special version of high order svd ( hosvd ) , and it simultaneously performs subspace identification using 2dsvd and data clustering using k means . the experimental results on synthetic data and real world data demonstrate the effectiveness of asi t.
sat based model checking for security protocols analysis . <eos> we present a model checking technique for security protocols based on a reduction to propositional logic . at the core of our approach is a procedure that , given a description of the protocol in a multi set rewriting formalism and a positive integer k , builds a propositional formula whose models ( if any ) correspond to attacks on the protocol . thus , finding attacks on protocols boils down to checking a propositional formula for satisfiability , problem that is usually solved very efficiently by modern sat solvers . experimental results indicate that the approach scales up to industrial strength security protocols with performance comparable with ( and in some cases superior to ) that of other state of the art protocol analysers .
existence of positive solutions for 2n <digit> n th order singular superlinear boundary value problems . <eos> this paper investigates the existence of positive solutions for 2n <digit> n th order ( n > <digit> n > <digit> ) singular superlinear boundary value problems . a necessary and sufficient condition for the existence of c2n <digit> 0,1 c <digit> n <digit> <digit> , <digit> as well as c2n <digit> 0,1 c <digit> n <digit> <digit> , <digit> positive solutions is given by constructing a special cone and with the e e norm .
embedding the internet in the lives of college students online and offline behavior . <eos> the internet is increasingly becoming embedded in the lives of most american citizens . college students constitute a group who have made particularly heavy use of the technology for everything from downloading music to distance education to instant messaging . researchers know a lot about the uses made of the internet by this group of people but less about the relationship between their offline activities and online behavior . this study reports the results of a web survey of a group of university undergraduates exploring the nature of both online and offline in five areas the use of news and information , the discussion of politics , the seeking of health information , the use of blogs , and the downloading of media and software .
a capacitive tactile sensor array for surface texture discrimination . <eos> this paper presents a silicon mems based capacitive sensing array , which has the ability to resolve forces in the sub mn range , provides directional response to applied loading and has the ability to differentiate between surface textures . texture recognition is achieved by scanning surfaces over the sensing array and assessing the frequency spectrum of the sensor outputs .
segmenting , modeling , and matching video clips containing multiple moving objects . <eos> this paper presents a novel representation for dynamic scenes composed of multiple rigid objects that may undergo different motions and are observed by a moving camera . multiview constraints associated with groups of affine covariant scene patches and a normalized description of their appearance are used to segment a scene into its rigid components , construct three dimensional models of these components , and match instances of models recovered from different image sequences . the proposed approach has been applied to the detection and matching of moving objects in video sequences and to shot matching , i.e. , the identification of shots that depict the same scene in a video clip .
weighted fuzzy interpolative reasoning systems based on interval type <digit> fuzzy sets . <eos> in this paper , we present a weighted fuzzy interpolative reasoning method for sparse fuzzy rule based systems based on interval type <digit> fuzzy sets . we also apply the proposed weighted fuzzy interpolative reasoning method to deal with the truck backer upper control problem . the proposed method satisfies the seven evaluation indices for fuzzy interpolative reasoning . the experimental results show that the proposed method outperforms the existing methods . it provides us with a useful way for dealing with fuzzy interpolative reasoning in sparse fuzzy rule based systems .
vestibular prehab . <eos> a sudden unilateral loss or impairment of vestibular function causes vertigo , dizziness , and impaired postural function . in most occasions , everyday activities supported or not by vestibular rehabilitation programs will promote a compensation and the symptoms subside . as the compensatory process requires sensory input , matching performed motor activity , both motor learning of exercises and matching to sensory input are required . if there is a simultaneous cerebellar lesion caused by the tumor or the surgery of the posterior cranial fossa , there may be a risk of a combined vestibulocerebellar lesion , with reduced compensatory abilities and with prolonged or sometimes permanent disability . on the other hand , a slow gradual loss of unilateral function occurring as the subject continues well learned everyday activities may go without any prominent symptoms . a pretreatment plan was therefore implemented before planned vestibular lesions , that is , prehab . this was first done in subjects undergoing gentamicin treatment for morbus mnire . subjects would perform vestibular exercises for <digit> days before the first gentamicin installation , and then continue doing so until free of symptoms . most subjects would only experience slight dizziness while losing vestibular function . the approachwhich is reported herewas then expanded to patients with pontine angle tumors requiring surgery , but with remaining vestibular function to ease postoperative symptoms and reduce risk of combined cerebellovestibular lesions . twelve patients were treated with prehab and had gentamicin installations transtympanically . in all cases there was a caloric loss , loss of vor in head impulse tests , and impaired subjective vertical and horizontal . spontaneous , positional nystagmus , subjective symptoms , and postural function were normalized before surgery and postoperative recovery was swift . pretreatment training with vestibular exercises continued during the successive loss of vestibular function during gentamicin treatment , and pre op gentamicin ablation of vestibular function offers a possibility to reduce malaise and speed up recovery .
sw1pers sliding windows and <digit> persistence scoring discovering periodicity in gene expression time series data . <eos> identifying periodically expressed genes across different processes ( e.g. the cell and metabolic cycles , circadian rhythms , etc ) is a central problem in computational biology . biological time series may contain ( multiple ) unknown signal shapes of systemic relevance , imperfections like noise , damping , and trending , or limited sampling density . while there exist methods for detecting periodicity , their design biases ( e.g. toward a specific signal shape ) can limit their applicability in one or more of these situations .
parallel generation of unstructured surface grids . <eos> in this paper , a new grid generation system is presented for the parallel generation of unstructured triangular surface grids . the object oriented design and implementation of the system , the internal components and the parallel meshing process itself are described . initially in a rasterisation stage , the geometry to be meshed is analysed and a smooth distribution of local element sizes in <digit> d space is set up automatically and stored in a cartesian mesh . this background mesh is used by the advancing front surface mesher as spacing definition for the triangle generation . both the rasterisation and the meshing are mpi parallelised . the underlying principles and strategies will be outlined together with the advantages and limitations of the approach . the paper will be concluded with examples demonstrating the capabilities of the presented approach .
h structured model reduction algorithms for linear discrete systems via lmi based optimisation . <eos> in this article , h structured model reduction is addressed for linear discrete systems . two important classes of systems are considered for structured model reduction , i.e. markov jump systems and uncertain systems . the problem we deal with is the development of algorithms with the flexibility to allow any structure in the reduced order system design , such as the structure of an original system , decentralisation of a networked system , pole assignment of the reduced system , etc. the algorithms are derived such that an associated model reduction error guarantees to satisfy a prescribed h norm bound constraint . a new condition for the existence of desired reduced order models preserving a certain structure is presented in a set of linear matrix inequalities ( lmi ) and non convex equality constraints . effective computational algorithms involving lmi are suggested to solve the matrix inequalities characterising a solution of the structured model reduction problem . numerical examples demonstrate the advantages of the proposed model reduction method .
a power aware code compression design for risc vliw architecture . <eos> we studied the architecture of embedded computing systems from the viewpoint of power consumption in memory systems and used a selective code compression ( scc ) approach to realize our design . based on the lzw ( lempel ziv welch ) compression algorithm , we propose a novel cost effective compression and decompression method . the goal of our study was to develop a new scc approach with an extended decision policy based on the prediction of power consumption . our decompression method had to be easily implemented in hardware and to collaborate with the embedded processor . the hardware implementation of our decompression engine uses the tsmc 0.18 mu m 2p6m model and its cell based libraries . to calculate power consumption more accurately , we used a static analysis method to estimate the power overhead of the decompression engine . we also used variable sized branch blocks and considered several features of very long instruction word ( vliw ) processors for our compression , including the instruction level parallelism ( ilp ) technique and the scheduling of instructions . our code compression methods are not limited to vliw machines , and can be applied to other kinds of reduced instruction set computer ( risc ) architecture .
globallocal negotiations for implementing configurable packages the power of initial organizational decisions . <eos> the purpose of this paper is to draw attention to the critical influence that initial organizational decisions regarding power and knowledge balance between internal members and external consultants have on the globallocal negotiation that characterizes configurable packages implementation . to do this , we conducted an intensive research study of a configurable information technology ( it ) implementation project in a canadian firm .
communication in random geometric radio networks with positively correlated random faults . <eos> we study the feasibility and time of coin communication in random geometric radio networks , where nodes fail randomly with positive correlation . we consider a set of radio stations with the same communication range , distributed in a random uniform way on a unit square region . in order to capture fault dependencies , we introduce the ranged spot model in which damaging events , called spots , occur randomly and independently on the region , causing faults in all nodes located within distance s from them . node faults within distance 2s become dependent in this model and are positively correlated . we investigate the impact of the spot arrival rate on the feasibility and the time of communication in the fault free part of the network . we provide an algorithm which broadcasts correctly with probability l epsilon in faulty random geometric radio networks of diameter d in time o ( d log l epsilon ) .
consumer complaint behaviour in telecommunications the case of mobile phone users in spain . <eos> consumer complaint behaviour theory is used to analyze spanish telecommunications data . the main determinants are the type of problem , socio demographic factors and the user s type of contract . proper complaint handling leads to satisfied , loyal and profitable consumers .
combining owl ontologies using epsilon connections . <eos> the standardization of the web ontology language ( owl ) leaves ( at least ) two crucial issues for web based ontologies unsatisfactorily resolved , namely how to represent and reason with multiple distinct , but linked ontologies , and how to enable effective knowledge reuse and sharing on the semantic web . in this paper , we present a solution for these fundamental problems based on e connections . we aim to use e connections to provide modelers with suitable means for developing web ontologies in a modular way and to provide an alternative to the owl imports construct . with such motivation , we present in this paper a syntactic and semantic extension of the web ontology language that covers e connections of owl dl ontologies . we show how to use such an extension as an alternative to the owl imports construct in many modeling situations . we investigate different combinations of the logics shin ( d ) , shon ( d ) and shio ( d ) for which it is possible to design and implement reasoning algorithms , well suited for optimization . finally , we provide support for e connections in both an ontology editor , swoop , and an owl reasoner , pellet . ( c ) <digit> elsevier b. v. all rights reserved .
model with artificial neural network to predict the relationship between the soil resistivity and dry density of compacted soil . <eos> this paper presents a technique to obtain the outcomes of soil dry density and optimum moisture contents with artificial neural network ( ann ) for compacted soil monitoring through soil resistivity measurement in geotechnical engineering . the compacted soil monitoring through soil electrical resistivity shows the important role in the construction of highway embankments , earth dams and many other engineering structure . generally , soil compaction is estimated through the determination of maximum dry density at optimum moisture contents in laboratory test . to estimate the soil compaction in conventional soil monitoring technique is time consuming and costly for the laboratory testing with a lot of samples of compacted soil . in this work , an ann model is developed for predicting the relationship between dry density of compacted soil and soil electrical resistivity based on experimental data in soil profile . the regression analysis between the output and target values shows that the r <digit> values are 0.99 and 0.93 for the training and testing sets respectively for the implementation of ann in soil profile . the significance of our research is to obtain an intelligent model for getting faster , cost effective and consistent outcomes in soil compaction monitoring through electrical resistivity for a wide range of applications in geotechnical investigation .
a fuzzy bi criteria transportation problem . <eos> in this paper , a fuzzy bi criteria transportation problem is studied . here , the model concentrates on two criteria total delivery time and total profit of transportation . the delivery times on links are fuzzy intervals with increasing linear membership functions , whereas the total delivery time on the network is a fuzzy interval with a decreasing linear membership function . on the other hand , the transporting profits on links are fuzzy intervals with decreasing linear membership functions and the total profit of transportation is a fuzzy number with an increasing linear membership function . supplies and demands are deterministic numbers . a nonlinear programming model considers the problem using the max min criterion suggested by bellman and zadeh . we show that the problem can be simplified into two bi level programming problems , which are solved very conveniently . a proposed efficient algorithm based on parametric linear programming solves the bi level problems . to explain the algorithm two illustrative examples are provided , systematically . ( c ) <digit> elsevier ltd. all rights reserved .
capacity gain of mixed multicast unicast transport schemes in a tv distribution network . <eos> this paper presents three approaches to estimate the required resources in an infrastructure where digital tv channels can be delivered in unicast or multicast ( broadcast ) mode . such situations arise for example in cable tv , iptv distribution networks or in ( future ) hybrid mobile tv networks . the three approaches presented are an exact calculation , a gaussian approximation and a simulation tool . we investigate two scenarios that allow saving bandwidth resources . in a static scenario , the most popular channels are multicast and the less popular channels rely on unicast . in a dynamic scenario , the list of multicast channels is dynamic and governed by the users ' behavior . we prove that the dynamic scenario always outperforms the static scenario . we demonstrate the robustness , versatility and the limits of our three approaches . the exact calculation application is limited because it is computationally expensive for cases with large numbers of users and channels , while the gaussian approximation is good exactly for such systems . the simulation tool takes long to yield results for small blocking probabilities . we explore the capacity gain regions under varying model parameters . finally , we illustrate our methods by discussing some realistic network scenarios using channel popularities based on measurement data as much as possible .
quantitatively evaluating the influence of online social interactions in the community assisted digital library . <eos> online social interactions are useful in information seeking from digital libraries , but how to measure their influence on the user 's information access actions has not yet been revealed . studies on this problem give us interesting insights into the workings of human dynamics in the context of information access from digital libraries . on the basis , we wish to improve the technological supports to provide more intelligent services in the ongoing china america million books digital library so that it can reach its potential in serving human needs . our research aims at developing a common framework to model online social interaction process in community assisted digital libraries . the underlying philosophy of our work is that the online social interaction can be viewed as a dynamic process , and the next state of each participant in this process ( e.g. , personal information access competency ) depends on the value of the previous states of all participants involving interactions in the period . hence , considering the dynamics of interaction process , we model each participant with a hidden markov model ( hmm ) chain and then employ the influence model , which was developed by c. asavathiratham as a dynamic bayes net ( dbn ) of representing the influences a number of markov chains have on each other , to analyze the effects of participants influencing each other . therefore , one can think of the entire interaction process as a dbn framework having two levels of structure the local level and the network level . each participant i has a local hmm chain ggr ( a ) which characterizes the transition of his internal states in the interaction process with state transition probability sum over j d ij p ( s i t s j t <digit> ) ( here states are his personal information access competence in different periods , while observations are his information access actions ) . meanwhile , the network level , which is described by a network graph ggr ( d t ) where d d ij is the influence factor matrix , represents the interacting relations between participants . the strength of each connection , d ij , describes the influence factor of the participant j at its begin on the one i at its end . hence , this model describes the dynamic inter influence process of the internal states of all participants involving online interactions . to automatically build the model , we need firstly to extract observed features from the data of online social interactions and information access actions . obviously , the effects of interactions are stronger if messages are exchanged more frequently , or the participants access more information in the online digital libraries during the period of time . based on this consideration , we select the interaction measure im i , j t and the amount of information ia j t as the estimation features of x i t . the interaction measure ia i t and the amount of information parameterize the features calculated automatically from the data of online social interactions between the participants i and j , and the features calculated from the data of information access actions respectively . secondly , we need to develop a mechanism for learning the parameters d ij and p ( s i t s j t <digit> . given sequences of observations x i t for each chain i , we may easily utilize the expectation maximization algorithm or the gradient based learning algorithm to get their estimation equations . we ran our experiments in the online digital library of w3c consortium ( www.w3c.org ) , which contains a mass of news , electronic papers or other materials related to web technologies . users may access and download any information and materials in this digital library , and also may free discuss on any related technological problems by means of its mailing lists . six users were selected in our experiments to collaboratively perform paper gathering tasks related to four given topics . any user might call for help from the others through the mailing lists when had difficulties in this process . all participants were required to record subjective evaluations of the effects that the others influenced his tasks . each experiment was scheduled by ten phases . and in each phase , we sampled im i , j t and ia i t for each participant and then fed them into the learning algorithms to automatically build the influence model . by comparing with the subjective influence graphs , the experimental results show that the influence model can estimate approximately the influences of online social interactions .
the ceo cio relationship revisited an empirical assessment of satisfaction with is . <eos> the necessity of integrating information systems ( is ) into corporate strategy has received widespread attention in recent years . strategic planning has moved is from serving primarily as a support function to a point where it may influence corporate strategy . the strength of this influence , however , usually is determined by the nature of the relationship between the chief information officer ( cio ) and the ceo . generally the more satisfied ceos are with cios , the greater the influence is has on top level decisions . results of a nationwide survey of motor carrier ceos and cios indicate that ceos are generally satisfied with their cios ' activities , and that cios perceive ceos as placing a high priority on strategic is plans . however , is does not appear to be truly a part of corporate strategy formulation .
simulations of photosynthesis by a k subset transforming system with membrane . <eos> by considering the inner regions of living cells ' membranes , p systems with inner regions are introduced . then , a new type of membrane computing systems are considered , called k subset transforming systems with membranes , which can treat nonintegral multiplicities of objects . as an application , a k subset transforming system is proposed in order to model the light reactions of the photosynthesis . the behaviour of such systems is simulated on a computer ,
technologische innovation und die auswirkung auf geschftsmodell , organisation und unternehmenskultur die transformation der ibm zum global integrierten , dienstleistungsorientierten unternehmen . <eos> im vorliegenden beitrag wird der einfluss von innovationen der informations und kommunikationstechnologie ( ikt ) auf die transformation von unternehmen untersucht . zunchst werden die allgemeinen ikt getriebenen entwicklungslinien der globalisierung und der dienstleistungsorientierung beschrieben . die nachfolgende analyse der transformation der ibm corporation ber die letzten 50jahre zu einem global integrierten , dienstleistungsorientierten unternehmen macht deutlich , dass ikt innovationen mit gleichzeitigen anpassungen des geschftsmodells , der organisation und der unternehmenskultur begegnet werden muss . die fhigkeit zu derartiger adaption gewinnt eine zunehmend zentrale bedeutung fr unternehmen .
modular robotics as a tool for education and entertainment . <eos> we developed i blocks , a modular electronic building block system and here we show how this system has proven useful , especially as an educational tool that allows hands on learning in an easy manner . through user studies we find limitations of the first i blocks system , and we show how the system can be improved by introducing a graphical user interface for authoring the contents of the individual i block . this is done by developing a new cubic block shape with new physical and electrical connectors , and by including new embedded electronics . we developed and evaluated the i blocks as a manipulative technology through studies in both schools and hospitals , and in diverse cultures such as in denmark , finland , italy and tanzania .
the selective use of redundancy for video streaming over vehicular ad hoc networks . <eos> video streaming over vehicular ad hoc networks ( vanets ) offers the opportunity to deploy many interesting services . these services , however , are strongly prone to packet loss due to the highly dynamic topology and shared wireless medium inherent in the vanets . a possible strategy to enhance the delivery rate is to use redundancy for handling packet loss . this is a suitable technique for vanets as it does not require any interaction between the source and receivers . in this work , we discuss novel approaches for the use of redundancy based on the particularities of video streaming over vanets . a thorough study on the use of redundancy through erasure coding and network coding in both video unicast and video broadcast in vanets is provided . we investigate each strategy , design novel solutions and compare their performance . we evaluated the proposed solutions from the perspective not only of cost as bandwidth utilization , but also the offered receiving rate of unique video content at the application layer . this perspective is fundamental to understanding how redundancy can be used without limiting the video quality that can be displayed to end users . furthermore , we propose the selective use of redundancy solely on data that is more relevant to the video quality . this approach offers increases in overall video quality without leading to an excessive overhead nor to a substantial decrease in the receiving rate of unique video content .
syntactic recognition of ecg signals by attributed finite automata . <eos> a syntactic pattern recognition method of electrocardiograms ( ecg ) is described in which attributed automata are used to execute the analysis of ecg signals . an ecg signal is first encoded into a string of primitives and then attributed automata are used to analyse the string . we have found that we can perform fast and reliable analysis of ecg signals by attributed automata .
lane mark extraction for automobiles under complex conditions . <eos> we proposed a vision based lane mark extraction method . we used multi adaptive thresholds for different blocks . based on the results , our method was robust for complex conditions . the proposed system could operate in real time .
cultural differences explaining the differences in results in gss implications for the next decade . <eos> for the next decade , the support that comes from group support systems ( gss ) will be increasingly directed towards culturally diversified groups . while there have been many gss studies concerning culture and cultural differences , no dedicated review of gss researches exists for the identification of current gaps and opportunities of doing cross cultural gss research . for this purpose , this paper provides a comprehensive review utilizing a taxonomy of six categories research type , gss technology used , independent variables , dependent variables , use of culture , and findings . additionally , this study also aims to illustrate how differences in experimental results arising from comparable studies , but from a different cultural setting , can be explained consistently using hofstede 's dimensions . to do so , we presented a comparative study on the use of gss in australia and singapore and explain the differences in results using hofstede 's g. hofstede , culture 's consequencesinternational differences in work related values , sage , beverly hills , ca ( <digit> ) . cultural dimensions . last , but not least , we present the implications of the impact of culture on gss research for the next decade from the viewpoint of the three gss stakeholders the facilitators , gss software designers , and the gss researchers . with the above , this paper seeks ( i ) to prepare a comprehensive map of gss research involving culture , and ( ii ) to prepare a picture of what all these mean and where we should be heading in the next decade .
building an ip based community wireless mesh network assessment of pacman as an ip address autoconfiguration protocol . <eos> wireless mesh networks are experiencing rapid progress and inspiring numerous applications in different scenarios . due to features such as autoconfiguration , self healing , connectivity coverage extension and support for dynamic topologies these particular characteristics make wireless mesh networks an appropriate architectural basis for the design of easy to deploy community or neighbourhood networks one of the main challenges in building a community network using mesh networks is the minimisation of user intervention in the ip address configuration of the network nodes in this paper we first consider the process of building an ip based mesh network using typical residential routers , exploring the options for the configuration of their wireless interfaces . then we focus on ip address autoconfiguration , identifying the specific requirements for community mesh networks and analysing the applicability of existing solutions . as a result of that analysis , we select pacman , an efficient distributed address autoconfiguration mechanism originally designed for ad hoc networks . and we perform an experimental study using off the shelf routers and assuming worst case scenarios analysing its behaviour as an ip address autoconfiguration mechanism for community wireless mesh networks the results of the conducted assessment show that pacman meets all the identified requirements of the community scenario ( c ) <digit> elsevier b v all rights reserved .
approximating partition functions of the two state spin system . <eos> two state spin system is a classical topic in statistical physics . we consider the problem of computing the partition function of the system on a bounded degree graph . based on the self avoiding tree , we prove the system exhibits strong correlation decay under the condition that the absolute value of inverse temperature is small . due to strong correlation decay property , an fptas for the partition function is presented and uniqueness of gibbs measure of the two state spin system on a bounded degree infinite graph is proved , under the same condition . this condition is sharp for ising model . ( c ) <digit> elsevier b.v. all rights reserved .
efficient memory utilization for high speed fpga based hardware emulators with sdrams . <eos> fpga based hardware . emulators are often used for the verification of lsi functions . they generally have dedicated external memories , such as sdrams , to compensate for the lack of memory capacity in fpgas . in such a case , access between the fpgas and the dedicated external memory may represent a major bottleneck with respect to emulation speed since the dedicated external memory may have to emulate a large number of memory blocks . in this paper , we propose three methods , dynamic clock control ( dcc ) , memory mapping optimization ( mmo ) , and efficient access scheduling ( eas ) , to avoid this bottleneck . dcc controls an emulation clock dynamically in accord with the number of memory accesses within one emulation clock cycle . eas optimizes the ordering of memory access to the dedicated external memory , and mmo optimizes the arrangement of the dedicated external memory addresses to which respective memories will be emulated . with them , emulation speed can be made 29.0 times faster , as evaluated in actual lsi emulations .
minimum stress optimal design with the level set method . <eos> this paper is devoted to minimum stress design in structural optimization . we propose a simple and efficient numerical algorithm for shape and topology optimization based on the level set method coupled with the topological derivative . we compute a shape derivative , as well as a topological derivative , for a stress based objective function . using an adjoint equation we implement a gradient algorithm for the minimization of the objective function . several numerical examples in <digit> d and <digit> d are discussed .
determination of wire recovery length in steel cables and its practical applications . <eos> in the presence of relatively significant states of radial pressures between the helical wires of a steel cable ( spiral strand and or wire rope ) , and significant levels of interwire friction , the individual broken wires tend to take up their appropriate share of the axial load within a certain length from the fractured end , which is called the recovery ( or development ) length . the paper presents full details of the formulations for determining the magnitude of recovery length in any layer of an axially loaded multi layered spiral strand with any construction details . the formulations are developed for cases of fully bedded in ( old ) spiral strands within which the pattern of interlayer contact forces and associated significant values of line contact normal forces between adjacent wires in any layer , are fully stabilised , and also for cases when ( in the presence of gaps between adjacent wires ) hoop line contact forces do not exist and only radial forces are present . based on a previously reported extensive series of theoretical parametric studies using a wide range of spiral strand constructions with widely different wire ( and cable ) diameters and lay angles , a very simple method ( aimed at practising engineers ) for determining the magnitude of recovery length in any layer of an axially loaded spiral strand with any type of construction details is prestented . using the final outcome of theoretical parametric studies , the minimum length of test specimens for axial fatigue tests whose test data may safely be used for estimating the axial fatigue lives of the much longer cables under service conditions may now be determined in a straightforward fashion . moreover , the control length over which one should count the number of broken wires for cable discard purposes is suggested to be equal to one recovery length whose upper bound value for both spiral strands and or wire ropes with any construction details is theoretically shown to be equal to 2.5 lay lengths .
generalized pcm coding of images . <eos> pulse code modulation ( pcm ) with embedded quantization allows the rate of the pcm bitstream to be reduced by simply removing a fixed number of least significant bits from each codeword . although this source coding technique is extremely simple , it has poor coding efficiency . in this paper , we present a generalized pcm ( gpcm ) algorithm for images that simply removes bits from each codeword . in contrast to pcm , however , the number and the specific bits that a gpcm encoder removes in each codeword depends on its position in the bitstream and the statistics of the image . since gpcm allows the encoding to be performed with different degrees of computational complexity , it can adapt to the computational resources that are available in each application . experimental results show that gpcm outperforms pcm with a gain that depends on the rate , the computational complexity of the encoding , and the degree of inter pixel correlation of the image .
influence of motor and converter non linearities on dynamic properties of dc drive with field weakening range . <eos> improvement of the dynamic properties of dc drive in the field weakening range was the aim of investigation . the non linear model of the drive system was applied . in the paper results of the comparative analysis of two emf control structures are presented . the classic emf control structure with subordinated excitation current control loop was compared with this one consisting of a non linear compensation block . for both control structures different kinds of the parameter designing for the emf and excitation controllers are considered . verification of the theoretical assumptions and synthesis methods of the investigated control structures are made by simulation tests using the pspice language .
rental software valuation in it investment decisions . <eos> the growth of application service providers ( asps ) is very rapid , leading to a number of options to organizations interested in developing new information technology services . the advantages of an asp include spreading out payments over a contract period and flexibility in terms of responding to changes in technology . likewise , newer risks are associated with asps , including pricing variability . some of the more common capital budgeting models may not be appropriate in this volatile marketplace . however , option models allow for many of the quirks to be considered . modification of the option pricing model and an analytical solution method incorporated into a spreadsheet for decision support are described and illustrated . the analytical tool allows for better decisions compared to traditional value analysis methods which do not fully account for the entry and exit options of the market .
an averaging scheme for macroscopic numerical simulation of nonconvex minimization problems . <eos> averaging or gradient recovery techniques , which are a popular tool for improved convergence or superconvergence of finite element methods in elliptic partial differential equations , have not been recommended for nonconvex minimization problems as the energy minimization process enforces finer and finer oscillations and hence at the first glance , a smoothing step appears even counterproductive . for macroscopic quantities such as the stress field , however , this counterargument is no longer true . in fact , this paper advertises an averaging technique for a surprisingly improved convergence behavior for nonconvex minimization problems . similar to a finite volume scheme , numerical experiments on a double well benchmark example provide empirical evidence of superconvergence phenomena in macroscopic numerical simulations of oscillating microstructures .
which app a recommender system of applications in markets implementation of the service for monitoring users ' interaction . <eos> users face the information overload problem when downloading applications in markets . this is mainly due to ( i ) the increasing unmanageable number of applications and ( ii ) the lack of an accurate and fine grained categorization of the applications in the markets . to address this issue , we present an integrated solution which recommends to the users applications by considering a big amount of information that is , according to their previously consumed applications , use pattern , tags used to annotate resources and history of ratings . we focus this paper on the service for monitoring users ' interaction . ( c ) <digit> elsevier ltd. all rights reserved .
new statistical features for the design of fiber optic statistical mode sensors . <eos> novel statistical features are proposed for the design of statistical mode sensors . proposed statistical features are first and second order moments . features are compared in terms of precision error , non linearity , and hysteresis .
an efficient indexing method for content based image retrieval . <eos> in this paper , we propose an efficient indexing method for content based image retrieval . the proposed method introduces the ordered quantization to increase the distinction among the quantized feature descriptors . thus , the feature point correspondences can be determined by the quantized feature descriptors , and they are used to measure the similarity between query image and database image . to implement the above scheme efficiently , a multi dimensional inverted index is proposed to compute the number of feature point correspondences , and then approximate ransac is investigated to estimate the spatial correspondences of feature points between query image and candidate images returned from the multi dimensional inverted index . the experimental results demonstrate that our indexing method improves the retrieval efficiency while ensuring the retrieval accuracy in the content based image retrieval .
prediction intervals in linear regression taking into account errors on both axes . <eos> this study reports the expressions for the variances in the prediction of the response and predictor variables calculated with the bivariate least squares ( bls ) regression technique . this technique takes into account the errors on both axes . our results are compared with those of a simulation process based on six different real data sets . the mean error in the results from the new expressions is between <digit> % and <digit> % . with weighted least squares , ordinary least squares , the constant variance ratio approach and orthogonal regression , on the other hand , mean errors can be as high as <digit> % , <digit> % , <digit> % and <digit> % respectively . an important property of the prediction intervals calculated with bls is that the results are not affected when the axes are switched . copyright ( c ) <digit> john wiley sons , ltd .
the piam approach to modular integrated assessment modelling . <eos> the next generation of integrated assessment modelling is envisaged as being organised as a modular process , in which modules encapsulating knowledge from different scientific disciplines are independently developed at distributed institutions and coupled afterwards in accordance with the question raised by the decision maker . such a modular approach needs to respect several stages of the model development process , approaching modularisation and integration on a conceptual , numerical , and technical level . the paper discusses the challenges at each level and presents partial solutions developed by the piam ( potsdam integrated assessment modules ) project at the potsdam institute for climate impact research ( pik ) . the challenges at each level differ greatly in character and in the work done addressing them . at the conceptual level , the notion of conceptual consistency of modular integrated models is discussed . at the numerical level , it is shown how an adequate modularisation of a problem from climateeconomy leads to a modular configuration into which independently developed climate and economic modules can be plugged . at the technical level , a software tool is presented which provides a simple consistent interface for data transfer between modules running on distributed and heterogeneous computer platforms .
finding multivariate outliers in fmri time series data . <eos> multivariate outlier detection methods are applicable to fmri time series data . removing outliers increases spatial specificity without hurting classification . simulation shows pcout is more sensitivity to small outliers than hd bacon .
cardinal consistency of reciprocal preference relations a characterization of multiplicative transitivity . <eos> consistency of preferences is related to rationality , which is associated with the transitivity property . many properties suggested to model transitivity of preferences are inappropriate for reciprocal preference relations . in this paper , a functional equation is put forward to model the cardinal consistency in the strength of preferences of reciprocal preference relations . we show that under the assumptions of continuity and monotonicity properties , the set of representable uninorm operators is characterized as the solution to this functional equation . cardinal consistency with the conjunctive representable cross ratio uninorm is equivalent to tanino 's multiplicative transitivity property . because any two representable uninorms are order isomorphic , we conclude that multiplicative transitivity is the most appropriate property for modeling cardinal consistency of reciprocal preference relations . results toward the characterization of this uninorm consistency property based on a restricted set of ( n <digit> ) preference values , which can be used in practical cases to construct perfect consistent preference relations , are also presented .
infomarker a new internet information service system . <eos> as the web grows , the massive increase in information is placing severe burdens on information retrieval and sharing . automated search engines and directories with small editorial staff are unable to keep up with the increasing submission of web sites . to address the problem , this paper presents infomarker an internet information service system based on open directory and zero keyword inquiry . the open directory sets up a net community in which the increasing net citizens can each organize a small portion of the web and present it to the others . by means of zero keyword inquiry , user can get the information he is interested in without inputting any keyword that is often required by search engines . in infomarker , user can record the web address he likes and can put forward an information request based on his web records . the information matching engine checks the information in the open directory to find what fits user 's needs and adds it to user 's web address records . the key to the matching process is layered keyword mapping . infomarker provides people with a whole new approach to getting information and shows a wide prospect .
grain flow measurements with x ray techniques . <eos> the use of low energy x rays , up to <digit> kev , densitometry is demonstrated for grain flow rate measurements through laboratory experiments . mass flow rates for corn were related to measured x ray intensity in gray scale units with a 0.99 correlation coefficient for flow rates ranging from <digit> to <digit> kg s. larger flow rate values can be measured by using higher energy or a higher tube current . measurements were done in real time at a <digit> hz sampling rate . flow rate measurements are relatively independent of grain moisture due to a negligible change in the x ray attenuation coefficients at typical moisture content values from <digit> to <digit> % . grain flow profile changes did not affect measurement accuracy . x rays easily capture variations in the corn thickness profile . due to the low energy of the x ray photons , biological shielding can be accomplished with <digit> mm thick lead foil or <digit> mm of steel .
dynamic performance enhancement of microgrids by advanced sliding mode controller . <eos> dynamics are the most important problems in the microgrid operation . in the islanded microgrid , the mismatch of parallel operations of inverters during dynamics can result in the instability . this paper considers severe dynamics which can occur in the microgrid . microgrid can have different configurations with different load and generation dynamics which are facing voltage disturbances . as a result , microgrid has many uncertainties and is placed in the distribution network where is full of voltage disturbances . moreover , characteristics of the distribution network and distributed energy resources in the islanded mode make microgrid vulnerable and easily lead to instability . the main aim of this paper is to discuss the suitable mathematical modeling based on microgrid characteristics and to design properly inner controllers to enhance the dynamics of microgrid with uncertain and changing parameters . this paper provides a method for inner controllers of inverter based distributed energy resources to have a suitable response for different dynamics . parallel inverters in distribution networks were considered to be controlled by nonlinear robust voltage and current controllers . theoretical prove beyond simulation results , reveal evidently the effectiveness of the proposed controller .
new identification procedure for continuous time radio frequency power amplifier model . <eos> in this paper , we present a new method for characterization of radio frequency power amplifier ( pa ) in the presence of nonlinear distortions which affect the modulated signal in radiocommunication transmission system . the proposed procedure uses a gray box model where pa dynamics are modeled with a mimo continuous filter and the nonlinear characteristics are described as general polynomial functions , approximated by means of taylor series . using the baseband input and output data , model parameters are obtained by an iterative identification algorithm based on output error method . initialization and excitation problems are resolved by an association of a new technique using initial values extraction with a multi level binary sequence input exciting all pa dynamics . finally , the proposed estimation method is tested and validated on experimental data .
manufacturing lead time rules customer retention versus tardiness costs . <eos> inaccurate production backlog information is a major cause of late deliveries , which can result in penalty fees and loss of reputation . we identify conditions when it is particularly worthwhile to improve an information system to provide good lead time information . we first analyze a sequential decision process model of lead time decisions at a firm which manufactures standard products to order , and has complete backlog information . there are poisson arrivals , stochastic processing times , customers may balk in response to quoted delivery dates , and revenues are offset by tardiness penalties . we characterize an optimal policy and show how to accelerate computations . the second part of the paper is a computational comparison of this optimum ( with full backlog information ) with a lead time quotation rule that is optimal with statistical shop status information . this reveals when the partial information method does well and when it is worth implementing measures to improve information transfer between operations and sales .
on the wiberg algorithm for matrix factorization in the presence of missing components . <eos> this paper considers the problem of factorizing a matrix with missing components into a product of two smaller matrices , also known as principal component analysis with missing data ( pcamd ) . the wiberg algorithm is a numerical algorithm developed for the problem in the community of applied mathematics . we argue that the algorithm has not been correctly understood in the computer vision community . although there are many studies in our community , almost every one of which refers to the wiberg study , as far as we know , there is no literature in which the performance of the wiberg algorithm is investigated or the detail of the algorithm is presented . in this paper , we present derivation of the algorithm along with a problem in its implementation that needs to be carefully considered , and then examine its performance . the experimental results demonstrate that the wiberg algorithm shows a considerably good performance , which should contradict the conventional view in our community , namely that minimization based algorithms tend to fail to converge to a global minimum relatively frequently . the performance of the wiberg algorithm is such that even starting with random initial values , it converges in most cases to a correct solution , even when the matrix has many missing components and the data are contaminated with very strong noise . our conclusion is that the wiberg algorithm can also be used as a standard algorithm for the problems of computer vision .
semi supervised local fisher discriminant analysis for dimensionality reduction . <eos> when only a small number of labeled samples are available , supervised dimensionality reduction methods tend to perform poorly because of overfitting . in such cases , unlabeled samples could be useful in improving the performance . in this paper , we propose a semi supervised dimensionality reduction method which preserves the global structure of unlabeled samples in addition to separating labeled samples in different classes from each other . the proposed method , which we call semi supervised local fisher discriminant analysis ( self ) , has an analytic form of the globally optimal solution and it can be computed based on eigen decomposition . we show the usefulness of self through experiments with benchmark and real world document classification datasets .
a stable fluidstructure interaction solver for low density rigid bodies using the immersed boundary projection method . <eos> dispersion of low density rigid particles with complex geometries is ubiquitous in both natural and industrial environments . we show that while explicit methods for coupling the incompressible navierstokes equations and newton 's equations of motion are often sufficient to solve for the motion of cylindrical particles with low density ratios , for more complex particles such as a body with a protrusion they become unstable . we present an implicit formulation of the coupling between rigid body dynamics and fluid dynamics within the framework of the immersed boundary projection method . similarly to previous work on this method , the resulting matrix equation in the present approach is solved using a block lu decomposition . each step of the block lu decomposition is modified to incorporate the rigid body dynamics . we show that our method achieves second order accuracy in space and first order in time ( third order for practical settings ) , only with a small additional computational cost to the original method . our implicit coupling yields stable solution for density ratios as low as <digit> <digit> . we also consider the influence of fictitious fluid located inside the rigid bodies on the accuracy and stability of our method .
latent word context model for information retrieval . <eos> the application of word sense disambiguation ( wsd ) techniques to information retrieval ( ir ) has yet to provide convincing retrieval results . major obstacles to effective wsd in ir include coverage and granularity problems of word sense inventories , sparsity of document context , and limited information provided by short queries . in this paper , to alleviate these issues , we propose the construction of latent context models for terms using latent dirichletallocation . we propose building one latent context per word , using a well principled representation of local context based on word features . in particular , context words are weighted using a decaying function according to their distance to the target word , which is learnt from data in an unsupervised manner . the resulting latent features are used to discriminate word contexts , so as to constrict querys semantic scope . consistent and substantial improvements , including on difficult queries , are observed on trec test collections , and the techniques combines well with blind relevance feedback . compared to traditional topic modeling , wsd and positional indexing techniques , the proposed retrieval model is more effective and scales well on large scale collections .
clusterization , frustration and collectivity in random networks . <eos> we consider the random erdos renyi network with enhanced clusterization and ising spins s <digit> at the network nodes . mutually linked spins interact with energy j. magnetic properties of the system that are dependent on the clustering coefficient c are investigated with the monte carlo heat bath algorithm . for j > <digit> the curie temperature t ( c ) increases from 3.9 to 5.5 when c increases from almost zero to 0.18 . these results deviate only slightly from the mean field theory . for j < <digit> the spin glass phase appears below t ( sg ) this temperature decreases with c , on the contrary to the mean field calculations . the results are interpreted in terms of social systems .
gps ins integration utilizing dynamic neural networks for vehicular navigation . <eos> recently , methods based on artificial intelligence ( ai ) have been suggested to provide reliable positioning information for different land vehicle navigation applications integrating the global positioning system ( gps ) with the inertial navigation system ( ins ) . all existing ai based methods are based on relating the ins error to the corresponding ins output at certain time instants and do not consider the dependence of the error on the past values of ins . this study , therefore , suggests the use of input delayed neural networks ( idnn ) to model both the ins position and velocity errors based on current and some past samples of ins position and velocity , respectively . this results in a more reliable positioning solution during long gps outages . the proposed method is evaluated using road test data of different trajectories while both navigational and tactical grade ins are mounted inside land vehicles and integrated with gps receivers . the performance of the idnn based model is also compared to both conventional ( based mainly on kalman filtering ) and recently published al based techniques . the results showed significant improvement in positioning accuracy especially for cases of tactical grade ins and long gps outages . ( c ) <digit> elsevier b.v. all rights reserved .
a unified probabilistic framework for automatic 3d facial expression analysis based on a bayesian belief inference and statistical feature models . <eos> textured 3d face models capture precise facial surfaces along with the associated textures , making it possible for an accurate description of facial activities . in this paper , we present a unified probabilistic framework based on a novel bayesian belief network ( bbn ) for 3d facial expression and action unit ( au ) recognition . the proposed bbn performs bayesian inference based on statistical feature models ( sfm ) and gibbs boltzmann distribution and feature a hybrid approach in fusing both geometric and appearance features along with morphological ones . when combined with our previously developed morphable partial face model ( sfam ) , the proposed bbn has the capacity of conducting fully automatic facial expression analysis . we conducted extensive experiments on the two public databases , namely the bu 3dfe dataset and the bosphorus dataset . when using manually labeled landmarks , the proposed framework achieved an average recognition rate of 94.2 % and 85.6 % for the <digit> and <digit> au on face data from the bosphorus dataset respectively , and 89.2 % for the six universal expressions on the bu 3dfe dataset . using the landmarks automatically located by sfam , the proposed bbn still achieved an average recognition rate of 84.9 % for the six prototypical facial expressions . these experimental results demonstrate the effectiveness of the proposed approach and its robustness in landmark localization errors . published by elsevier b.v.
mpml3d scripting agents for the 3d internet . <eos> the aim of this paper is two fold . first , it describes a scripting language for specifying communicative behavior and interaction of computer controlled agents ( bots ) in the popular three dimensional ( 3d ) multiuser online world of second life and the emerging opensimulator project . while tools for designing avatars and in world objects in second life exist , technology for nonprogrammer content creators of scenarios involving scripted agents is currently missing . therefore , we have implemented new client software that controls bots based on the multimodal presentation markup language 3d ( mpml3d ) , a highly expressive xml based scripting language for controlling the verbal and nonverbal behavior of interacting animated agents . second , the paper compares second life and opensimulator platforms and discusses the merits and limitations of each from the perspective of agent control . here , we also conducted a small study that compares the network performance of both platforms .
tlb and snoop energy reduction using virtual caches in low power chip multiprocessors . <eos> in our quest to bring down the power consumption in low power chip multiprocessors , we have found that tlb and snoop accesses account for about <digit> % of the energy wasted by all l1 data cache accesses . we have investigated the prospects of using virtual caches to bring down the number of tlb accesses . a key observation is that while the energy wasted in the tlbs are cut , the energy associated with snoop accesses becomes higher . we then contribute with two techniques to reduce the number of snoop accesses and their energy cost . virtual caches together with the proposed techniques are shown to reduce the energy wasted in the l1 caches and the tlbs by about <digit> % .
extracting tennis statistics from wireless sensing environments . <eos> creating statistics from sporting events is now widespread with most efforts to automate this process using various sensor devices . the problem with many of these statistical applications is that they require proprietary applications to process the sensed data and there is rarely an option to express a wide range of query types . instead , applications tend to contain built in queries with predefined outputs . in the research presented in this paper , data from a wireless network is converted to a structured and highly interoperable format to facilitate user queries by expressing high level queries in a standard database language and automatically generating the results required by coaches .
critical success factors of inter organizational information systems a case study of cisco and xiao tong in china . <eos> this paper reports a case study of an inter organizational information system ( ios ) of cisco and xiao tong in china . we interviewed their senior managers , heads of departments and employees who have been directly affected in their work . other sources of information are company documents and publicly available background information . the study examines the benefits of the ios for both corporations . the research also reveals seven critical success factors for the ios , namely intensive stimulation , shared vision , cross organizational implementation team , high integration with internal information systems , inter organizational business process re engineering , advanced legacy information system and infrastructure and shared industry standard . ( c ) <digit> elsevier b.v. all rights reserved .
maize grain shape approaches for dem modelling . <eos> the shape of a grain of maize was approached using the multi sphere method . models with single spherical particles and with rolling friction were also used . results from two dem software codes were compared . recommendations on the shape approach for dem modelling were provided .
multi sector antenna performance in dense wireless networks . <eos> sectorized antennas provide an attractive solution to increase wireless network capacity through higher spatial reuse . despite their increasing popularity , the real world performance characteristics of such antennas in dense wireless mesh networks are not well understood . in this demo , we demonstrate our multi sector antenna prototypes and their performance through video streaming over an indoor wireless network in the presence of interfering nodes . we use our graphical tool to vary the sender , receiver , and interferer antenna configurations and the resulting performance is directly visible in the video quality displayed at the receiver .
improvement of 3p and 6r mechanical robots reliability and quality applying fmea and qfd approaches . <eos> in the past few years , extending usage of robotic systems has increased the importance of robot reliability and quality . to improve the robot reliability and quality by applying standard approaches such as failure mode and effect analysis ( fmea ) and quality function deployment ( qfd ) during the design of robot is necessary . fmea is a qualitative method which determines the critical failure modes in robot design . in this method risk priority number is used to sort failures with respect to critical situation . two examples of mechanical robots are analyzed by using this method and critical failure modes are determined for each robot . corrective actions are proposed for critical items to modify robots reliability and reduce their risks . finally by using qfd , quality of these robots is improved according to the customers requirements . in this method by making four matrixes , optimum values for all technical parameters are determined and the final product has the desired quality .
informatics methodologies for evaluation research in the practice setting . <eos> a continuing challenge in health informatics and health evaluation is to enable access to the practice of health care so that the determinants of successful care and good health outcomes can be measured , evaluated and analysed . furthermore the results of the analysis should be available to the health care practitioner or to the patient as might be appropriate , so that he or she can use this information for continual improvement of practice and optimisation of outcomes . in this paper we review two experiences , one in primary care , the famus project , and the other in hospital care , the autocontrol project . each project demonstrates an informatics approach for evaluation research in the clinical setting and indicates ways in which useful information can be obtained which with appropriate feed back and education can be used towards the achievement of better health . emphasis is given to data collection methods compatible with practice and to high quality information feedback , particularly in the team context , to enable the formulation of strategies for practice improvement .
an improved som algorithm and its application to color feature extraction . <eos> reducing the redundancy of dominant color features in an image and meanwhile preserving the diversity and quality of extracted colors is of importance in many applications such as image analysis and compression . this paper presents an improved self organization map ( som ) algorithm namely mfd som and its application to color feature extraction from images . different from the winner take all competitive principle held by conventional som algorithms , mfd som prevents , to a certain degree , features of non principal components in the training data from being weakened or lost in the learning process , which is conductive to preserving the diversity of extracted features . besides , mfd som adopts a new way to update weight vectors of neurons , which helps to reduce the redundancy in features extracted from the principal components . in addition , we apply a linear neighborhood function in the proposed algorithm aiming to improve its performance on color feature extraction . experimental results of feature extraction on artificial datasets and benchmark image datasets demonstrate the characteristics of the mfd som algorithm .
a motion planning system for mobile robots . <eos> in this paper , a motion planning system for a mobile robot is proposed . path planning tries to find a feasible path for mobile robots to move from a starting node to a target node in an environment with obstacles . a genetic algorithm is used to generate an optimal path by taking the advantage of its strong optimization ability . mobile robot , obstacle and target localizations are realized by means of camera and image processing . a graphical user interface ( gui ) is designed for the motion planning system that allows the user to interact with the robot system and to observe the robot environment . all the software components of the system are written in matlab that provides to use non predefined accessories rather than the robot firmware has , to avoid confusing in c libraries of robot 's proprietary software , to control the robot in detail and not to re compile the programs frequently in real time dynamic operations .
a unified strategy for search and result representation for an online bibliographical catalogue . <eos> purpose one of the biggest concerns of modem information retrieval systems is reducing the user effort required for manual traversal and filtering of long matching document lists . thus , the first goal of this research is to propose an improved scheme for representation of search results . further , it aims to explore the impact of various user information needs on the searching process with the aim of finding a unified searching approach well suited for different query types and retrieval tasks . design methodology approach the bow online bibliographical catalogue is based on a hierarchical concept index to which entries are linked . the key idea is that searching in the hierarchical catalogue should take advantage of the catalogue structure and return matching topics from the hierarchy , rather than just a long list of entries . likewise , when new entries are inserted , a search for relevant topics to which they should be linked is required . therefore , a similar hierarchical scheme for query topic matching can be applied for both tasks . findings the experiments show that different query types used for the above tasks are best treated by different topic ranking functions . to further examine this phenomenon a user study was conducted , where various statistical weighting factors were incorporated and their impact on the performance for different query types was measured . finally , it is found that the mixed strategy that applies the most suitable ranking function to each query type yielded a significant increase in precision relative to the baseline and to employing any examined strategy in isolation on the entire set of user queries . originality value the main contributions of this paper are the alternative approach for compact and concise representation of search results , which were implemented in the bow online bibliographical catalogue and the unified or mixed strategy for search and result representation applying the most suitable ranking function to each query type , which produced superior results compared to different single strategy based approaches .
robust multiple phase switched capacitor dc dc converter with digital interleaving regulation scheme . <eos> an integrated switched capacitor ( sc ) dc dc converter with a digital interleaving regulation scheme is presented . by interleaving the newly structured charge pump ( cp ) cells in multiple phases , the input current ripple and output voltage ripple are reduced significantly . the converter exhibits excellent robustness , even when one of the cp cells fails to operate . a fully digital controller is employed with a hysteretic control algorithm . it features dead beat system stability and fast transient response . hspice post layout simulation shows that , with a 1.5 v input power supply , the sc converter accurately provides an adjustable regulated power output in a range of 1.6 to 2.7 v. the maximum output ripple is <digit> mv when a full load of 0.54 w is supplied . transient response of 1.8 ms is observed when the load current switches from half to full load ( from <digit> to <digit> ma ) .
teeth recognition based on multiple attempts in mobile device . <eos> most traditional biometric approaches generally utilize a single image for personal identification . however , these approaches sometimes failed to recognize users in practical environment due to false detected or undetected subject . therefore , this paper proposes a novel recognition approach based on multiple frame images that are implemented in mobile devices . the aim of this paper is to improve the recognition accuracy and to reduce computational complexity through multiple attempts . here , multiple attempts denote that multiple frame images are used in time of recognition procedure . among sequential frame images , an adequate subject , i.e. , teeth image , is chosen by subject selection module which is operated based on differential image entropy . the selected subject is then utilized as a biometric trait of traditional recognition algorithms including pca , lda , and ehmm . the performance evaluation of proposed method is performed using two teeth databases constructed by a mobile device . through experimental results , we confirm that the proposed method exhibits improved recognition accuracy of about 3.64.8 % , and offers the advantage of lower computational complexity than traditional biometric approaches .
a conceptual approach for the die structure design . <eos> a large number of decisions are made during the conceptual design stage which is characterized by a lack of complete geometric information . while existing cad systems supporting the geometric aspects of design have had little impact at the conceptual design stage . to support the conceptual die design and the top down design process , a new concept called conceptual assembly modeling framework ( camf ) is presented in this paper . firstly , the framework employs the zigzag function symbol mapping to implement the function design of the die . from the easily understood analytical results of the function symbol mapping matrix , the designer can evaluate the quality of a proposed die concept . secondly , a new method logic assembly modeling is proposed using logic components in this framework to satisfy the characteristic of the conceptual die design . representing shapes and spatial relations in logic can provide a natural , intuitive method of developing complete computer systems for reasoning about die construction design at the conceptual stage . the logic assembly which consists of logic components is an innovative representation that provides a natural link between the function design of the die and the detailed geometric design .
approximation algorithm for coloring of dotted interval graphs . <eos> dotted interval graphs were introduced by aumann et al. y. aumann , m. lewenstein , o. melamud , r. pinter , z. yakhini , dotted interval graphs and high throughput genotyping , in acm siam symposium on discrete algorithms . soda <digit> , pp. <digit> <digit> as a generalization of interval graphs . the problem of coloring these graphs found application in high throughput genotyping . jiang m. jiang , approximating minimum coloring and maximum independent set in dotted interval graphs , information processing letters <digit> ( <digit> ) <digit> <digit> improves the approximation ratio of aumann et al. y. aumann , m. lewenstein , o. melamud , r. pinter , z. yakhini , dotted interval graphs and high throughput genotyping , in acm siam symposium on discrete algorithms , soda <digit> , pp. <digit> <digit> . in this work we improve the approximation ratio of jiang m. jiang , approximating minimum coloring and maximum independent set in dotted interval graphs , information processing letters <digit> ( <digit> ) <digit> <digit> and aumarm et al. y. aumann , m. lewenstein , o. melamud , r. pinter , z. yakhini , dotted interval graphs and high throughput genotyping , in acm siam symposium on discrete algorithms , soda <digit> , pp. <digit> <digit> . in the exposition we develop a generalization of the problem of finding the maximum number of non attacking queens on a triangle . ( c ) <digit> elsevier b.v. all rights reserved .
scalable visibility color map construction in spatial databases . <eos> recent advances in 3d modeling provide us with real 3d datasets to answer queries , such as what is the best position for a new billboard and which hotel room has the best view in the presence of obstacles . these applications require measuring and differentiating the visibility of an object ( target ) from different viewpoints in a dataspace , e.g. , a billboard may be seen from many points but is readable only from a few points closer to it . in this paper , we formulate the above problem of quantifying the visibility of ( from ) a target object from ( of ) the surrounding area with a visibility color map ( vcm ) . a vcm is essentially defined as a surface color map of the space , where each viewpoint of the space is assigned a color value that denotes the visibility measure of the target from that viewpoint . measuring the visibility of a target even from a single viewpoint is an expensive operation , as we need to consider factors such as distance , angle , and obstacles between the viewpoint and the target . hence , a straightforward approach to construct the vcm that requires visibility computation for every viewpoint of the surrounding space of the target is prohibitively expensive in terms of both i os and computation , especially for a real dataset comprising thousands of obstacles . we propose an efficient approach to compute the vcm based on a key property of the human vision that eliminates the necessity for computing the visibility for a large number of viewpoints of the space . to further reduce the computational overhead , we propose two approximations namely , minimum bounding rectangle and tangential approaches with guaranteed error bounds . our extensive experiments demonstrate the effectiveness and efficiency of our solutions to construct the vcm for real 2d and 3d datasets .
toward a neurogenetic theory of neuroticism . <eos> recent advances in neuroscience and molecular biology have begun to identify neural and genetic correlates of complex traits . future theories of personality need to integrate these data across the behavioral , neural , and genetic level of analysis and further explain the underlying epigenetic processes by which genes and environmental variables interact to shape the structure and function of neural circuitry . in this chapter , i will review some of the work that has been conducted at the cognitive , neural , and molecular genetic level with respect to one specific personality traitneuroticism . i will focus particularly on individual differences with respect to memory , self reference , perception , and attention during processing of emotional stimuli and the significance of gene by environment interactions . this chapter is intended to serve as a tutorial bridge for psychologists who may be intrigued by molecular genetics and for molecular biologists who may be curious about how to apply their research to the study of personality .
technological means of communication and collaboration in archives and records management . <eos> this study explores the international collaboration efforts of archivists and records managers starting with the hypothesis that internet technologies have had a significant impact on both national and international communication for this previously conservative group . the use and importance of mailing lists for this purpose is studied in detail . a quantitative analysis looks globally at the numbers of lists in these fields and the numbers of subscribers . a qualitative analysis of list content is also described . the study finds that archivists and records managers have now created more than <digit> mailing lists related to their profession and have been contributing to these lists actively . it also ' estimates ' that about half of the profession follows a list relating to their work and that archivists seem to like lists more than records managers do . the study concludes that mailing lists can be seen as a virtual college binding these groups together to develop the field .
privacy preserving decision tree learning using unrealized data sets . <eos> privacy preservation is important for machine learning and data mining , but measures designed to protect private information often result in a trade off reduced utility of the training samples . this paper introduces a privacy preserving approach that can be applied to decision tree learning , without concomitant loss of accuracy . it describes an approach to the preservation of the privacy of collected data samples in cases where information from the sample database has been partially lost . this approach converts the original sample data sets into a group of unreal data sets , from which the original samples can not be reconstructed without the entire group of unreal data sets . meanwhile , an accurate decision tree can be built directly from those unreal data sets . this novel approach can be applied directly to the data storage as soon as the first sample is collected . the approach is compatible with other privacy preserving approaches , such as cryptography , for extra protection .
selected topics on assignment problems . <eos> we survey recent developments in the fields of bipartite matchings , linear sum assignment and bottleneck assignment problems and applications , multidimensional assignment problems , quadratic assignment problems , in particular lower bounds , special cases and asymptotic results , biquadratic and communication assignment problems .
fast parameter free region growing segmentation with application to surgical planning . <eos> in this paper , we propose a self assessed adaptive region growing segmentation algorithm . in the context of an experimental virtual reality surgical planning software platform , our method successfully delineates main tissues relevant for reconstructive surgery , such as fat , muscle , and bone . we rely on a self tuning approach to deal with a great variety of imaging conditions requiring limited user intervention ( one seed ) . the detection of the optimal parameters is managed internally using a measure of the varying contrast of the growing region , and the stopping criterion is adapted to the noise level in the dataset thanks to the sampling strategy used for the assessment function . sampling is referred to the statistics of a neighborhood around the seed ( s ) , so that the sampling period becomes greater when images are noisier , resulting in the acquisition of a lower frequency version of the contrast function . validation is provided for synthetic images , as well as real ct datasets . for the ct test images , validation is referred to manual delineations for <digit> cases and to subjective assessment for another <digit> . high values of sensitivity and specificity , as well as dice 's coefficient and jaccard 's index on one hand , and satisfactory subjective evaluation on the other hand , prove the robustness of our contrast based measure , even suggesting suitability for calibration of other region based segmentation algorithms .
accuracy and efficiency in computing electrostatic potential for an ion channel model in layered dielectric electrolyte media . <eos> this paper will investigate the numerical accuracy and efficiency in computing the electrostatic potential for a finite height cylinder , used in an explicit implicit hybrid solvation model for ion channel and embedded in a layered dielectric electrolyte medium representing a biological membrane and ionic solvents . a charge locating inside the cylinder cavity , where ion channel proteins and ions are given explicit atomistic representations , will be influenced by the polarization field of the surrounding implicit dielectric electrolyte medium . two numerical techniques , a specially designed boundary integral equation method and an image charge method , will be investigated and compared in terms of accuracy and efficiency for computing the electrostatic potential . the boundary integral equation method based on the three dimensional layered green s functions provides a highly accurate solution suitable for producing a benchmark reference solution , while the image charge method is found to give reasonable accuracy and highly efficient and viable to use the fast multipole method for interactions of a large number of charges in the atomistic region of the hybrid solvation model .
the social sharing of emotion ( sse ) in online social networks a case study in live journal . <eos> using content analysis , we gauge the occurrence of social sharing of emotion ( sse ) in live journal . we present a theoretical model of a three cycle process for online sse . a large part of emotional blog posts showed full initiation of social sharing . affective feedback provided empathy , emotional support and admiration . this study is the first one to empirically assess the occurrence and structure of online sse .
non testing approaches under reach help or hindrance perspectives from a practitioner within industry . <eos> legislation such as reach strongly advocates the use of alternative approaches including invitro , ( q ) sars , and chemical categories as a means to satisfy the information requirements for risk assessment . one of the most promising alternative approaches is that of chemical categories , where the underlying hypothesis is that the compounds within the category are similar and therefore should have similar biological activities . the challenge lies in characterizing the chemicals , understanding the mode mechanism of action for the activity of interest and deriving a way of relating these together to form inferences about the likely activity outcomes . ( q ) sars are underpinned by the same hypothesis but are packaged in a more formalized manner . since the publication of the white paper for reach , there have been a number of efforts aimed at developing tools , approaches and techniques for ( q ) sars and read across for regulatory purposes . while technical guidance is available , there still remains little practical guidance about how these approaches can or should be applied in either the evaluation of existing ( q ) sars or in the formation of robust categories . here we provide a perspective of how some of these approaches have been utilized to address our in house reach requirements .
realtime performance analysis of different combinations of fuzzypid and bias controllers for a two degree of freedom electrohydraulic parallel manipulator . <eos> development of a <digit> dof electrohydraulic motion simulator as a parallel manipulator . control of heave , pitch and combined heave and pitch motion of the parallel manipulator . design of pid , fuzzypid , self tuning fuzzypid and self tuning fuzzypid with bias controllers . use of different combinations of fuzzypid and bias controllers for study of real time control performance . best control response found for the self tuning fuzzypid with bias controller .
on the depth distribution of linear codes . <eos> the depth distribution of a linear code was recently introduced by etzion . in this correspondence , a number of basic and interesting properties for the depth of finite words and the depth distribution of linear codes are obtained . in addition , we study the enumeration problem of counting the number of linear subcodes with the prescribed depth constraints , and derive some explicit and interesting enumeration formulas . furthermore , we determine the depth distribution of reed muller code rm ( m , r ) . finally , we show that there are exactly nine depth equivalence classes for the ternary <digit> , <digit> , <digit> golay codes .
are we there yet . <eos> statistical approaches to artificial intelligence are behind most success stories of the field in the past decade . the idea of generating non trivial behaviour by analysing vast amounts of data has enabled recommendation systems , search engines , spam filters , optical character recognition , machine translation and speech recognition , among other things . as we celebrate the spectacular achievements of this line of research , we need to assess its full potential and its limitations . what are the next steps to take towards machine intelligence
the complexity of the matroid greedoid partition problem . <eos> we show that the maximum matroid greedoid partition problem is np hard to approximate to within <digit> <digit> epsilon for any epsilon > <digit> , which matches the trivial factor <digit> <digit> approximation algorithm . the main tool in our hardness of approximation result is an extractor code with polynomial rate , alphabet size and list size , together with an efficient algorithm for list decoding . we show that the recent extractor construction of guruswami , umans and vadhan v. guruswami . c. umans , s.p. vadhan , unbalanced expanders and randomness extractors from parvaresh vardy codes , in ieee conference on computational complexity , ieee computer society , <digit> , pp. <digit> <digit> can be used to obtain a code with these properties . we also show that the parameterized matroid greedoid partition problem is fixed parameter tractable . ( c ) <digit> elsevier b.v. all rights reserved .
exploring the ncrnancrna patterns based on bridging rules . <eos> ncrnas play an important role in the regulation of gene expression . however , many of their functions have not yet been fully discovered . there are complicated relationships between ncrnas in different categories . finding these relationships can contribute to identify ncrnas functions and properties . we extend the association rule to represent the relationship between two ncrnas . based on this rule , we can speculate the ncrnas function when it interacts with other ncrnas . we propose two measures to explore the relationships between ncrnas in different categories . entropy theory is to calculate how close two ncrnas are . association rule is to represent the interactions between ncrnas . we use three datasets from mirbase and rnadb . two from mirbase are designed for finding relationships between mirnas the other from rnadb is designed for relationships among mirna , snorna and pirna . we evaluate our measures from both biological significance and performance perspectives . all the cross species patterns regarding mirna that we found are proven correct using mirnamap 2.0 . in addition , we find novel cross genomes patterns such as ( hsa mir 190b hsa mir <digit> <digit> ) . according to the patterns we find , we can ( <digit> ) explore one ncrnas function from another with known function and ( <digit> ) speculate the functions of both of them based on the relationship even we do no understand either of them . our methods merits also include ( <digit> ) they are suitable for any ncrna datasets and ( <digit> ) they are not sensitive to the parameters .
gaussian mixture modelling to detect random walks in capital markets . <eos> in this paper , gaussian mixture modelling is used to detect random walks in capital markets with the kolmogorov smirnov test . the main idea is to use gaussian mixture modelling to fit asset return distributions and then use the kolmogorov smirnov test to determine the number of components . several quantities are used to characterize gaussian mixture models and ascertain whether random walks exist in capital markets . empirical studies on china securities markets and forex markets are used to demonstrate the proposed procedure . ( c ) <digit> elsevier ltd. all rights reserved .
scientific design rationale . <eos> design rationale should be regarded both as a tool for the practice of design , and as a method to enable the science of design . design rationale answers questions about why a given design takes the form that it does . answers to these why questions represent a significant portion of the knowledge generated from design research . this knowledge , along with that from empirical studies of designs in use , contributes to what simon called the sciences of the artificial . most research on the nature and use of design rationale has been analytic or theoretical . in this article , we describe an empirical study of the roles that design rationale can play in the conduct of design research . we report results from an interview study with <digit> design researchers investigating how they construe and carry out design as research . the results include an integrated framework of the affordances design rationale can contribute to design research . the framework and supporting qualitative data provide insight into how design rationale might be more effectively leveraged as a first class methodology for research into the creation and use of artifacts .
high flowability monomer resists for thermal nanoimprint lithography . <eos> in this paper , we have been using polymer and thermally curable monomer resists in a full 8in . wafer thermal nanoimprint lithography process . using exactly the same imprinting conditions , we observed that a monomer solution provides a much larger resist redistribution than a polymer resist . imprinting fresnel zone plates , composed of micro and nano meter features , was possible only with the monomer resist . in order to reduce the shrinkage ratio of the monomer resists , acrylatesilsesquioxane materials were synthesised . with a simple diffusion like model , we could extract a mean free path of 1.1 mm for the monomer resist , while a polymer flows only on distances below <digit> m in the same conditions .
binarized support vector machines . <eos> the widely used support vector machine ( svm ) method has shown to yield very good results in supervised classification problems . other methods such as classification trees have become more popular among practitioners than svm thanks to their interpretability , which is an important issue in data mining . in this work , we propose an svm based method that automatically detects the most important predictor variables and the role they play in the classifier . in particular , the proposed method is able to detect those values and intervals that are critical for the classification . the method involves the optimization of a linear programming problem in the spirit of the lasso method with a large number of decision variables . the numerical experience reported shows that a rather direct use of the standard column generation strategy leads to a classification method that , in terms of classification ability , is competitive against the standard linear svm and classification trees . moreover , the proposed method is robust i.e. , it is stable in the presence of outliers and invariant to change of scale or measurement units of the predictor variables . when the complexity of the classifier is an important issue , a wrapper feature selection method is applied , yielding simpler but still competitive classifiers .
ambrosio tortorelli segmentation of stochastic images model extensions , theoretical investigations and numerical methods . <eos> we discuss an extension of the ambrosio tortorelli approximation of the mumford shah functional for the segmentation of images with uncertain gray values resulting from measurement errors and noise . our approach yields a reliable precision estimate for the segmentation result , and it allows us to quantify the robustness of edges in noisy images and under gray value uncertainty . we develop an ansatz space for such images by identifying gray values with random variables . the use of these stochastic images in the minimization of energies of ambrosio tortorelli type leads to stochastic partial differential equations for a stochastic smoothed version of the original image and a stochastic phase field for the edge set . for the discretization of these equations we utilize the generalized polynomial chaos expansion and the generalized spectral decomposition ( gsd ) method . in contrast to the simple classical sampling technique , this approach allows for an efficient determination of the stochastic properties of the output image and edge set by computations on an optimally small set of random variables . also , we use an adaptive grid approach for the spatial dimensions to further improve the performance , and we extend an edge linking method for the classical ambrosio tortorelli model for use with our stochastic model . the performance of the method is demonstrated on artificial data and a data set from a digital camera as well as real medical ultrasound data . a comparison of the intrusive gsd discretization with a stochastic collocation and a monte carlo sampling is shown .
a provably convergent heuristic for stochastic bicriteria integer programming . <eos> we propose a general purpose algorithm aps ( adaptive pareto sampling ) for determining the set of pareto optimal solutions of bicriteria combinatorial optimization ( co ) problems under uncertainty , where the objective functions are expectations of random variables depending on a decision from a finite feasible set . aps is iterative and population based and combines random sampling with the solution of corresponding deterministic bicriteria co problem instances . special attention is given to the case where the corresponding deterministic bicriteria co problem can be formulated as a bicriteria integer linear program ( ilp ) . in this case , well known solution techniques such as the algorithm by chalmet et al. can be applied for solving the deterministic subproblem . if the execution of aps is terminated after a given number of iterations , only an approximate solution is obtained in general , such that aps must be considered a metaheuristic . nevertheless , a strict mathematical result is shown that ensures , under rather mild conditions , convergence of the current solution set to the set of pareto optimal solutions . a modification replacing or supporting the bicriteria ilp solver by some metaheuristic for multicriteria co problems is discussed . as an illustration , we outline the application of the method to stochastic bicriteria knapsack problems by specializing the general framework to this particular case and by providing computational examples .
the impact of a simulation game on operations management education . <eos> this study presents a new simulation game and analyzes its impact on operations management education . the proposed simulation was empirically tested by comparing the number of mistakes during the first and second halves of the game . data were gathered from <digit> teams of four or five undergraduate students in business administration , taking their first course in operations management . to assess learning , instead of relying solely on an overall performance measurement , as is usually done in the skill based learning literature , we analyzed the evolution of different types of mistakes that were made by students in successive rounds of play . our results show that although simple decision making skills can be acquired with traditional teaching methods , simulation games are more effective when students have to develop decision making abilities for managing complex and dynamic situations . ( c ) <digit> elsevier ltd. all rights reserved .
covering a set of points in a plane using two parallel rectangles . <eos> in this paper we consider the problem of finding two parallel rectangles in arbitrary orientation for covering a given set of n points in a plane , such that the area of the larger rectangle is minimized . we propose an algorithm that solves the problem in o ( n ( <digit> ) ) time using o ( n ( <digit> ) ) space . without altering the complexity , our approach can be used to solve another optimization problem namely , minimize the sum of the areas of two arbitrarily oriented parallel rectangles covering a given set of points in a plane . ( c ) <digit> elsevier b.v. all rights reserved .
investigating the extreme programming system an empirical study . <eos> in this paper we discuss our empirical study about the advantages and difficulties <digit> greek software companies experienced applying extreme programming ( xp ) as a holistic system in software development . based on a generic xp system including feedback influences and using a cause effect model including social technical affecting factors , as our research tool , the study statistically evaluates the application of xp practices in the software companies being studied . data were collected from <digit> managers and developers , using the sample survey technique with questionnaires and interviews , in a time period of six months . practices were analysed individually , using descriptive statistics ( ds ) , and as a whole by building up different models using stepwise discriminant analysis ( da ) . the results have shown that companies , facing various problems with common code ownership , on site customer , <digit> hour week and metaphor , prefer to develop their own tailored xp method and way of working practices that met their requirements . pair programming and test driven development were found to be the most significant success factors . interactions and hidden dependencies for the majority of the practices as well as communication and synergy between skilled personnel were found to be other significant success factors . the contribution of this preliminary research work is to provide some evidence that may assist companies in evaluating whether the xp system as a holistic framework would suit their current situation .
an optimal gts scheduling algorithm for time sensitive transactions in ieee 802.15.4 networks . <eos> ieee 802.15.4 is a new enabling standard for low rate wireless personal area networks and has been widely accepted as a de facto standard for wireless sensor networking . while primary motivations behind 802.15.4 are low power and low cost wireless communications , the standard also supports time and rate sensitive applications because of its ability to operate in tdma access modes . the tdma mode of operation is supported via the guaranteed time slot ( gts ) feature of the standard . in a beacon enabled network topology , the personal area network ( pan ) coordinator reserves and assigns the gts to applications on a first come first served ( fcfs ) basis in response to requests from wireless sensor nodes . this fixed fcfs scheduling service offered by the standard may not satisfy the time constraints of time sensitive transactions with delay deadlines . such operating scenarios often arise in wireless video surveillance and target detection applications running on sensor networks . in this paper , we design an optimal work conserving scheduling algorithm for meeting the delay constraints of time sensitive transactions and show that the proposed algorithm outperforms the existing scheduling model specified in ieee 802.15.4 .
controlled dense coding with cluster state . <eos> two schemes for controlled dense coding with a one dimensional four particle cluster state are investigated . in this protocol , the supervisor ( cliff ) can control the channel and the average amount of information transmitted from the sender ( alice ) to the receiver ( bob ) by adjusting the local measurement angle theta . it is shown that the results for the average amounts of information are unique from the different two schemes .
slope stability analysis using the limit equilibrium method and two finite element methods . <eos> in this paper , the factors of safety and critical slip surfaces obtained by the limit equilibrium method ( lem ) and two finite element methods ( the enhanced limit strength method ( elsm ) and strength reduction method ( srm ) ) are compared . several representative two dimensional slope examples are analysed . using the associated flow rule , the results showed that the two finite element methods were generally in good agreement and that the lem yielded a slightly lower factor of safety than the two finite element methods did . moreover , a key condition regarding the stress field is shown to be necessary for elsm analysis .
deformation invariant attribute vector for deformable registration of longitudinal brain mr images . <eos> this paper presents a novel approach to define deformation invariant attribute vector ( diav ) for each voxel in 3d brain image for the purpose of anatomic correspondence detection . the diav method is validated by using synthesized deformation in 3d brain mri images . both theoretic analysis and experimental studies demonstrate that the proposed diav is invariant to general nonlinear deformation . moreover , our experimental results show that the diav is able to capture rich anatomic information around the voxels and exhibit strong discriminative ability . the diav has been integrated into a deformable registration algorithm for longitudinal brain mr images , and the results on both simulated and real brain images are provided to demonstrate the good performance of the proposed registration algorithm based on matching of diavs .
carbapenem resistant enterobacteriaceae biology , epidemiology , and management . <eos> introduced in the 1980s , carbapenem antibiotics have served as the last line of defense against multidrug resistant gram negative organisms . over the last decade , carbapenem resistant enterobacteriaceae ( cre ) have emerged as a significant public health threat . this review summarizes the molecular genetics , natural history , and epidemiology of cre and discusses approaches to prevention and treatment .
hypergraph based inductive learning for generating implicit key phrases . <eos> this paper presents a novel approach to generate implicit key phrases which are ignored in previous researches . recent researches prefer to extract key phrases with semi supervised transductive learning methods , which avoid the problem of training data . in this paper , based on a transductive learning method , we formulate the phrases in the document as a hypergraph and expand the hypergraph to include implicit phrases , which are ranked by an inductive learning approach . the highest ranked phrases are seen as implicit key phrases , and experimental results demonstrate the satisfactory performance of this approach .
strategic commitment to price to stimulate downstream innovation in a supply chain . <eos> it is generally in a firms interest for its supply chain partners to invest in innovations . to the extent that these innovations either reduce the partners variable costs or stimulate demand for the end product , they will tend to lead to higher levels of output for all of the firms in the chain . however , in response to the innovations of its partners , a firm may have an incentive to opportunistically increase its own prices . the possibility of such opportunistic behavior creates a hold up problem that leads supply chain partners to underinvest in innovation . clearly , this hold up problem could be eliminated by a pre commitment to price . however , by making an advance commitment to price , a firm sacrifices an important means of responding to demand uncertainty . in this paper we examine the trade off that is faced when a firms channel partner has opportunities to invest in either cost reduction or quality improvement , i.e. demand enhancement . should it commit to a price in order to encourage innovation , or should it remain flexible in order to respond to demand uncertainty . we discuss several simple wholesale pricing mechanisms with respect to this trade off .
mutation based software testing using program schemata . <eos> mutation analysis is a powerful technique for assessing the quality of test data used in unit testing software . unfortunately , current automated mutation analysis systems suffer from severe performance problems . in this paper the principles of mutation analysis are reviewed , current automation approaches are described , and a new method of performing mutation analysis is outlined . performance improvements of over <digit> % are reported and other advantages of this new method are highlighted .
bamboo a data centric , object oriented approach to many core software . <eos> traditional data oriented programming languages such as dataflow languages and stream languages provide a natural abstraction for parallel programming . in these languages , a developer focuses on the flow of data through the computation and these systems free the developer from the complexities of low level , thread oriented concurrency primitives . this simplification comes at a cost traditional data oriented approaches restrict the mutation of state and , in practice , the types of data structures a program can effectively use . bamboo borrows from work in typestate and software transactions to relax the traditional restrictions of data oriented programming models to support mutation of arbitrary data structures . we have implemented a compiler for bamboo which generates code for the tilepro64 many core processor . we have evaluated this implementation on six benchmarks tracking , a feature tracking algorithm from computer vision kmeans , a k means clustering algorithm montecarlo , a monte carlo simulation filterbank , a multi channel filter bank fractal , a mandelbrot set computation and series , a fourier series computation . we found that our compiler generated implementations that obtained speedups ranging from 26.2 x to 61.6 x when executed on <digit> cores .
performance optimization problem in speculative prefetching . <eos> speculative prefetching has been proposed to improve the response time of network access . previous studies in speculative prefetching focus on building and evaluating access models for the purpose of access prediction . this paper investigates a complementary area which has been largely ignored , that of performance modeling . we analyze the performance of a prefetcher that has uncertain knowledge about future accesses . our performance metric is the improvement in access time , for which we derive a formula in terms of resource parameters ( time available and time required for prefetching ) and speculative parameters ( probabilities for next access ) . we develop a prefetch algorithm to maximize the improvement in access time . the algorithm is based on finding the best solution to a stretch knapsack problem , using theoretically proven apparatus to reduce the search space . an integration between speculative prefetching and caching is also investigated .
inspiring collaboration through the use of videoconferencing technology . <eos> at the beginning of <digit> the university of washington opened the odegaard videoconference studio which allowed groups on campus to communicate with colleagues that were physically in different locations . the opening of this facility inspired all sorts of collaborating on a more frequent basis as traveling , and more importantly the time and expense involved with traveling , was now not as necessary in order to have a meeting . many boundaries for collaboration were removed through the use of different types of technology that allowed for video and audio conferencing , and , data and application sharing . this provided for a way to share ideas in more detail , make decisions , and receive feedback quicker , making the overall process more efficient , personal , and overall more effective .
expanders , sorting in rounds and superconcentrators of limited depth . <eos> expanding graphs and superconcentrators are relevant to theoretical computer science in several ways . here we use finite geometries to construct explicitly highly expanding graphs with essentially the smallest possible number of edges . our graphs enable us to improve significantly previous results on a parallel sorting problem , by describing an explicit algorithm to sort n elements in k time units using ogr ( n agr k ) processors , where , e.g. , agr <digit> <digit> <digit> . using our graphs we can also construct efficient n superconcentrators of limited depth . for example , we construct an n superconcentrator of depth <digit> with ogr ( n <digit> <digit> ) edges better than the previous known results .
synchrony and frequency regulation by synaptic delay in networks of self inhibiting neurons . <eos> we show that a pair of mutually coupled self inhibitory neurons can display stable synchronous oscillations provided only that the delay to the onset of inhibition is sufficiently long . the frequency of these oscillations is determined either entirely by the length of the synaptic delay , or by the synaptic delay and intrinsic time constants . we also show how cells can exhibit transient synchronous oscillations where the length of the transients is determined by the synaptic delay , but where the frequency is largely independent of the delay .
minimizing power dissipation during write operation to register files . <eos> this paper presents a power reduction mechanism for the write operation in register files ( regfiles ) , which adds a conditional charge sharing structure to the pair of complementary bit lines in each column of the regfile . because the read and write ports for the regfile are separately implemented , it is possible to avoid pre charging the bit line pair for consecutive writes . more precisely , when writing same values to some cells in the same column of the regfile , it is possible to eliminate energy consumption due to precharging of the bit line pair . at the same time , when writing opposite values to some cells in the same column of the regfile , it is possible to reduce energy consumed in charging the bit line pair thanks to charge sharing . motivated by these observations , we modify the bit line structure of the write ports in the regfile such that i ) we remove per cycle bitline pre charging and ii ) we employ conditional data dependent charge sharing . experimental results on a set of spec2000int mediabench benchmarks show an average of 61.5 % energy savings with 5.1 % area overhead and 16.2 % increase in write access delay .
a decision support framework for metrics selection in goal based measurement programs gqm dsfms . <eos> complex gqm based measurement programs lead to the need for decision support in metric selection . we provide an decision support framework in choosing an optimal set of metrics to maximize measurement goal achievement for a given budget . the framework was evaluated by comparison with expert opinion in a cmmi level <digit> company . extent of addressing information needs under a fixed budged was higher when selecting metrics using the framework .
energy area delay trade offs in the physical design of on chip segmented bus architecture . <eos> the increasing gap between design productivity and chip complexity , and the emerging systems on chip ( soc ) architectural template have led to the wide utilization of reusable intellectual property ( ip ) cores . the physical design implementation of the macro cells ( ip blocks or pre designed blocks ) in general needs to find a well balanced solution among chip area , on chip interconnect energy and critical path delay . we are especially interested in the entire trade off curve among these three criteria at the floorplanning stage . we show this concept for a real communication scheme based on segmented bus , rather than just an extreme solution . a fast exploration design flow from the memory organization to the final layout is introduced to explore the design space .
system integration of a miniature rotorcraft for aerial tele operation research . <eos> this paper describes the development and integration of the systems required for research into human interaction with a tele operated miniature rotorcraft . because of the focus on vehicles capable of operating indoors , the size of the vehicle was limited to <digit> cm , and therefore the hardware had to be carefully chosen to meet the ensuing size and weight constraints , while providing sufficient flight endurance . the components described in this work include the flight hardware , electronics , sensors , and software necessary to conduct tele operation experiments . the integration tasks fall into three main areas . first , the paper discusses the choice of rotorcraft platform best suited for indoor operation addressing the issues of size , payload capabilities , and power consumption . the second task was to determine what electronics and sensing could be integrated into a rotorcraft with significant payload limitations . finally , the third task involved characterizing the various components both individually and as a complete system . the paper concludes with an overview of ongoing tele operation research performed with the embedded rotorcraft platform . ( c ) <digit> elsevier ltd. all rights reserved .
rank inclusion in criteria hierarchies . <eos> this paper presents a method called rank inclusion in criteria hierarchies ( rich ) for the analysis of incomplete preference information in hierarchical weighting models . in rich , the decision maker is allowed to specify subsets of attributes which contain the most important attribute or , more generally , to associate a set of rankings with a given set of attributes . such preference statements lead to possibly non convex sets of feasible attribute weights , allowing decision recommendations to be obtained through the computation of dominance relations and decision rules . an illustrative example on the selection of a subcontractor is presented , and the computational properties of rich are considered .
automatic relative orientation of large scale imagery over urban areas using modified iterated hough transform . <eos> the automation of relative orientation ( ro ) has been the major focus of the photogrammetric research community in the last decade . despite the reported progress , there is no reliable ( robust ) approach that can perform automatic relative orientation ( aro ) using large scale imagery over urban areas . a reliable and general method for solving matching problems in various photogrammetric activities has been developed at the ohio state university . this approach has been used to solve single photo resection using free form linear features , surface matching and relative orientation . the approach estimates the parameters of a mathematical model relating the entities of two datasets when the correspondence of the involved entities is unknown . when applied to relative orientation , the coplanarity model is used to relate extracted edge pixels and or feature points from a stereo pair . in its execution , the relative orientation parameters are solved sequentially , using the coplanarity model to evaluate all possible pairings of the input primitives and choosing the most probable solution . as a result of this technique , the matched entities that correspond to the parameter solution are implicitly determined . experiments using real data conclude that this is a robust method for relative orientation for both urban and rural scenes .
emergency railway wagon scheduling by hybrid biogeography based optimization . <eos> railway transportation plays an important role in many disaster relief and other emergency supply chains . based on the analysis of several recent disaster rescue operations in china , the paper proposes a mathematical model for emergency railway wagon scheduling , which considers multiple target stations requiring relief supplies , source stations for providing supplies , and central stations for allocating railway wagons . under the emergency environment , the aim of the problem is to minimize the weighted time for delivering all the required supplies to the targets . for efficiently solving the problem , we develop a new hybrid biogeography based optimization ( bbo ) algorithm , which uses a local ring topology of population to avoid premature convergence , includes the differential evolution ( de ) mutation operator to perform effective exploration , and takes some problem specific mechanisms for fine tuning the search process and handling the constraints . computational experiments show that our algorithm is robust and scalable , and outperforms some state of the art heuristic algorithms on a set of problem instances .
biasvariance analysis in estimating true query model for information retrieval . <eos> we study the retrieval effectiveness stability tradeoff in query model estimation . this tradeoff is investigated through a novel angle , i.e. , biasvariance tradeoff . we formulate the performance biasvariance and estimation biasvariance . we investigate various query estimation methods using biasvariance analysis . experiments have been conducted to verify hypotheses on biasvariance analysis .
qualitative constraint satisfaction problems an extended framework with landmarks . <eos> dealing with spatial and temporal knowledge is an indispensable part of almost all aspects of human activity . the qualitative approach to spatial and temporal reasoning , known as qualitative spatial and temporal reasoning ( qstr ) , typically represents spatial temporal knowledge in terms of qualitative relations ( e.g. , to the east of , after ) , and reasons with spatial temporal knowledge by solving qualitative constraints . when formulating qualitative constraint satisfaction problems ( csps ) , it is usually assumed that each variable could be here , there and everywhere . ( <digit> ) practical applications such as urban planning , however , often require a variable to take its value from a certain finite domain , i.e. it is required to be ' here or there , but not everywhere ' . entities in such a finite domain often act as reference objects and are called landmarks in this paper . the paper extends the classical framework of qualitative csps by allowing variables to take values from finite domains . the computational complexity of the consistency problem in this extended framework is examined for the five most important qualitative calculi , viz. point algebra , interval algebra , cardinal relation algebra , rcc5 , and rcc8 . we show that all these consistency problems remain in np and provide , under practical assumptions , efficient algorithms for solving basic constraints involving landmarks for all these calculi . ( c ) <digit> elsevier b.v. all rights reserved .
the interaction of software prefetching with ilp processors in shared memory systems . <eos> current microprocessors aggressively exploit instruction level parallelism ( ilp ) through techniques such as multiple issue , dynamic scheduling , and non blocking reads . recent work has shown that memory latency remains a significant performance bottleneck for shared memory multiprocessor systems built of such processors.this paper provides the first study of the effectiveness of software controlled non binding prefetching in shared memory multiprocessors built of state of the art ilp based processors . we find that software prefetching results in significant reductions in execution time ( <digit> % to <digit> % ) for three out of five applications on an ilp system . however , compared to previous generation system , software prefetching is significantly less effective in reducing the memory stall component of execution time on an ilp system . consequently , even after adding software prefetching , memory stall time accounts for over <digit> % of the total execution time in four out of five applications on our ilp system.this paper also investigates the interaction of software prefetching with memory consistency models on ilp based multiprocessors . in particular , we seek to determine whether software prefetching can equalize the performance of sequential consistency ( sc ) and release consistency ( rc ) . we find that even with software prefetching , for three out of five applications , rc provides a significant reduction in execution time ( <digit> % to <digit> % ) compared to sc .
a scalable and extensible framework for query answering over rdf . <eos> the semantic web is gaining increasing interest to fulfill the need of sharing , retrieving , and reusing information . in this context , the resource description framework ( rdf ) has been conceived to provide an easy way to represent any kind of data and metadata , according to a lightweight model and syntaxes for serialization ( rdf xml , n3 , etc. ) . despite rdf has the advantage of being general and simple , it can not be used as a storage model as it is , since it can be easily shown that even simple management operations involve serious performance limitations . in this paper we present a framework which provides a flexible and persistent layer relying on a novel storage model that guarantees good scalability and performance of query evaluation . the approach is based on the notion of construct , that represents a concept of the domain of interest . this makes the approach easily extensible and independent from the specific knowledge representation language . based on this representation , reasoning capabilities are supported by a rule based engine . finally we present experimental results over real world scenarios to demonstrate the feasibility of the approach .
using interactive <digit> d visualization for public consultation . <eos> <digit> d models are often developed to aid the design and development of indoor and outdoor environments . this study explores the use of interactive <digit> d visualization for public consultation for outdoor environments . two visualization techniques ( interactive <digit> d visualization and static visualization ) were compared using the method of individual testing . visualization technique had no effect on the perception of the represented outdoor environment , but there was a preference for using interactive <digit> d. previously established mechanisms for a preference for interactive <digit> d visualization in other domains were confirmed in the perceived strengths and weaknesses of visualization techniques . in focus group discussion , major preferences included provision of more information through interactive <digit> d visualization and wider access to information for public consultation . from a users ' perspective , the findings confirm the strong potential of interactive <digit> d visualization for public consultation . ( c ) <digit> elsevier b.v. all rights reserved .
polymorphic nodal elements and their application in discontinuous galerkin methods . <eos> in this work , we discuss two different but related aspects of the development of efficient discontinuous galerkin methods on hybrid element grids for the computational modeling of gas dynamics in complex geometries or with adapted grids . in the first part , a recursive construction of different nodal sets for hp finite elements is presented . they share the property that the nodes along the sides of the two dimensional elements and along the edges of the three dimensional elements are the legendregausslobatto points . the different nodal elements are evaluated by computing the lebesgue constants of the corresponding vandermonde matrix . in the second part , these nodal elements are applied within the modal discontinuous galerkin framework . we still use a modal based formulation , but introduce a nodal based integration technique to reduce computational cost in the spirit of pseudospectral methods . we illustrate the performance of the scheme on several large scale applications and discuss its use in a recently developed space time expansion discontinuous galerkin scheme .
deployment based solution for prolonging lifetime in sensor networks with multiple mobile sinks . <eos> enhancing sensor network lifetime is an important research topic for wireless sensor networks . solutions based on linear programming , clustering , controlled non uniform node distributions and mobility are presented separately in the literature . even thought , the problem is still open and not fully solved . drawbacks exist for all the above solutions when considered separately . perhaps a solution that is able to provide composite benefits of some of them could better solve the problem . in this paper , we introduce a solution for prolonging the lifetime of sensor networks . the proposed solution is based on a deployment strategy of multiple mobile sinks . in our proposal , data traffic is directed away from the network center toward the network peripheral where sinks would be initially deployed . sinks stay stationary while collecting the data reports that travel over the network perimeter toward them . eventually perimeter nodes would be exposed to a peeling phenomenon which results in partitioning one or more sinks from their one hop neighbors . the partitioned sinks move discrete steps following the direction of the progressive peeling towards the network center . the mechanism maintains the network connectivity and delays the occurrence of partition . moreover , it balances the load among nodes and reduces the energy consumption . the performance of the proposed protocol is evaluated using intensive simulations . the results show the efficiency ( in terms of both reliability and connectivity ) of our deployment strategy with the associated data collection protocol .
design and applications of an algorithm benchmark system in a computational problem solving environment . <eos> benchmark tests are often used to evaluate the quality of products by a set of common criteria . in this paper we describe a computational problem solving environment based on open source codes and an algorithm benchmark system , which is embedded in the environment as a plug in system . the algorithm benchmark system can be used to compare the performance of various algorithms or to evaluate the behavior of an algorithm with different input instances . the current implementation allows users to compare or evaluate algorithms written in c c . some examples of the algorithm benchmark system that evaluates the memory utilization , time complexity and the output of algorithms are also presented . algorithm benchmark impresses the learning effect students can not only comprehend the performance of respective algorithms but also write their own programs to challenge the best known results .
automated performance tuning . <eos> this tutorial presents automated techniques for implementing and optimizing numeric and symbolic libraries on modern computing platforms including sse , multicore , and gpu . obtaining high performance requires effective use of the memory hierarchy , short vector instructions , and multiple cores . highly tuned implementations are difficult to obtain and are platform dependent . for example , intel core i7 <digit> xe has a peak floating point performance of over <digit> gflops and the nvidia tesla c870 has a peak floating point performance of over <digit> gflops , however , achieving close to peak performance on such platforms is extremely difficult . consequently , automated techniques are now being used to tune and adapt high performance libraries such as atlas ( math atlas.sourceforge.net ) , plasma ( icl.cs.utk.edu plasma ) and magma ( icl.cs.utk.edu magma ) for dense linear algebra , oski ( bebop.cs.berkeley.edu oski ) for sparse linear algebra , fftw ( www.fftw.org ) for the fast fourier transform ( fft ) , and spiral ( www.spiral.net ) for wide class of digital signal processing ( dsp ) algorithms . intel currently uses spiral to generate parts of their mkl and ipp libraries .
explicit solutions for a class indirect pharmacodynamic response models . <eos> explicit solutions for four , ordinary differential equation ( ode ) based , types of indirect response models are presented . these response models were introduced by dayneka et al in <digit> j. pharmacokinet . biopharm . <digit> ( <digit> ) <digit> to describe pharmacodynamic responses utilizing inhibitory or stimulatory em , x type functions . the explicit solutions are expressed in terms of hypergeometric f <digit> ( <digit> ) functions and their analytical continuations . a practical application is demonstrated for modeling the kinetics of drug action for ibandronate , a potent bisphosphonate that suppresses bone turnover resulting in a reduction in the markers of bone turnover . ten times shorter model evaluation times , with the explicit solution compared with the differential equation implementation , may enhance situations where a large number of model evaluations are needed , such as clinical trial simulations and parameter estimation . ( c ) <digit> elsevier ireland ltd. all rights reserved .
a web based consumer oriented intelligent decision support system for personalized e services . <eos> due to the rapid advancement of electronic commerce and web technologies in recent years , the concepts and applications of decision support systems have been significantly extended . one quickly emerging research topic is the consumer oriented decision support system that provides functional supports to consumers for efficiently and effectively making personalized decisions . in this paper we present an integrated framework for developing web based consumer oriented intelligent decision support systems to facilitate all phases of consumer decision making process in business to consumer e services applications . major application functional modules comprised in the system framework include consumer and personalized management , navigation and search , evaluation and selection , planning and design , community and collaboration management , auction and negotiation , transactions and payments , quality and feedback control , as well as communications and information distributions . system design and implementation methods will be illustrated using an example . also explored are various potential e services application domains including e tourism and e investment .
efficient segment based video transcoding proxy for mobile multimedia services . <eos> to support various bandwidth requirements for mobile multimedia services for future heterogeneous mobile environments , such as portable notebooks , personal digital assistants ( pdas ) , and 3g cellular phones , a transcoding video proxy is usually necessary to provide mobile clients with adapted video streams by not only transcoding videos to meet different needs on demand , but also caching them for later use . traditional proxy technology is not applicable to a video proxy because it is less cost effective to cache the complete videos to fit all kinds of clients in the proxy . since transcoded video objects have inheritance dependency between different bit rate versions , we can use this property to amortize the retransmission overhead from transcoding other objects cached in the proxy . in this paper , we propose the object relation graph ( org ) to manage the static relationships between video versions and an efficient replacement algorithm to dynamically manage video segments cached in the proxy . specifically , we formulate a transcoding time constrained profit function to evaluate the profit from caching each version of an object . the profit function considers not only the sum of the costs of caching individual versions of an object , but also the transcoding relationship among these versions . in addition , an effective data structure , cached object relation tree ( cort ) , is designed to facilitate the management of multiple versions of different objects cached in the transcoding proxy . experimental results show that the proposed algorithm outperforms companion schemes in terms of the byte hit ratios and the startup latency .
automated process planning method to machine a b spline free form feature on a mill turn center . <eos> in this paper , we present a methodology for automating the process planning and nc code generation for a widely encountered class of free form features that can be machined on a <digit> axis mill turn center . the free form feature family that is considered is that of extruded protrusions whose cross section is a closed , periodic b spline curve . in this methodology , for machining a part with b spline protrusion located at the free end , the part is first rough turned to the maximum profile diameter of the b spline , followed by rough profile cutting and finish profiling with axially mounted end mill tools . the identification and sequencing of machining volumes is completely automated , as is the generation of actual nc code . the approach supports both convex and non convex profiles . in the case of non convex profiles , the process planning algorithm ensures that there is no gouging of the work piece by the tool . the algorithm also identifies when sections of the tool path lie outside the work piece and utilizes rapid traverses in these regions to reduce cutting time . this methodology presents an integrated turn mill process planning where by making the process fully automated from design with no user intervention making the overall process planning efficient . the algorithm was tested on several examples and test parts using the unmodified nc code obtained from the implementation were run on a moriseiki mill turn center . the parts that were produced met the dimensional specifications of the desired part . ( c ) <digit> elsevier ltd. all rights reserved .
stability results for two classes of linear time delay and hybrid systems . <eos> the stability of linear time delay systems with point internal delays is difficult to deal with in practice because of the fact that their characteristic equation is usually of transcendent type rather than of polynomial type . this feature causes usually the system to possess an infinite number of poles . in this paper , stability tests for this class of systems are obtained either based on extensions of classical tests applicable to delay free systems or on approaches within the framework of two dimensional digital filters . some of those two dimensional stability tests are also proved to be useful for stability testing of a common class of linear hybrid systems which involve coupled continuous and digital substates after a slight ad hoc adaptation of the tests for that situation .
a pseudo nearest neighbor approach for missing data recovery on gaussian random data sets . <eos> missing data handling is an important preparation step for most data discrimination or mining tasks . inappropriate treatment of missing data may cause large errors or false results . in this paper , we study the effect of a missing data recovery method , namely the pseudo nearest neighbor substitution approach , on gaussian distributed data sets that represent typical cases in data discrimination and data mining applications . the error rate of the proposed recovery method is evaluated by comparing the clustering results of the recovered data sets to the clustering results obtained on the originally complete data sets . the results are also compared with that obtained by applying two other missing data handling methods , the constant default value substitution and the missing data ignorance ( non substitution ) methods . the experiment results provided a valuable insight to the improvement of the accuracy for data discrimination and knowledge discovery on large data sets containing missing values .
a lagrangian relaxation approach to the edge weighted clique problem . <eos> the b clique polytope cpnb is the convex hull of the node and edge incidence vectors of all subcliques of size at most b of a complete graph on n nodes . including the boolean quadric polytope qpn cpnn as a special case and being closely related to the quadratic knapsack polytope , it has received considerable attention in the literature . in particular , the max cut problem is equivalent with optimizing a linear function over cpnn . the problem of optimizing linear functions over cpnb has so far been approached via heuristic combinatorial algorithms and cutting plane methods . we study the structure of cpnb in further detail and present a new computational approach to the linear optimization problem based on the idea of integrating cutting planes into a lagrangian relaxation of an integer programming problem that balas and christofides had suggested for the traveling salesman problem . in particular , we show that the separation problem for tree inequalities becomes polynomial in our lagrangian framework . finally , computational results are presented .
resource aware programming in the pixie os . <eos> this paper presents pixie , a new sensor node operating system designed to support the needs of data intensive applications . these applications , which include high resolution monitoring of acoustic , seismic , acceleration , and other signals , involve high data rates and extensive in network processing . given the fundamentally resource limited nature of sensor networks , a pressing concern for such applications is their ability to receive feedback on , and adapt their behavior to , fluctuations in both resource availability and load . the pixie os is based on a dataflow programming model based on the concept of resource tickets , a core abstraction for representing resource availability and reservations . by giving the system visibility and fine grained control over resource management , a broad range of policies can be implemented . to shield application programmers from the burden of managing these details , pixie provides a suite of resource brokers , which mediate between low level physical resources and higher level application demands . pixie is implemented in nesc and supports limited backwards compatibility with tinyos . we describe pixie in the context of two applications limb motion analysis for patients undergoing treatment for motion disorders , and acoustic target detection using a network of microphones . we present a range of experiments demonstrating pixie 's ability to accurately account for resource availability at runtime and enable a range of both generic and application specific adaptations .
highly undersampled magnetic resonance image reconstruction via homotopic l ( <digit> ) minimization . <eos> in clinical magnetic resonance imaging ( mri ) , any reduction in scan time offers a number of potential benefits ranging from high temporal rate observation of physiological processes to improvements in patient comfort . following recent developments in compressive sensing ( cs ) theory , several authors have demonstrated that certain classes of mr images which possess sparse representations in some transform domain can be accurately reconstructed from very highly undersampled k space data by solving a convex l ( <digit> ) minimization problem . although l ( <digit> ) based techniques are extremely powerful , they inherently require a degree of oversampling above the theoretical minimum sampling rate to guarantee that exact reconstruction can be achieved . in this paper , we propose a generalization of the cs paradigm based on homotopic approximation of the l ( <digit> ) quasi norm and show how mr image reconstruction can be pushed even further below the nyquist limit and significantly closer to the theoretical bound . following a brief review of standard cs methods and the developed theoretical extensions , several example mri reconstructions from highly undersampled k space data are presented .
an incremental verification algorithm for real time systems . <eos> we present an incremental algorithm for model checking the red time systems against the requirements specified in the real time extension of modal mu calculus . using this algorithm , we avoid the repeated construction and analysis of the whole state space during the course of evolution of the system from time to time . we use a finite representation of the system , like most other algorithms on real time systems . we construct and update a graph ( called tsg ) that is derived from the region graph and the formula . this allows us to halt the construction of this graph when enough nodes have been explored to determine the truth of the formula . tsg is minimal in the sense of partitioning the infinite state space into regions and it expresses a relation on the set of regions of the partition . we use the structure of the formula to derive this partition . when a change is applied to the timed automaton of the system , we find a new partition from the current partition and the tsg with minimum cost .
a survey on transport protocols for wireless multimedia sensor networks . <eos> wireless networks composed of multimedia enabled resource constrained sensor nodes have enriched a large set of monitoring sensing applications . in such communication scenario , however , new challenges in data transmission and energy efficiency have arisen due to the stringent requirements of those sensor networks . generally , congested nodes may deplete the energy of the active congested paths toward the sink and incur in undesired communication delay and packet dropping , while bit errors during transmission may negatively impact the end to end quality of the received data . many approaches have been proposed to face congestion and provide reliable communications in wireless sensor networks , usually employing some transport protocol that address one or both of these issues . nevertheless , due to the unique characteristics of multimedia based wireless sensor networks , notably minimum bandwidth demand , bounded delay and reduced energy consumption requirement , communication protocols from traditional scalar wireless sensor networks are not suitable for multimedia sensor networks . in the last decade , such requirements have fostered research in adapting existing protocols or proposing new protocols from scratch . we survey the state of the art of transport protocols for wireless multimedia sensor networks , addressing the recent developments and proposed strategies for congestion control and loss recovery . future research directions are also discussed , outlining the remaining challenges and promising investigation areas .
two integrable couplings of the tu hierarchy and their hamiltonian structures . <eos> the double integrable couplings of the tu hierarchy are worked out by use of vector loop algebras g <digit> and g <digit> respectively . also the hamiltonian structures of the obtained system are given by the quadratic form identity .
dynamic simulation of bioreactor systems using orthogonal collocation on finite elements . <eos> the dynamics of continuous biological processes is addressed in this paper . numerical simulation of a conventional activated sludge process shows that despite the large differences in the dynamics of the species investigated . the orthogonal collocation on finite element technique with three internal collocation and four elements ( ocfe <digit> ) gives excellent numerical results for bioreactor models up to a peclet number of <digit> . it is shown that there is little improvement in numerical accuracy when a much larger internal collocation point is introduced . over and above peclet number of <digit> , considered to be large for this process . simulation with the global orthogonal collocation ( goc ) technique is infeasible . due to the banded nature of its structural matrix , the method of lines ( mol ) technique requires the lowest computing time , typically four times less than that required by the ocfe <digit> . validation of the hydraulics of an existing pilot scale subsurface flow ( ssf ) constructed wetland process using the aforementioned numerical techniques suggested that the ocfe is superior to the mol and goc in terms of numerical stability , ( c ) <digit> elsevier science ltd. all rights reserved .
detecting regularities on grammar compressed strings . <eos> we address the problems of detecting and counting various forms of regularities in a string represented as a straight line program ( slp ) which is essentially a context free grammar in the chomsky normal form . given an slp of size n that represents a string s of length n , our algorithm computes all runs and squares in s in o ( n3h ) o ( n <digit> h ) time and o ( n2 ) o ( n <digit> ) space , where h is the height of the derivation tree of the slp . we also show an algorithm to compute all gapped palindromes in o ( n3h gnhlog n ) o ( n <digit> h g n h log n ) time and o ( n2 ) o ( n <digit> ) space , where g is the length of the gap . as one of the main components of the above solution , we propose a new technique called approximate doubling which seems to be a useful tool for a wide range of algorithms on slps . indeed , we show that the technique can be used to compute the periods and covers of the string in o ( n2h ) o ( n <digit> h ) time and o ( nh ( n log2 n ) ) o ( n h ( n log <digit> n ) ) time , respectively .
achieving reusability and composability with a simulation conceptual model . <eos> reusability and composability ( r c ) are two important quality characteristics that have been very difficult to achieve in the modelling and simulation ( m s ) discipline . reuse provides many technical and economical benefits . composability has been increasingly crucial for m s of a system of systems , in which disparate systems are composed with each other . the purpose of this paper is to describe how r c can be achieved by using a simulation conceptual model ( cm ) in a community of interest ( coi ) . we address r c in a multifaceted manner covering many m s areas ( types ) . m s is commonly employed where r c are very much needed by many cois . we present how a cm developed for a coi can assist in r c for the design of any type of large scale complex m s application in that coi . a cm becomes an asset for a coi and offers significant economic benefits through its broader applicability and more effective utilization .
wavelength decomposition approach for computing blocking probabilities in multicast wdm optical networks . <eos> we present an approximate analytical method to evaluate the blocking probabilities in multicast wavelength division multiplexing ( wdm ) networks without wavelength converters . our approach is based on the wavelength decomposition approach in which the wdm network is divided into layers ( colors ) and the moment matching method is used to characterize the overflow traffic from one layer to another . analyzing blocking probabilities for unicast and multicast calls in each layer of the network is derived from an exact approach . we assume static routing with either first fit or random wavelength assignment algorithm . results are presented which indicate the accuracy of our method .
a new local meshless method for steady state heat conduction in heterogeneous materials . <eos> in this paper a truly meshless method based on the integral form of energy equation is presented to study the steady state heat conduction in the anisotropic and heterogeneous materials . the presented meshless method is based on the satisfaction of the integral form of energy balance equation for each sub particle ( sub domain ) inside the material . moving least square ( mls ) approximation is used for approximation of the field variable over the randomly located nodes inside the domain . in the absence of heat generation , the domain integration is eliminated from the formulation of presented method and the computational efforts are reduced substantially with respect to the conventional mlpg method . a direct method is presented for treatment of material discontinuity at the heterogeneous material in the presented meshless method . as a practical problem the heat conduction in fibrous composite material is studied and the steady state heat conduction in unidirectional fibermatrix composites is investigated . the solution domain includes a small area of the composite system called representative volume element ( rve ) . comparison of numerical results shows that the presented meshless method is simple , effective , accurate and less costly method for micromechanical analysis of heat conduction in heterogeneous materials .
a territory defining multiobjective evolutionary algorithms and preference incorporation . <eos> we have developed a steady state elitist evolutionary algorithm to approximate the pareto optimal frontiers of multiobjective decision making problems . the algorithms define a territory around each individual to prevent crowding in any region . this maintains diversity while facilitating the fast execution of the algorithm . we conducted extensive experiments on a variety of test problems and demonstrated that our algorithm performs well against the leading multiobjective evolutionary algorithms . we also developed a mechanism to incorporate preference information in order to focus on the regions that are appealing to the decision maker . our experiments show that the algorithm approximates the pareto optimal solutions in the desired region very well when we incorporate the preference information .
a holistic frame of reference for modelling social systems . <eos> purpose to outline a philosophical system of inquiry that may be used as a frame of reference for modelling social systems . design methodology approach the paper draws on insights from cognitive science , autopoiesis , management cybernetics and non linear dynamics . findings the outcome of this paper is an outline of a frame of reference to be used as a starting point ( or a frame of orientation ) for any problem solving modelling intent or act . the framework highlights the importance of epistemological reflection and the need to avoid any separation of the process of knowing from that of modelling . it also emphasises the importance of inquiry into the assumptions that underpin the methods , tools and techniques that we employ , and into the tacit beliefs of the human actors who use them . research limitations implications the presented frame of reference should be regarded as an evolving system of inquiry , one that seeks to incorporate contemporary human insight . practical implications exactly , how the frame of reference presented in this paper should be exploited within an organisational or educational context , is a question to which there is no single correct answer . what is primarily important , however , is that it should be used to raise the profile of , and disseminate the benefits that accrue from , inquiry which goes beyond the simple application of tools and methods . originality value this paper proposes a new framework of reference for modelling social systems that draws on insights from cognitive science , autopoiesis , management cybernetics and non linear dynamics .
a source synchronous double data rate parallel optical transceiver ic . <eos> source synchronous double data rate ( ddr ) signaling is widely used in electrical interconnects to eliminate clock recovery and to double communication bandwidth . this paper describes the design of a parallel optical transceiver integrated circuit ( ic ) that uses source synchronous ddr optical signaling . on the transmit side , two <digit> b electrical inputs are multiplexed , encoded , and sent over two high speed optical links . on the receive side , the procedure is reversed to produce two <digit> b electrical outputs . the proposed ic integrates analog vertical cavity surface emitting lasers ( vcsels ) , drivers and optical receivers with digital ddr multiplexing , serialization , and deserialization circuits . it was fabricated in a 0.5 mu m silicon on sapphire ( sos ) complementary metal oxide semiconductor ( cmos ) process . linear arrays of quad vcsels and photodetectors were attached to the proposed transceiver ic using hip chip bonding . a free space optical link system was constructed to demonstrate correct ic functionality . the test results show successful transceiver operation at a data rate of <digit> mb s with a <digit> mhz ddr clock , achieving a gigabit of aggregate bandwidth . while the proposed ddr scheme is well suited for low skew fiber ribbon , free space , and waveguide optical links , it can also be extended to links with higher skew with the addition of skew compensation circuitry . to the authors ' knowledge , this is the first demonstration of parallel optical transceivers that use source synchronous ddr signaling .
fpcode an efficient approach for multi modal biometrics . <eos> although face recognition technology has progressed substantially , its performance is still not satisfactory due to the challenges of great variations in illumination , expression and occlusion . this paper aims to improve the accuracy of personal identification , when only few samples are registered as templates , by integrating multiple modal biometrics , i.e. face and palmprint . we developed in this paper a feature code , namely fpcode , to represent the features of both face and palmprint . though feature code has been used for palmprint recognition in literature , it is first applied in this paper for face recognition and multi modal biometrics . as the same feature is used , fusion is much easier . experimental results show that both feature level and decision level fusion strategies achieve much better performance than single modal biometrics . the proposed approach uses fixed length <digit> <digit> bits coding scheme that is very efficient in matching , and at the same time achieves higher accuracy than other fusion methods available in literature .
kinetics and energetics during uphill and downhill carrying of different weights . <eos> during physically heavy work tasks the musculoskeletal tissues are exposed to both mechanical and metabolic loading . the aim of the present study was to test a biomechanical model for prediction of whole body energy turnover from kinematic and anthropometric data during load carrying . total loads of <digit> , <digit> and <digit> kg were carried symmetrically or asymmetrically in the hands , while walking on a treadmill ( 4.5 kmh <digit> ) horizontally , uphill , or downhill the slopes being <digit> % . mean values for the directly measured oxygen uptake ranged for all trials from 0.5 to 2.1 l o2min <digit> , and analysis of variance showed significant differences regarding slope , load carried , and symmetry . the calculated values of oxygen uptake based on the biomechanical model correlated significantly with the directly measured values , fitting to the line y 0.990 x 0.144 , where y is the estimated and x is the measured oxygen uptake in lmin <digit> . the close relationship between energy turnover rate measured directly and estimated based on a biomechanical model justifies the assessment of the metabolic load from kinematic data .
granular prototyping in fuzzy clustering . <eos> we introduce a logic driven clustering in which prototypes are formed and evaluated in a sequential manner . the way of revealing a structure in data is realized by maximizing a certain performance index ( objective function ) that takes into consideration an overall level of matching ( to be maximized ) and a similarity level between the prototypes ( the component to be minimized ) . the prototypes identified in the process come with the optimal weight vector that serves to indicate the significance of the individual features ( coordinates ) in the data grouping represented by the prototype . since the topologies of these groupings are in general quite diverse the optimal weight vectors are reflecting the anisotropy of the feature space , i.e. , they show some local ranking of features in the data space . having found the prototypes we consider an inverse similarity problem and show how the relevance of the prototypes translates into their granularity .
empirical evaluation of latency sensitive application performance in the cloud . <eos> cloud computing platforms enable users to rent computing and storage resources on demand to run their networked applications and employ virtualization to multiplex virtual servers belonging to different customers on a shared set of servers . in this paper , we empirically evaluate the efficacy of cloud platforms for running latency sensitive multimedia applications . since multiple virtual machines running disparate applications from independent users may share a physical server , our study focuses on whether dynamically varying background load from such applications can interfere with the performance seen by latency sensitive tasks . we first conduct a series of experiments on amazon 's ec2 system to quantify the cpu , disk , and network jitter and throughput fluctuations seen over a period of several days . we then turn to a laboratory based cloud and systematically introduce different levels of background load and study the ability to isolate applications under different settings of the underlying resource control mechanisms . we use a combination of micro benchmarks and two real world applications the doom <digit> game server and apple 's darwin streaming server for our experimental evaluation . our results reveal that the jitter and the throughput seen by a latency sensitive application can indeed degrade due to background load from other virtual machines . the degree of interference varies from resource to resource and is the most pronounced for disk bound latency sensitive tasks , which can degrade by nearly <digit> % under sustained background load . we also find that careful configuration of the resource control mechanisms within the virtualization layer can mitigate , but not eliminate , this interference .
the role of chineseamerican scientists in chinaus scientific collaboration a study in nanotechnology . <eos> in this paper , we use bibliometric methods and social network analysis to analyze the pattern of chinaus scientific collaboration on individual level in nanotechnology . results show that chineseamerican scientists have been playing an important role in chinaus scientific collaboration . we find that chinaus collaboration in nanotechnology mainly occurs between chinese and chineseamerican scientists . in the co authorship network , chineseamerican scientists tend to have higher betweenness centrality . moreover , the series of polices implemented by the chinese government to recruit oversea experts seems to contribute a lot to chinaus scientific collaboration .
localization of spherical fruits for robotic harvesting . <eos> the orange picking robot ( opr ) is a project for developing a robot that is able to harvest oranges automatically . one of the key tasks in this robotic application is to identify the fruit and to measure its location in three dimensions . this should be performed using image processing techniques which must be sufficiently robust to cope with variations in lighting conditions and a changing environment . this paper describes the image processing system developed so far to guide automatic harvesting of oranges , which here has been integrated in the first complete full scale prototype opr .
game based learning for computer science education . <eos> today , learners increasingly demand for innovative and motivating learning scenarios that strongly respond to their habits of using media . one of the many possible solutions to this demand is the use of computer games to support the acquisition of knowledge . this paper reports on chances and challenges of applying a game based learning scenario for the acquisition of it knowledge as realized by the german bmbf project spitkom . after briefly describing the learning potential of multiplayer browser games as well as the educational objectives and target group of the spitkom project , we will present the main results of a study that was carried out in the first phase of the project to guide the game design . in the course of the study , data were collected regarding ( a ) the computer game preferences of the target group and ( b ) the target group 's competencies in playing computer games . we will then introduce recommendations that were deduced from the study 's findings and that outline the concept and the prototype of the game .
efficient evaluation functions for evolving coordination . <eos> this paper presents fitness evaluation functions that efficiently evolve coordination in large multi component systems . in particular , we focus on evolving distributed control policies that are applicable to dynamic and stochastic environments . while it is appealing to evolve such policies directly for an entire system , the search space is prohibitively large in most cases to allow such an approach to provide satisfactory results . instead , we present an approach based on evolving system components individually where each component aims to maximize its own fitness function . though this approach sidesteps the exploding state space concern , it introduces two new issues ( <digit> ) how to create component evaluation functions that are aligned with the global evaluation function and ( <digit> ) how to create component evaluation functions that are sensitive to the fitness changes of that component , while remaining relatively insensitive to the fitness changes of other components in the system . if the first issue is not addressed , the resulting system becomes uncoordinated if the second issue is not addressed , the evolutionary process becomes either slow to converge or worse , incapable of converging to good solutions . this paper shows how to construct evaluation functions that promote coordination by satisfying these two properties . we apply these evaluation functions to the distributed control problem of coordinating multiple rovers to maximize aggregate information collected . we focus on environments that are highly dynamic ( changing points of interest ) , noisy ( sensor and actuator faults ) , and communication limited ( both for observation of other rovers and points of interest ) forcing the rovers to evolve generalized solutions . on this difficult coordination problem , the control policy evolved using aligned and component sensitive evaluation functions outperforms global evaluation functions by up to <digit> % . more notably , the performance improvements increase when the problems become more difficult ( larger , noisier , less communication ) . in addition we provide an analysis of the results by quantifying the two characteristics ( alignment and sensitivity discussed above ) leading to a systematic study of the presented fitness functions .
contention free communication scheduling for array redistribution . <eos> array redistribution is required often in programs on distributed memory parallel computers . it is essential to use efficient algorithms for redistribution , otherwise the performance of the programs may degrade considerably . the redistribution overheads consist of two parts index computation and interprocessor communication . if there is no communication scheduling in a redistribution algorithm , the communication contention may occur , which increases the communication waiting time . in order to solve this problem , in this paper , we propose a technique to schedule the communication so that it becomes contention free . our approach initially generates a communication table to represent the communication relations among sending nodes and receiving nodes . according to the communication table , we then generate another table named communication scheduling table . each column of communication scheduling table is a permutation of receiving node numbers in each communication step . thus the communications in our redistribution algorithm are contention free . our approach can deal with multi dimensional shape changing redistribution .
quadratic weighted median filters for edge enhancement of noisy images . <eos> quadratic volterra filters are effective in image sharpening applications . the linear combination of polynomial terms , however , yields poor performance in noisy environments . weighted median ( wm ) filters , in contrast , are well known for their outlier suppression and detail preservation properties . the wm sample selection methodology is naturally extended to the quadratic sample case , yielding a filter structure referred to as quadratic weighted median ( qwm ) that exploits the higher order statistics of the observed samples while simultaneously being robust to outliers arising in the higher order statistics of environment noise . through statistical analysis of higher order samples , it is shown that , although the parent gaussian distribution is light tailed , the higher order terms exhibit heavy tailed distributions . the optimal combination of terms contributing to a quadratic system , i.e. , cross and square , is approached from a maximum likelihood perspective which yields the wm processing of these terms . the proposed qwm filter structure is analyzed through determination of the output variance and breakdown probability . the studies show that the qwm exhibits lower variance and breakdown probability indicating the robustness of the proposed structure . the performance of the qwm filter is tested on constant regions , edges and real images , and compared to its weighted sum dual , the quadratic volterra filter . the simulation results show that the proposed method simultaneously suppresses the noise and enhances image details . compared with the quadratic volterra sharpener , the qwm filter exhibits superior qualitative and quantitative performance in noisy image sharpening .
elra european language resources association background , recent developments and future perspectives . <eos> the european language resources association ( elra ) was founded in <digit> with the mission of providing language resources ( lr ) to european research institutions and companies . in this paper we describe the background , the mission and the major activities since then .
multiple concurrence of multi partite quantum system . <eos> we propose a new way of description of the global entanglement property of a multi partite pure state quantum system . based on the idea of bipartite concurrence , by dividing the multi partite quantum system into two subsystems , a combination of all the bipartite concurrences of a multipartite quantum system is used to describe the entanglement property of the multi partite system . we derive the analytical results for ghz state , w state with arbitrary number of qubits , and cluster state with the number of particles no greater than <digit> .
tolerant information retrieval with backpropagation networks . <eos> neural networks can learn fi om human decisions and preferences . especially in , human computer interaction , adaptation to the behaviour and expectations of the user is necessary . ih information retrieval , an important area within human computer interaction , expectations are difficult to meet . the inherently vague nature of information retrieval has bed to the application of vague processing techniques . neural networks seem to have great potential to model the cognitive processes involved more appropriately . current models based on neural networks and their implications for human computer interaction ar e analysed . cosimir ( cognitive similarity learning in information retrieval ) , an innovative model integrating human knowledge into the core of the retrieval process , is presented . it applies backpropagation to information retrieval , integrating human centred and soft and tolerant computing into the core of the retrieval process . a further backpropagation model , the transformation network for heterogeneous data sources , is discussed . empirical evaluations have provided promising results .
approximation of mean time between failure when a system has periodic maintenance . <eos> this paper describes a simple technique for estimating the mean time between failure ( mtbf ) of a system that has periodic maintenance at regular intervals . this type of maintenance is typically found in high reliability , mission oriented applications where it is convenient to perform maintenance after the completion of the mission . this approximation technique can greatly simplify the mtbf analysis for large systems . the motivation for this analysis was to understand the nature of the error in the approximation and to develop a means for quantifying that error . this paper provides the derivation of the equations that bound the error that can result when using this approximation method . it shows that , for most applications , the mtbf calculations can be greatly simplified with only a very small sacrifice in accuracy .
supplying web 2.0 an empirical investigation of the drivers of consumer transmutation of culture oriented digital information goods . <eos> this paper describes an empirical study of behaviors associated with consumers ' creative modification of digital information goods found in web 2.0 and elsewhere . they are products of culture such as digital images , music , video , news and computer games . we will refer to them as digital culture products . how do consumers who transmute such products differ from those who do not , and from each other this study develops and tests a theory of consumer behavior in transmuting digital culture products , separating consumers into different groups based on how and why they transmute . with our theory , we posit these groups as having differences of motivation , as measured by product involvement and innovativeness , and of ability as measured by computer skills . a survey instrument to collect data from internet capable computer users on the relevant constructs , and on their transmutation activities , is developed and distributed using a web based survey hosting service . the data are used to test hypotheses that consumers ' enduring involvement and innovativeness are positively related to transmutation behaviors , and that computer self efficacy moderates those relationships . the empirical results support the hypotheses that enduring involvement and innovativeness do motivate transmutation behavior . the data analysis also supports the existence of a moderating relationship of computer self efficacy with respect to enduring involvement , but not of computer self efficacy with respect to innovativeness . the findings further indicate that transmutation activities should be expected to impact web 2.0 oriented companies , both incumbents and start ups , as they make decisions about how to incorporate consumers into their business models not only as recipients of content , but also as its producers . ( c ) <digit> elsevier b. v. all rights reserved .
polynomial cost for solving ivp for high index dae . <eos> we show that the cost of solving initial value problems for high index differential algebraic equations is polynomial in the number of digits of accuracy requested . the algorithm analyzed is built on a taylor series method developed by pryce for solving a general class of differential algebraic equations . the problem may be fully implicit , of arbitrarily high fixed index and contain derivatives of any order . we give estimates of the residual which are needed to design practical error control algorithms for differential algebraic equations . we show that adaptive meshes are always more efficient than non adaptive meshes . finally , we construct sufficiently smooth interpolants of the discrete solution .
a novel wavelength hopping passive optical network ( wh pon ) for provision of enhanced physical security . <eos> a novel secure wavelength hopping passive optical network ( wh pon ) is presented in which physical layer security is introduced to the access network . the wh pon design uses a pair of matched tunable lasers in the optical line terminal to create a time division multiplexed signal in which each data frame is transmitted at a unique wavelength . the transmission results for a <digit> channel wh pon operating at a data rate of 2.5 gb s are presented in this paper . the inherent security of the wh pon design is verified through an attempted cross channel eavesdropping attempt at an optical network unit . the results presented verify that the wh pon provides secure broadband service in the access network .
on the information flow required for tracking control in networks of mobile sensing agents . <eos> we design controllers that permit mobile agents with distributed or networked sensing capabilities to track ( follow ) desired trajectories , identify what trajectory information must be distributed to each agent for tracking , and develop methods to minimize the communication needed for the trajectory information distribution .
analysis of timing based mutual exclusion with random times . <eos> various timing based mutual exclusion algorithms have been proposed that guarantee mutual exclusion if certain timing assumptions hold . in this paper , we examine how these algorithms behave when the time for the basic operations is governed by probability distributions . in particular , we are concerned with how often such algorithms succeed in allowing a processor to obtain a critical region and how this success rate depends on the random variables involved . we explore this question in the case where operation times are governed by exponential and gamma distributions , using both theoretical analysis and simulations .
modeling virtual worlds in databases . <eos> a method of modeling virtual worlds in databases is presented . the virtual world model is conceptually divided into several distinct elements , which are separately represented in a database . the model pen nits to dynamically generate virtual scenes . ( c ) <digit> published by elsevier b.v.
an efficient scheduling algorithm for scalable video streaming over p2p networks . <eos> during recent years , the internet has witnessed rapid advancement in peer to peer ( p2p ) media streaming . in these applications , an important issue has been the block scheduling problem , which deals with how each node requests the media data blocks from its neighbors . in most streaming systems , peers are likely to have heterogeneous upload download bandwidths , leading to the fact that different peers probably perceive different streaming quality . layered ( or scalable ) streaming in p2p networks has recently been proposed to address the heterogeneity of the network environment . in this paper , we propose a novel block scheduling scheme that is aimed to address the p2p layered video streaming . we define a soft priority function for each block to be requested by a node in accordance with the blocks significance for video playback . the priority function is unique in that it strikes good balance between different factors , which makes the priority of a block well represent the relative importance of the block over a wide variation of block size between different layers . the block scheduling problem is then transformed to an optimization problem that maximizes the priority sum of the delivered video blocks . we develop both centralized and distributed scheduling algorithms for the problem . simulation of two popular scalability types has been conducted to evaluate the performance of the algorithms . the simulation results show that the proposed algorithm is effective in terms of bandwidth utilization and video quality .
a threshold for a polynomial solution of 2sat . <eos> the sat problem is a classical p complete problem even for monotone , horn and two conjunctive formulas ( the last known as 2sat ) . we present a novel branch and bound algorithm to solve the 2sat problem exactly . our procedure establishes a new threshold where 2sat can be computed in polynomial time . we show that for any <digit> cf formula f with n variables where 2sat ( f ) < p ( n ) , for some polynomial p , 2sat ( f ) is computed in polynomial time . this is a new way to measure the degree of difficulty for solving 2sat and , according to such measure our algorithm allows to determine a boundary between ' hard ' and ' easy ' instances of the 2sat problem .
bagging and boosting statistical machine translation systems . <eos> in this article we address the issue of generating diversified translation systems from a single statistical machine translation ( smt ) engine for system combination . unlike traditional approaches , we do not resort to multiple structurally different smt systems , but instead directly learn a strong smt system from a single translation engine in a principled way . our approach is based on bagging and boosting which are two instances of the general framework of ensemble learning . the basic idea is that we first generate an ensemble of weak translation systems using a base learning algorithm , and then learn a strong translation system from the ensemble . one of the advantages of our approach is that it can work with any of current smt systems and make them stronger almost for free . beyond this , most system combination methods are directly applicable to the proposed framework for generating the final translation system from the ensemble of weak systems . we evaluate our approach on chinese english translation in three state of the art smt systems , including a phrase based system , a hierarchical phrase based system and a syntax based system . experimental results on the nist mt evaluation corpora show that our approach leads to significant improvements in translation accuracy over the baselines . more interestingly , it is observed that our approach is able to improve the existing system combination systems . the biggest improvements are obtained by generating weak systems using bagging boosting , and learning the strong system using a state of the art system combination method . ( c ) <digit> elsevier b.v. all rights reserved .
functional dimensioning and tolerancing software for concurrent engineering applications . <eos> this paper describes the development of a prototype software package for solving functional dimensioning and tolerancing ( fd t ) problems in a concurrent engineering environment . it provides a systematic way of converting functional requirements of a product into dimensional specifications by means of the following steps firstly , the relationships necessary for solving fd t problems are represented in a matrix form , known as functional requirements dimensions ( fr d ) matrix . secondly , the values of dimensions and tolerances are then determined by satisfying all these relationships represented in a fr d matrix by applying a comprehensive strategy which includes tolerance allocation strategies for different types of fd t problems and for determining an optimum solution order for coupled functional equations . the prototype software is evaluated by its potential users , and the results indicate that it can be an effective computer based tool for solving fd t problems in a ce environment . ( c ) <digit> elsevier b.v. all rights reserved .
parametric model checking of stopwatch petri nets . <eos> at the border between control and verification , parametric verification can be used to synthesize constraints on the parameters to ensure that a system verifies given specifications . in this paper we propose a new framework for the parametric verification of time petri nets with stopwatches . we first introduce a parametric extension of time petri nets with inhibitor arcs ( itpns ) with temporal parameters and we define a symbolic representation of the parametric state space based on the classical state class graph method . then , we propose semi algorithms for the parametric model checking of a subset of parametric tctl formulae on itpns . these results have been implemented in the tool romeo and we illustrate them in a case study based on a scheduling problem .
modelling the interaction of catecholamines with the alpha ( 1a ) adrenoceptor towards a ligand induced receptor structure . <eos> adrenoceptors are members of the important g protein coupled receptor family for which the detailed mechanism of activation remains unclear . in this study , we have combined docking and molecular dynamics simulations to model the ligand induced effect on an homology derived human alpha ( 1a ) adrenoceptor . analysis of agonist alpha ( 1a ) adrenoceptor complex interactions focused on the role of the charged amine group , the aromatic ring , the n methyl group of adrenaline , the beta hydroxyl group and the catechol meta and para hydroxyl groups of the catecholamines . the most critical interactions for the binding of the agonists are consistent with many earlier reports and our study suggests new residues possibly involved in the agonist binding site , namely thr <digit> and cys <digit> . we further observe a number of structural changes that occur upon agonist binding including a movement of tm v away from tm iii and a change in the interactions of asp <digit> of the conserved dry motif . this may cause arg <digit> to move out of the tm helical bundle and change the orientation of residues in ic ii and ic iii , allowing for increased affinity of coupling to the g protein .
probabilistic string similarity joins . <eos> edit distance based string similarity join is a fundamental operator in string databases . increasingly , many applications in data cleaning , data integration , and scientific computing have to deal with fuzzy information in string attributes . despite the intensive efforts devoted in processing ( deterministic ) string joins and managing probabilistic data respectively , modeling and processing probabilistic strings is still a largely unexplored territory . this work studies the string join problem in probabilistic string databases , using the expected edit distance ( eed ) as the similarity measure . we first discuss two probabilistic string models to capture the fuzziness in string values in real world applications . the string level model is complete , but may be expensive to represent and process . the character level model has a much more succinct representation when uncertainty in strings only exists at certain positions . since computing the eed between two probabilistic strings is prohibitively expensive , we have designed efficient and effective pruning techniques that can be easily implemented in existing relational database engines for both models . extensive experiments on real data have demonstrated order of magnitude improvements of our approaches over the baseline .
a high performance simulator of the immune response . <eos> the application of concepts and methods of statistical mechanics to biological problems is one of the most promising frontiers of computational physics . for instance cellular automata ( ca ) , i.e. fully discrete dynamical systems evolving according to boolean laws , appear to be extremely well suited to the simulation of the immune system dynamics . a prominent example of immunological ca is represented by the celadaseiden automaton that has proven capable of providing several new insights into the dynamics of the immune system response . in the present paper we describe a parallel version of the celadaseiden automaton . details on the parallel implementation as well as performance data on the ibm sp2 parallel platform are presented and commented on .
speaker adaptation of language and prosodic models for automatic dialog act segmentation of speech . <eos> speaker dependent modeling has a long history in speech recognition , but has received less attention in speech understanding . this study explores speaker specific modeling for the task of automatic segmentation of speech into dialog acts ( das ) , using a linear combination of speaker dependent and speaker independent language and prosodic models . data come from <digit> frequent speakers in the icsi meeting corpus adaptation data per speaker ranges from 5k to 115k words . we compare performance for both reference transcripts and automatic speech recognition output . we find that ( <digit> ) speaker adaptation in this domain results both in a significant overall improvement and in improvements for many individual speakers , ( <digit> ) the magnitude of improvement for individual speakers does not depend on the amount of adaptation data , and ( <digit> ) language and prosodic models differ both in degree of improvement , and in relative benefit for specific da classes . these results suggest important future directions for speaker specific modeling in spoken language understanding tasks .
an efficient method for electromagnetic scattering analysis . <eos> we present a novel method to solve the magnetic field integral equation ( mfie ) using the method of moments ( mom ) efficiently . this method employs a linear combination of the divergence conforming raowiltonglisson ( rwg ) function and the curl conforming nrwg function to test the mfie in mom . the discretization process and the relationship of this new testing function with the previously employed rwg and nrwg testing functions are presented . numerical results of radar cross section ( rcs ) data for objects with sharp edges and corners show that accuracy of the mfie can be improved significantly through the use of the new testing functions . at the same time , only the commonly used rwg basis functions are needed for this method .
empirical mode decomposition synthesis of fractional processes in 1d and 2d space . <eos> we report here on image texture analysis and on numerical simulation of fractional brownian textures based on the newly emerged empirical mode decomposition ( emd ) . emd introduced by n.e. huang et al. is a promising tool to non stationary signal representation as a sum of zero mean am fm components called intrinsic mode functions ( imf ) . recent works published by p. flandrin et al. relate that , in the case of fractional gaussian noise ( fgn ) , emd acts essentially as a dyadic filter bank that can be compared to wavelet decompositions . moreover , in the context of fgn identification , p. handrin et al. show that variance progression across imfs is related to hurst exponent h through a scaling law . starting with these recent results , we propose a new algorithm to generate fgn , and fractional brownian motion ( fbm ) of hurst exponent h from imfs obtained from emd of a white noise , i.e. ordinary gaussian noise ( fgn with h <digit> <digit> ) . ( c ) <digit> elsevier b.v. all rights reserved .
flow topology in a steady three dimensional lid driven cavity . <eos> we present in this paper a thorough investigation of three dimensional flow in a cubical cavity , subject to a constant velocity lid on its roof . in this steady state analysis , we adopt the mixed formulation on tri quadratic elements to preserve mass conservation . to resolve difficulties in the asymmetric and indefinite large size matrix equations , we apply the bicgstab solution solver . to achieve stability , weighting functions are designed in favor of variables on the upstream side . to achieve accuracy , the weighting functions are properly chosen so that false diffusion errors can be largely suppressed by the equipped streamline operator . our aim is to gain some physical insight into the vortical flow using a theoretically rigorous topological theory . to broaden our understanding of the vortex dynamics in the cavity , we also study in detail the longitudinal spiralling motion in the flow interior . ( c ) <digit> elsevier science ltd. all rights reserved .
image object classification using saccadic search , spatio temporal pattern encoding and self organisation . <eos> a method for extracting features from photographic images is investigated . the input image is through a saccadic search algorithm divided into a set of sub images , segmented and coded by a spatio temporal encoding engine . the input image is thus represented by a set of characteristic pattern signatures , well suited for classification by an unsupervised neural network . a strategy using multiple self organising feature maps ( som ) in a hierarchical manner is used . with this approach , using a certain degree of user selection , a database of sub images is grouped according to similarities in signature space .
theoretical properties of lfsrs for built in self test . <eos> linear feedback shift registers have been studied for a long time as interesting solutions for error detection and correction techniques in transmissions . in the test domain , and principally in built in self test applications , they are often used as generators of pseudo random test sequences . conversely , their potential to generate prescribed deterministic test sequences is dealt within more recent works , and nowadays , allows the investigation of efficient test with a pseudo deterministic bist technique . pseudo deterministic test sequences are composed of both deterministic and pseudo random test patterns and offer high fault coverage with a tradeoff between test length and hardware cost . in this paper , synthesis techniques for lfsrs that embed such kind of sequences are described .
two fixed parameter algorithms for vertex covering by paths on trees . <eos> vertex covering by paths on trees with applications in machine translation is the task to cover all vertices of a tree t ( v , e ) by choosing a minimum weight subset of given paths in the tree . the problem is np hard and has recently been solved by an exact algorithm running in o ( <digit> ( c ) center dot vertical bar v vertical bar ( <digit> ) ) time , where c denotes the maximum number of paths covering a tree vertex . we improve this running time to o ( <digit> ( c ) center dot c center dot vertical bar v vertical bar ) . on the route to this , we introduce the problem tree like weighted hitting set which might be of independent interest . in addition , for the unweighted case of vertex covering by paths on trees , we present an exact algorithm using a search tree of size o ( <digit> ( k ) center dot k ) , where k denotes the number of chosen covering paths . finally , we briefly discuss the existence of a size o ( k ( <digit> ) ) problem kernel . ( c ) <digit> elsevier b.v. all rights reserved .
graphical dynamic linear models specification , use and graphical transformations . <eos> in this work , we propose a dynamic graphical model as a tool for bayesian inference and forecasting in dynamic systems described by a series which is dependent on a state vector evolving according to a markovian law . we build sequential algorithms for the probabilities propagation . this sequentiality turns out to be represented by the dynamic graphical structure alter carrying out several goal oriented sequential graphical transformations . ( c ) <digit> elsevier science inc. all rights reserved . msg 60j99 68t30 62h99 .
alignment with non overlapping inversions and translocations on two strings . <eos> an inversion and a translocation are important in bio sequence analysis and motivate researchers to consider the sequence alignment problem using these operations . based on inversion and translocation , we introduce a new alignment problem with non overlapping inversions and translocationsgiven two strings x and y , find an alignment with non overlapping inversions and translocations for x and y. this problem has interesting application for finding a common sequence from two mutated sequences . we , in particular , consider the alignment problem when non overlapping inversions and translocations are allowed for both x and y. we design an efficient algorithm that determines the existence of such an alignment and retrieves an alignment , if exists .
a twist to partial least squares regression . <eos> a modification of the pls1 algorithm is presented . stepwise optimization over a set of candidate loading weights obtained by taking powers of the y x correlations and x standard deviations generalizes the classical pls1 based on y x covariances and hence adds flexibility to the modelling . when good linear predictions can be obtained , the suggested approach often finds models with fewer and more interpretable components . good performance is demonstrated when compared with the classical pls1 on calibration benchmark data sets . an important part of the comparisons is managed by a novel model selection strategy . the selection is based on choosing the simplest model among those with a cross validation error smaller than the pre specified significance limit of a chi ( <digit> ) statistic . copyright ( c ) <digit> john wiley sons , ltd .
a robust and efficient finite volume scheme for the discretization of diffusive flux on extremely skewed meshes in complex geometries . <eos> in this paper an improved finite volume scheme to discretize diffusive flux on a non orthogonal mesh is proposed . this approach , based on an iterative technique initially suggested by khosla p.k. khosla , s.g. rubin , a diagonally dominant second order accurate implicit scheme , computers and fluids <digit> ( <digit> ) <digit> and known as deferred correction , has been intensively utilized by muzaferija s. muzaferija , adaptative finite volume method for flow prediction using unstructured meshes and multigrid approach , ph.d. thesis , imperial college , <digit> and later fergizer and peric j.h. fergizer , m. peric , computational methods for fluid dynamics , springer , <digit> to deal with the non orthogonality of the control volumes . using a more suitable decomposition of the normal gradient , our scheme gives accurate solutions in geometries where the basic idea of muzaferija fails . first the performances of both schemes are compared for a poisson problem solved in quadrangular domains where control volumes are increasingly skewed in order to test their robustness and efficiency . it is shown that convergence properties and the accuracy order of the solution are not degraded even on extremely skewed mesh . next , the very stable behavior of the method is successfully demonstrated on a randomly distorted grid as well as on an anisotropically distorted one . finally we compare the solution obtained for quadrilateral control volumes to the ones obtained with a finite element code and with an unstructured version of our finite volume code for triangular control volumes . no differences can be observed between the different solutions , which demonstrates the effectiveness of our approach .
evaluation of trend localization with multi variate visualizations . <eos> multi valued data sets are increasingly common , with the number of dimensions growing . a number of multi variate visualization techniques have been presented to display such data . however , evaluating the utility of such techniques for general data sets remains difficult . thus most techniques are studied on only one data set . another criticism that could be levied against previous evaluations of multi variate visualizations is that the task does n't require the presence of multiple variables . at the same time , the taxonomy of tasks that users may perform visually is extensive . we designed a task , trend localization , that required comparison of multiple data values in a multi variate visualization . we then conducted a user study with this task , evaluating five multi variate visualization techniques from the literature ( brush strokes , data driven spots , oriented slivers , color blending , dimensional stacking ) and juxtaposed grayscale maps . we report the results and discuss the implications for both the techniques and the task .
an efficient reconfigurable multiplier architecture for galois field gf ( 2m ) . <eos> this paper describes an efficient architecture of a reconfigurable bit serial polynomial basis multiplier for galois field gf ( 2m ) , where <digit> < m m. the value m , of the irreducible polynomial degree , can be changed and so , can be configured and programmed . the value of m determines the maximum size that the multiplier can support . the advantages of the proposed architecture are ( i ) the high order of flexibility , which allows an easy configuration for different field sizes , and ( ii ) the low hardware complexity , which results in small area . by using the gated clock technique , significant reduction of the total multiplier power consumption is achieved .
experiences in building a grid based platform to serve earth observation training activities . <eos> earth observation data processing and storing can be done nowadays only using distributed systems . experiments dealing with a large amount of data are possible within the timeframe of a lesson and can give trainees the freedom to innovate . following these trends and ideas , we have built a proof of the concept platform , named gisheo , for earth observation educational tasks . it uses grid computing technologies to analyze and store remote sensing data , and combines them with elearning facilities . this paper provides an overview of the gisheo 's platform architecture and of its technical and innovative solutions . ( c ) <digit> elsevier b.v. all rights reserved .
field d path finding on weighted triangulated and tetrahedral meshes . <eos> classic shortest path algorithms operate on graphs , which are suitable for problems that can be represented by weighted nodes or edges . finding a shortest path through a set of weighted regions is more difficult and only approximate solutions tend to scale well . the field d algorithm efficiently calculates an approximate , interpolated shortest path through a set of weighted regions and was designed for navigating robots through terrains with varying characteristics . field d operates on unit grid or quad tree data structures , which require high resolutions to accurately model the boundaries of irregular world structures . in this paper , we extend the field d cost functions to 2d triangulations and 3d tetrahedral meshes structures which model polygonal world structures more accurately . since robots typically have limited resources available for computation and storage , we pay particular attention to computation and storage overheads when detailing our extensions . we begin by providing analytic solutions to the minimum of each cost function for 2d triangles and 3d tetrahedra . our triangle implementation provides a <digit> % improvement in performance over an existing triangle implementation . while our 3d extension to tetrahedra is the first full analytic extension of field d to 3d , previous work only provided an approximate minimization for a single cost function on a 3d cube with unit lengths . each cost function is expressed in terms of a general function whose characteristics can be exploited to reduce the calculations required to find a minimum . these characteristics can also be exploited to cache the majority of cost functions , producing a speedup of up to <digit> % in the 3d tetrahedral case . we demonstrate that , in environments composed of non grid aligned data , multi resolution quad tree field d requires an order of magnitude more faces and between <digit> and <digit> times more node expansions , to produce a path of similar cost to one produced by a triangle implementation of field d on a lower resolution triangulation . we provide examples of 3d pathing through models of complex topology , including pathing through anatomical structures extracted from a medical data set . to summarise , this paper details a robust and efficient extension of field d pathing to data sets represented by weighted triangles and tetrahedra , and also provides empirical data which demonstrates the reduction in storage and computation costs that accrue when one chooses such a representation over the more commonly used quad tree and grid based alternatives .
correlation analysis of signal flow in a model prefrontal cortical circuit representing multiple target locations . <eos> in spite of the recent cross correlation analyses of the monkey prefrontal cortical neurons performing spatial working memory tasks ( j. neurosci . <digit> ( <digit> ) <digit> cerebr . cortex <digit> ( <digit> ) <digit> ) , it is uncertain as to how much degree the correlation data reflect the circuitry of highly recurrent networks . we did a computer simulation of a model cortical circuit , whose connectivity is fully known , and analyzed the cross correlations of the spikes of pairs of neurons in the model . the result shows that cross correlation histograms ( cchs ) of pyramidalpyramidal pairs tend to mask higher order synaptic interactions , yielding cchs with central peaks or almost flat cchs . however , cchs of pyramidalinterneuron pairs show displaced positive and or negative peaks , depending on the connectivity of these neurons .
slow dynamic finite element simulation of manufacturing processes . <eos> explicit time integration and dynamic finite element formulations are increasingly being used to analyze nonlinear static problems in solid and structural mechanics . this is particularly true in the simulation of sheer metal manufacturing processes . employment of slow dynamic , quasi static techniques in static problems can introduce undesirable dynamic effects that originate from the inertia forces of the governing equations . in this paper , techniques and guidelines are presented , analyzed and demonstrated , which enable the minimization of the undesirable dynamic effects . the effect of the duration and functional form of the time histories of the loads and boundary conditions is quantified by the analysis of a linear spring mass oscillator . the resulting guidelines and techniques are successfully demonstrated in the nonlinear finite element simulation of a sheet metal deep drawing operation . the accuracy of the quasi static , slow dynamic finite element analyses is evaluated by comparison to results of laboratory experiments and purely static analyses . various measures that quantify the dynamic effects , including kinetic energy , also are discussed . ( c ) <digit> elsevier science ltd .
improving tcp performance in integrated wireless communications networks . <eos> many analytical and simulation based studies of tcp performance in wireless environments assume an error free and congestion free reverse channel that has the same capacity as the forward channel . such an assumption does not hold in many real world scenarios , particularly in the hybrid networks consisting of various wireless lan ( wlan ) and cellular technologies . in this paper , we first study , through extensive simulations , the performance characteristics of four representative tcp schemes , namely tcp new reno , sack , veno , and westwood , under the network conditions of asymmetric end to end link capacities , correlated wireless errors , and link congestion in both forward and reverse directions . we then propose a new tcp scheme , called tcp new jersey , which is capable of distinguishing wireless packet losses from congestion packet losses , and reacting accordingly . tcp new jersey consists of two key components , the timestamp based available bandwidth estimation ( tabe ) algorithm and the congestion warning ( cw ) router configuration . tabe is a tcp sender side algorithm that continuously estimates the bandwidth available to the connection and guides the sender to adjust its transmission rate when the network becomes congested . tabe is immune to the ack drops as well as ack compression . cw is a configuration of network routers such that routers alert end stations by marking all packets when there is a sign of an incipient congestion . the marking of packets by the cw configured routers helps the sender of the tcp connection to effectively differentiate packet losses caused by network congestion from those caused by wireless link errors . our simulation results show that tcp new jersey is able to accurately estimate the available bandwidth of the bottleneck link of an end to end path and the tabe estimator is immune to link asymmetry , bi directional congestion , and the relative position of the bottleneck link in the multi hop end to end path . the proactive congestion avoidance control mechanism proposed in our scheme minimizes the network congestion , reduces the network volatility , and stabilizes the queue lengths while achieving more throughput than other tcp schemes .
a hopfield neural network based task mapping method . <eos> with a prior knowledge of a program , static mapping aims to identify an optimal clustering strategy that can produce the best performance . in this paper we present a static method that uses hopfield neural network to cluster the tasks of a parallel program for a given system . this method takes into account both load balancing and communication minimization . the method has been tested on a distributed shared memory system against other three clustering methods . four programs , sor , n body , gaussian elimination and vq , are used in the test . the result shows that our method is superior to the other three . ( c ) <digit> elsevier science b.v. all rights reserved .
investigating the evolution of code smells in object oriented systems . <eos> software design problems are known and perceived under many different terms , such as code smells , flaws , non compliance to design principles , violation of heuristics , excessive metric values and anti patterns , signifying the importance of handling them in the construction and maintenance of software . once a design problem is identified , it can be removed by applying an appropriate refactoring , improving in most cases several aspects of quality such as maintainability , comprehensibility and reusability . this paper , taking advantage of recent advances and tools in the identification of non trivial code smells , explores the presence and evolution of such problems by analyzing past versions of code . several interesting questions can be investigated such as whether the number of problems increases with the passage of software generations , whether problems vanish by time or only by targeted human intervention , whether code smells occur in the course of evolution of a module or exist right from the beginning and whether refactorings targeting at smell removal are frequent . in contrast to previous studies that investigate the application of refactorings in the history of a software project , we attempt to analyze the evolution from the point of view of the problems themselves . to this end , we classify smell evolution patterns distinguishing deliberate maintenance activities from the removal of design problems as a side effect of software evolution . results are discussed for two open source systems and four code smells .
issues in parallelizing multigrid based substrate model extraction and analysis . <eos> accurate modeling of coupling effects via the substrate is an increasingly important concern in the design of mixed signal systems such as communication , biomedical and analog signal processing circuits . fast switching digital blocks inject noise into the common substrate hindering the performance of high precision sensible analog circuitry . miniaturization effects on ics complexity inevitably make the accuracy requirements for substrate coupling simulation increase . due in part to the global nature of such couplings , model extraction and analysis is a computation intensive task requiring the availability of fast and accurate substrate model extraction and analysis tools . one way to deal with this problem is to take further advantage of available computational technologies and distributed computing emerges as an interesting solution.in this paper we discuss several issues related to the parallelization of a multigrid based substrate model extraction and analysis tool . this tool is used as a proxy for generic computations on a 3d discretized volume . the results presented indicate potential avenues for successfully exploiting parallelism as well as pitfalls to avoid in such a quest .
hypoxia induced phrenic long term facilitation emergent properties . <eos> as in other neural systems , plasticity is a hallmark of the neural system controlling breathing . one spinal mechanism of respiratory plasticity is phrenic long term facilitation ( pltf ) following acute intermittent hypoxia . although cellular mechanisms giving rise to pltf occur within the phrenic motor nucleus , different signaling cascades elicit pltf under different conditions . these cascades , referred to as q and s pathways to phrenic motor facilitation ( pmf ) , interact via cross talk inhibition . whereas the q pathway dominates pltf after mild to moderate hypoxic episodes , the s pathway dominates after severe hypoxic episodes . the biological significance of multiple pathways to pmf is unknown . this review will discuss the possibility that interactions between pathways confer emergent properties to pltf , including pattern sensitivity and metaplasticity . understanding these mechanisms and their interactions may enable us to optimize intermittent hypoxia induced plasticity as a treatment for patients that suffer from ventilatory impairment or other motor deficits .
bivariate mellin convolution operators quantitative approximation theorems . <eos> in this paper we study some qualitative and quantitative versions of the voronovskaja approximation formulae for a class of bivariate mellin convolution operators of type ( t ( w ) f ) ( x , y ) integral ( r <digit> ) k ( w ) ( tx ( <digit> ) , vy ( <digit> ) ) f ( t , v ) dtdv tv . moreover we apply the general theory to some particular cases leading to various asymptotic formulae and involving various differential operators . ( c ) <digit> elsevier ltd. all rights reserved .
e government evolution in eu local governments a comparative perspective . <eos> purpose the purpose of this paper is to describe an empirical study of the advances and trends of e government in transparency , openness and hence accountability in european union ( eu ) local governments to determine the extent to which the internet promotes the convergence towards more transparent and accountable government . the paper also tests the extent to which different factors related to the implementation of information and communication technologies ( icts ) , the number of inhabitants and the type of public administration style have influenced e government developments in the cities studied . design methodology approach a comprehensive content analysis of <digit> local government web sites was conducted using a <digit> item evaluation questionnaire . the evaluations were performed in <digit> and <digit> and <digit> eu countries were covered ( five per country ) . to analyse the evolution of e government , several techniques were used tests of difference of means , multidimensional scaling and cluster analysis . the contribution of the different contextual factors to the development of government web sites was tested with ols regression analysis . findings the results show noticeable progress in the application of icts and increasing eu local government concern for bringing government closer to citizens and for giving an image of modernity and responsiveness , although few web sites show clear signs of real openness to encouraging citizen dialogue . the evolution of the e government initiatives analysed shows that , at present , they are still overlapped with the public administration style of each country as an extension of traditional front offices with potential benefits in speed and accessibility . originality value although a growing number of e government studies are appearing , previous research has not analysed the evolution of eu local governments from a comparative perspective .
soccer video processing for the detection of advertisement billboards . <eos> billboards are placed on the sides of a soccer field for advertisement during match telecast . unlike regular commercials , which are introduced during a break , on field billboards appear on the tv screen at uncertain time instances , in different sizes , and also for different durations . automated processing of soccer telecasts for detection and analysis of such billboards can provide important information on the effectiveness of this mode of advertising . we propose a method in which shot boundaries are first identified and the type of each shot is determined . frames within each shot are then segmented to locate possible regions of interests ( rois ) locations in a frame where billboards are potentially present . finally , we use a combination of local and global features for detecting individual billboards by matching with a set of given templates .
multi component image segmentation in homogeneous regions based on description length minimization application to speckle , poisson and bernoulli noise . <eos> in this article , a minimum description length ( mdl ) criterion adapted to independent multi component image segmentation into homogeneous regions is proposed . this approach , based on a deformable polygonal grid , allows us to segment noisy multi component images perturbed with spatially independent speckle , poisson or bernoulli noise . the advantages of using such a multi component approach rather than a mono component one is demonstrated on synthetic and real images . this segmentation method is also applicable to multi component images whose components do not follow the same noise statistics or have not been previously registered .
language dominance in interpersonal deception in computer mediated communication . <eos> dominance is not only a complicated social phenomenon that involves interpersonal dynamics , but also an effective strategy used in various applications such as deception detection , negotiation , and online community . the extensive literature on dominance has primarily focused on the personality traits and socio biological influence , as well as various nonverbal and paralinguistic behaviors associated with dominance . nonetheless , language dominance manifested through dynamically acquired linguistic capability and strategies has not been fully investigated . the exploration of language dominance in the context of deception is even rarer . with the increasing use of computer mediated communication ( cmc ) in all aspects of modern life , language dominance in cmc has emerged as an important issue . this study examines language dominance in the context of deception via cmc . the experimental results show that deceivers ( <digit> ) demonstrate a different trend of language dominance from truthtellers over time ( <digit> ) manipulate the level of language dominance by initiating communication with low dominance and gradually increasing the level over the course of interaction , and ( <digit> ) display higher levels of dominance in terms of some linguistic behaviors than truthtellers . they suggest that in cmc , deceivers not only adjust the level of language dominance more frequently , but also change it more remarkably than truthtellers .
benchmarking short sequence mapping tools . <eos> the development of next generation sequencing instruments has led to the generation of millions of short sequences in a single run . the process of aligning these reads to a reference genome is time consuming and demands the development of fast and accurate alignment tools . however , the current proposed tools make different compromises between the accuracy and the speed of mapping . moreover , many important aspects are overlooked while comparing the performance of a newly developed tool to the state of the art . therefore , there is a need for an objective evaluation method that covers all the aspects . in this work , we introduce a benchmarking suite to extensively analyze sequencing tools with respect to various aspects and provide an objective comparison .

causality of frontal and occipital alpha activity revealed by directed coherence . <eos> recently there has been increased attention to the causality among biomedical signals . the causality between brain structures involved in the generation of alpha activity is examined based on eeg signals acquired simultaneously in the frontal and occipital regions of the scalp . the concept of directed coherence ( dc ) is introduced as a means of resolving two signal observations into the constituent components of original signals , the interaction between signals and the influence of one signal source on the other , through autoregressive modeling . the technique was applied to eeg recorded from <digit> normal subjects with eyes closed . through an analysis of the directed coherence , it was found that in both the left and right hemispheres , alpha rhythms with relatively low frequency had a significantly higher correlation in the frontal occipital direction than in the opposite direction . in the upper alpha frequency band , a significantly higher dc was observed in the occipital frontal direction , and the right left dc in the occipital area was consistently higher . the activity of rhythms near <digit> hz was widespread . these results suggest that there is a difference in the genesis and the structure of information transmission in the lower and upper band , and for <digit> hz alpha waves .
learning finite binary sequences from half space data . <eos> the problem of inferring a finite binary sequence w is an element of ( <digit> , <digit> ) ( n ) is considered . it is supposed that at epochs t <digit> , <digit> , ... , the learner is provided with random half space data in the form of finite binary sequences u ( ( t ) is an element of ) <digit> , <digit> ( n ) which have positive inner product with w . the goal of the learner is to determine the underlying sequence w in an efficient , on line fashion from the data u ( ( t ) ) , t greater than or equal to <digit> . in this context , it is shown that the randomized , on line directed drift algorithm produces a sequence of hypotheses w ( ( t ) ) is an element of <digit> , <digit> ( n ) , t greater than or equal to <digit> which converges to w in finite time with probability <digit> . it is shown that while the algorithm has a minimal space complexity of 2n bits of scratch memory , it has exponential time complexity with an expected mistake bound of order ohm ( e ( 0.139 n ) ) . batch incarnations of the algorithm are introduced which allow for massive improvements in running time with a relatively small cost in space ( batch size ) . in particular , using a batch of o ( n log n ) examples at each update epoch reduces the expected mistake bound of the ( batch ) algorithm to o ( n ) ( in an asynchronous bit update mode ) and o ( <digit> ) ( in a synchronous bit update mode ) . the problem considered here is related to binary integer programming and to learning in a mathematical model of a neuron . ( c ) <digit> john wiley sons , inc .
projection based statistical analysis of full chip leakage power with non log normal distributions . <eos> in this paper we propose a novel projection based algorithm to estimate the full chip leakage power with consideration of both inter die and intra die process variations . unlike many traditional approaches that rely on log normal approximations , the proposed algorithm applies a novel projection method to extract a low rank quadratic model of the logarithm of the full chip leakage current and , therefore , is not limited to log normal distributions . by exploring the underlying sparse structure of the problem , an efficient algorithm is developed to extract the non log normal leakage distribution with linear computational complexity in circuit size . in addition , an incremental analysis algorithm is proposed to quickly update the leakage distribution after changes to a circuit are made . our numerical examples in a commercial 90nm cmos process demonstrate that the proposed algorithm provides 4x error reduction compared with the previously proposed log normal approximations , while achieving orders of magnitude more efficiency than a monte carlo analysis with <digit> samples .
hybrid numerical methods for convection diffusion problems in arbitrary geometries . <eos> the hybrid nodal integral finite element method ( ni fem ) and the hybrid nodal integral finite analytic method ( ni fam ) are developed to solve the steady state , two dimensional convection diffusion equation ( cde ) . the hybrid ni fam for the steady state problem is then extended to solve the more general time dependent , two dimensional , cde . these hybrid coarse mesh methods , unlike the conventional nodal integral approach , are applicable in arbitrary geometries and maintain the high efficiency of the conventional nodal integral method ( nim ) . in steady state problems , the computational domain for both hybrid methods is discretized using rectangular nodes in the interior of the domain and along vertical and horizontal boundaries , while triangular nodes are used along the boundaries that are not parallel to the x or y axes . in time dependent problems , the rectangular and triangular nodes become space time parallelepiped and wedge shaped nodes , respectively . the difference schemes for the variables on the interfaces of adjacent rectangular parallelepiped nodes are developed using the conventional nim . for the triangular nodes in the hybrid ni fem , a trial function is written in terms of the edge averaged concentration of the three edges and made to satisfy the cde in an integral sense . in the hybrid ni fam , the concentration over the triangular wedge shaped nodes is represented using a finite analytic approximation , which is based on the analytic solution of the one dimensional cde . the difference schemes for both hybrid methods are then developed for the interfaces between the rectangular parallelepiped and triangular wedge shaped nodes by imposing continuity of the flux across the interfaces . a formal derivation of these hybrid methods and numerical results for several test problems are presented and discussed . ( c ) <digit> elsevier science ltd. all rights reserved .
constrained ellipse fitting with center on a line . <eos> fitting an ellipse to given data points is a common optimization task in computer vision problems . however , the possibility of incorporating the prior constraint the ellipses center is located on a given line into the optimization algorithm has not been examined so far . this problem arises , for example , by fitting an ellipse to data points representing the path of the image positions of an adhesion inside a rotating vessel whose position of the rotational axis in the image is known . our new method makes use of a constrained algebraic cost function with the incorporated ellipse center on given line prior condition in a global convergent one dimensional optimization approach . further advantages of the algorithm are computational efficiency and numerical stability .
trellis portability across architectures with a high level framework . <eos> trellis shows programmability benefits of a common and portable set of directives . we illustrate descriptive capability of directives that can support portable codes . we enhance the openacc model with more efficient mapping and synchronization . we implement prototype source translation of trellis to openmp , openacc and cuda .
pedagogical content knowledge in programming education for secondary school . <eos> dissertation overview , addressing the concept of pedadogical content knowledge for the teaching and learning of programming for secondary education .
interference analysis for highly directional <digit> ghz mesh networks the case for rethinking medium access control . <eos> we investigate spatial interference statistics for multigigabit outdoor mesh networks operating in the unlicensed <digit> ghz millimeter ( mm ) wave band . the links in such networks are highly directional because of the small carrier wavelength ( an order of magnitude smaller than those for existing cellular and wireless local area networks ) , narrow beams are essential for overcoming higher path loss and can be implemented using compact electronically steerable antenna arrays . directionality drastically reduces interference , but it also leads to deafness , making implicit coordination using carrier sense infeasible . in this paper , we make a quantitative case for rethinking medium access control ( mac ) design in such settings . unlike existing mac protocols for omnidirectional networks , where the focus is on interference management , we contend that mac design for <digit> ghz mesh networks can essentially ignore interference and must focus instead on the challenge of scheduling half duplex transmissions with deaf neighbors . our main contribution is an analytical framework for estimating the collision probability in such networks as a function of the antenna patterns and the density of simultaneously transmitting nodes . the numerical results from our interference analysis show that highly directional links can indeed be modeled as pseudowired , in that the collision probability is small even with a significant density of transmitters . furthermore , simulation of a rudimentary directional slotted aloha protocol shows that packet losses due to failed coordination are an order of magnitude higher than those due to collisions , confirming our analytical results and highlighting the need for more sophisticated coordination mechanisms .
the influence of skeletal muscle anisotropy on electroporation in vivo study and numerical modeling . <eos> the aim of this study was to theoretically and experimentally investigate electroporation of mouse tibialis cranialis and to determine the reversible electroporation threshold values needed for parallel and perpendicular orientation of the applied electric field with respect to the muscle fibers . our study was based on local electric field calculated with three dimensional realistic numerical models , that we built , and in vivo visualization of electroporated muscle tissue . we established that electroporation of muscle cells in tissue depends on the orientation of the applied electric field the local electric field threshold values were determined ( pulse parameters <digit> s , 1hz ) to be 80v cm and 200v cm for parallel and perpendicular orientation , respectively . our results could be useful electric field parameters in the control of skeletal muscle electroporation , which can be used in treatment planning of electroporation based therapies such as gene therapy , genetic vaccination , and electrochemotherapy .
the role of commutativity in constraint propagation algorithms . <eos> constraint propagation algorithms form an important part of most of the constraint programming systems . we provide here a simple , yet very general framework that allows us to explain several constraint propagation algorithms in a systematic way . in this framework we proceed in two steps . first , we introduce a generic iteration algorithm on partial orderings and prove its correctness in an abstract setting . then we instantiate this algorithm with specific partial orderings and functions to obtain specific constraint propagation algorithms . in particular , using the notions commutativity and semi commutativity , we show that the ac <digit> , pc <digit> , dac , and dpc algorithms for achieving ( directional ) are consistency and ( directional ) path consistency are instances of a single generic algorithm . the work reported here extends and simplifies that of apt 1999a .
smartrank a smart scheduling tool for mobile cloud computing . <eos> resource scarcity is a major obstacle for many mobile applications , since devices have limited energy power and processing potential . as an example , there are applications that seamlessly augment human cognition and typically require resources that far outstrip mobile hardwares capabilities , such as language translation , speech recognition , and face recognition . a new trend has been explored to tackle this problem , the use of cloud computing . this study presents smartrank , a scheduling framework to perform load partitioning and offloading for mobile applications using cloud computing to increase performance in terms of response time . we first explore a benchmarking of face recognition application using mobile cloud and confirm its suitability to be used as case study with smartrank . we have applied the approach to a face recognition process based on two strategies cloudlet federation and resource ranking through balanced metrics ( level of cpu utilization and round trip time ) . second , using a full factorial experimental design we tuned the smartrank with the most suitable partitioning decision calibrating scheduling parameters . nevertheless , smartrank uses an equation that is extensible to include new parameters and make it applicable to other scenarios .
new global exponential stability conditions for inertial cohengrossberg neural networks with time delays . <eos> in this paper , global exponential stability of inertial cohengrossberg neural networks with time delays is investigated . by using homeomorphism theorem and inequality technique , a lmi based global exponential stability condition and inequality form global exponential stability condition are obtained for the above neural networks . in our result , the assumptions for the differentiability and monotonicity on the behaved functions in ke and miao ( <digit> ) <digit> are removed . thus our results are less conservative than those obtained in ke and miao ( <digit> ) <digit> . hence , we obtain new global exponential stability for this neural network .
extracting the fetal heart rate variability using a frequency tracking algorithm . <eos> in this work , we propose an algorithm to extract the fetal heart rate variability from an ecg measured from the mother abdomen . the algorithm consists of two methods a separation algorithm based on second order statistics that extracts the desired signal in one shot through the data , and a heart instantaneous frequency ( hif ) estimator . the hif algorithm is used to extract the mother heart rate which serves as reference to extract the fetal heart rate . we carried out simulations where the signals overlap in frequency and time , and showed that it worked efficiently .
education and training in health informatics the it eductra project . <eos> in this contribution both the eductra project of the european advanced informatics in medicine programme and the it eductra project of the telematics applications programme ( health sector ) are described . eductra had as aim to investigate which gaps in knowledge health professionals have with respect to health informatics and to suggest ways to remedy this . it was assumed that health professionals had a basic understanding of health informatics and that additional educational material only had to cover the knowledge necessary for appreciating the new products coming from the aim programme . a state of the art survey revealed that the knowledge of health professionals with respect to health informatics was deplorable . guidelines for curricula were therefore proposed to enable potential teachers to design courses . it eductra is a continuation of the eductra project . it has as aim to create learning materials covering a broad area of health informatics .
methods for reasoning from geometry about anatomic structures injured by penetrating trauma . <eos> this paper presents the methods used for three dimensional ( 3d ) reasoning about anatomic structures affected by penetrating trauma in traumascan web , a platform independent decision support system for evaluating the effects of penetrating trauma to the chest and abdomen . in assessing outcomes for an injured patient , traumascan web utilizes 3d models of anatomic structures and 3d models of the regions of damage associated with stab and gunshot wounds to determine the probability of injury to anatomic structures . probabilities estimated from 3d reasoning about affected anatomic structures serve as input to a bayesian network which calculates posterior probabilities of injury based on these initial probabilities together with available information about patient signs , symptoms and test results . in addition to displaying textual descriptions of conditions arising from penetrating trauma to a patient , traumascan web allows users to visualize the anatomy suspected of being injured in 3d , in this way providing a guide to its reasoning process .
use of effective stiffness matrix for the free vibration analyses of a non uniform cantilever beam carrying multiple two degree of freedom springdampermass systems . <eos> this paper investigates the free vibration characteristics of a non uniform cantilever beam carrying multiple two degree of freedom ( dof ) springdampermass systems by means of two finite element methods , fem1 and fem2 . where the fem1 is the conventional finite element method ( fem ) with each two dof springdampermass system being considered as a finite element possessing stiffness , damping and mass matrices , while the fem2 is an alternative approach with each two dof springdampermass system being replaced by an effective stiffness matrix composed of four massless effective springs . instead of using both the real part ( ) and the imaginary part ( ) of a complex eigenvalue ( ) to derive the mathematical expressions , this paper directly employs the implicit form of the complex eigenvalue ( ) to formulate the problem . in the fem1 , since each springdampermass system has two dof , the total dof of the entire system increases two if the beam carries one more two dof springdampermass system . however , in the fem2 , the total dof of the entire system remains unchanged , because all dof of each springdampermass system are suppressed by the effective stiffness matrix . good agreement between the natural frequencies obtained from fem2 and the corresponding ones from fem1 confirms the reliability of the presented theory .
a dtc strategy dedicated to three switch three phase inverter fed induction motor drives . <eos> purpose the put pose of this paper is to describe the implementation of a direct torque control strategy dedicated to three switch three phase delta shaped inverter ( tstpi ) fed induction motor drives as well as the comparison of its performance with those yielded by six switch three phase inverter ( sstpi ) fed induction motor drives under the takahashi dtc strategy . design methodology approach referring to the asymmetrical stator voltage vectors and in order to reach high dynamic with low ripple of the electromagnetic torque response , the design of the vector selection table should include virtual voltage vectors by the subdivision of each sector into two equal sub sectors . findings it has been shown that the implementation of the proposed dtc strategy in tstpi fed induction motor drives leads to higher transient behaviour and better steady state features than those exhibited by the takahashi dtc strategy implemented in sstpi fed induction motor drives . research limitations implications the research should be extended to a comparison of the obtained simulation results with experimental measurements . practical implications a <digit> per cent reduction of cost and compactness associated with a <digit> per cent increase of reliability makes the tstpi an interesting candidate , especially in large scale production applications such as the automotive industry . originality value the paper proposes an approach to improve the cost effectiveness , the compactness and the reliability of tstpi fed induction motor drives , which represents a crucial benefit in electric and hybrid propulsion systems .
hybrid generative discriminative classifier for unconstrained character recognition . <eos> handwriting recognition for hand held devices like pdas requires very accurate and adaptive classifiers . it is such a complex classification problem that it is quite usual now to make co operate several classification methods . in this paper , we present an original two stages recognizer . the first stage is a model based classifier which store an exhaustive set of character models . the second stage is a pairwise classifier which separate the most ambiguous pairs of classes . this hybrid architecture is based on the idea that the correct class almost systematically belongs to the two more relevant classes found by the first classifier . experiments on a 80,000 examples database show a <digit> % improvement on a <digit> classes recognition problem . moreover , we show experimentally that such an architecture suits perfectly for incremental classification .
optimal sleep patterns for serving delay tolerant jobs . <eos> sleeping is an important method to reduce energy consumption in many information and communication systems . in this paper we focus on a typical server under dynamic load , where entering and leaving sleeping mode incurs an energy and a response time penalty . we seek to understand under what kind of system configuration and control method will sleep mode obtain a pareto optimal tradeoff between energy saving and average response time . we prove that the optimal sleeping policy has a simple hysteretic structure . simulation results then show that this policy results in significant energy savings , especially for relatively delay insensitive applications and under low traffic load . however , we demonstrate that seeking the maximum energy saving presents another tradeoff it drives up the peak temperature in the server , with potential reliability consequences .
on channel discontinuity constraint routing in wireless networks . <eos> multi channel wireless networks are increasingly deployed as infrastructure networks , e.g. in metro areas . network nodes frequently employ directional antennas to improve spatial throughput . in such networks , between two nodes , it is of interest to compute a path with a channel assignment for the links such that the path and link bandwidths are the same . this is achieved when any two consecutive links are assigned different channels , termed as channel discontinuity constraint ( cdc ) . cdc paths are also useful in tdma systems , where , preferably , consecutive links are assigned different time slots . in the first part of this paper , we develop a t spanner for cdc paths using spatial properties a sub network containing <digit> ( n10 ) links , for any <digit> > <digit> , such that cdc paths increase in cost by at most a factor t ( <digit> <digit> sin ( <digit> <digit> ) ) ( <digit> ) . we propose a novel distributed algorithm to compute the spanner using an expected number of <digit> ( n log n ) fixed size messages . in the second part , we present a distributed algorithm to find minimum cost cdc paths between two nodes using <digit> ( n ( <digit> ) ) fixed size messages , by developing an extension of edmonds ' algorithm for minimum cost perfect matching . in a centralized implementation , our algorithm runs in <digit> ( n ( <digit> ) ) time improving the previous best algorithm which requires <digit> ( n ( <digit> ) ) running time . moreover , this running time improves to <digit> ( n <digit> ) when used in conjunction with the spanner developed . ( c ) <digit> elsevier b.v. all rights reserved .
composition of aspects based on a relation model synergy of multiple paradigms . <eos> software composition for timely and affordable software development and evolution is one of the oldest pursuits of software engineering . in current software composition techniques , component based software development ( cbsd ) and aspect oriented software development ( aosd ) have attracted academic and industrial attention . blackbox composition used in cbsd provides simple and safe modularization for its strong information hiding , which is , however , the main obstacle for a black box composite to evolve later . this implies that an application developed through black box composition can not take advantage of aspect oriented programming ( aop ) used in aosd . on the contrary , aop enhances maintainability and comprehensibility by modularizing concerns crosscutting multiple components but lacks the support for the hierarchical and external composition of aspects themselves and compromises the important software engineering principles such as encapsulation , which is almost perfectly supported in black box composition . the role and role model have been recognized to have many similarities with cbsd and aop but have significant differences with those composition techniques as well . although each composition paradigm has its own advantages and disadvantages , there is no substantial support to realize the synergy of these composition paradigms the black box composition , aop , and role model . in this paper , a new composition technique based on representational abstraction of the relationship between component instances is introduced . the model supports the simple , elegant , and dynamic composition of components with its declarative form and provides the hooks through which an aspect can evolve and a parallel developed aspect can be merged at the instance level .
generalization performance of magnitude preserving semi supervised ranking with graph based regularization . <eos> semi supervised ranking is a relatively new and important learning problem inspired by many applications . we propose a novel graph based regularized algorithm which learns the ranking function in the semi supervised learning framework . it can exploit geometry of the data while preserving the magnitude of the preferences . the least squares ranking loss is adopted and the optimal solution of our model has an explicit form . we establish error analysis of our proposed algorithm and demonstrate the relationship between predictive performance and intrinsic properties of the graph . the experiments on three datasets for recommendation task and two quantitative structureactivity relationship datasets show that our method is effective and comparable to some other state of the art algorithms for ranking .
accessible haptic user interface design approach for users with visual impairments . <eos> with the number of people with visual impairments ( e.g. , low vision and blind ) continuing to increase , vision loss has become one of the most challenging disabilities . today , haptic technology , using an alternative sense to vision , is deemed an important component for effectively accessing information systems . the most appropriately designed assistive technology is critical for those with visual impairments to adopt assistive technology and to access information , which will facilitate their tasks in personal and professional life . however , most of the existing design approaches are inapplicable and inappropriate to such design contexts as users with visual impairments interacting with non graphical user interfaces ( i.e. , haptic technology ) . to resolve such design challenges , the present study modified a participatory design approach ( i.e. , pictive , plastic interface for collaborative technology initiatives video exploration ) to be applicable to haptic technologies , by considering the brain plasticity theory . the sense of touch is integrated into the design activity of pictive . participants with visual impairments were able to effectively engage in designing non visual interfaces ( e.g. , haptic interfaces ) through non visual communication methods ( e.g. , touch modality ) .
effect of probabilistic task allocation based on statistical analysis of bid values . <eos> this paper presents the effect of adaptively introducing appropriate strategies into the award phase of the contract net protocol ( cnp ) in a massively multi agent system ( mmas ) .
higher order concurrent programs with finite communication topology ( extended abstract ) . <eos> concurrent ml ( cml ) is an extension of the functional language standard ml ( sml ) with primitives for the dynamic creation of processes and channels and for the communication of values over channels . because of the powerful abstraction mechanisms the communication topology of a given program may be very complex and therefore an efficient implementation may be facilitated by knowledge of the topology . this paper presents an analysis for determining when a bounded number of processes and channels will be generated . the analysis proceeds in two stages . first we extend a polymorphic type system for sml to deduce not only the type of cml programs but also their communication behaviour expressed as terms in a new process algebra . next we develop an analysis that given the communication behaviour predicts the number of processes and channels required during the execution of the cml program . the correctness of the analysis is proved using a subject reduction property for the type system .
on the polyhedral structure of a multi item production planning model with setup times . <eos> we present and study a mixed integer programming model that arises as a substructure in many industrial applications . this model generalizes a number of structured mip models previously studied , and it provides a relaxation of various capacitated production planning problems and other fixed charge network flow problems . we analyze the polyhedral structure of the convex hull of this model , as well as of a strengthened lp relaxation . among other results , we present valid inequalities that induce facets of the convex hull under certain conditions . we also discuss how to strengthen these inequalities by using known results for lifting valid inequalities for <digit> <digit> continuous knapsack problems .

coherence between one random and one periodic signal for measuring the strength of responses in the electro encephalogram during sensory stimulation . <eos> coherence between a pulse train representing periodic stimuli and the eeg has been used in the objective detection of steady state evoked potentials . this work aimed to quantify the strength of the stimulus responses based on the statistics of coherence estimate between one random and one periodic signal focusing on the confidence limits and power of significance tests in detecting responses . to detect the responses in <digit> % of cases , a signal to noise ratio of about 7.9 db was required when using <digit> windows ( m ) in the coherence estimation . the ratio , however , increased to 1.2 db when m was <digit> . the results were tested in monte carlo simulations and applied to eegs obtained from <digit> subjects during visual stimulation . the method showed differences in the strength of responses at the stimulus frequency and its harmonics , as well as variations between individuals and over cortical regions . in contrast to those from the parietal and temporal regions , results for the occipital region gave confidence limits ( with m <digit> ) that were above zero for all subjects , indicating statistically significant responses . the proposed technique extends the usefulness of coherence as a measure of stimulus responses and allows statistical analysis that could also be applied usefully in a range of other biological signals .
exploring the dynamics of adaptation with evolutionary activity plots . <eos> evolutionary activity statistics and their visualization are introduced , and their motivation is explained . examples of their use are described , and their strengths and limitations are discussed . references to more extensive or general accounts of these techniques are provided .
repeated exposure to the abused inhalant toluene alters levels of neurotransmitters and generates peroxynitrite in nigrostriatal and mesolimbic nuclei in rat . <eos> toluene , a volatile hydrocarbon found in a variety of chemical compounds , is misused and abused by inhalation for its euphorigenic effects . toluene 's reinforcing properties may share a common characteristic with other drugs of abuse , namely , activation of the mesolimbic dopamine system . prior studies in our laboratory found that acutely inhaled toluene activated midbrain dopamine neurons in the rat . moreover , single systemic injections of toluene in rats produced a dose dependent increase in locomotor activity which was blocked by depletion of nucleus accumbens dopamine or by pretreatment with a d2 dopamine receptor antagonist . here we examined the effects of seven daily intraperitoneal injections of <digit> mg kg toluene on the content of serotonin and dopamine in the caudate nucleus ( cn ) and nucleus accumbens ( nac ) , substantia nigra , and ventral tegmental area at <digit> , <digit> , and <digit> h after the last injection . also , the roles of nitric oxide , peroxynitrite , and the production of <digit> nitrosotyrosine ( <digit> nt ) , in the cn and nac were assessed at the same time points . toluene treatments increased dopamine levels in the cn and nac , and serotonin levels in cn , nac , and ventral tegmental area . measurements of the dopamine metabolite dihydroxyphenylacetic acid ( dopac ) further suggested a change in transmitter utilization in cn and nac . lastly , <digit> nt levels also showed a differential change between cn and nac , but at different time points post toluene injection . these results point out the complexity of action of toluene on neurotransmitter function following a course of chronic exposure . changes in the production of <digit> nt also suggest that toluene induced neurotoxicity may mediate via generation of peroxynitrite .
supporting ad hoc ranking aggregates . <eos> this paper presents a principled framework for efficient processing of ad hoc top k ( ranking ) aggregate queries , which provide the k groups with the highest aggregates as results . essential support of such queries is lacking in current systems , which process the queries in a nave materialize group sort scheme that can be prohibitively inefficient . our framework is based on three fundamental principles . the upper bound principle dictates the requirements of early pruning , and the group ranking and tuple ranking principles dictate group ordering and tuple ordering requirements . they together guide the query processor toward a provably optimal tuple schedule for aggregate query processing . we propose a new execution framework to apply the principles and requirements . we address the challenges in realizing the framework and implementing new query operators , enabling efficient group aware and rank aware query plans . the experimental study validates our framework by demonstrating orders of magnitude performance improvement in the new query plans , compared with the traditional plans .
meaningful and meaningless solutions for cooperative n person games . <eos> game values often represent data that can be measured in more than one acceptable way ( e.g. , monetary amounts ) . we point out that in such a case a statement about cooperative n person game models might be meaningless in the sense that its truth or falsity depends on the choice of an acceptable way to measure game values . in particular , we analyze statements about solution concepts such as the core , stable sets , the nucleolus , the shapley value ( and some of its generalizations ) .
numerical optimization algorithm for rotationally invariant multi orbital slave boson method . <eos> we develop a generalized numerical optimization algorithm for the rotationally invariant multi orbital slave boson approach , which is applicable for arbitrary boundary constraints of high dimensional objective function by combining several classical optimization techniques . after constructing the calculation architecture of rotationally invariant multi orbital slave boson model , we apply this optimization algorithm to find the stable ground state and magnetic configuration of two orbital hubbard models . the numerical results are consistent with available solutions , confirming the correctness and accuracy of our present algorithm . furthermore , we utilize it to explore the effects of the transverse hunds coupling terms on metalinsulator transition , orbital selective mott phase and magnetism . these results show the quick convergency and robust stable character of our algorithm in searching the optimized solution of strongly correlated electron systems .
molecular dynamics simulation of large scale carbon nanotubes on a shared memory architecture . <eos> carbon nanotubes are expected to play a significant role in the design and manufacture of many nano mechanical and nano electronic devices of future . it is important , therefore , that atomic level elastomechanical response properties of both single and multiwall nanotubes be investigated in detail . classical molecular dynamics simulations employing brenner 's reactive potential with long range van der waals interactions have been used in mechanistic response studies of carbon nanotubes to external strains . the studies of single and multiwalled carbon nanotubes under compressive strains show the instabilities beyond elastic response . due to inclusion of non bonded long range interactions , the simulations also show the redistribution of strain and strain energy from sideways bucklng to the formation of highly localized strained kink sites . bond rearrangements occur at the kink sites , leading to formation of topological defects , preventing the tube from relaxing fully back to it 's original configuration . elastomechanic response behavior of single and multiwall carbon nanotubes to externally applied compressive strains is simulated and studied in detail . we will describe the results and discuss their implication towards the stability of any molecular mechanical structure made of carbon nanotubes .
nonprimitive recursive complexity and undecidability for petri net equivalences . <eos> the aim of this note is twofold . firstly , it shows that the undecidability result for bisimilarity in theor . comput . sci . <digit> ( <digit> ) <digit> <digit> can be immediately extended for the whole range of equivalences land preorders ) on labelled petri nets . secondly , it shows that restricting our attention to nets with finite reachable space , the respective ( decidable ) problems are nonprimitive recursive this approach also applies to mayr and meyer 's result j. acm <digit> ( <digit> ) <digit> <digit> for the reachability set equality , yielding a more direct proof . ( c ) <digit> elsevier science b.v. all rights reserved .
time decaying aggregates in out of order streams . <eos> processing large data streams is now a major topic in data management . the data involved can be truly massive , and the required analyses complex . in a stream of sequential events such as stock feeds , sensor readings , or ip traffic measurements , data tuples pertaining to recent events are typically more important than older ones . this can be formalized via time decay functions , which assign weights to data based on the age of data . decay functions such as sliding windows and exponential decay have been studied under the assumption of well ordered arrivals , i.e. , data arrives in non decreasing order of time stamps . however , data quality issues are prevalent in massive streams ( due to network asynchrony and delays etc. ) , and correct arrival order is not guaranteed . we focus on the computation of decayed aggregates such as range queries , quantiles , and heavy hitters on out of order streams , where elements do not necessarily arrive in increasing order of timestamps . existing techniques such as exponential histograms and waves are unable to handle out of order streams . we give the first deterministic algorithms for approximating these aggregates under popular decay functions such as sliding window and polynomial decay . we study the overhead of allowing out of order arrivals when compared to well ordered arrivals , both analytically and experimentally . our experiments confirm that these algorithms can be applied in practice , and compare the relative performance of different approaches for handling out of order arrivals .
the village telco project a reliable and practical wireless mesh telephony infrastructure . <eos> voip ( voice over ip ) over mesh networks could be a potential solution to the high cost of making phone calls in most parts of africa . the village telco ( vt ) is an easy to use and scalable voip over meshed wlan ( wireless local area network ) telephone infrastructure . it uses a mesh network of mesh potatoes to form a peer to peer network to relay telephone calls without landlines or cell phone towers . this paper discusses the village telco infrastructure , how it addresses the numerous difficulties associated with wireless mesh networks , and its efficient deployment for voip services in some communities around the globe . the paper also presents the architecture and functions of a mesh potato and a novel combined analog telephone adapter ( ata ) and wifi access point that routes calls . lastly , the paper presents the results of preliminary tests that have been conducted on a mesh potato . the preliminary results indicate very good performance and user acceptance of the mesh potatoes . the results proved that the infrastructure is deployable in severe and under resourced environments as a means to make cheap phone calls and render internet and ip based services . as a result , the vt project contributes to bridging the digital divide in developing areas .
general subspace learning with corrupted training data via graph embedding . <eos> we address the following subspace learning problem supposing we are given a set of labeled , corrupted training data points , how to learn the underlying subspace , which contains three components an intrinsic subspace that captures certain desired properties of a data set , a penalty subspace that fits the undesired properties of the data , and an error container that models the gross corruptions possibly existing in the data . given a set of data points , these three components can be learned by solving a nuclear norm regularized optimization problem , which is convex and can be efficiently solved in polynomial time . using the method as a tool , we propose a new discriminant analysis ( i.e. , supervised subspace learning ) algorithm called corruptions tolerant discriminant analysis ( ctda ) , in which the intrinsic subspace is used to capture the features with high within class similarity , the penalty subspace takes the role of modeling the undesired features with high between class similarity , and the error container takes charge of fitting the possible corruptions in the data . we show that ctda can well handle the gross corruptions possibly existing in the training data , whereas previous linear discriminant analysis algorithms arguably fail in such a setting . extensive experiments conducted on two benchmark human face data sets and one object recognition data set show that ctda outperforms the related algorithms .
closure properties of hyper minimized automata . <eos> two deterministic finite automata are almost equivalent if they disagree in acceptance only for finitely many inputs . an automaton a is hyper minimized if no automaton with fewer states is almost equivalent to a. a regular language l is canonical if the minimal automaton accepting l is hyper minimized . the asymptotic state complexity s ( l ) of a regular language l is the number of states of a hyper minimized automaton for a language finitely different from l. in this paper we show that ( <digit> ) the class of canonical regular languages is not closed under intersection , union , concatenation , kleene closure , difference , symmetric difference , reversal , homomorphism , and inverse homomorphism ( <digit> ) for any regular languages l ( <digit> ) and l ( <digit> ) the asymptotic state complexity of their sum l ( <digit> ) boolean or l ( <digit> ) , intersection l ( <digit> ) boolean and l ( <digit> ) , difference l ( <digit> ) l ( <digit> ) , and symmetric difference l ( <digit> ) circle plus l ( <digit> ) can be bounded by s ( l ( <digit> ) ) . s ( l ( <digit> ) ) . this bound is tight in binary case and in unary case can be met in infinitely many cases . ( <digit> ) for any regular language l the asymptotic state complexity of its reversal l ( r ) can be bounded by <digit> ( s ) ( l ) . this bound is tight in binary case . ( <digit> ) the asymptotic state complexity of kleene closure and concatenation can not be bounded . namely , for every k > <digit> , there exist languages k , l , and m such that s ( k ) s ( l ) s ( m ) <digit> and s ( k ) s ( l . m ) k. these are answers to open problems formulated by back et al. rairo theor . inf . appl . <digit> ( <digit> ) <digit> <digit>
on equivalent parameter learning in simplified feature space based on bayesian asymptotic analysis . <eos> parametric models for sequential data , such as hidden markov models , stochastic context free grammars , and linear dynamical systems , are widely used in time series analysis and structural data analysis . computation of the likelihood function is one of primary considerations in many learning methods . iterative calculation of the likelihood such as the model selection is still time consuming though there are effective algorithms based on dynamic programming . the present paper studies parameter learning in a simplified feature space to reduce the computational cost . simplifying data is a common technique seen in feature selection and dimension reduction though an oversimplified space causes adverse learning results . therefore , we mathematically investigate a condition of the feature map to have an asymptotically equivalent convergence point of estimated parameters , referred to as the vicarious map . as a demonstration to find vicarious maps , we consider the feature space , which limits the length of data , and derive a necessary length for parameter learning in hidden markov models .
a dual scale lattice gas automata model for gas solid two phase flow in bubbling fluidized beds . <eos> modelling the hydrodynamics of gas solid flow is important for the design and scale up of fluidized bed reactors . a novel gas solid dual scale model based on lattice gas cellular automata ( lgca ) is proposed to describe the macroscopic behaviour through microscopic gas solid interactions . solid particles and gas pseudo particles are aligned in lattices with different scales for solid and gas . in addition to basic lgca rules , additional rules for collision and propagation are specifically designed for gas solid systems . the solid 's evolution is then motivated by the temporal and spatial average momentum gained through solid solid and gas solid interactions . a statistical method , based on the similarity principle , is derived for the conversion between model parameters and hydrodynamic properties . simulations for bubbles generated from a vertical jet in a bubbling fluidized bed based on this model agree well with experimental results , as well as with the results of two fluid approaches and discrete particle simulations . ( c ) <digit> elsevier ltd. all rights reserved .
scalable algorithms for global snapshots in distributed systems . <eos> existing algorithms for global snapshots in distributed systems are not scalable when the underlying topology is complete . in a network with n processors , these algorithms require o ( n ) space and o ( n ) messages per processor . as a result , these algorithms are not efficient in large systems when the logical topology of the communication layer such as mpi is complete . in this paper , we propose three algorithms for global snapshot a grid based , a tree based and a centralized algorithm . the grid based algorithm uses o ( n ) space but only o ( n ) messages per processor . the tree based algorithm requires only o ( <digit> ) space and o ( log n log w ) messages per processor where w is the average number of messages in transit per processor . the centralized algorithm requires only o ( <digit> ) space and o ( log w ) messages per processor . we also have a matching lower bound for this problem . our algorithms have applications in checkpointing , detecting stable predicates and implementing synchronizers . we have implemented our algorithms on top of the mpi library on the blue gene l supercomputer . our experiments confirm that the proposed algorithms significantly reduce the message and space complexity of a global snapshot .
a smart tcp acknowledgment approach for multihop wireless networks . <eos> reliable data transfer is one of the most difficult tasks to be accomplished in multihop wireless networks . traditional transport protocols like tcp face severe performance degradation over multihop networks given the noisy nature of wireless media as well as unstable connectivity conditions in place . the success of tcp in wired networks motivates its extension to wireless networks . a crucial challenge faced by tcp over these networks is how to operate smoothly with the 802.11 wireless mac protocol which also implements a retransmission mechanism at link level in addition to short rts cts control frames for avoiding collisions . these features render tcp acknowledgments ( ack ) transmission quite costly . data and ack packets cause similar medium access overheads despite the much smaller size of the acks . in this paper , we further evaluate our dynamic adaptive strategy for reducing ack induced overhead and consequent collisions . our approach resembles the sender side 's congestion control . the receiver is self adaptive by delaying more acks under nonconstrained channels and less otherwise . this improves not only throughput but also power consumption . simulation evaluations exhibit significant improvement in several scenarios .
vasopressin and social odor processing in the olfactory bulb and anterior olfactory nucleus . <eos> central vasopressin facilitates social recognition and modulates numerous complex social behaviors in mammals , including parental behavior , aggression , affiliation , and pair bonding . in rodents , social interactions are primarily mediated by the exchange of olfactory information , and there is evidence that vasopressin signaling is important in brain areas where olfactory information is processed . we recently discovered populations of vasopressin neurons in the main and accessory olfactory bulbs and anterior olfactory nucleus that are involved in the processing of social odor cues . in this review , we propose a model of how vasopressin release in these regions , potentially from the dendrites , may act to filter social odor information to facilitate odor based social recognition . finally , we discuss recent human research linked to vasopressin signaling and suggest that our model of priming facilitated vasopressin signaling would be a rewarding target for further studies , as a failure of priming may underlie pathological changes in complex behaviors .
ticks , tick borne rickettsiae , and coxiella burnetii in the greek island of cephalonia . <eos> domestic animals are the hosts of several tick species and the reservoirs of some tick borne pathogens hence , they play an important role in the circulation of these arthropods and their pathogens in nature . they may act as vectors , but , also , as reservoirs of spotted fever group ( sfg ) rickettsiae , which are the causative agents of sfg rickettsioses . q fever is a worldwide zoonosis caused by coxiella burnetii ( c. burnetii ) , which can be isolated from ticks . a total of 1,848 ticks ( <digit> female , <digit> male , and <digit> nymph ) were collected from dogs , goats , sheep , cattle , and horses in <digit> different localities of the greek island of cephalonia . rhipicephalus ( rh . ) bursa , rh . turanicus , rh . sanguineus , dermacentor marginatus ( d. marginatus ) , ixodes gibbosus ( i. gibbosus ) , haemaphysalis ( ha . ) punctata , ha . sulcata , hyalomma ( hy . ) anatolicum excavatum and hy . marginatum marginatum were the species identified . c. burnetii and four different sfg rickettsiae , including rickettsia ( r. ) conorii , r. massiliae , r. rhipicephali , and r. aeschlimannii were detected using molecular methods . double infection with r. massiliae and c. burnetii was found in one of the positive ticks
quiver polynomials in iterated residue form . <eos> degeneracy loci polynomials for quiver representations generalize several important polynomials in algebraic combinatorics . in this paper we give a nonconventional generating sequence description of these polynomials when the quiver is of dynkin type .
the relationship among soft sets , soft rough sets and topologies . <eos> molodtsovs soft set theory is a newly emerging tool to deal with uncertain problems . based on the novel granulation structures called soft approximation spaces , feng et al. initiated soft rough approximations and soft rough sets . fengs soft rough sets can be seen as a generalized rough set model based on soft sets , which could provide better approximations than pawlaks rough sets in some cases . this paper is devoted to establishing the relationship among soft sets , soft rough sets and topologies . we introduce the concept of topological soft sets by combining soft sets with topologies and give their properties . new types of soft sets such as keeping intersection soft sets and keeping union soft sets are defined and supported by some illustrative examples . we describe the relationship between rough sets and soft rough sets . we obtain the structure of soft rough sets and the topological structure of soft sets , and reveal that every topological space on the initial universe is a soft approximating space .
a highly efficient vlsi architecture for h. <digit> avc cavlc decoder . <eos> in this paper , an efficient algorithm is proposed to improve the decoding efficiency of the context based adaptive variable length coding ( cavlc ) procedure . due to the data dependency among symbols in the decoding how , the cavlc decoder requires large computation time , which dominates the overall decoder system performance . to expedite its decoding speed , the critical path in the cavlc decoder is first analyzed and then reduced by forwarding the adaptive detection for succeeding symbols . with a shortened critical path , the cavlc architecture is further divided into two segments , which can be easily implemented by a pipeline structure . consequently , the overall performance is effectively improved . in the hardware implementation , a low power combined lut and single output buffer have been adopted to reduce the area as well as power consumption without affecting the decoding performance . experimental results show that the proposed architecture surpassing other recent designs can approximately reduce power consumption by <digit> % and achieve three times decoding speed in comparison to the original decoding procedure suggested in the h. <digit> standard . the maximum frequency can be larger than <digit> mhz , which can easily support the real time requirement for resolutions higher than the hd1080 format .
building database applications of virtual reality with x vrml . <eos> a new method of building active database driven virtual reality applications is presented . the term active is used to describe applications that allow server side user interaction , dynamic composition of virtual scenes , access to on line data , continuous visualization , and implementation of persistency.the use the x vrml language for building active applications of virtual reality is proposed . x vrml is a high level xml based language that overcomes the main limitations of the current virtual reality systems by providing convenient access to databases , object orientation , parameterization , and imperative programming techniques . applications of x vrml include on line data visualization , geographical information systems , scientific visualization , virtual games , and e commerce applications such as virtual shops . in this paper , methods of accessing databases from x vrml are described , architectures of x vrml systems for different application domains are discussed , and examples of database applications of virtual reality implemented in x vrml are presented .
distributed h infinity filtering for sensor networks with switching topology . <eos> in this article , the distributed h filtering problem is investigated for a class of sensor networks under topology switching . the main purpose is to design the distributed h filter that allows one to regulate the sensor 's working modes . firstly , a switched system model is proposed to reflect the working mode change of the sensors . then , a stochastic sequence is adopted to model the packet dropout phenomenon occurring in the channels from the plant to the networked sensors . by utilising the lyapunov functional method and stochastic analysis , some sufficient conditions are established to ensure that the filtering error system is mean square exponentially stable with a prescribed h performance level . furthermore , the filter parameters are determined by solving a set of linear matrix inequalities ( lmis ) . our results relates the decay rate of the filtering error system to the switching frequency of the topology directly and shows the existence of such a distributed filter when the topology is not varying very frequently , which is helpful for the sensor state regulation . finally , the effectiveness of the proposed design method is demonstrated by two numerical examples .
the evolution of goal based information modelling literature review . <eos> purpose the first in a series on goal based information modelling , this paper presents a literature review of two goal based measurement methods . the second article in the series will build on this background to present an overview of some recent case based research that shows the applicability of the goal based methods for information modelling ( as opposed to measurement ) . the third and concluding article in the series will present a new goal based information model the goal based information framework ( gbif ) that is well suited to the task of documenting and evaluating organisational information flow . design methodology approach following a literature review of the goal question metric ( gqm ) and goal question indicator measure ( gqim ) methods , the paper presents the strengths and weaknesses of goal based approaches . findings the literature indicates that the goal based methods are both rigorous and adaptable . with over <digit> years of use , goal based methods have achieved demonstrable and quantifiable results in both practitioner and academic studies . the down side of the methods are the potential expense and the expansiveness of goal based models . the overheads of managing the goal based process , from early negotiations on objectives and goals to maintaining the model ( adding new goals , questions and indicators ) , could make the method unwieldy and expensive for organisations with limited resources . an additional challenge identified in the literature is the narrow focus of top down ( i.e. goal based ) methods . since the methods limit the focus to a pre defined set of goals and questions , the opportunity for discovery of new information is limited . research limitations implications much of the previous work on goal based methodologies has been confined to software measurement contexts in larger organisations with well established information gathering processes . although the next part of the series presents goal based methods outside of this native context , and within low maturity organisations , further work needs to be done to understand the applicability of these methods in the information science discipline . originality value this paper presents ail overview of goal based methods . the next article in the series will present the method outside the native context of software measurement . with the universality of the method established , information scientists will have a new tool to evaluate and document organisational information flow .
a communication reduction approach to iteratively solve large sparse linear systems on a gpgpu cluster . <eos> finite element methods ( fem ) are widely used in academia and industry , especially in the fields of mechanical engineering , civil engineering , aerospace , and electrical engineering . these methods usually convert partial difference equations into large sparse linear systems . for complex problems , solving these large sparse linear systems is a time consuming process . this paper presents a parallelized iterative solver for large sparse linear systems implemented on a gpgpu cluster . traditionally , these problems do not scale well on gpgpu clusters . this paper presents an approach to reduce the communications between cluster compute nodes for these solvers . additionally , computation and communication are overlapped to reduce the impact of data exchange . the parallelized system achieved a speedup of up to 15.3 times on <digit> nvidia tesla gpus , compared to a single gpu . an analytical evaluation of the algorithm is conducted in this paper , and the analytical equations for predicting the performance are presented and validated .
transductive inference using multiple experts for brushwork annotation in paintings domain . <eos> many recent studies perform annotation of paintings based on brushwork . in these studies the brushwork is modeled indirectly as part of the annotation of high level artistic concepts such as the artist name using low level texture . in this paper , we develop a serial multi expert framework for explicit annotation of paintings with brushwork classes . in the proposed framework , each individual expert implements transductive inference by exploiting both labeled and unlabelled data . to minimize the problem of noise in the feature space , the experts select appropriate features based on their relevance to the brushwork classes . the selected features are utilized to generate several models to annotate the unlabelled patterns . the experts select the best performing model based on vapnik combined bound . the transductive annotation using multiple experts out performs the conventional baseline method in annotating patterns with brushwork classes .
on the integration of equations of motion for particle in cell codes . <eos> an area preserving implementation of the 2nd order runge kutta integration method for equations of motion is presented . for forces independent of velocity the scheme possesses the same numerical simplicity and stability as the leapfrog method , and is not implicit for forces which do depend on velocity . it can be therefore easily applied where the leapfrog method in general can not . we discuss the stability of the new scheme and test its performance in calculations of particle motion in three cases of interest . first , in the ubiquitous and numerically demanding example of nonlinear interaction of particles with a propagating plane wave , second , in the case of particle motion in a static magnetic field and , third , in a nonlinear dissipative case leading to a limit cycle . we compare computed orbits with exact orbits and with results from the leapfrog and other low order integration schemes . of special interest is the role of intrinsic stochasticity introduced by time diferencing , which can destroy orbits of an otherwise exactly integrable system and therefore constitutes a restriction on the applicability of an integration scheme in such a context a. friedman , s.p. auerbach , j. comput . phys . <digit> ( <digit> ) <digit> . in particular , we show that for a plane wave the new scheme proposed herein can be reduced to a symmetric standard map . this leads to the nonlinear stability condition delta t omega ( b ) < <digit> , where delta t is the time step and omega ( b ) the particle bounce frequency . ( c ) <digit> elsevier inc. all rights reserved .
system support for mobile augmented reality services . <eos> developing and deploying augmented reality ( ar ) services in pervasive computing environments is quite difficult because almost of all current systems require heavy and bulky head mounted displays ( hmds ) and are based on inflexible centralized architectures for detecting service locations and superimposing ar images . we propose a light weight mobile ar service framework that combines personal mobile devices most of people own nowadays , visual tags as inexpensive ar techniques , and mobile code that enables easy to deploy environments . our framework enables developers to easily deploy mobile ar services in pervasive computing environments and users to interact them in a both of practical and intuitive way .
fabrication of the wireless systems for controlling movements of the electrical stimulus capsule in the small intestines . <eos> diseases of the gastro intestinal tract are becoming more prevalent . new techniques and devices , such as the wireless capsule endoscope and the telemetry capsule , that are able to measure the various signals of the digestive organs ( temperature , ph , and pressure ) , have been developed for the observation of the digestive organs . in these capsule devices , there are no methods of moving and grasping them . in order to make a swift diagnosis and to give proper medication , it is necessary to control the moving speed of the capsule . this paper presents a wireless system for the control of movements of an electrical stimulus capsule . this includes an electrical stimulus capsule which can be swallowed and an external transmitting control system . a receiver , a receiving antenna ( small multi loop ) , a transmitter , and a transmitting antenna ( monopole ) were designed and fabricated taking into consideration the mpe , power consumption , system size , signal to noise ratio and the modulation method . the wireless system , which was designed and implemented for the control of movements of the electrical stimulus capsule , was verified by in vitro experiments which were performed on the small intestines of a pig . as a result , we found that when the small intestines are contracted by electrical stimuli , the capsule can move to the opposite direction , which means that the capsule can go up or down in the small intestines .
a continuous wavelet based approach to detect anisotropic properties in spatial point processes . <eos> a two dimensional stochastic point process can be regarded as a random measure and thus represented as a ( countable ) sum of delta dirac measures concentrated at some points . integration with respect to the point process itself leads to the concept of the continuous wavelet transform of a point process . applying then suitable translation , rotation and dilation operations through a non unitary operator , we obtain a transformed point process which highlights main properties of the original point process . the choice of the mother wavelet is relevant and we thus conduct a detailed analysis proposing three two dimensional mother wavelets . we use this approach to detect main directions present in the point process , and to test for anisotropy .
robust object tracking using joint color texture histogram . <eos> a novel object tracking algorithm is presented in this paper by using the joint color texture histogram to represent a target and then applying it to the mean shift framework . apart from the conventional color histogram features , the texture features of the object are also extracted by using the local binary pattern ( lbp ) technique to represent the object . the major uniform lbp patterns are exploited to form a mask for joint color texture feature selection . compared with the traditional color histogram based algorithms that use the whole target region for tracking , the proposed algorithm extracts effectively the edge and corner features in the target region , which characterize better and represent more robustly the target . the experimental results validate that the proposed method improves greatly the tracking accuracy and efficiency with fewer mean shift iterations than standard mean shift tracking . it can robustly track the target under complex scenes , such as similar target and background appearance , on which the traditional color based schemes may fail to track .
quasi resonant interconnects a low power , low latency design methodology . <eos> design and analysis guidelines for quasi resonant interconnect networks ( qrn ) are presented in this paper . the methodology focuses on developing an accurate analytic distributed model of the on chip interconnect and inductor to obtain both low power and low latency . excellent agreement is shown between the proposed model and spectras simulations . the analysis and design of the inductor , insertion point , and driver resistance for minimum power delay product is described . a case study demonstrates the design of a quasi resonant interconnect , transmitting a <digit> gb s data signal along a <digit> mm line in a tsmc 0.18 mu m cmos technology . as compared to classical repeater insertion , an average reduction of 91.1 % and 37.8 % is obtained in power consumption and delay , respectively . as compared to optical links , a reduction of 97.1 % and 35.6 % is observed in power consumption and delay , respectively .
combining hashing and enciphering algorithms for epidemiological analysis of gathered data . <eos> objectives compiling individual records coming from different sources is necessary for multi center studies . legal aspects can be satisfied by implementing anonymization procedures . when using these procedures with a different key for each study it becomes almost impossible to link records from separate data collections . methods the originality of the method relies on the way the combination of hashing and enciphering techniques is performed like in asymmetric encryption , two keys are used but the private key depends on the patient 's identity . results the combination of hashing and enciphering techniques provides a great improvement in the overall security of the proposed scheme . conclusion this methodology makes stored data available for use in the field of public health , while respecting legal security requirements .
a personalized english learning recommender system for esl students . <eos> this paper has developed an online personalized english learning recommender system capable of providing esl students with reading lessons that suit their different interests and therefore increase the motivation to learn . the system , using content based analysis , collaborative filtering , and data mining techniques , analyzes real students reading data and generates recommender scores , based on which to help select appropriate lessons for respective students . its performance having been tracked over a period of one year , this recommender system has proved to be very useful in heightening esl learners motivation and interest in reading .
graph based hierarchical conceptual clustering . <eos> hierarchical conceptual clustering has proven to be a useful , although under explored , data mining technique . a graph based representation of structural information combined with a substructure discovery technique has been shown to be successful in knowledge discovery . the subdue substructure discovery system provides one such combination of approaches . this work presents subdue and the development of its clustering functionalities . several examples are used to illustrate the validity of the approach both in structured and unstructured domains , as well as to compare subdue to the cobweb clustering algorithm . we also develop a new metric for comparing structurally defined clusterings . results show that subdue successfully discovers hierarchical clusterings in both structured and unstructured data .
an intelligent system employing an enhanced fuzzy c means clustering model application in the case of forest fires . <eos> fuzzy c means is a well established clustering algorithm . according to this approach instead of having each data point dpi ( x , y ) belonging only to a specific cluster in a crisp manner , each dpi belongs to all of the determined clusters with a different degree of membership . in this way cluster overlapping is allowed . this research effort enhances the fuzzy c means model in an intelligent manner , employing a flexible fuzzy termination criterion . the enhanced fuzzy c means clustering algorithm performs several iterations before the proper centers of the clusters more or less stabilize , which means that their coordinates remain almost equal to the previous ones . in this way the algorithm is expanded to perform in a more flexible and human like intelligent way , avoiding the chance of infinite loops and the performance of unnecessary iterations . a corresponding software system has been developed in c programming language applying the extended model . the system has been applied for the clustering of the greek forest departments according to their forest fire risk . two risk factors were taken into consideration , namely the number of forest fires and the annual burned forested areas . the design and the development of the innovative model system and the results of its application are presented and discussed in this research paper .
miniaturization of uwb antennas and its influence on antenna transceiver performance in impulse uwb communication . <eos> in this paper , a co design methodology and the effect of antenna miniaturization in an impulse uwb system transceiver is presented . modified small size printed tapered monopole antennas ( ptma ) are designed in different scaling sizes . in order to evaluate the performance and functionality of these antennas , the effect of each antenna is studied in a given impulse uwb system . the uwb system includes an impulse uwb transmitter and two kinds of uwb receivers are considered , one based on correlation detection and one on energy detection schemes . a tunable low power impulse uwb transmitter is designed and the benefit of co designing it with the ptma antenna is investigated for the 3.110.6 ghz band . a comparison is given between a <digit> ( omega ) design and a co designed version . our antenna transceiver co design methodology shows improvement in both transmitter efficiency and whole system performance . the simulation results show that the ptma antenna and its miniaturized geometries are suitable for uwb applications .
induced quasi arithmetic uncertain linguistic aggregation operator . <eos> induced quasi arithmetic aggregation operators are considered to aggregate uncertain linguistic information by using order inducing variables . we introduce the induced correlative uncertain linguistic aggregation operator with choquet integral and we also present the induced uncertain linguistic aggregation operator by using the dempster shafer theory of evidence . the special cases of the new proposed operators are investigated . many existing linguistic aggregation operators are special cases of our new operators and more new uncertain linguistic aggregation operators can be derived from them . decision making methods based on the new aggregation operators are proposed and architecture material supplier selection problems are presented to illustrate the feasibility and efficiency of the new methods .
on fuzzy congruence of a near ring module . <eos> the aim of this paper is to introduce fuzzy submodule and fuzzy congruence of an r module ( near ring module ) , to obtain the correspondence between fuzzy congruences and fuzzy submodules of an r module , to define quotient r module of an r module over a fuzzy submodule and to obtain correspondence between fuzzy congruences of an r module and fuzzy congruences of quotient r module over a fuzzy submodule of an r module . ( c ) <digit> elsevier science b.v. all rights reserved .
self bounded controlled invariant subspaces in measurable signal decoupling with stability minimal order feedforward solution . <eos> the structural properties of self bounded controlled invariant subspaces are fundamental to the synthesis of a dynamic feedforward compensator achieving insensitivity of the controlled output to a disturbance input accessible for measurement , on the assumption that the system is stable or pre stabilized by an inner feedback . the control system herein devised has several important features i ) minimum order of the feedforward compensator ii ) minimum number of unassignable dynamics internal to the feedforward compensator iii ) maximum number of dynamics , external to the feedforward compensator , arbitrarily assignable by a possible inner feedback . from the numerical point of view , the design method herein detailed does not involve any computation of eigenspaces , which may be critical for systems of high order . the procedure is first presented for left invertible systems . then , it is extended to non left invertible systems by means of a simple , original , squaring down technique .
hypergraph based multilevel matrix approximation for text information retrieval . <eos> in latent semantic indexing ( lsi ) , a collection of documents is often pre processed to form a sparse term document matrix , followed by a computation of a low rank approximation to the data matrix . a multilevel framework based on hypergraph coarsening is presented which exploits the hypergraph that is canonically associated with the sparse term document matrix representing the data . the main goal is to reduce the cost of the matrix approximation without sacrificing accuracy . because coarsening by multilevel hypergraph techniques is a form of clustering , the proposed approach can be regarded as a hybrid of factorization based lsi and clustering based lsi . experimental results indicate that our method achieves good improvement of the retrieval performance at a reduced cost
balanced paths in acyclic networks tractable cases and related approaches . <eos> given a weighted acyclic network g and two nodes s and t in g , we consider the problem of computing k balanced paths from s to t , that is , k paths such that the difference in cost between the longest and the shortest path is minimized . the problem has several variants . we show that , whereas the general problem is solvable in pseudopolynomial time , both the arc disjoint and the node disjoint variants ( i.e. , the variants where the k paths are required to be arc disjoint and node disjoint , respectively ) are strongly np hard . we then address some significant special cases of such variants , and propose exact as well as approximate algorithms for their solution . the proposed approaches are also able to solve versions of the problem in which k origin destination pairs are provided , and a set of k paths linking the origin destination pairs has to be computed in such a way to minimize the difference in cost between the longest and the shortest path in the set . ( c ) <digit> wiley periodicals , inc. networks , vol . <digit> ( <digit> ) , 104 111 2005
the connected assignment problem . <eos> given a graph and costs of assigning to each vertex one of k different colors , we want to find a minimum cost assignment such that no color q induces a subgraph with more than a given number ( q ) of connected components . this problem arose in the context of contiguity constrained clustering , but also has a number of other possible applications . we show the problem to be np hard . nevertheless , we derive a dynamic programming algorithm that proves the case where the underlying graph is a tree to be solvable in polynomial time . next , we propose mixed integer programming formulations for this problem that lead to branch and cut and branch and price algorithms . finally , we introduce a new class of valid inequalities to obtain an enhanced branch and cut . extensive computational experiments are reported .
stable spaces for real time clothing . <eos> we present a technique for learning clothing models that enables the simultaneous animation of thousands of detailed garments in real time . this surprisingly simple conditional model learns and preserves the key dynamic properties of a cloth motion along with folding details . our approach requires no a priori physical model , but rather treats training data as a black box . we show that the models learned with our method are stable over large time steps and can approximately resolve cloth body collisions . we also show that within a class of methods , no simpler model covers the full range of cloth dynamics captured by ours . our method bridges the current gap between skinning and physical simulation , combining benefits of speed from the former with dynamic effects from the latter . we demonstrate our approach on a variety of apparel worn by male and female human characters performing a varied set of motions typically used in video games ( e.g. , walking , running , jumping , etc. ) .
using topes to validate and reformat data in end user programming tools . <eos> end user programming tools offer no data types except string for many categories of data , such as person names and street addresses . consequently , these tools can not automatically validate or reformat these data . to address this problem , we have developed a user extensible model for string like data . each tope in this model is a user defined abstraction that guides the interpretation of strings as a particular kind of data . specifically , each tope implementation contains software functions for recognizing and reformatting instances of that tope 's kind of data . this makes it possible at runtime to distinguish between invalid data , valid data , and questionable data that could be valid or invalid . once identified , questionable and or invalid data can be double checked and possibly corrected , thereby increasing the overall reliability of the data . valid data can be automatically reformatted to any of the formats appropriate for that kind of data . to show the general applicability of topes , we describe new features that topes have enabled us to provide in four tools .
rough sets and the role of the monetary policy in financial stability ( macroeconomic problem ) and the prediction of insolvency in insurance sector ( microeconomic problem ) . <eos> this paper faces two questions related with financial stability . the first one is a macroeconomic problem in which we try to further investigate the role of monetary policy in explaining banking sector fragility and , ultimately , systemic banking crisis . it analyses a large sample of countries in the period <digit> . we find that the degree of central bank independence is one of the key variables to explain financial crisis . however , the effects of the degree of independence are not linear . surprisingly , either a high degree of independence or a high degree of dependence are compatible with a situation of financial stability , while intermediate levels of independence are more likely associated with financial crisis . it seems that it is the uncertainty related with a non clear allocation of monetary policy responsibilities that contributes to financial crisis episodes . the second one is a microeconomic problem the prediction of insolvency in insurance companies . this question has been a concern of several parties stemmed from the perceived need to protect general public and to minimize the costs associated such as the effects on state insurance guaranty funds or the responsibilities for management and auditors . we have developed a bankruptcy prediction model for spanish non life insurance companies and the results obtained are very encouraging in comparison with previous analysis . this model could be used as an early warning system for supervisors in charge of the soundness of these entities and or in charge of the financial system stability . most methods applied in the past to tackle these two problems are techniques of statistical nature and , variables employed in these models do not usually satisfy statistical assumptions what complicates the analysis . we propose an approach to undertake these questions based on rough set theory .
classification of self dual codes of length <digit> . <eos> a complete classification of binary self dual codes of length <digit> is given .
supporting pervasive computing applications with active context fusion and semantic context delivery . <eos> future pervasive computing applications are envisioned to adapt the applications behaviors by utilizing various contexts of an environment and its users . such context information may often be ambiguous and also heterogeneous , which make the delivery of unambiguous context information to real applications extremely challenging . thus , a significant challenge facing the development of realistic and deployable context aware services for pervasive computing applications is the ability to deal with these ambiguous contexts . in this paper , we propose a resource optimized quality assured context mediation framework based on efficient context aware data fusion and semantic based context delivery . in this framework , contexts are first fused by an active fusion technique based on dynamic bayesian networks and ontology , and further mediated using a composable ontological rule based model with the involvement of users or application developers . the fused context data are then organized into an ontology based semantic network together with the associated ontologies in order to facilitate efficient context delivery . experimental results using sunspot and other sensors demonstrate the promise of this approach .
on computing the minimum <digit> path vertex cover and dissociation number of graphs . <eos> the dissociation number of a graph g is the number of vertices in a maximum size induced subgraph of g with vertex degree at most <digit> . a k path vertex cover of a graph g is a subset s of vertices of g such that every path of order k in g contains at least one vertex from s. the minimum <digit> path vertex cover is a dual problem to the dissociation number . for this problem , we present an exact algorithm with a running time of o ( 1.5171 ( n ) ) on a graph with n vertices . we also provide a polynomial time randomized approximation algorithm with an expected approximation ratio of <digit> <digit> for the minimum <digit> path vertex cover . ( c ) <digit> elsevier b.v. all rights reserved .
interval multiplicative transitivity for consistency , missing values and priority weights of interval fuzzy preference relations . <eos> in this paper , the concept of multiplicative transitivity of a fuzzy preference relation , as defined by tanino t. tanino , fuzzy preference orderings in group decision making , fuzzy sets and systems <digit> ( <digit> ) <digit> , is extended to discover whether an interval fuzzy preference relation is consistent or not , and to derive the priority vector of a consistent interval fuzzy preference relation . we achieve this by introducing the concept of interval multiplicative transitivity of an interval fuzzy preference relation and show that , by solving numerical examples , the test of consistency and the weights derived by the simple formulas based on the interval multiplicative transitivity produce the same results as those of linear programming models proposed by xu and chen z.s. xu , j. chen , some models for deriving the priority weights from interval fuzzy preference relations , european journal of operational research <digit> ( <digit> ) <digit> . in addition , by taking advantage of interval multiplicative transitivity of an interval fuzzy preference relation , we put forward two approaches to estimate missing value ( s ) of an incomplete interval fuzzy preference relation , and present numerical examples to illustrate these two approaches .
an o ( n log n ) algorithm for finding a shortest central link segment . <eos> a central link segment of a simple n vertex polygon p is a segment s inside p that minimizes the quantity max ( x epsilon p ) min ( y epsilon s ) d ( l ) ( x , y ) , where d ( l ) ( x , y ) is the link distance between points a and y of p. in this paper we present an o ( n log n ) algorithm for finding a central link segment of p. this generalizes previous results for finding an edge or a segment of p from which p is visible . moreover , in the same time bound , our algorithm finds a central link segment of minimum length . constructing a central link segment has applications to the problems of finding an optimal robot placement in a simply connected polygonal region and determining the minimum value k for which a given polygon is k visible from some segment .
deconstructing switch reference . <eos> this paper develops a new view on switch reference , a phenomenon commonly taken to involve a morphological marker on a verb indicating whether the subject of this verb is coreferent with or disjoint from the subject of another verb . ipropose a new structural source of switch reference marking , which centers around coordination at different heights of the clausal structure , coupled with distinct morphological realizations of the syntactic coordination head . conjunction of two vps has two independent consequences first , only a single external argument is projected second , the coordinator head is realized by some marker a ( the same subject marker ) . conjunction of two vps , by contrast , leads to projection of two independent external arguments and a different realization of the coordination by a marker b ( the different subject marker ) . the hallmark properties of this analysis are that ( i ) subject identity or disjointness is only indirectly tied to the switch reference markers , furnishing a straightforward account of cases where this correlation breaks down ( ii ) switch reference does not operate across fully developed clauses , which accounts for the widely observed featural defectiveness of switch reference clauses ( iii ) same subject and different subject constructions differ in their syntactic structure , thus accommodating cases where the choice of the switch reference markers has an impact on event structure . the analysis is mainly developed on the basis of evidence from the mexican language seri , the papuan language amele , and the north american language kiowa .
an optimized parallel lsqr algorithm for seismic tomography . <eos> the lsqr algorithm developed by paige and saunders ( <digit> ) is considered one of the most efficient and stable methods for solving large , sparse , and ill posed linear ( or linearized ) systems . in seismic tomography , the lsqr method has been widely used in solving linearized inversion problems . as the amount of seismic observations increase and tomographic techniques advance , the size of inversion problems can grow accordingly . currently , a few parallel lsqr solvers are presented or available for solving large problems on supercomputers , but the scalabilities are generally weak because of the significant communication cost among processors . in this paper , we present the details of our optimizations on the lsqr code for , but not limited to , seismic tomographic inversions . the optimizations we have implemented to our lsqr code include reordering the damping matrix to reduce its band width for simplifying the communication pattern and reducing the amount of communication during calculations adopting sparse matrix storage formats for efficiently storing and partitioning matrices using the mpi i o functions to parallelize the date reading and result writing processes providing different data partition strategies for efficiently using computational resources . a large seismic tomographic inversion problem , the full 3d waveform tomography for southern california , is used to explain the details of our optimizations and examine the performance on yellowstone supercomputer at the ncar wyoming supercomputing center ( nwsc ) . the results showed that the required wall time of our code for the same inversion problem is much less than that of the lsqr solver from the petsc library ( balay et al. , <digit> ) .
on computer assisted classification of coupled integrable equations . <eos> we show how the triangularization method of the second author can be successfully applied to the problem of classification of homogeneous coupled integrable equations . the classifications rely on the recent algorithm developed by the first author that requires solving <digit> systems of polynomial equations . we show that these systems can be completely resolved in the case of coupled korteweg de vries , sawada kotera and kaup kupershmidttype equations .
a novel method for fingerprint verification that approaches the problem as a two class pattern recognition problem . <eos> we present a system for fingerprint verification that approaches the problem as a two class pattern recognition problem . the distances of the test fingerprint to the reference fingerprints are normalized by the corresponding mean values obtained from the reference set , to form a five dimensional feature vector . this feature vector is then projected onto a one dimensional karhunen loeve space and then classified into one of the two classes ( genuine or impostor ) .
the uncovering of hidden structures by latent semantic analysis . <eos> latent semantic analysis ( lsa ) is a well known method for information retrieval . it has also been applied as a model of cognitive processing and word meaning acquisition . this dual importance of lsa derives from its capacity to modulate the meaning of words by contexts , dealing successfully with polysemy and synonymy . the underlying reasons that make the method work are not clear enough . we propose that the method works because it detects an underlying block structure ( the blocks corresponding to topics ) in the term by document matrix . in real cases this block structure is hidden because of perturbations . we propose that the correct explanation for lsa must be searched in the structure of singular vectors rather than in the profile of singular values . using the perronfrobenius theory we show that the presence of disjoint blocks of documents is marked by sign homogeneous entries in the vectors corresponding to the documents of one block and zeros elsewhere . in the case of nearly disjoint blocks , perturbation theory shows that if the perturbations are small , the zeros in the leading vectors are replaced by small numbers ( pseudo zeros ) . since the singular values of each block might be very different in magnitude , their order does not mirror the order of blocks . when the norms of the blocks are similar , lsa works fine , but we propose that when the topics have different sizes , the usual procedure of selecting the first k singular triplets ( k being the number of blocks ) should be replaced by a method that selects the perturbed perron vectors for each block .
computing monodromy groups defined by plane algebraic curves . <eos> we present a symbolic numeric method to compute the monodromy group of a plane algebraic curve viewed as a ramified covering space of the complex plane . following the definition , our algorithm is based on analytic continuation of algebraic functions above paths in the complex plane . our contribution is three fold first of all , we show how to use a minimum spanning tree to minimize the length of paths then , we propose a strategy that gives a good compromise between the number of steps and the truncation orders of puiseux expansions , obtaining for the first time a complexity result about the number of steps finally , we present an efficient numerical modular algorithm to compute puiseux expansions above critical points , which is a non trivial task .
stone like representation theorems and three valued filters in r <digit> algebras ( nilpotent minimum algebras ) . <eos> nilpotent minimum algebras ( nm algebras ) are algebraic counterpart of a formal deductive system where conjunction is modeled by the nilpotent minimum t norm , a logic also independently introduced by guo jun wang in the mid 1990s . such algebras are to this logic just what boolean algebras are to the classical propositional logic . in this paper , by introducing respectively the stone topology and a three valued fuzzy stone topology on the set of all maximal filters in an nm algebra , we first establish two analogues for an nm algebra of the well known stone representation theorem for a boolean algebra , which state that the boolean skeleton of an nm algebra is isomorphic to the algebra of all clopen subsets of its stone space and the three valued skeleton is isomorphic to the algebra of all clopen fuzzy subsets of its three valued fuzzy stone space , respectively . then we introduce the notions of boolean filter and of three valued filter in an nm algebra , and finally we prove that three valued filters and closed subsets of the stone space of an nm algebra are in one to one correspondence and boolean filters uniquely correspond to closed subsets of the subspace consisting of all ultrafilters . ( c ) <digit> elsevier b.v. all rights reserved .
an adaptive learning scheme for load balancing with zone partition in multi sink wireless sensor network . <eos> in many researches on load balancing in multi sink wsn , sensors usually choose the nearest sink as destination for sending data . however , in wsn , events often occur in specific area . if all sensors in this area all follow the nearest sink strategy , sensors around nearest sink called hotspot will exhaust energy early . it means that this sink is isolated from network early and numbers of routing paths are broken . in this paper , we propose an adaptive learning scheme for load balancing scheme in multi sink wsn . the agent in a centralized mobile anchor with directional antenna is introduced to adaptively partition the network into several zones according to the residual energy of hotspots around sink nodes . in addition , machine learning is applied to the mobile anchor to make it adaptable to any traffic pattern . through interactions with the environment , the agent can discovery a near optimal control policy for movement of mobile anchor . the policy can achieve minimization of residual energys variance among sinks , which prevent the early isolation of sink and prolong the network lifetime .
interactive visual tools to explore spatio temporal variation . <eos> commongis is a developing software system for exploratory analysis of spatial data . it includes a multitude of tools applicable to different data types and helping an analyst to find answers to a variety of questions . commongis has been recently extended to support exploration of spatio temporal data , i.e. temporally variant data referring to spatial locations . the set of new tools includes animated thematic maps , map series , value flow maps , time graphs , and dynamic transformations of the data . we demonstrate the use of the new tools by considering different analytical questions arising in the course of analysis of thematic spatio temporal data .
multiprocessor system on chip ( mpsoc ) technology . <eos> the multiprocessor system on chip ( mpsoc ) uses multiple cpus along with other hardware subsystems to implement a system . a wide range of mpsoc architectures have been developed over the past decade . this paper surveys the history of mpsocs to argue that they represent an important and distinct category of computer architecture . we consider some of the technological trends that have driven the design of mpsocs . we also survey computer aided design problems relevant to the design of mpsocs .
statistical behavior of joint least square estimation in the phase diversity context . <eos> the images recorded by optical telescopes are often degraded by aberrations that induce phase variations in the pupil plane . several wavefront sensing techniques have been proposed to estimate aberrated phases . one of them is phase diversity , for which the joint least square approach introduced by gonsalves et al. is a reference method to estimate phase coefficients from the recorded images . in this paper , we rely on the asymptotic theory of toeplitz matrices to show that gonsalves ' technique provides a consistent phase estimator as the size of the images grows . no comparable result is yielded by the classical joint maximum likelihood interpretation ( e.g. , as found in the work by paxman et al. ) . finally , our theoretical analysis is illustrated through simulated problems .
integrated in silico approaches for the prediction of ames test mutagenicity . <eos> the bacterial reverse mutation assay ( ames test ) is a biological assay used to assess the mutagenic potential of chemical compounds . in this paper approaches for the development of an in silico mutagenicity screening tool are described . three individual in silico models , which cover both structure activity relationship methods ( sars ) and quantitative structure activity relationship methods ( qsars ) , were built using three different modelling techniques ( <digit> ) an in house alert model which uses sar approach where alerts are generated based on experts judgements ( <digit> ) a knn approach ( k nearest neighbours ) , which is a qsar model where a prediction is given based on outcomes of its k chemical neighbours ( <digit> ) a naive bayesian model ( nb ) , which is another qsar model , where a prediction is derived using a bayesian formula through preselected identified informative chemical features ( e.g. , physico chemical , structural descriptors ) . these in silico models , were compared against two well known alert models ( derek and toxtree ) and also against three different consensus approaches ( categorical bayesian integration approach ( cbi ) , partial least squares discriminate analysis ( pls da ) and simple majority vote approach ) . by applying these integration methods on the validation sets it was shown that both integration models ( pls da and cbi ) achieved better performance than any of the individual models or consensus obtained by simple majority rule . in conclusion , the recommendation of this paper is that when obtaining consensus predictions for ames mutagenicity , approaches like pls da or cbi should be the first choice for the integration as compared to a simple majority vote approach .
visualization and clustering of categorical data with probabilistic self organizing map . <eos> this paper introduces a self organizing map dedicated to clustering , analysis and visualization of categorical data . usually , when dealing with categorical data , topological maps use an encoding stage categorical data are changed into numerical vectors and traditional numerical algorithms ( som ) are run . in the present paper , we propose a novel probabilistic formalism of kohonen map dedicated to categorical data where neurons are represented by probability tables . we do not need to use any coding to encode variables . we evaluate the effectiveness of our model in four examples using real data . our experiments show that our model provides a good quality of results when dealing with categorical data .
stiffness analysis of parallelogram type parallel manipulators using a strain energy method . <eos> stiffness analysis of a general ptpm using an algebraic method . result comparison between the proposed method and a finite element analysis method . a new stiffness index relating the stiffness property to the wrench experienced in a task .
simulation of natural and social process interactions an example from bronze age mesopotamia . <eos> new multimodel simulations of bronze age mesopotamian settlement system dynamics , using advanced object based simulation frameworks , are addressing fine scale interaction of natural processes ( crop growth , hydrology , etc. ) and social processes ( kinship driven behaviors , farming and herding practices , etc. ) on a daily basis across multi enerational model runs . key components of these simulations are representations of initial settlement populations that are demographically and socially plausible , and detailed models of social mechanisms that can produce and maintain realistic textures of social structure and dynamics over time . the simulation engine has broad applicability and is also being used to address modern problems such as agroeconomic sustainability in southeast asia . this article describes the simulation framework and presents results of initial studies , highlighting some social system representations .
newton like dynamics and forward backward methods for structured monotone inclusions in hilbert spaces . <eos> in a hilbert space setting we introduce dynamical systems , which are linked to newton and levenbergmarquardt methods . they are intended to solve , by splitting methods , inclusions governed by structured monotone operators m a b , where a is a general maximal monotone operator , and b is monotone and locally lipschitz continuous . based on the minty representation of a as a lipschitz manifold , we show that these dynamics can be formulated as differential systems , which are relevant to the cauchylipschitz theorem , and involve separately b and the resolvents of a. in the convex subdifferential case , by using lyapunov asymptotic analysis , we prove a descent minimizing property and weak convergence to equilibria of the trajectories . time discretization of these dynamics gives algorithms combining newtons method and forward backward methods for solving structured monotone inclusions .
damage identification of a target substructure with moving load excitation . <eos> this paper presents a substructural damage identification approach under moving vehicular loads based on a dynamic response reconstruction technique . the relationship between two sets of time response vectors from the substructure subject to moving loads is formulated with the transmissibility matrix based on impulse response function in the wavelet domain . only the finite element model of the intact target substructure and the measured dynamic acceleration responses from the target substructure in the damaged state are required . the time histories of moving loads and interface forces on the substructure are not required in the proposed algorithm . the dynamic response sensitivity based method is adopted for the substructural damage identification with the local damage modeled as a reduction in the elemental stiffness factor . the adaptive tikhonov regularization technique is employed to have an improved identification result when noise effect is included in the measurements . numerical studies on a three dimensional box section girder bridge deck subject to a single moving force or a two axle three dimensional moving vehicle are conducted to investigate the performance of the proposed substructural damage identification approach . the simulated local damage can be identified with <digit> % noise in the measured data .
randomized parallel communication ( preliminary version ) . <eos> using a simple finite degree interconnection network among n processors and a straightforward randomized algorithm for packet delivery , it is possible to deliver a set of n packets travelling to unique targets from unique sources in <digit> ( log n ) expected time . the expected delivery time is in other words the depth of the interconnection graph . the b way shufile networks are examples of such . this represents a crude analysis of the transient response to a sudden but very uniform request load on the network . variations in the uniformity of the load are also considered . consider s i packets with randomly chosen targets beginning at a source labelled i . the expected overall delay is then equation where the labelling is chosen so that s <digit> s <digit> . these ideas can be used to guage the asymptotic efficiency of various synchronous parallel algorithms which use such a randomized communications system . the only important assumption is that variations in the physical transmission time along any connection link are negligible in comparison to the amount of work done at a processor .
feature selection for fast speech emotion recognition . <eos> in speech based emotion recognition , both acoustic features extraction and features classification are usually time consuming , which obstruct the system to be real time . in this paper , we proposea novel feature selection ( fsalgorithm to filter out the low efficiency features towards fast speech emotion recognition.firstly , each acoustic feature 's discriminative ability , time consumption and redundancy are calculated . then , we map the original feature space into a nonlinear one to select nonlinear features , which can exploit the underlying relationship among the original features . thirdly , high discriminative nonlinear feature with low time consumption is initially preserved . finally , a further selection is followed to obtain low redundant features based on these preserved features . the final selected nonlinear features are used in features ' extraction and features ' classification in our approach , we call them qualified features . the experimental results demonstrate that recognition time consumption can be dramatically reduced in not only the extraction phase but also the classification phase . moreover , a competitive of recognition accuracy has been observed in the speech emotion recognition .
automated inspection planning of free form shape parts by laser scanning . <eos> the inspection operation accounts for a large portion of manufacturing lead time , and its importance in quality control can not be overemphasized . in recent years , due to the development of laser technology , the accuracy of laser scanners has been improved significantly so that they can be used in a production environment . they are noncontact type measuring devices and usually have the scanning speed that is <digit> times faster than that of coordinate measuring machines . this laser scanning technology provides us a platform that enables us to perform a <digit> % inspection of complicated shape parts . this research proposes algorithms that lead to the automation of laser scanner based inspection operations . the proposed algorithms consist of three steps firstly , all possible accessible directions at each sampled point on a part surface are generated considering constraints existing in a laser scanning operation . the constraints include satisfying the view angle , the depth of view , checking interference with a part , and avoiding collision with the probe . secondly , the number of scans and the most desired direction for each scan are calculated . finally , the scan path that gives the minimum scan time is generated . the proposed algorithms are applied to sample parts and the results are discussed .
gbf a grammar based filter for internet applications . <eos> observing network traffic is necessary for achieving different purposes such as system performance , network debugging and or information security . observations , as such , are obtained from low level monitors that may record a large volume of relevant and irrelevant events . thus adequate filters are needed to pass interesting information only . this work presents a multilayer system , gbf that integrates both packet ( low level ) and document ( high level ) filters . actually , the design of gbf is grammar based so that it relies upon a set of context free grammars to carry out various processes , specially the document reconstruction process . gbf consists of three layers , acquisition layer , packet filter layer , and reconstruction layer . the performance of the reconstruction process is evaluated in terms of the time consumed during service separation and session separation tasks .
enhanced particle swarm optimizer incorporating a weighted particle . <eos> this study proposes an enhanced particle swarm optimizer incorporating a weighted particle ( epsowp ) to improve the evolutionary performance for a set of benchmark functions . in conventional particle swarm optimizer ( pso ) , there are two principal forces to guide the moving direction of each particle . however , if the current particle lies too close to either the personal best particle or the global best particle , the velocity is mainly updated by only one term . as a result , search step becomes smaller and the optimization of the swarm is likely to be trapped into a local optimum . to address this problem , we define a weighted particle for incorporation into the particle swarm optimization . because the weighted particle has a better opportunity getting closer to the optimal solution than the global best particle during the evolution , the epsowp is capable of guiding the swarm to a better direction to search the optimal solution . simulation results show the effectiveness of the epsowp , which outperforms various evolutionary algorithms on a selected set of benchmark functions . furthermore , the proposed epsowp is applied to controller design and parameter identification for an inverted pendulum system as well as parameter learning of neural network for function approximation to show its viability to solve practical design problems .
media access protocol for a coexisting cognitive femtocell network . <eos> femtocell networks are widely deployed to extend cellular network coverage into indoor environments such as large office spaces and homes . cognitive radio functionality can be implemented in femtocell networks based on an overlay mechanism under the assumption of a hierarchical access scenario . this study introduces a novel femtocell network architecture , that is characterized by a completely autonomous femtocell bandwidth access and a distributed media access control protocol for supporting data and real time traffic . the detailed description of the architecture and media access protocol is presented . furthermore , in depth theoretical analysis is performed on the proposed media access protocol using discrete time markov chain modeling to validate the effectiveness of the proposed protocol and architecture .
integrating computer animation and multimedia . <eos> multimedia provides an immensely powerful tool for the dissemination of both information and entertainment . current multimedia presentations consist of synchronised excerpts of media ( such as sound , video gi text ) which are coordinated by an author to ensure a clear narrative is presented to the audience . however each of the segments of the presentation consist of previously recorded footage , only the timing and synchronisation are dynamically constructed . the next logical advance for such systems is therefore to include the capability of generating material ' on the fly ' in response to the actions of the audience . this paper describes a mechanism for using computer animation to generate this interactive material . unlike previous animation techniques the approach presented here is suitable for use in constructing a storyline which the author can control , but the user can influence . in order to allow such techniques to be used we also present a multimedia authoring gr playback system which incorporates interactive animation with existing media .
an ontology for supporting communities of practice . <eos> in the context of the palette project aimed at enhancingallindividual and organizational learning in communities of practice ( cops ) , we are developing knowledge management ( km ) services . our approach is based on an ontology dedicated to cops and built from analysis of information sources about eleven cops available in palette project . this ontology aims both at modeling the members of the cop and at annotating the cop knowledge resources . the paper describes our method for building this ontology , its structure and contents and it analyses our experience feedback from the cooperative building of this ontology .
a set of neural tools for human computer interactions application to the handwritten character recognition , and visual speech recognition problems . <eos> this paper presents a new technique of data coding and an associated set of homogenous processing tools for the development of human computer interactions ( hci ) . the proposed technique facilitates the fusion of different sensorial modalities and simplifies the implementations . the coding takes into account the spatio temporal nature of the signals to be processed in the framework of a sparse representation of data . neural networks adapted to such a representation of data are proposed to perform the recognition tasks . their development is illustrated by two examples one of on line handwritten character recognition and the other of visual speech recognition .
impact of sub optimal checkpoint intervals on application efficiency in computational clusters . <eos> as computational clusters rapidly grow in both size and complexity , system reliability and , in particular , application resilience have become increasingly important factors to consider in maintaining efficiency and providing improved computational performance over predecessor systems . one commonly used mechanism for providing application fault tolerance in parallel systems is the use of checkpointing . by making use of a multi cluster simulator , we study the impact of sub optimal checkpoint intervals on overall application efficiency . by using a model of a <digit> node cluster and workload statistics from los alamos national laboratory to parameterize the simulator , we find that dramatically overestimating the amtti has a fairly minor impact on application efficiency while potentially having a much more severe impact on user centric performance metrics such a queueing delay . we compare and contrast these results with the trends predicted by an analytical model .
an approach to a content based retrieval of multimedia data . <eos> this paper presents a data model tailored for multimedia data representation , along with the main characteristics of a multimedia query language that exploits the features of the proposed model . the model addresses data presentation , manipulation and content based retrieval . it consists of three parts a multimedia description model , which provides a structural view of raw multimedia data , a multimedia presentation model , and a multimedia interpretation model which allows semantic information to be associated with multimedia data . the paper focuses on the structuring of a multimedia data model which provides support for content based retrieval of multimedia data . the query language is an extension of a traditional query language which allows restrictions to be expressed on features , concepts , and the structural aspects of the objects of multimedia data and the formulation of queries with imprecise conditions . the result of a query is an approximate set of database objects which partially match such a query .
monte carlo em with importance reweighting and its applications in random effects models1 . <eos> in this paper we propose a new monte carlo em algorithm to compute maximum likelihood estimates in the context of random effects models . the algorithm involves the construction of efficient sampling distributions for the monte carlo implementation of the e step , together with a reweighting procedure that allows repeatedly using a same sample of random effects . in addition , we explore the use of stochastic approximations to speed up convergence once stability has been reached . our algorithm is compared with that of mcculloch ( <digit> ) . extensions to more general problems are discussed .
a perceptual approach for stereoscopic rendering optimization . <eos> the traditional way of stereoscopic rendering requires rendering the scene for left and right eyes separately which doubles the rendering complexity . in this study , we propose a perceptually based approach for accelerating stereoscopic rendering . this optimization approach is based on the binocular suppression theory , which claims that the overall percept of a stereo pair in a region is determined by the dominant image on the corresponding region . we investigate how binocular suppression mechanism of human visual system can be utilized for rendering optimization . our aim is to identify the graphics rendering and modeling features that do not affect the overall quality of a stereo pair when simplified in one view . by combining the results of this investigation with the principles of visual attention , we infer that this optimization approach is feasible if the high quality view has more intensity contrast . for this reason , we performed a subjective experiment , in which various representative graphical methods were analyzed . the experimental results verified our hypothesis that a modification , applied on a single view , is not perceptible if it decreases the intensity contrast , and thus can be used for stereoscopic rendering .
using traditional loop unrolling to fit application on a new hybrid reconfigurable architecture . <eos> this paper presents a strategy to modify a sequential implementation of an h. <digit> avc motion estimation to run on a new reconfigurable architecture called rosa . the modifications aim to provide more parallelism that will be exploited by the architecture . in the strategy presented in this paper we used traditional loop unrolling and profile information as techniques to modify the application and to generate a best fit solution to rosa architecture .
evaluating fluid semantics for passive stochastic process algebra cooperation . <eos> fluid modelling is a next generation technique for analysing massive performance models . passive cooperation is a popular cooperation mechanism frequently used by performance engineers . therefore having an accurate translation of passive cooperation into a fluid model is of direct practical application . we compare different existing styles of fluid model translations of passive cooperation in a stochastic process algebra and show how the previous model can be improved upon significantly . we evaluate the new passive cooperation fluid semantics and show that the first order fluid model is a good approximation to the dynamics of the underlying continuous time markov chain . we show that in a family of possible translations to the fluid model , there is an optimal translation which can be expected to introduce least error . finally , we use these new techniques to show how the scalability of a passively cooperating distributed software architecture could be assessed .
using new media to improve self help for clients and staff . <eos> one of the most common frustrations for any person looking for technical support is actually finding effective technical support . even if a solution seems clear , it can be misunderstood if the vernacular is not just right . a large part of a successful support call involves being able to determine the actual problem based on the information the client provides . help desk analysts must have the ability to translate non tech descriptions to identify a problem in technical terms and then communicate a solution using vernacular the client can understand . this process is always a little different . if we aim to be successful analysts , we must speak different languages in order to help our clients . based on this logic , it stands to reason that our self help documentation must do the same . providing a variety of methods to get self help ensures a message will be received by a wider audience . in the world of modern media , audiences are presented with many ways to consume information . this ensures the message is heard by the most people in a manner that is the most appealing and the most clear . new methods of consuming information have become possible as the face of mainstream media has become democratized over the last few years . this is thanks largely to the fact that the tools needed to create and distribute content have become affordable and readily available to anyone with a bit of technical skill . anyone with a laptop , a webcam and a little imagination can and do create content . considering all of this , we asked ourselves , why should n't we . we have found that creating content in new media is relatively easy and fun . finding and creating new methods to deliver content positively engages and challenges our help desk team . thinking about how to best use new media requires help desk analysts to rethink otherwise standardized and mundane processes and create fresh perspectives . the creation and production of new media establishes stronger ownership of procedures and process . we would like to share the following from our ongoing experiences with new media at our help desk general issues we see with clients finding help how creating new media creates stronger ownership and morale with staff expanding the technical skills of help desk staff how using new media improves our client experience casting a wider net ( ensuring a message gets to the most people ) how we use new media and what we have done with it how to make your own video podcast in 1,345 easy steps
a framework for preservation of cloud users data privacy using dynamic reconstruction of metadata . <eos> in the rising paradigm of cloud computing , attainment of sustainable levels of cloud users trust in using cloud services is directly dependent on effective mitigation of its associated impending risks and resultant security threats . among the various indispensible security services required to ensure effective cloud functionality leading to enhancement of users confidence in using cloud offerings , those related to the preservation of cloud users data privacy are significantly important and must be matured enough to withstand the imminent security threats , as emphasized in this research paper . this paper highlights the possibility of exploiting the metadata stored in cloud 's database in order to compromise the privacy of users data items stored using a cloud provider 's simple storage service . it , then , proposes a framework based on database schema redesign and dynamic reconstruction of metadata for the preservation of cloud users data privacy . using the sensitivity parameterization parent class membership of cloud database attributes , the database schema is modified using cryptographic as well as relational privacy preservation operations . at the same time , unaltered access to database files is ensured for the cloud provider using dynamic reconstruction of metadata for the restoration of original database schema , when required . the suitability of the proposed technique with respect to private cloud environments is ensured by keeping the formulation of its constituent steps well aligned with the recommendations proposed by various standards development organizations working in this domain .
a systematic literature review on soa migration . <eos> when service orientation was introduced as the solution for retaining and rehabilitating legacy assets , both researchers and practitioners proposed techniques , methods , and guidelines for soa migration . with so much hype surrounding soa , it is not surprising that the concept was interpreted in many different ways , and consequently , different approaches to soa migration were proposed . accordingly , soon there was an abundance of methods that were hard to compare and eventually adopt . against this backdrop , this paper reports on a systematic literature review that was conducted to extract the categories of soa migration proposed by the research community . we provide the state of the art in soa migration approaches , and discuss categories of activities carried out and knowledge elements used or produced in those approaches . from such categorization , we derive a reference model , called soa migration frame of reference , that can be used for selecting and defining soa migration approaches . as a co product of the analysis , we shed light on how soa migration is perceived in the field , which further points to promising future research directions . copyright <digit> john wiley sons , ltd .
application of projection pursuit learning to boundary detection and deblurring in images . <eos> projection pursuit learning networks ( pplns ) have been used in many fields of research but have not been widely used in image processing . in this paper we demonstrate how this highly promising technique may be used to connect edges and produce continuous boundaries . we also propose the application of ppln to deblurring a degraded image when little or no a priori information about the blur is available . the ppln was successful at developing an inverse blur filter to enhance blurry images . theory and background information on projection pursuit regression ( ppr ) and ppln are also presented .
learning to transform time series with a few examples . <eos> we describe a semisupervised regression algorithm that learns to transform one time series into another time series given examples of the transformation . this algorithm is applied to tracking , where a time series of observations from sensors is transformed to a time series describing the pose of a target . instead of defining and implementing such transformations for each tracking task separately , our algorithm learns a memoryless transformation of time series from a few example input output mappings . the algorithm searches for a smooth function that fits the training examples and , when applied to the input time series , produces a time series that evolves according to assumed dynamics . the learning procedure is fast and lends itself to a closed form solution . it is closely related to nonlinear system identification and manifold learning techniques . we demonstrate our algorithm on the tasks of tracking rfid tags from signal strength measurements , recovering the pose of rigid objects , deformable bodies , and articulated bodies from video sequences . for these tasks , this algorithm requires significantly fewer examples compared to fully supervised regression algorithms or semisupervised learning algorithms that do not take the dynamics of the output time series into account .
almost periodic solutions to abstract semilinear evolution equations with stepanov almost periodic coefficients . <eos> in this paper , almost periodicity of the abstract semilinear evolution equation u ' ( t ) a ( t ) u ( t ) f ( t , u ( t ) ) with stepanov almost periodic coefficients is discussed . we establish a new composition theorem of stepanov almost periodic functions and , with its help , we study the existence and uniqueness of almost periodic solutions to the above semilinear evolution equation . our results are even new for the case of a ( t ) equivalent to a.
phoenix based clone detection using suffix trees . <eos> a code clone represents a sequence of statements that are duplicated in multiple locations of a program . clones often arise in source code as a result of multiple cut paste operations on the source , or due to the emergence of crosscutting concerns . programs containing code clones can manifest problems during the maintenance phase . when a fault is found or an update is needed on the original copy of a code section , all similar clones must also be found so that they can be fixed or updated accordingly . the ability to detect clones becomes a necessity when performing maintenance tasks . however , if done manually , clone detection can be a slow and tedious activity that is also error prone . a tool that can automatically detect clones offers a significant advantage during software evolution . with such an automated detection tool , clones can be found and updated in less time . moreover , restructuring or refactoring of these clones can yield better performance and modularity in the program.this paper describes an investigation into an automatic clone detection technique developed as a plug in for microsoft 's new phoenix framework . our investigation finds function level clones in a program using abstract syntax trees ( asts ) and suffix trees . an ast provides the structural representation of the code after the lexical analysis process . the ast nodes are used to generate a suffix tree , which allows analysis on the nodes to be performed rapidly . we use the same methods that have been successfully applied to find duplicate sections in biological sequences to search for matches on the suffix tree that is generated , which in turn reveal matches in the code .
slimeware engineering devices with slime mold . <eos> the plasmodium of the acellular slime mold physarum polycephalum is a gigantic single cell visible to the unaided eye . the cell shows a rich spectrum of behavioral patterns in response to environmental conditions . in a series of simple experiments we demonstrate how to make computing , sensing , and actuating devices from the slime mold . we show how to program living slime mold machines by configurations of repelling and attracting gradients and demonstrate the workability of the living machines on tasks of computational geometry , logic , and arithmetic .
automated aspect oriented decomposition of process control systems for ultra high dependability assurance . <eos> this paper presents a method for decomposing process control systems . this decomposition method is automated , meaning that a series of principles that can be evolved to support automated tools are given to help a designer decompose complex systems into a collection of simpler components . each component resulting from the decomposition process can be designed and implemented independently of the other components . also , these components can be tested or verified by the end user independently of each other . moreover , the system properties , such as safety , stability , and reliability , can be mathematically inferred from the properties of the individual components . these components are referred to as ideal ( independently developable end user assessable logical ) components . this decomposition method is applied to a case study specified by the high integrity systems group at sandia national labs , which involves the control of a future version of the bay area rapid transit ( bart ) system .
diffusion confusion based light weight security for item rfid tag reader communication . <eos> in this paper we propose a challenge response protocol called dcstar , which takes a novel approach to solve security issues that are specific to low cost item rfid tags . our dcstar protocol is built upon light weight primitives such as <digit> bit random number generator , exclusive or , and cyclic redundancy check and utilizing these primitives it also provides a simple diffusion confusion cipher to encrypt the challenge and response from the tag to the rfid reader . as a result our protocol achieves rfid tag reader server mutual authentication , communicating data confidentiality and integrity , secure key distribution and key protection . it also provides an efficient way for consumers to verify whether tagged items are genuine or fake and to protect consumers ' privacy while carrying tagged items .
on solutions of functional integral equations of urysohn type on an unbounded interval . <eos> in this paper we establish the existence of solutions of functional integral and quadratic urysohn integral on the interval r ( ) <digit> , infinity ) . the technique of proving applied in this paper is based on the concept of measure of noncompactness and the fixed point theorem . some new results are given . ( c ) <digit> elsevier ltd. all rights reserved .
a cultural probes study on video sharing and social communication on the internet . <eos> the focus of this article is the link between video sharing and interpersonal communication on the internet . previous works on social television systems belong to two categories <digit> ) studies on how collocated groups of viewers socialize while watching tv , and <digit> ) studies on novel social tv applications ( e.g. experimental set ups ) and devices ( e.g. ambient displays ) that provide technological support for tv sociability over a distance . the main shortcoming of those studies is that they have not considered the dominant contemporary method of social tv . early adopters of technology have been watching and sharing video online . we employed cultural probes in order to gain in depth information about the social aspect of video sharing on the internet . our sample consisted of six heavy users of internet video , watching an average of at least one hour of internet video a day . in particular , we explored how they are integrating video into their daily social communication practices . we found that internet video is shared and discussed with distant friends . moreover , the results of the study indicate several opportunities and threats for the development of integrated mass and interpersonal communication applications and services .
phenotypic modulation of vascular smooth muscle cells . <eos> the smooth muscle myosin heavy chain ( mhc ) gene and its isoforms are excellent molecular markers that reflect smooth muscle phenotypes . the smemb nonmuscle myosin heavy chain b ( nmhc b ) is a distinct mhc gene expressed predominantly in phenotypically modulated smcs ( synthetic type smc ) . to dissect the molecular mechanisms governing phenotypic modulation of smcs , we analyzed the transcriptional regulatory mechanisms underlying expression of the smemb gene . we previously reported two transcription factors , bteb2 iklf and hex , which transactivate the smemb gene promoter based on the transient reporter transfection assays . bteb2 iklf is a zinc finger transcription factor , whereas hex is a homeobox protein . bteb2 iklf expression in smcs is downregulated with vascular development in vivo but upregulated in cultured smcs and in neointima in response to vascular injury after balloon angioplasty . bteb2 iklf and hex activate not only the smemb gene but also other genes activated in synthetic smcs including plasminogen activator inhibitor <digit> ( pai <digit> ) , inos , pdgf a , egr <digit> , and vegf receptors . mitogenic stimulation activates bteb2 iklf gene expression through mek1 and egr <digit> . elevation of intracellular camp is also important in phenotypic modulation of smcs , because the smemb promoter is activated under cooperatively by camp response element binding protein ( creb ) and hex .
intent specifications an approach to building human centered specifications . <eos> this paper examines and proposes an approach to writing software specifications , based on research in systems theory , cognitive psychology , and human machine interaction . the goal is to provide specifications that support human problem solving and the tasks that humans must perform in software development and evolution . a type of specification , called intent specifications , is constructed upon this underlying foundation .
point equivalence of second order odes maximal invariant classification order . <eos> we show that the local equivalence problem of second order ordinary differential equations under point transformations is completely characterized by differential invariants of order at most <digit> and that this upper bound is sharp . we also demonstrate that , modulo cartan duality and point transformations , the painlev i equation can be characterized as the simplest second order ordinary differential equation belonging to the class of equations requiring 10th order jets for their classification .
an improved evaluation of ladder logic diagrams and petri nets for the sequence controller design in manufacturing systems . <eos> sequence controller designs play a key role in advanced manufacturing systems . traditionally , the ladder logic diagram ( lld ) has been widely applied to programmable logic controllers ( plc ) , while recently the petri net ( pn ) has emerged as an alternative tool for the sequence control of complex systems . the evaluation of both approaches has become crucial and has thus attracted attention .
lightweight detection of node presence in manets . <eos> while mobility in the sense of node movement has been an intensively studied aspect of mobile ad hoc networks ( manets ) , another aspect of mobility has not yet been subjected to systematic research nodes may not only move around but also enter and leave the network . in fact , many proposed protocols for manets exhibit worst case behavior when an intended communication partner is currently not present . therefore , knowing whether a given node is currently present in the network can often help to avoid unnecessary overhead . in this paper , we present a solution to the presence detection problem . it uses a bloom filter based beaconing mechanism to aggregate and distribute information about the presence of network nodes . we describe the algorithm and discuss design alternatives . we assess the algorithms properties both analytically and through simulation , and thereby underline the effectiveness and applicability of our approach .
an integrated toolchain for model based functional safety analysis . <eos> we design a complete toolchain for integrating fault tolerance analysis into modeling . the goal of this work is to bridge the gap between the different specialized tools available . having an integrated environment will reduce errors , ensure coherence and simplify analysis .
scale invariant feature matching using rotation invariant distance for remote sensing image registration . <eos> scale invariant feature transform ( sift ) has been widely used in image matching . but when sift is introduced in the registration of remote sensing images , the keypoint pairs which are expected to be matched are often assigned two different value of main orientation owing to the significant difference in the image intensity between remote sensing image pairs , and therefore a lot of incorrect matches of keypoints will appear . this paper presents a method using rotation invariant distance instead of euclid distance to match the scale invariant feature vectors associated with the keypoints . in the proposed method , the feature vectors are reorganized into feature matrices , and fast fourier transform ( fft ) is introduced to compute the rotation invariant distance between the matrices . much more correct matches are obtained by the proposed method since the rotation invariant distance is independent of the main orientation of the keypoints . experimental results indicate that the proposed method improves the match performance compared to other state of art methods in terms of correct match rate and aligning accuracy .
computer related gender differences . <eos> computer related gender differences are examined using survey responses from <digit> college students . issues studied include gender differences regarding interest and enjoyment of both using a computer and computer programming . interesting gender differences with implications for teaching are examined for the groups ( family , teachers , friends , others ) that have the most influence on students ' interest in computers . traditional areas such as confidence , career understanding and social bias are also discussed . preliminary results for a small sample of technology majors indicate that computer majors have unique interests and attitudes compared to other science majors .
analysis of eeg signals by combining eigenvector methods and multiclass support vector machines . <eos> a new approach based on the implementation of multiclass support vector machine ( svm ) with the error correcting output codes ( ecoc ) is presented for classification of electroencephalogram ( eeg ) signals . in practical applications of pattern recognition , there are often diverse features extracted from raw data which needs recognizing . decision making was performed in two stages feature extraction by eigenvector methods and classification using the classifiers trained on the extracted features . the aim of the study is classification of the eeg signals by the combination of eigenvector methods and multiclass svm . the purpose is to determine an optimum classification scheme for this problem and also to infer clues about the extracted features . the present research demonstrated that the eigenvector methods are the features which well represent the eeg signals and the multiclass svm trained on these features achieved high classification accuracies .
lambda rbac programming with role based access control . <eos> we study mechanisms that permit program components to express role constraints on clients , focusing on programmatic security mechanism , which permit access controls to be expressed , in situ , as part of the code realizing basic functionality . in this setting , two questions immediately arise . ( <digit> ) the user of a component faces the issue of safety is a particular role sufficient to use the component ( <digit> ) the component designer faces the dual issue of protection is a particular role demanded in all execution paths of the component we provide a formal calculus and static analysis to answer both questions .
output only modal analysis using continuous scan laser doppler vibrometry and application to a 20kw wind turbine . <eos> continuous scan laser doppler vibrometry ( csldv ) is a technique where the measurement point continuously sweeps over a structure while measuring , capturing both spatial and temporal information . the continuous scan approach can greatly accelerate measurements , allowing one to capture spatially detailed mode shapes in the same amount of time that conventional methods require to measure the response at a single point . the method is especially beneficial when testing large structures , such as wind turbines , that have low natural frequencies and hence may require very long time records at each measurement point . several csldv methods have been presented that use sinusoidal excitation or impulse excitation , but csldv has not previously been employed with an unmeasured , broadband random input . this work extends csldv to that class of input , developing an output only modal analysis method ( oma csldv ) . a recently developed algorithm for linear time periodic system identification , which makes use of harmonic power spectra and the harmonic transfer function concept developed by wereley <digit> , is used in conjunction with csldv measurements . one key consideration , the choice of the scan frequency , is explored . the proposed method is validated on a randomly excited free free beam , where one dimensional mode shapes are captured by scanning the laser along the length of the beam . the first seven natural frequencies and mode shapes are extracted from the harmonic power spectrum of the vibrometer signal and show good agreement with the analytically derived modes of the beam . the method is then applied to identify the mode shapes of a parked 20kw wind turbine using a ground based laser and with only a light breeze providing excitation .
preferences in wikipedia abstracts empirical findings and implications for automatic entity summarization . <eos> we empirically study how wikipedians summarize entity descriptions in practice . we compare entity descriptions in dbpedia with their wikipedia abstracts . we analyze the length of a summary and the priorities of property values . we analyze the priorities of , diversity of , and correlation between properties . implications for automatic entity summarization are drawn from the findings .
multi organ localization with cascaded global to local regression and shape prior . <eos> we propose a fast and robust method for multiple organs localization . our method provides organ dedicated confidence maps for each organ . it extends the cascade of random forest with additional shape prior . the values of the testing and learning parameters can be explained physically . we evaluate our method on <digit> ct volumes and show its good accuracy .
ordered interval routing schemes . <eos> an interval routing scheme ( irs ) represents the routing tables in a network in a space efficient way by labeling each vertex with an unique integer address , and the outgoing edges at each vertex with disjoint subintervals of these addresses . an irs that has at most k intervals per edge label is called a k irs . in this paper , we propose a new type of interval routing scheme , called an ordered interval routing scheme ( oirs ) , that uses an ordering of the outgoing edges at each vertex and allows non disjoint intervals in the labels of those edges . we show for a number of graph classes that using an oirs instead of an irs reduces the size of the routing tables in the case of optimal routing , i.e. , routing along shortest paths . we show that optimal routing in any k tree is possible using an oirs with at most 2k <digit> <digit> k <digit> intervals per edge label , although the best known result for an irs is 2k <digit> <digit> k <digit> intervals per edge label . any torus has an optimal <digit> oirs , although it may not have an optimal <digit> irs . we present similar results for the petersen graph , k garland graphs and a few other graphs .
performance analysis in non rayleigh and non rician communications channels . <eos> this paper investigates the probability of erasure for mobile communication channels containing limited number of scatterers . two kinds of channels with and without line of sight are examined . the resultant data is depicted by graphs to express the differences in existing theoretical models more clearly . the results indicate that the probability of erasure is different from that of predicted by both rayleigh and rician models for small number of scatterers .
computational geometry column <digit> . <eos> the recent result that n congruent balls in r ( d ) have at most <digit> distinct geometric permutations is described .
trends of environmental information systems in the context of the european water framework directive . <eos> in europe , the development of environmental information systems for the water domain is heavily influenced by the need to support the processes of the european water framework directive ( wfd ) . the aim of the wfd is to ensure that all european waters , these being groundwater , surface or coastal waters , are protected according to a common standard . while the wfd itself does only include concrete information technology ( it ) recommendations on a very high level of data exchange , regional and or national environmental agencies build or adapt their information systems according to their specific requirements in order to deliver the results for the first wfd reporting phase on time . moreover , as the wfd requires a water management policy centered on natural river basin districts instead of administrative and political regions , the agencies have to co ordinate their work , possibly across national borders . on this background , the present article analyses existing it recommendations for the wfd implementation strategy and motivates the need to develop an it framework architecture that comprises different views such as an organisational , a process , a data and a functional view . after having presented representative functions of operational water body information systems for the thematic and the co operation layer , the article concludes with a summary of future it developments that are required to efficiently support the wfd implementation .
a finite volume method for viscous incompressible flows using a consistent flux reconstruction scheme . <eos> an incompressible navier stokes solver using curvilinear body fitted collocated grid has been developed to solve unconfined flow past arbitrary two dimensional body geometries . in this solver , the full navier stokes equations have been solved numerically in the physical plane itself without using any transformation to the computational plane . for the proper coupling of pressure and velocity field on collocated grid , a new scheme , designated ' consistent flux reconstruction ' ( cfr ) scheme , has been developed . in this scheme , the cell face centre velocities are obtained explicitly by solving the momentum equations at the centre of the cell faces . the velocities at the cell centres are also updated explicitly by solving the momentum equations at the cell centres . by resorting to such a fully explicit treatment considerable simplification has been achieved compared to earlier approaches . in the present investigation the solver has been applied to unconfined flow past a square cylinder at zero and non zero incidence at low and moderate reynolds numbers and reasonably good agreement has been obtained with results available from literature . copyright ( c ) <digit> john wiley sons , ltd .
practical online retrieval evaluation . <eos> online evaluation is amongst the few evaluation techniques available to the information retrieval community that is guaranteed to reflect how users actually respond to improvements developed by the community . broadly speaking , online evaluation refers to any evaluation of retrieval quality conducted while observing user behavior in a natural context . however , it is rarely employed outside of large commercial search engines due primarily to a perception that it is impractical at small scales . the goal of this tutorial is to familiarize information retrieval researchers with state of the art techniques in evaluating information retrieval systems based on natural user clicking behavior , as well as to show how such methods can be practically deployed . in particular , our focus will be on demonstrating how the interleaving approach and other click based techniques contrast with traditional offline evaluation , and how these online methods can be effectively used in academic scale research . in addition to lecture notes , we will also provide sample software and code walk throughs to showcase the ease with which interleaving and other click based methods can be employed by students , academics and other researchers .
a privacy preserving clustering approach toward secure and effective data analysis for business collaboration . <eos> the sharing of data has been proven beneficial in data mining applications . however , privacy regulations and other privacy concerns may prevent data owners from sharing information for data analysis . to resolve this challenging problem , data owners must design a solution that meets privacy requirements and guarantees valid data clustering results . to achieve this dual goal , we introduce a new method for privacy preserving clustering called dimensionality reduction based transformation ( drbt ) . this method relies on the intuition behind random projection to protect the underlying attribute values subjected to cluster analysis . the major features of this method are ( a ) it is independent of distance based clustering algorithms ( b ) it has a sound mathematical foundation and ( c ) it does not require cpu intensive operations . we show analytically and empirically that transforming a data set using drbt , a data owner can achieve privacy preservation and get accurate clustering with a little overhead of communication cost .
unified read requests . <eos> most work on multimedia storage systems has assumed that clients will be serviced using a round robin strategy . the server services the clients in rounds and each client is allocated a time slice within that round . furthermore , most such algorithms are evaluated on the basis of a tightly specified cost function . this is the basis for well known algorithms such as fcfs , scan , scan edf , etc. in this paper , we describe a request merging ( rm ) module that takes as input , a set of client requests , and a set of constraints on the desired performance such as client waiting time or maximum disk bandwidth , and a cost function . it produces as output , a unified read request ( urr ) , telling the storage server which data items to read , and when the clients would like these data items to be delivered to them . given a cost function cf , a urr is optimal if there is no other urr satisfying the constraints with a lower cost . we present three algorithms in this paper , each of which accomplishes this kind of request merging . the first algorithm opturr is guaranteed to produce minimal cost urrs with respect to arbitrary cost functions . in general , the problem of computing an optimal urr is np complete , even when only two data objects are considered . to alleviate this problem , we develop two other algorithms , called greedyurr and fasturr that may produce sub optimal urrs , but which have some nicer computational properties . we will report on the pros and cons of these algorithms through an experimental evaluation .
brain computer evolutionary multiobjective optimization a genetic algorithm adapting to the decision maker . <eos> the centrality of the decision maker ( dm ) is widely recognized in the multiple criteria decision making community . this translates into emphasis on seamless human computer interaction , and adaptation of the solution technique to the knowledge which is progressively acquired from the dm . this paper adopts the methodology of reactive search optimization ( rso ) for evolutionary interactive multiobjective optimization . rso follows to the paradigm of learning while optimizing , through the use of online machine learning techniques as an integral part of a self tuning optimization scheme . user judgments of couples of solutions are used to build robust incremental models of the user utility function , with the objective to reduce the cognitive burden required from the dm to identify a satisficing solution . the technique of support vector ranking is used together with a k fold cross validation procedure to select the best kernel for the problem at hand , during the utility function training procedure . experimental results are presented for a series of benchmark problems .
linear separability of gene expression data sets . <eos> we study simple geometric properties of gene expression data sets , where samples are taken from two distinct classes ( e.g. , two types of cancer ) . specifically , the problem of linear separability for pairs of genes is investigated . if a pair of genes exhibits linear separation with respect to the two classes , then the joint expression level of the two genes is strongly correlated to the phenomena of the sample being taken from one class or the other . this may indicate an underlying molecular mechanism relating the two genes and the phenomena ( e. g. , a specific cancer ) . we developed and implemented novel efficient algorithmic tools for finding all pairs of genes that induce a linear separation of the two sample classes . these tools are based on computational geometric properties and were applied to <digit> publicly available cancer data sets . for each data set , we computed the number of actual separating pairs and compared it to an upper bound on the number expected by chance and to the numbers resulting from shuffling the labels of the data at random empirically . seven out of these <digit> data sets are highly separable . statistically , this phenomenon is highly significant , very unlikely to occur at random . it is therefore reasonable to expect that it manifests a functional association between separating genes and the underlying phenotypic classes .
a language for representing and extracting 3d geometry semantics from paper based sketches . <eos> the key contribution is a visual language to formally represent form geometry semantics on paper . parsing the language allows for the automatic generation of 3d virtual models . a proof of concept prototype tool was implemented . the language is capable to roughly model forms with linear topological ordering . evaluation results show that practising designers would use the language .
decentralized list scheduling . <eos> classical list scheduling is a very popular and efficient technique for scheduling jobs for parallel and distributed platforms . it is inherently centralized . however , with the increasing number of processors , the cost for managing a single centralized list becomes too prohibitive . a suitable approach to reduce the contention is to distribute the list among the computational units each processor only has a local view of the work to execute . thus , the scheduler is no longer greedy and standard performance guarantees are lost .
antenna impedance matching with neural networks . <eos> impedance matching between transmission lines and antennas is an important and fundamental concept in electromagnetic theory . one definition of antenna impedance is the resistance and reactance seen at the antenna terminals or the ratio of electric to magnetic fields at the input . the primary intent of this paper is real time compensation for changes in the driving point impedance of an antenna due to frequency deviations . in general , the driving point impedance of an antenna or antenna array is computed by numerical methods such as the method of moments or similar techniques . some configurations do lend themselves to analytical solutions , which will be the primary focus of this work . this paper employs a neural control system to match antenna feed lines to two common antennas during frequency sweeps . in practice , impedance matching is performed off line with smith charts or relatively complex formulas but they rarely perform optimally over a large bandwidth . there have been very few attempts to compensate for matching errors while the transmission system is in operation and most techniques have been targeted to a relatively small range of frequencies . the approach proposed here employs three small neural networks to perform real time impedance matching over a broad range of frequencies during transmitter operation . double stub tuners are being explored in this paper but the approach can certainly be applied to other methodologies . the ultimate purpose of this work is the development of an inexpensive microcontroller based system .
cellular automata over group alphabets undergraduate education and the pascgalois project . <eos> this purpose of this note is to report efforts underway in the pascgalois project ( www.pascgalois.org ) to provide connections between standard courses in the undergraduate mathematics curriculum ( e.g. abstract algebra , number theory , discrete mathematics ) and cellular automata . the value of these connections to the mathematical education of undergraduates will be described . project course supplements , supporting software , and areas of student research will also be summarized .
from structured documents to novel query facilities . <eos> structured documents ( e.g. , sgml ) can benefit a lot from database support and more specifically from object oriented database ( oodb ) management systems . this paper describes a natural mapping from sgml documents into oodb 's and a formal extension of two oodb query languages ( one sql like and the other calculus ) in order to deal with sgml document retrieval . although motivated by structured documents , the extensions of query languages that we present are general and useful for a variety of other oodb applications . a key element is the introduction of paths as first class citizens . the new features allow to query data ( and to some extent schema ) without exact knowledge of the schema in a simple and homogeneous fashion .
determination of oxidized low density lipoproteins ( ox ldl ) versus ox ldl 2gpi complexes for the assessment of autoimmune mediated atherosclerosis . <eos> the immunolocalization of oxidized low density lipoproteins ( ox ldl ) , <digit> glycoprotein i ( 2gpi ) , cd4 cd8 immunoreactive lymphocytes , and immunoglobulins in atherosclerotic lesions strongly suggested an active participation of the immune system in atherogenesis . oxidative stress leading to ox ldl production is thought to play a central role in both the initiation and progression of atherosclerosis . ox ldl is highly proinflammatory and chemotactic for macrophage monocyte and immune cells . enzyme linked immunosorbent assays ( elisas ) to measure circulating ox ldl have been developed and are being currently used to assess oxidative stress as risk factor or marker of atherosclerotic disease . ox ldl interacts with 2gpi and circulating ox ldl 2gpi complexes have been demonstrated in patients with systemic lupus erythematosus ( sle ) and antiphospholipid syndrome ( aps ) . it has been postulated that 2gpi binds ox ldl to neutralize its proinflammatory and proatherosclerotic effects . because 2gpi is ubiquitous in plasma , its interaction with ox ldl may mask oxidized epitopes recognized by capture antibodies potentially interfering with immunoassays results . the measurement of ox ldl 2gpi complexes may circumvent this interference representing a more physiological and accurate way of measuring ox ldl
text document clustering based on frequent word meaning sequences . <eos> most of existing text clustering algorithms use the vector space model , which treats documents as bags of words . thus , word sequences in the documents are ignored , while the meaning of natural languages strongly depends on them . in this paper , we propose two new text clustering algorithms , named clustering based on frequent word sequences ( cfws ) and clustering based on frequent word meaning sequences ( cfwms ) . a word is the word form showing in the document , and a word meaning is the concept expressed by synonymous word forms . a word ( meaning ) sequence is frequent if it occurs in more than certain percentage of the documents in the text database . the frequent word ( meaning ) sequences can provide compact and valuable information about those text documents . for experiments , we used the reuters <digit> text collection , cisi documents of the classic data set classic data set , ftp ftp.cs.cornell.edu pub smart , and a corpus of the text retrieval conference ( trec ) high accuracy retrieval from documents ( hard ) track of text retrieval conference , <digit> . our experimental results show that cfws and cfwms have much better clustering accuracy than bisecting k means ( bkm ) m. steinbach , g. karypis , v. kumar , a comparison of document clustering techniques , kdd <digit> workshop on text mining , <digit> , a modified bisecting k means using background knowledge ( bbk ) a. hotho , s. staab , g. stumme , ontologies improve text document clustering , in proceedings of the 3rd ieee international conference on data mining , <digit> , pp. <digit> and frequent itemset based hierarchical clustering ( fihc ) b.c.m. fung , k. wang , m. ester , hierarchical document clustering using frequent itemsets , in proceedings of siam international conference on data mining , <digit> algorithms .
an online approach based on locally weighted learning for short term traffic flow prediction . <eos> traffic flow prediction is a basic function of intelligent transportation system . due to the complexity of traffic phenomenon , most existing methods build complex models such as neural networks for traffic flow prediction . as a model may lose effect with time lapse , it is important to update the model on line . however , the high computational cost of maintaining a complex model puts great challenge for model updating . the high computation cost lies in two aspects computation of complex model coefficients and huge amount training data for it . in this paper , we propose to use a nonparametric approach based on locally weighted learning to predict traffic flow . our approach incrementally incorporates new data to the model and is computationally efficient , which makes it suitable for online model updating and predicting . in addition , we adopt wavelet analysis to extract the periodic characteristic of the traffic data , which is then used for the input of the prediction model instead of the raw traffic flow data . the primary experiments on real data demonstrate the effectiveness and efficiency of our approach .
usable computing on open distributed systems . <eos> an open distributed system provides a best effort guarantee on the quality of service provided to applications . this has worked well for throughput based applications of the kind typically executed in condor or boincstyle environments . for other applications , the absence of timeliness of correctness guarantees limit the utility or appeal of this environment . computational results that are too late or erroneous are not usable to the application . we present techniques designed to efficiently promote usable computing in open distributed systems .
a note on the not <digit> choosability of some families of planar graphs . <eos> a graph g is l list colorable if for a given list assignment l l ( v ) v epsilon v , there exists a proper coloring c of g such that c ( v ) epsilon l ( v ) for all v epsilon v. if g is l list colorable for any list assignment with vertical bar l ( v ) vertical bar > k for all v epsilon v , then g is said k choosable . in m. voigt , a not <digit> choosable planar graph without <digit> cycles , discrete math . <digit> ( <digit> ) <digit> <digit> and m. voigt , a non <digit> choosable planar graph without cycles of length <digit> and <digit> , <digit> , manuscript , voigt gave a planar graph without <digit> cycles and a planar graph without <digit> cycles and <digit> cycles which are not <digit> choosable . in this note , we give smaller and easier graphs than those proposed by voigt and suggest an extension of erdos ' relaxation of steinberg 's conjecture to <digit> choosability . ( c ) <digit> elsevier b.v. all rights reserved .
impedance spectroscopy studies of moisture uptake in low k dielectrics and its relation to reliability . <eos> water incursion into low k beol capacitors was monitored via impedance spectroscopy . it is a non destructive , zero dc field , low ac field probe ( < 0.5 v ) . samples are tested at device operation conditions and are re testable . thermal activation energies related to water bonding with dielectric are measured . the increase in ac loss is correlated with poorer reliability , i.e. early failure .
a multi level depiction method for painterly rendering based on visual perception cue . <eos> increasing the level of detail ( lod ) in brushstrokes within areas of interest improved the realism of painterly rendering . using a modified quad tree , we segmented an image into areas with similar levels of saliency each of these segments was then used to control the brush strokes during rendering . we could also simulate real oil painting steps based on saliency information . our method runs in a reasonable fine and produces results that are visually appealing and competitive with previous techniques .
preventive replacement for systems with condition monitoring and additional manual inspections . <eos> researched a problem of both condition monitoring and inspection . defined two types of preventive replacements . utilized the delay time concept to model the failure process . formulated a decision problem of two decision variables simultaneously .
redundant and force differentiated systems in engineering and nature . <eos> sophisticated load carrying structures , in nature as well as man made , share some common properties . a clear differentiation of tension , compression and shear is in nature primarily manifested in the properties of materials adapted to the efforts , whereas they in engineering are distributed on different components . for stability and failure safety , redundancy on different levels is also commonly used . the paper aims at collecting and expanding previous methods for the computational treatment of redundant and force differentiated systems . a common notation is sought , giving and developing criteria for describing the diverse problems from a common structural mechanical viewpoint . from this , new criteria for the existence of solutions , and a method for treatment of targeted dynamic solutions are developed . added aspects to previously described examples aim at emphasizing similarities and differences between engineering and nature , in the forms of a tension truss structure and the human musculoskeletal system .
simultaneous optimization of the material properties and the topology of functionally graded structures . <eos> a level set based method is proposed for the simultaneous optimization of the material properties and the topology of functionally graded structures . the objective of the present study is to determine the optimal material properties ( via the material volume fractions ) and the structural topology to maximize the performance of the structure in a given application . in the proposed method , the volume fraction and the structural boundary are considered as the design variables , with the former being discretized as a scalar field and the latter being implicitly represented by the level set method . to perform simultaneous optimization , the two design variables are integrated into a common objective functional . sensitivity analysis is conducted to obtain the descent directions . the optimization process is then expressed as the solution to a coupled hamiltonjacobi equation and diffusion partial differential equation . numerical results are provided for the problem of mean compliance optimization in two dimensions .
the impact of head movements on user involvement in mediated interaction . <eos> we examine engagement within conversational behaviours of the subject when interacting with a socially expressive system . we found real time communication requires more than verbal communication , and head nodding . head nodding effects depend on precise on screen movement by synchronize the on screen movement with the head movement .
parsing images into regions , curves , and curve groups . <eos> in this paper , we present an algorithm for parsing natural images into middle level vision representations regions , curves , and curve groups ( parallel curves and trees ) . this algorithm is targeted for an integrated solution to image segmentation and curve grouping through bayesian inference . the paper makes the following contributions . ( <digit> ) it adopts a layered ( or <digit> . <digit> d sketch ) representation integrating both region and curve models which compete to explain an input image . the curve layer occludes the region layer and curves observe a partial order occlusion relation . ( <digit> ) a markov chain search scheme metropolized gibbs samplers ( mgs ) is studied . it consists of several pairs of reversible jumps to traverse the complex solution space . an mgs proposes the next state within the jump scope of the current state according to a conditional probability like a gibbs sampler and then accepts the proposal with a metropolis hastings step . this paper discusses systematic design strategies of devising reversible jumps for a complex inference task . ( <digit> ) the proposal probability ratios in jumps are factorized into ratios of discriminative probabilities . the latter are computed in a bottom up process , and they drive the markov chain dynamics in a data driven markov chain monte carlo framework . we demonstrate the performance of the algorithm in experiments with a number of natural images .
laparoscopic management of adnexal masses . <eos> suspected ovarian neoplasm is a common clinical problem affecting women of all ages . although the majority of adnexal masses are benign , the primary goal of diagnostic evaluation is the exclusion of malignancy . it has been estimated that approximately <digit> % of women in the united states will undergo a surgical procedure for a suspected ovarian neoplasm during their lifetime . despite the magnitude of the problem , there is still considerable disagreement regarding the optimal surgical management of these lesions . traditional management has relied on laparotomy to avoid undertreatment of a potentially malignant process . advances in detection , diagnosis , and minimally invasive surgical techniques make it necessary now to review this practice in an effort to avoid unnecessary morbidity among patients . here , we review the literature on the laparosopic approach to the treatment of the adnexal mass without sacrificing the principles of oncologic surgery . we highlight potentials of minimally invasive surgery and address the risks associated with the laparoscopic approach .
dealing with plagiarism in the information systems research community a look at factors that drive plagiarism and ways to address them . <eos> imagine yourself spending years conducting a research project and having it published as an article in a refereed journal , only to see a plagiarized copy of the article later published in another journal . then imagine yourself being left to fight for your rights alone , and eventually finding out that it would be very difficult to hold the plagiarist accountable for what he or she did . the recent decision by the association of information systems to create a standing committee on member misconduct suggests that while this type of situation may sound outrageous , it is likely to become uncomfortably frequent in the information systems research community if proper measures are not taken by a community backed organization . in this article , we discuss factors that can drive plagiarism , as well as potential measures to prevent it . our goal is to discuss alternative ways in which plagiarism can be prevented and dealt with when it arises . we hope to start a debate that provides the basis on which broader mechanisms to deal with plagiarism can be established , which we envision as being associated with and complementary to the committee created by the association for information systems .
percolation in the secrecy graph . <eos> the secrecy graph is a random geometric graph which is intended to model the connectivity of wireless networks under secrecy constraints . directed edges in the graph are present whenever a node can talk to another node securely in the presence of eavesdroppers , which , in the model , is determined solely by the locations of the nodes and eavesdroppers . in the case of infinite networks , a critical parameter is the maximum density of eavesdroppers that can be accommodated while still guaranteeing an infinite component in the network , i.e. , the percolation threshold . we focus on the case where the locations of the nodes and eavesdroppers are given by poisson point processes , and present bounds for different types of percolation , including in , out and undirected percolation .
evaluation of region of interest coders using perceptual image quality assessments . <eos> perceptual image assessment is proposed for coder performance evaluation . proposed assessment uses a linear combination of perceptual measures just based on features . region of interest coder perceptual evaluation aims at identifying coder behavior . some perceptual assessments are adequate to evaluate test coders .
achieving anycast in dtns by enhancing existing unicast protocols . <eos> many dtn environments , such as emergency response networks and pocket switched networks , are based on human mobility and communication patterns , which naturally lead to groups . in these scenarios , group based communication is central , and hence a natural and useful routing paradigm is anycast , where a node attempts to communicate with at least one member of a particular group . unfortunately , most existing anycast solutions assume connectivity , and the few specifically for dtns are single copy in nature and have only been evaluated in highly limited mobility models . in this paper , we propose a protocol independent method of enhancing a large number of existing dtn unicast protocols , giving them the ability to perform anycast communication . this method requires no change to the unicast protocols themselves and instead changes their world view by adding a thin layer beneath the routing layer . through a thorough set of simulations , we also evaluate how different parameters and network conditions affect the performance of these newly transformed anycast protocols .
a framework for supporting data integration using the materialized and virtual approaches . <eos> this paper presents a framework for data integration currently under development in the squirrel project . the framework is based on a special class of mediators , called squirrel integration mediators . these mediators can support the traditional virtual and materialized approaches , and also hybrids of them.in the squirrel mediators , a relation in the integrated view can be supported as ( a ) fully materialized , ( b ) fully virtual , or ( c ) partially materialized ( i.e. , with some attributes materialized and other attributes virtual ) . in general , ( partially ) materialized relations of the integrated view are maintained by incremental updates from the source databases . squirrel mediators provide two approaches for doing this ( <digit> ) materialize all needed auxiliary data , so that data sources do not have to be queried when processing the incremental updates or ( <digit> ) leave some or all of the auxiliary data virtual , and query selected source databases when processing incremental updates.the paper presents formal notions of consistency and freshness for integrated views defined over multiple autonomous source databases . it is shown that squirrel mediators satisfy these properties .
validation and verification of intelligent systems what are they and how are they different . <eos> researchers and practitioners in the field of expert systems all generally agree that to be useful , any fielded intelligent system must be adequately verified and validated . but what does this mean in concrete terms what exactly is verification what exactly is validation how are they different many authors have attempted to define these terms and , as a result , several interpretations have surfaced . it is our opinion that there is great confusion as to what these terms mean . how they are different , and how they are implemented . this paper . therefore , has two aims to clarify the meaning of the terms validation and verification as they apply to intelligent systems , and to describe how several researchers are implementing these . the second part of the paper , therefore , details some techniques that can be used to perform the verification and validation of systems . also discussed is the role of testing as part of the above mentioned processes .
multiple blocking sets and multisets in desarguesian planes . <eos> in ag ( <digit> , q ( <digit> ) ) , the minimum size of a minimal ( q <digit> ) fold blocking set is known to be q ( <digit> ) <digit> . here , we construct minimal ( q <digit> ) fold blocking sets of size q ( <digit> ) in ag ( <digit> , q ( <digit> ) ) . as a byproduct , we also obtain new two character multisets in pg ( <digit> , q ( <digit> ) ) . the essential idea in this paper is to investigate q ( <digit> ) sets satisfying the opposite of ebert 's discriminant condition .
a simple weighting scheme for classification in two group discriminant problems . <eos> this paper introduces a new weighted linear programming model , which is simple and has strong intuitive appeal for two group classifications . generally , in applying weights to solve a classification problem in discriminant analysis where the relative importance of every observation is known , larger weights ( penalties ) will be assigned to those more important observations . the perceived importance of an observation is measured here as the willingness of the decision maker to misclassify this observation . for instance , a decision maker is least willing to see a classification rule that misclassifies a top financially strong firm to the group that contains bankrupt firms . our weighted linear programming model provides an objective weighting scheme whereby observations can be weighted according to their perceived importance . the more important this observation , the heavier its assigned weight . results of a simulation experiment that uses contaminated data show that the weighted linear programming model consistently and significantly outperforms existing linear programming and standard statistical approaches in attaining higher average hit ratios in the <digit> replications for each of the <digit> cases tested . scope and purpose generally , in applying weights to solve a discriminant problem where the relative importance of every observation is known , larger weights ( penalties ) will be assigned to those more important observations . however , if decision makers do not have prior or additional information about the observations , it is very difficult to assign weights to the observations . subjective judgements from decision makers may be a way of obtaining those weights . an alternative way is to suggest an objective weighting scheme for obtaining classification weights of observations from the data matrix of the training sample . we suggest a new approach , which provides an objective weighting scheme whereby individual observations can be weighted according to their perceived importance . the more important the observation , the heavier its assigned weight will be . the importance of individual observation is first determined in one of two stages of our model using more than one discriminant function . simulation experiments are run to test this new approach .
hybrid intelligent packing system ( hips ) through integration of artificial neural networks , artificial intelligence , and mathematical programming . <eos> a successful solution to the packing problem is a major step toward material savings on the scrap that could be avoided in the cutting process and therefore money savings . although the problem is of great interest , no satisfactory algorithm has been found that can be applied to all the possible situations . this paper models a hybrid intelligent packing system ( hips ) by integrating artificial neural networks ( anns ) , artificial intelligence ( ai ) , and operations research ( or ) approaches for solving the packing problem . the hips consists of two main modules , an intelligent generator module and a tester module . the intelligent generator module has two components ( i ) a rough assignment module and ( ii ) a packing module . the rough assignment module utilizes the expert system and rules concerning cutting restrictions and allocation goals in order to generate many possible patterns . the packing module is an ann that packs the generated patterns and performs post solution adjustments . the tester module , which consists of a mathematical programming model , selects the sets of patterns that will result in a minimum amount of scrap .
distributed scheduling and resource allocation for cognitive ofdma radios . <eos> scheduling spectrum access and allocating power and rate resources are tasks affecting critically the performance of wireless cognitive radio ( cr ) networks . the present contribution develops a primal dual optimization framework to schedule any to any cr communications based on orthogonal frequency division multiple access and allocate power so as to maximize the weighted average sum rate of all users . fairness is ensured among cr communicators and possible hierarchies are respected by guaranteeing minimum rate requirements for primary users while allowing secondary users to access the spectrum opportunistically . the framework leads to an iterative channel adaptive distributed algorithm whereby nodes rely only on local information exchanges with their neighbors to attain global optimality . simulations confirm that the distributed online algorithm does not require knowledge of the underlying fading channel distribution and converges to the optimum almost surely from any initialization .
physically based hydraulic erosion simulation on graphics processing unit . <eos> visual simulation of natural erosion on terrains has always been a fascinating research topic in the field of computer graphics . while there are many algorithms already developed to improve the visual quality of terrain , the recent simulation methods revolve around physically based hydraulic erosion because it can generate realistic natural looking terrains . however , many of such algorithms were tested only on low resolution terrains . when simulated on a higher resolution terrain , most of the current algorithms become computationally expensive . this is why in many applications today , terrains are generated off line and loaded during the application runtime . this method restricts the number of terrains which can be stored if there is a limitation on storage capacity . recently , graphics hardware has evolved into an indispensable tool in improving the speed of computation . this has motivated us to develop an erosion algorithm to map to graphics hardware for faster terrain generation . in this paper , we propose a fast and efficient hydraulic erosion procedural technique that utilizes the gpus powerful computation capability in order to generate high resolution erosion on terrains . our method is based on the newtonian physics approach that is implemented on a two dimensional data structure which stores height fields , water amount , and dissolved sediment and water velocities . we also present a comprehensive comparison between the cpu and gpu implementations together with the visual results and the statistics on simulation time taken .
novel immune based framework for securing ad hoc networks . <eos> one of the main security issues in mobile ad hoc networks ( manets ) is a malicious node that can falsify a route advertisement , overwhelm traffic without forwarding it , help to forward corrupted data and inject false or uncompleted information , and many other security problems . mapping immune system mechanisms to networking security is the main objective of this paper which may significantly contribute in securing manets . in a step for providing secured and reliable broadband services , formal specification logic along with a novel immuneinspired security framework ( i <digit> manets ) are introduced . the different immune components are synchronized with the framework through an agent that has the ability to replicate , monitor , detect , classify , and block isolate the corrupted packets and or nodes in a federated domain . the framework functions as the human immune system in first response , second response , adaptability , distributability , and survivability and other immune features and properties . interoperability with different routing protocols is considered . the framework has been implemented in a real environment . desired and achieved results are presented .
slabpose columnsort a new oblivious algorithm for out of core sorting on distributed memory clusters . <eos> our goal is to develop a robust out of core sorting program for a distributed memory cluster . the literature contains two dominant paradigms for out of core sorting algorithms merging based and partitioning based . we explore a third paradigm , that of oblivious algorithms . unlike the two dominant paradigms , oblivious algorithms do not depend on the input keys and therefore lead to predetermined i o and communication patterns in an out of core setting . predetermined i o and communication patterns facilitate overlapping i o , communication , and computation for efficient implementation . we have developed several out of core sorting programs using the paradigm of oblivious algorithms . our baseline implementation , <digit> pass columnsort , was based on leighton 's columnsort algorithm . though efficient in terms of i o and communication , <digit> pass columnsort has a restriction on the maximum problem size . as our first effort toward relaxing this restriction , we developed two implementations subblock columnsort and m columnsort . both of these implementations incur substantial performance costs subblock columnsort performs additional disk i o , and m columnsort needs substantial amounts of extra communication and computation . in this paper we present slabpose columnsort , a new oblivious algorithm that we have designed explicitly for the out of core setting . slabpose columnsort relaxes the problem size restriction at no extra i o or communication cost . experimental evidence on a beowulf cluster shows that unlike subblock columnsort and m columnsort , slabpose columnsort runs almost as fast as <digit> pass columnsort . to the best of our knowledge , our implementations are the first out of core multiprocessor sorting algorithms that make no assumptions about the keys and produce output that is perfectly load balanced and in the striped order assumed by the parallel disk model .
a real time kinematics on the translational crawl motion of a quadruped robot . <eos> it is known that the kinematics of a quadruped robot is complex due to its topology and the redundant actuation in the robot . however , it is fundamental to compute the inverse and direct kinematics for the sophisticated control of the robot in real time . in this paper , the translational crawl gait of a quadruped robot is introduced and the approach to find the solution of the kinematics for such a crawl motion is proposed . since the resulting kinematics is simplified , the formulation can be used for the real time control of the robot . the results of simulation and experiment shows that the present method is feasible and efficient .
geographical classification of olive oils by the application of cart and svm to their ft ir . <eos> this paper reports the application of fourier transform infrared ( ft ir ) spectroscopy to the geographical classification of extra virgin olive oils . two chemometrical techniques , classification and regression trees ( cart ) and support vector machines ( svm ) based on the gaussian kernel and the recently introduced euclidean distance based pearson vii universal kernel ( puk ) , were applied to discriminate between italian and non italian and between ligurian and non ligurian olive oils . the puk is applied in literature with success on regression problems . in this paper the mapping power of this universal kernel for classification was investigated . in this study it was observed that svm performed better than cart . svm based on the puk provide models with a high selectivity and sensitivity ( thus a better accuracy ) as compared to those obtained using the gaussian kernel . the wave numbers selected in the classification trees were interpreted demonstrating that the trees were chemically justified . this study also shows that ft ir spectroscopy associated with svm and cart can be used to correctly discriminate between various origins of olive oils , demonstrating that the combination of techniques might be a powerful tool for supporting the claimed origin of olive oils . copyright ( c ) <digit> john wiley sons , ltd .
arfnns under different types svr for identification of nonlinear magneto rheological damper systems with outliers . <eos> this paper demonstrates different types support vector regression ( svr ) for annealing robust fuzzy neural networks ( arfnns ) to identification of nonlinear magneto rheological ( mr ) damper with outliers . a svr has the good performances to determine the number of rule in the simplified fuzzy inference system and initial weights for the fuzzy neural networks . in this paper , we independently proposed two different types svr for the arfnns . hence , a combination model that fuses simplified fuzzy inference system , svr and radial basis function networks is used . based on these initial structures , and then annealing robust learning algorithm ( arla ) can be used effectively to adjust the parameters of structures . simulation results show the superiority of the proposed method with the different types svr for the nonlinear mr damper systems with outliers .
fuzzy linear regression model based on fuzzy scalar product . <eos> the new concept and method of imposing imprecise ( fuzzy ) input and output data upon the conventional linear regression model is proposed in this paper . we introduce the fuzzy scalar ( inner ) product to formulate the fuzzy linear regression model . in order to invoke the conventional approach of linear regression analysis for real valued data , we transact the alpha level linear regression models of the fuzzy linear regression model . we construct the membership functions of fuzzy least squares estimators via the form of resolution identity which is a well known formula in fuzzy sets theory . in order to obtain the membership value of any given least squares estimate taken from the fuzzy least squares estimator , we transform the original problem into the optimization problems . we also provide two computational procedures to solve the optimization problems .
relating torque and slip in an odometric model for an autonomous agricultural vehicle . <eos> this paper describes a method of considering the slip that is experienced by the wheels of an agricultural autonomous guided vehicle such that the accuracy of dead reckoning navigation may be improved . traction models for off road locomotion are reviewed . using experimental data from an agricultural agv , a simplified form suitable for vehicle navigation is derived . this simplified model relates measurements of the torques applied to the wheels with wheel slip , and is used as the basis of an observation model for odometric sensor data in the vehicle 's extended kalman filter ( ekf ) navigation system . the slip model parameters are included as states in the vehicle ekf so that the vehicle may adapt to changing surface properties . results using real field data and a simulation of the vehicle ekf show that positional accuracy can be increased by a slip aware odometric model , and that when used as part of a multi sensor navigation system , the consistency of the ekf state estimator is improved .
evaluation of folksonomy induction algorithms . <eos> algorithms for constructing hierarchical structures from user generated metadata have caught the interest of the academic community in recent years . in social tagging systems , the output of these algorithms is usually referred to as folksonomies ( from folk generated taxonomies ) . evaluation of folksonomies and folksonomy induction algorithms is a challenging issue complicated by the lack of golden standards , lack of comprehensive methods and tools as well as a lack of research and empirical simulation studies applying these methods . in this article , we report results from a broad comparative study of state of the art folksonomy induction algorithms that we have applied and evaluated in the context of five social tagging systems . in addition to adopting semantic evaluation techniques , we present and adopt a new technique that can be used to evaluate the usefulness of folksonomies for navigation . our work sheds new light on the properties and characteristics of state of the art folksonomy induction algorithms and introduces a new pragmatic approach to folksonomy evaluation , while at the same time identifying some important limitations and challenges of folksonomy evaluation . our results show that folksonomy induction algorithms specifically developed to capture intuitions of social tagging systems outperform traditional hierarchical clustering techniques . to the best of our knowledge , this work represents the largest and most comprehensive evaluation study of state of the art folksonomy induction algorithms to date .
teleportation of n qudit state . <eos> in this paper , we study the teleportation of arbitrary n qudit state with the tensor representation . the necessary and sufficient condition for realizing a successful or perfect teleportation is obtained , as will be shown , which is determined by the measurement matrix t delta and the quantum channel parameter matrix x. the general expressions of the measurement matrix t delta are written out and the quantum channel parameter matrix x are discussed . as an example , we show the details of three ququart state teleportation .
bio interactive healthcare service system using lifelog based context computing . <eos> intelligent bio sensor information processing was developed using lifelog based context aware technology to provide a flexible and dynamic range of diagnostic capabilities to satisfy healthcare requirements in ubiquitous and mobile computing environments . to accomplish this , various noise signals were grouped into six categories by context estimation and effectively reconfigured noise reduction filters by neural network and genetic algorithm . the neural network based control module effectively selected an optimal filter block by noise context based clustering in running mode , and filtering performance was improved by genetic algorithm in evolution mode . due to its adaptive criteria , genetic algorithm was used to explore the action configuration for each identified bio context to implement our concept . our proposed bio interactive healthcare service system adopts the concepts of biological context awareness with evolutionary computations in working environments modeled and identified as bio sensors based environmental contexts . we used an unsupervised learning algorithm for lifelog based context modeling and a supervised learning algorithm for context identification .
smb collision detection based on temporal coherence . <eos> the paper presents a novel collision detection algorithm , termed the sort moving boxes ( smb ) for large number of moving 2d 3d objects which are represented by their axis aligned bounding boxes ( aabbs ) . the main feature of the algorithm is the full exploitation of the temporal coherence of the objects exhibited in a dynamic environment . in the algorithm , the aabbs are first projected to each cartesian axis . the projected intervals on the axes are separately sorted by the diminishing increment sort ( dis ) and further divided into subsections . by processing all the intervals within the subsections to check if they overlap , a complete contact list can be built . the smb is a fast and robust collision detection algorithm , particularly for systems involving a large number of moving aabbs , and also supports for the dynamic insertion and deletion of objects . its performance in terms of both expected total detection time and memory requirements is proportional to the total number of aabbs , n , and is not influenced by size differences of aabbs , the space size and packing density over a large range up to ten times difference . the only assumption made is that the sorted list at one time step will remain an almost sorted list at the next time step , which is valid for most applications whose movement and deformation of each aabb and the dynamic change of the total number n are approximately continuous .
an international analysis of the extensions to the ieee lomv1 .0 metadata standard . <eos> we analyzed <digit> works using the ieee lomv1 .0 standard and found <digit> types of extensions made to it . due to mexico interoperability difficulties , we compared its extensions with the rest of the world . we found that local extensions do not help to increase the system 's interoperability ability . we found the action most important after implementing extensions is to publish them .
a unifying approach to goal directed evaluation . <eos> goal directed evaluation , as embodied in icon and snobol , is built on the notions of backtracking and of generating successive results , and therefore it has always been something of a challenge to specify and implement . in this article , we address this challenge using computational monads and partial evaluation . we consider a subset of icon and we specify it with a monadic semantics and a list monad . we then consider a spectrum of monads that also fit the bill , and we relate them to each other . for example , we derive a continuation monad as a church encoding of the list monad . the resulting semantics coincides with gudeman 's continuation semantics of icon . we then compile icon programs by specializing their interpreter ( i.e. , by using the first futamura projection ) , using type directed partial evaluation . through various back ends , including a run time code generator , we generate ml code , c code , and ocaml byte code . binding time analysis and partial evaluation of the continuation based interpreter automatically give rise to c programs that coincide with the result of proebsting 's optimized compiler .
an architectural history of metaphors . <eos> this paper presents a review and an historical perspective on the architectural metaphor . it identifies common characteristics and peculiaritiesas they apply to given historical periodsand analyses the similarities and divergences . the review provides a vocabulary , which will facilitate an appreciation of existing and new metaphors .
time based query performance predictors . <eos> query performance prediction is aimed at predicting the retrieval effectiveness that a query will achieve with respect to a particular ranking model . in this paper , we study query performance prediction for a ranking model that explicitly incorporates the time dimension into ranking . different time based predictors are proposed as analogous to existing keyword based predictors . in order to improve predicting performance , we combine different predictors using linear regression and neural networks . extensive experiments are conducted using queries and relevance judgments obtained by crowdsourcing .
asymptotically sufficient partitions and quantizations . <eos> we consider quantizations of observations represented by finite partitions of observation spaces . partitions usually decrease the sensitivity of observations to their probability distributions . a sequence of quantizations is considered to be asymptotically sufficient for a statistical problem if the loss of sensitivity is asymptotically negligible . the sensitivity is measured by f divergences of distributions or the closely related f informations including the classical shannon information . it is demonstrated that in some cases the maximization of f divergences means the same as minimization of distortion of observations in the classical sense considered in mathematical statistics and information theory . the main result of the correspondence is a general sufficient condition ford the asymptotic sufficiency of quantizations . selected applications of this condition are studied leading to new simple criteria of asymptotic optimality for quantizations of vector valued observations and observations on general poisson processes .
the topology aware file distribution problem . <eos> we present theoretical results for large file distribution on general networks of known topology ( known link bandwidths and router locations ) . we show that the problem of distributing a file in minimum time is np hard in this model , and we give an o ( log n ) approximation algorithm , where n is the number of workstations that require the file . we also characterize our method as optimal amongst the class of no link sharing algorithms .
achieving quality assurance functionality in the food industry using a hybrid case based reasoning and fuzzy logic approach . <eos> quality control of food inventories in the warehouse is complex as well as challenging due to the fact that food can easily deteriorate . currently , this difficult storage problem is managed mostly by using a human dependent quality assurance and decision making process . this has however , occasionally led to unimaginative , arduous and inconsistent decisions due to the injection of subjective human intervention into the process . therefore , it could be said that current practice is not powerful enough to support high quality inventory management . in this paper , the development of an integrative prototype decision support system , namely , intelligent food quality assurance system ( ifqas ) is described which will assist the process by automating the human based decision making process in the quality control of food storage . the system , which is composed of a case based reasoning ( cbr ) engine and a fuzzy rule based reasoning ( fbr ) engine , starts with the receipt of incoming food inventory . with the cbr engine , certain quality assurance operations can be suggested based on the attributes of the food received . further of this , the fbr engine can make suggestions on the optimal storage conditions of inventory by systematically evaluating the food conditions when the food is receiving . with the assistance of the system , a holistic monitoring in quality control of the receiving operations and the storage conditions of the food in the warehouse can be performed . it provides consistent and systematic quality assurance guidelines for quality control which leads to improvement in the level of customer satisfaction and minimization of the defective rate .
coverage and connectivity in three dimensional underwater sensor networks . <eos> unlike a terrestrial network . an underwater sensor network call have significant height which makes it a three dimensional network . there are many important sensor network design problems where the physical dimensionality of the network plays it significant role . one such problem is determining how to deploy minimum number of sensor nodes so that all points inside the network is within the sensing range of at least one sensor and all sensor nodes call communicate with each other , possibly over a multi hop path . the solution to this problem depends oil the ratio of the communication ran e and the sensing range of each sensor . under sphere based communication and sensing model , placing a node at the center of each virtual cell created by truncated octahedron based tessellation solves this problem when this ratio is greater than 1.7889 . however , for smaller values of this ratio , the solution depends on how much communication redundancy the network needs . we provide solutions for both limited and full communication redundancy requirements . copyright ( c ) <digit> john wiley sons , ltd .
error correction of voicemail transcripts in scanmail . <eos> despite its widespread use , voicemail presents numerous usability challenges people must listen to messages in their entirety , they can not search by keywords , and audio files do not naturally support visual skimming . scanmail overcomes these flaws by automatically generating text transcripts of voicemail messages and presenting them in an email like interface . transcripts facilitate quick browsing and permanent archive . however , errors from the automatic speech recognition ( asr ) hinder the usefulness of the transcripts . the work presented here specifically addresses these problems by evaluating user initiated error correction of transcripts . user studies of two editor interfaces a grammar assisted menu and simple replacement by typing reveal reduced audio playback times and an emphasis on editing important words with the menu , suggesting its value in mobile environments where limited input capabilities are the norm and user privacy is essential . the study also adds to the scarce body of work on asr confidence shading , suggesting that shading may be more helpful than previously reported .
study of stress waves in geomedia and effect of a soil cover layer on wave attenuation using a <digit> d finite difference method . <eos> the propagation and attenuation of blast induced stress waves differs between geomedia such as rock or soil mass . this paper numerically studies the propagation and attenuation of blast induced elastoplastic waves in deep geomedia by using a one dimensional ( i d ) finite difference code . firstly , the elastoplastic cap models for rock and soil masses are introduced into the governing equations of spherical wave motion and a fortran code based on the finite difference method is developed . secondly , an underground spherical blast is simulated with this code and verified by software , renewto . the propagation of stress waves in rock and soil masses is numerically investigated , respectively . finally , the effect of a soil cover layer on the attenuation of stress waves in the rear rock mass is studied . it is determined that large plastic deformation of geomedia can effectively dissipate the energy of stress waves inward and the developed i d finite difference code coupled with elastoplastic cap models is convenient and effective in the numerical simulations for underground spherical explosion . ( c ) <digit> elsevier ltd. all rights reserved .
keyed hash function based on a dynamic lookup table of functions . <eos> in this paper , we present a novel keyed hash function based on a dynamic lookup table of functions . more specifically , we first exploit the piecewise linear chaotic map ( pwlcm ) with secret keys used for producing four <digit> bit initial buffers and then elaborate the lookup table of functions used for selecting composite functions associated with messages . next , we convert the divided message blocks into ascii code values , check the equivalent indices and then find the associated composite functions in the lookup table of functions . for each message block , the four buffers are reassigned by the corresponding composite function and then the lookup table of functions is dynamically updated . after all the message blocks are processed , the final <digit> bit hash value is obtained by cascading the last reassigned four buffers . finally , we evaluate our hash function and the results demonstrate that the proposed hash algorithm has good statistical properties , strong collision resistance , high efficiency , and better statistical performance compared with existing chaotic hash functions .
exploring hierarchical multidimensional data with unified views of distribution and correlation . <eos> data analysts explore data by inspecting features such as clustering , distribution and correlation . much existing research has focused on different visualisations for different data exploration tasks . for example , a data analyst might inspect clustering and correlation with scatterplots , but use histograms to inspect a distribution . such visualisations allow an analyst to confirm prior expectations . for example , a scatterplot may confirm an expected correlation or may show deviations from the expected correlation . in order to better facilitate discovery of unexpected features in data , however , a combination of different perspectives may be needed . in this paper , we combine distributional and correlational views of hierarchical multidimensional data . our unified view supports the simultaneous exploration of data distribution and correlation . by presenting a unified view , we aim to increase the chances of discovery of unexpected data features , and to provide the means to explore such features in detail . further , our unified view is equipped with a small number of primitive interaction operators which a user composes to facilitate smooth and flexible exploration . ( c ) <digit> elsevier ltd. all rights reserved .
application driven network on chip architecture exploration refinement for a complex soc . <eos> this article presents an overview of the design process of an interconnection network , using the technology proposed by arteris . section <digit> summarizes the various features a noc is required to implement to be integrated in modern socs . section <digit> describes the proposed top down approach , based on the progressive refinement of the noc description , from its functional specification ( sect. <digit> ) to its verification ( sect. <digit> ) . the approach is illustrated by a typical use case of a noc embedded in a hand held gaming device . the methodology relies on the definition of the performance behavior and expectation ( sect. <digit> ) , which can be early and efficiently simulated against various noc architectures . the system architect is then able to identify bottle necks and converge towards the noc implementation fulfilling the requirements of the target application ( sect. <digit> ) .
real valued mvdr beamforming using spherical arrays with frequency invariant characteristic . <eos> complex valued minimum variance distortionless response ( mvdr ) beamforming for wideband signals has very high computational amount . in this paper , we design a novel real valued mvdr beamformer for spherical arrays . the dependence of the array steering matrix on source signal directions and frequencies is decoupled using spherical harmonic decomposition . then a compensation network is designed to solve the frequency dependence of the array response and to get a new array response only determined by the spherical harmonics of the source directions . all frequency bins of wideband signals can be used together instead of being processed independently . by exploiting the property of the conjugate spherical harmonics , a unitary transform can be found to acquire a real valued frequency invariant steering matrix ( fism ) . based on the fism , real valued mvdr ( rv mvdr ) is developed to obtain good performance with low computational amount . simulation results demonstrate the performance of our proposed method for beamforming and direction of arrival ( doa ) estimation by comparing with the complex valued and real weighted mvdr methods .
private database queries using quantum states with limited coherence times . <eos> we describe a method for private database queries using exchange of quantum states with bits encoded in mutually incompatible bases . for technology with limited coherence time , the database vendor can announce the encoding after a suitable delay to allow the user to privately learn one of two items in the database without the ability to also definitely infer the second item . this quantum approach also allows the user to choose to learn other functions of the items , such as the exclusive or of their bits , but not to gain more information than equivalent to learning one item , on average . this method is especially useful for items consisting of a few bits by avoiding the substantial overhead of conventional cryptographic approaches .
scheduling for information gathering on sensor network . <eos> we investigate a unique wireless sensor network scheduling problem in which all nodes in a cluster send exactly one packet to a designated sink node in an effort to minimize transmission time . however , node transmissions must be sufficiently isolated either in time or in space to avoid collisions . the problem is formulated and solved via graph representation . we prove that an optimal transmission schedule can be obtained efficiently through a pipeline like schedule when the underlying topology is either line or tree . the minimum time required for a line or tree topology with n nodes is <digit> ( n <digit> ) . we further prove that our scheduling problem is np hard for general graphs . we propose a heuristic algorithm for general graphs . our heuristic tries to schedule as many independent segments as possible to increase the degree of parallel transmissions . this algorithm is compared to an rts cts based distributed algorithm . preliminary simulated results indicate that our heuristic algorithm outperforms the rts cts based distributed algorithm ( up to <digit> % ) and exhibits stable behavior .
synchronization analysis and control in chaos system based on complex network . <eos> for a certain kind of complex network , lorenz chaos system is used to describe the state equation of nodes in network . by constructing a lyapunov function , it is proved that this network model can achieve synchronization under the adaptive control scheme . the control strategy is simple , effective and easy for the engineering design in the future . the simulation results show the effectiveness of control scheme .
improved property in organic light emitting diode utilizing two al alq3 layers . <eos> we reported on the fabrication of organic light emitting devices ( oleds ) utilizing the two al alq3 layers and two electrodes . this novel green device with structure of al ( 110nm ) tris ( <digit> hydroxyquinoline ) aluminum ( alq3 ) ( 65nm ) al ( 110nm ) alq3 ( 50nm ) n , n dipheny1 n , n bis ( <digit> methy1phyeny1 ) <digit> , <digit> bipheny1 <digit> , <digit> diamine ( tpd ) ( 60nm ) ito ( 60nm ) glass . tpd were used as holes transporting layer ( htl ) , and alq3 was used as electron transporting layer ( etl ) , at the same time , alq3 was also used as emitting layer ( el ) , al and ito were used as cathode and anode , respectively . the results showed that the device containing the two al alq3 layers and two electrodes had a higher brightness and electroluminescent efficiency than the device without this layer . at current density of 14ma cm2 , the brightness of the device with the two al alq3 layers reach 3693cd m2 , which is higher than the 2537cd m2 of the al alq3 tpd alq3 ito glass device and the 1504.0 cd m2 of the al alq3 tpd ito glass . turn on voltage of the device with two al alq3 layers was 7v , which is lower than the others .
concept development for kindergarten children through a health simulation . <eos> according to many dental professionals , the decay process resulting from the accumulation of sugar on teeth is a very difficult concept for young children to learn . playing the dental hygiene game with thinkingtags not only brings context into the classroom , but also allows children to work with digital manipulatives that provide rich personal experiences and instant feedback . instead of watching a demonstration of the accumulation of sugars on a computer screen , or being told about dental health , this simulation allows pre school children to experience improving or decaying dental health without any real adverse health effects . small , wearable , microprocessor driven tags were brought into the kindergarten classroom to simulate the decay process , providing information about sugars in foods and creating a discussion about teeth . preliminary analyses suggest that this program was effective and enthusiastically received by this age group .
high output impedance current mode four function filter with reduced number of active and passive elements using the dual output current conveyor . <eos> this paper reports a new single input multi output current mode multifunction filter which can simultaneously realise lp , hp , bp and br filter functions all at high impedance outputs . the circuit permits orthogonal adjustment of quality factor q and omega ( <digit> ) , employs only five grounded passive components and no element matching conditions are imposed . a second order all pass function can easily be obtained . the passive sensitivities are shown to be low .
constraint programming for itemset mining . <eos> the relationship between constraint based mining and constraint programming is explored by showing how the typical constraints used in pattern mining can be formulated for use in constraint programming environments . the resulting framework is surprisingly flexible and allows us to combine a wide range of mining constraints in different ways . we implement this approach in off the shelf constraint programming systems and evaluate it empirically . the results show that the approach is not only very expressive , but also works well on complex benchmark problems .
experiences mining open source release histories . <eos> software releases form a critical part of the life cycle of a software project . typically , each project produces releases in its own way , using various methods of versioning , archiving , announcing and publishing the release . understanding the release history of a software project can shed light on the project history , as well as the release process used by that project , and how those processes change . however , many factors make automating the retrieval of release history information difficult , such as the many sources of data , a lack of relevant standards and a disparity of tools used to create releases . in spite of the large amount of raw data available , no attempt has been made to create a release history database of a large number of projects in the open source ecosystem . this paper presents our experiences , including the tools , techniques and pitfalls , in our early work to create a software release history database which will be of use to future researchers who want to study and model the release engineering process in greater depth .
balancing throughput and response time in online scientific clouds via ant colony optimization ( sp2013 <digit> <digit> ) . <eos> the cloud computing paradigm focuses on the provisioning of reliable and scalable infrastructures ( clouds ) delivering execution and storage services . the paradigm , with its promise of virtually infinite resources , seems to suit well in solving resource greedy scientific computing problems . the goal of this work is to study private clouds to execute scientific experiments coming from multiple users , i.e. , our work focuses on the infrastructure as a service ( iaas ) model where custom virtual machines ( vm ) are launched in appropriate hosts available in a cloud . then , correctly scheduling cloud hosts is very important and it is necessary to develop efficient scheduling strategies to appropriately allocate vms to physical resources . the job scheduling problem is however np complete , and therefore many heuristics have been developed . in this work , we describe and evaluate a cloud scheduler based on ant colony optimization ( aco ) . the main performance metrics to study are the number of serviced users by the cloud and the total number of created vms in online ( non batch ) scheduling scenarios . besides , the number of intra cloud network messages sent are evaluated . simulated experiments performed using cloudsim and job data from real scientific problems show that our scheduler succeeds in balancing the studied metrics compared to schedulers based on random assignment and genetic algorithms .
exploring the cscw spectrum using process mining . <eos> process mining techniques allow for extracting information from event logs . for example , the audit trails of a workflow management system or the transaction logs of an enterprise resource planning system can be used to discover models describing processes , organizations , and products . traditionally , process mining has been applied to structured processes . in this paper , we argue that process mining can also be applied to less structured processes supported by computer supported cooperative work ( cscw ) systems . in addition , the prom framework is described . using prom a wide variety of process mining activities are supported ranging from process discovery and verification to conformance checking and social network analysis .
effects of spatial and temporal variation in environmental conditions on simulation of wildfire spread . <eos> implementation of a wildfire spread model based on the level set method . investigation of wildfire propagation under stochastic wind and fuel conditions . local variation in combustion condition slows the rate of propagation . local variation in wind direction is found to increase flank spread . a harmonic mean is preferential for spatially varying parameters in spread models .
utilization of spatial decision support systems decision making in dryland agriculture a tifton burclover case study . <eos> fsaw delineated wyoming agricultural land into relative ranks for burclover establishment . defuzzification produced final output map with crisp scores and calculated centroid . calculated centroid map demonstrated efficacy of sdss in agricultural decision making . effective land suitability ranking validated value of ex ante agricultural technologies . presented information has potential to determine burclover feasibility in wyoming .
propagation engine prototyping with a domain specific language . <eos> constraint propagation is at the heart of constraint solvers . two main trends co exist for its implementation variable oriented propagation engines and constraint oriented propagation engines . those two approaches ensure the same level of local consistency but their efficiency ( computation time ) can be quite different depending on the instance solved . however , it is usually accepted that there is no best approach in general , and modern constraint solvers implement only one . in this paper , we would like to go a step further providing a solver independent language at the modeling stage to enable the design of propagation engines . we validate our proposal with a reference implementation based on the choco solver and the minizinc constraint modeling language .
a projection pursuit framework for supervised dimension reduction of high dimensional small sample datasets . <eos> the analysis and interpretation of datasets with large number of features and few examples has remained as a challenging problem in the scientific community , owing to the difficulties associated with the curse of the dimensionality phenomenon . projection pursuit ( pp ) has shown promise in circumventing this phenomenon by searching low dimensional projections of the data where meaningful structures are exposed . however , pp faces computational difficulties in dealing with datasets containing thousands of features ( typical in genomics and proteomics ) due to the vast quantity of parameters to optimize . in this paper we describe and evaluate a pp framework aimed at relieving such difficulties and thus ease the construction of classifier systems . the framework is a two stage approach , where the first stage performs a rapid compaction of the data and the second stage implements the pp search using an improved version of the spp method ( guo et al. , <digit> , <digit> ) . in an experimental evaluation with eight public microarray datasets we showed that some configurations of the proposed framework can clearly overtake the performance of eight well established dimension reduction methods in their ability to pack more discriminatory information into fewer dimensions .
conservation functions for <digit> d automata efficient algorithms , new results , and a partial taxonomy . <eos> we present theorems that can be used for improved efficiency in the calculation of conservation functions for cellular automata . we report results obtained from implementations of algorithms based on these theorems that show conservation laws for <digit> d cellular automata of higher order than any previously known . we introduce the notion of trivial and core conservation functions to distinguish truly new conservation functions from simple extensions of lower order ones . we then present the complete list of conservation functions up to order <digit> for the <digit> elementary <digit> d binary cellular automata . these include cas that were not previously known to have nontrivial conservation functions .
a reference bacterial genome dataset generated on the minion portable single molecule nanopore sequencer . <eos> the minion is a new , portable single molecule sequencer developed by oxford nanopore technologies . it measures four inches in length and is powered from the usb 3.0 port of a laptop computer . the minion measures the change in current resulting from dna strands interacting with a charged protein nanopore . these measurements can then be used to deduce the underlying nucleotide sequence .
serial batching scheduling of deteriorating jobs in a two stage supply chain to minimize the makespan . <eos> for the scheduling problem with a buffer , an optimal algorithm is developed for solving it . for the scheduling problem without buffer , some useful properties are derived . a heuristic is designed for solving it , and a novel lower bound is also derived . two special cases are well analyzed , and two optimal algorithms are developed for solving them , respectively .
entanglement monotones and maximally entangled states in multipartite qubit systems . <eos> we present a method to construct entanglement measures for pure states of multipartite qubit systems . the key element of our approach is an antilinear operator that we call comb in reference to the hairy ball theorem . for qubits ( i.e. spin <digit> <digit> ) the combs are automatically invariant under sl ( <digit> , c ) . this implies that the filters obtained from the combs are entanglement monotones by construction . we give alternative formulae for the concurrence and the <digit> tangle as expectation values of certain antilinear operators . as an application we discuss inequivalent types of genuine four , five and six qubit entanglement .
automatic verification of java programs with dynamic frames . <eos> framing in the presence of data abstraction is a challenging and important problem in the verification of object oriented programs leavens et al. ( formal aspects comput ( facs ) 19 159 189 , <digit> ) . the dynamic frames approach is a promising solution to this problem . however , the approach is formalized in the context of an idealized logical framework . in particular , it is not clear the solution is suitable for use within a program verifier for a java like language based on verification condition generation and automated , first order theorem proving . in this paper , we demonstrate that the dynamic frames approach can be integrated into an automatic verifier based on verification condition generation and automated theorem proving . the approach has been proven sound and has been implemented in a verifier prototype . the prototype has been used to prove correctness of several programming patterns considered challenging in related work .
properties of the transmission of pulse sequences in a bistable chain of unidirectionally coupled neurons . <eos> we study the propagation of pulse sequences in a chain of neurons with sigmoidal inputoutput relations . the propagating speeds of pulse fronts depend on the widths of the preceding pulses and adjacent pulse fronts interact attractively . sequences of pulse widths are then modulated through transmission . equations for changes in pulse width sequences are derived with a kinematical model of propagating pulse fronts . the transmission of pulse width sequences in the chain is expressed as a linear system with additive noise . the gain of the system function increases exponentially with the number of neurons in a high frequency region . the power spectrum of variations in pulse widths due to spatiotemporal noise also increases in the same manner . further , the interaction between pulse fronts keeps the coherence and mutual information of initial and transmitted pulse sequences . results of an experiment on an analog circuit confirm these properties .
building geometric feature based maps for indoor service robots . <eos> this paper presents an efficient geometric approach to the simultaneous localization and mapping problem based on an extended kalman filter . the map representation and building process is formulated , fully implemented and successfully experimented in different indoor environments with different robots . the use of orthogonal shape constraints is proposed to deal with the inconsistency of the estimation . built maps are successfully used for the navigation of two different service robots an interactive tour guide robot , and an assistive walking aid for the frail elderly .
the cross entropy method with patching for rare event simulation of large markov chains . <eos> there are various importance sampling schemes to estimate rare event probabilities in markovian systems such as markovian reliability models and jackson networks . in this work , we present a general state dependent importance sampling method which partitions the state space and applies the cross entropy method to each partition . we investigate two versions of our algorithm and apply them to several examples of reliability and queueing models . in all these examples we compare our method with other importance sampling schemes . the performance of the importance sampling schemes is measured by the relative error of the estimator and by the efficiency of the algorithm . the results from experiments show considerable improvements both in running time of the algorithm and the variance of the estimator .
combined simulation for process control extension of a general purpose simulation tool . <eos> combined discrete event and continuous views of production processes are important in designing computer control systems for both process industries and manufacturing . the paper presents an extension of the popular matlab simulink simulation tool to facilitate the simulation of the discrete sequential control logic applied to continuous processes . the control system is modelled as a combined system where the discrete and the continuous parts of the system are separated and an interface is introduced between them . the sequential control logic is represented by a sequential function chart ( sfc ) . a sfc blockset is defined to enable graphical composition of the sfc and its integration into the simulink environment . a simulation mechanism is implemented which is called periodically from the standard simulink simulation engine and carries out the correct state transition sequence of the discrete model and executes corresponding sfc actions . two simulation case studies are given to illustrate the possible application of the developed simulation environment the simulation of a batch process cell , as an example from the area of process control and an example of a manufacturing system , i.e. the control of a laboratory scale modular production system . ( c ) <digit> elsevier science b.v. all rights reserved .
action recognition feedback based framework for human pose reconstruction from monocular images . <eos> a novel framework based on action recognition feedback for pose reconstruction of articulated human body from monocular images is proposed in this paper . the intrinsic ambiguity caused by perspective projection makes it difficult to accurately recover articulated poses from monocular images . to alleviate such ambiguity , we exploit the high level motion knowledge as action recognition feedback to discard those implausible estimates and generate more accurate pose candidates using large number of motion constraints during natural human movement . the motion knowledge is represented by both local and global motion constraints . the local spatial constraint captures motion correlation between body parts by multiple relevance vector machines while the global temporal constraint preserves temporal coherence between time ordered poses via a manifold motion template . experiments on the cmu mocap database demonstrate that our method performs better on estimation accuracy than other methods without action recognition feedback .
burr size reduction in drilling by ultrasonic assistance . <eos> accuracy and surface finish play an important role in modern industry . undesired projections of materials , known as burrs , reduce the part quality and negatively affect the assembly process . a recent and promising method for reducing burr size in metal cutting is the use of ultrasonic assistance , where high frequency and low amplitude vibrations are added in the feed direction during cutting . note that this cutting process is distinct from ultrasonic machining . this paper presents the design of an ultrasonically vibrated workpiece holder , and a two stage experimental investigation of ultrasonically assisted drilling of a1100 <digit> aluminum workpieces . the results of <digit> drilling experiments with uncoated and tin coated drills are reported and analyzed . the effect of ultrasonic assistance on burr size , chip formation , thrust forces and tool wear is studied . the results demonstrate that under suitable ultrasonic vibration conditions , the burr height and width can be reduced in comparison to conventional drilling .
selecting coherent and relevant plots in large scatterplot matrices . <eos> the scatterplot matrix ( splom ) is a well established technique to visually explore high dimensional data sets . it is characterized by the number of scatterplots ( plots ) of which it consists of . unfortunately , this number quadratically grows with the number of the data sets dimensions . thus , an splom scales very poorly . consequently , the usefulness of sploms is restricted to a small number of dimensions . for this , several approaches already exist to explore such small sploms . those approaches address the scalability problem just indirectly and without solving it . therefore , we introduce a new greedy approach to manage large sploms with more than <digit> dimensions . we establish a combined visualization and interaction scheme that produces intuitively interpretable sploms by combining known quality measures , a pre process reordering and a perception based abstraction . with this scheme , the user can interactively find large amounts of relevant plots in large sploms .
the random electrode selection ensemble for eeg signal classification . <eos> pattern classification methods are a crucial direction in the current study of braincomputer interface ( bci ) technology . a simple yet effective ensemble approach for electroencephalogram ( eeg ) signal classification named the random electrode selection ensemble ( rese ) is developed , which aims to surmount the instability demerit of the fisher discriminant feature extraction for bci applications . through the random selection of recording electrodes answering for the physiological background of user intended mental activities , multiple individual classifiers are constructed . in a feature subspace determined by a couple of randomly selected electrodes , principal component analysis ( pca ) is first used to carry out dimensionality reduction . successively fisher discriminant is adopted for feature extraction , and a bayesian classifier with a gaussian mixture model ( gmm ) approximating the feature distribution is trained . for a test sample the outputs from all the bayesian classifiers are combined to give the final prediction for its label . theoretical analysis and classification experiments with real eeg signals indicate that the rese approach is both effective and efficient .
robust schur stability of polynomials with polynomial parameter dependency . <eos> the paper considers the robust schur stability verification of polynomials with coefficients depending polynomially on parameters varying in given intervals . a new algorithm is presented which relies on the expansion of a multivariate polynomial into bernstein polynomials and is based on the decomposition of the family of polynomials into its symmetric and antisymmetric parts . it is shown how the inspection of both polynomial families on the upper half of the unit circle can be reduced to the analysis of two related polynomial families on the real interval <digit> , <digit> . then the bernstein expansion can be applied in order to check whether both polynomial families have a zero in this interval in common .
use of nano scale double gate mosfets in low power tunable current mode analog circuits . <eos> use of independently driven nano scale double gate ( dg ) mosfets for low power analog circuits is emphasized and illustrated . in independent drive configuration , the top gate response of dg mosfets can be altered by application of a control voltage on the bottom gate . we show that this could be a powerful method to conveniently tune the response of conventional cmos analog circuits especially for current mode design . several examples of such circuits , including current mirrors , a differential current amplifier and differential integrators are illustrated and their performance gauged using tcad simulations . the topologies and biasing schemes explored here show how the nano scale dg mosfets may pave way for efficient , mismatch tolerant and smaller circuits with tunable characteristics .
cooperative triangulation in msbns without revealing subnet structures . <eos> multiply sectioned bayesian networks ( msbns ) provide a coherent framework for probabilistic inference in a cooperative multiagent distributed interpretation system . inference in msbns can be performed effectively using a compiled representation . the compilation involves the triangulation of the collective dependency structure ( a graph ) defined in terms of the union of a set of local dependency structures ( a set of graphs ) . privacy of agents eliminates the option to assemble these graphs at a central location and to triangulate their union . earlier work solved distributed triangulation in a restricted case . the method is conceptually complex and the correctness of its extension to the general case is difficult to justify . in this paper , we present a new method that is conceptually simpler and is efficient . we prove its correctness in the general case and demonstrate its performance experimentally ( c ) <digit> john wiley sons , inc .
the complexity of parallel evaluation of linear recurrence . <eos> the concept of computers such as c.mmp and illiac iv is to achieve computational speed up by performing several operations simultaneously with parallel processors . this type of computer organization is referred to as a parallel computer . in this paper , we prove upper bounds on speed ups achievable by parallel computers for a particular problem , the solution of first order linear recurrences . we consider this problem because it is important in practice and also because it is simply stated so that we might obtain some insight into the nature of parallel computation by studying it .
leukocyte image segmentation using simulated visual attention . <eos> computer aided automatic analysis of microscopic leukocyte is a powerful diagnostic tool in biomedical fields which could reduce the effects of human error , improve the diagnosis accuracy , save manpower and time . however , it is a challenging to segment entire leukocyte populations due to the changing features extracted in the leukocyte image , and this task remains an unsolved issue in blood cell image segmentation . this paper presents an efficient strategy to construct a segmentation model for any leukocyte image using simulated visual attention via learning by on line sampling . in the sampling stage , two types of visual attention , bottom up and top down together with the movement of the human eye are simulated . we focus on a few regions of interesting and sample high gradient pixels to group training sets . while in the learning stage , the svm ( support vector machine ) model is trained in real time to simulate the visual neuronal system and then classifies pixels and extracts leukocytes from the image . experimental results show that the proposed method has better performance compared to the marker controlled watershed algorithms with manual intervention and thresholding based methods .
on the construction of an aggregated measure of the development of interval data . <eos> we analyse some possibilities for constructing an aggregated measure of the development of socio economical objects in terms of their composite phenomenon ( i.e. , phenomenon described by many statistical features ) if the relevant data are expressed as intervals . such a measure , based on the deviation of the data structure for a given object from the benchmark of development is a useful tool for ordering , comparing and clustering objects . we present the construction of a composite phenomenon when it is described by interval data and discuss various aspects of stimulation and normalization of the diagnostic features as well as a definition of a benchmark of development ( based usually on optimum or expected levels of these features ) . our investigation includes the following options for the realization of this purpose transformation of the interval model into a singlevalued version without any significant loss of its statistical properties , standardization of pure intervals as well as definition of the interval ideal object . for the determination of a distance between intervals , the hausdorff formula is applied . the simulation study conducted and the empirical analysis showed that the first two variants are especially useful in practice .
user requirements for a web based spreadsheet mediated collaboration . <eos> this paper reports the initial results of a research project to investigate how to develop a web based spreadsheet mediated business collaboration system that could notably enhance the business processes presently carried out by small to medium sized enterprises . using a scenario based design approach , a set of user 's requirements were extracted from an appropriate field study . these requirements were then analysed in the context of well known usability principles , and a set of design implications were derived based on a selected set of hci design patterns related to cooperative interaction design . starting from that knowledge , suitable interactive collaboration scenarios have been drawn , from which a list of user interface requirements for a web based spreadsheet mediated collaboration system has been formulated .
automated estimation and analyses of meteorological drought characteristics from monthly rainfall data . <eos> the paper describes a new software package for automated estimation , display and analyses of various drought indices continuous functions of precipitation that allow quantitative assessment of meteorological drought events to be made . the software at present allows up to five different drought indices to be estimated . they include the decile index ( di ) , the effective drought index ( edi ) , the standardized precipitation index ( spi ) and deviations from the long term mean and median value . each index can be estimated from point and spatially averaged rainfall data and a number of options are provided for months ' selection and the type of the analysis , including a running mean , single value or multiple annual values . the software also allows spell run analysis to be performed and maps of a specific index to be constructed . the software forms part of the comprehensive computer package , developed earlier and designed to perform the multitude of water resources analyses and hydro meteorological data processing . the <digit> step procedure of setting up and running a typical drought assessment application is described in detail . the examples of applications are given primarily in the specific context of south asia where the software has been used .
introduction to the special issue on statistical signal extraction and filtering . <eos> the papers of the special issue on statistical signal extraction and filtering are introduced briefly and the invitation to contribute to the next issue to be devoted to this topic is reiterated . there follows an account of the history and the current developments in the areas of wienerkolmogorov and kalman filtering , which is a leading topic of the present issue . other topics will be treated in like manner in subsequent introductions .
worst case optimal approximation algorithms for maximizing triplet consistency within phylogenetic networks . <eos> the study of phylogenetic networks is of great interest to computational evolutionary biology and numerous different types of such structures are known . this article addresses the following question concerning rooted versions of phylogenetic networks . what is the maximum value of p 0,1 p <digit> , <digit> such that for every input set t of rooted triplets , there exists some network n n such that at least p t p t of the triplets are consistent with n n we call an algorithm that computes such a network ( where p is maximum ) worst case optimal . here we prove that the set containing all triplets ( the full triplet set ) in some sense defines p . moreover , given a network n n that obtains a fraction p p for the full triplet set ( for any p p ) , we show how to efficiently modify n n to obtain a fraction p p for any given triplet set t . we demonstrate the power of this insight by presenting a worst case optimal result for level <digit> phylogenetic networks improving considerably upon the <digit> <digit> fraction obtained recently by jansson , nguyen and sung . for level <digit> phylogenetic networks we show that p 0.61 p 0.61 . we emphasize that , because we are taking t t as a ( trivial ) upper bound on the size of an optimal solution for each specific input t , the results in this article do not exclude the existence of approximation algorithms that achieve approximation ratio better than p. finally , we note that all the results in this article also apply to weighted triplet sets .
direct search of feasible region and application to a crashworthy helicopter seat . <eos> the paper proposes a novel approach to identify the feasible region for a constrained optimisation problem . in engineering applications the search for the feasible region turns out to be extremely useful in the understanding of the problem as the feasible region defines the portion of the domain where design parameters can be ranged to fulfil the constraints imposed on performances , manufacturing and regulations . the search for the feasible region is not a trivial task as non convex , irregular and disjointed shapes can be found . the algorithm presented in this paper moves from the above considerations and proposes a recursive feasible infeasible segment bisection algorithm combined with support vector machine ( svm ) techniques to reduce the overall computational effort . the method is discussed and then illustrated by means of three simple analytical test cases in the first part of the paper . a real world application is finally presented the search for the survivability zone of a crashworthy helicopter seat under different crash conditions . a finite element model , including an anthropomorphic dummy , is adopted to simulate impacts that are characterised by different deceleration pulses and the proposed algorithm is used to investigate the influence of pulse shape on impact survivability .
feasibly constructive proofs and the propositional calculus ( preliminary version ) . <eos> the motivation for this work comes from two general sources . the first source is the basic open question in complexity theory of whether p equals np ( see <digit> and <digit> ) . our approach is to try to show they are not equal , by trying to show that the set of tautologies is not in np ( of course its complement is in np ) . this is equivalent to showing that no proof system ( in the general sense defined in <digit> ) for the tautologies is super in the sense that there is a short proof for every tautology . extended resolution is an example of a powerful proof system for tautologies that can simulate most standard proof systems ( see <digit> ) . the main theorem ( 5.5 ) in this paper describes the power of extended resolution in a way that may provide a handle for showing it is not super . the second motivation comes from constructive mathematics . a constructive proof of , say , a statement a must provide an effective means of finding a proof of a for each value of x , but nothing is said about how long this proof is as a function of x. if the function is exponential or super exponential , then for short values of x the length of the proof of the instance of a may exceed the number of electrons in the universe . in section <digit> , i introduce the system pv for number theory , and it is this system which i suggest properly formalizes the notion of a feasibly constructive proof .
sina semantic interpretation of user queries for question answering on interlinked data . <eos> the architectural choices underlying linked data have led to a compendium of data sources which contain both duplicated and fragmented information on a large number of domains . one way to enable non experts users to access this data compendium is to provide keyword search frameworks that can capitalize on the inherent characteristics of linked data . developing such systems is challenging for three main reasons . first , resources across different datasets or even within the same dataset can be homonyms . second , different datasets employ heterogeneous schemas and each one may only contain a part of the answer for a certain user query . finally , constructing a federated formal query from keywords across different datasets requires exploiting links between the different datasets on both the schema and instance levels . we present sina , a scalable keyword search system that can answer user queries by transforming user supplied keywords or natural languages queries into conjunctive sparql queries over a set of interlinked data sources . sina uses a hidden markov model to determine the most suitable resources for a user supplied query from different datasets . moreover , our framework is able to construct federated queries by using the disambiguated resources and leveraging the link structure underlying the datasets to query . we evaluate sina over three different datasets . we can answer <digit> queries from the qald <digit> correctly . moreover , we perform as well as the best question answering system from the qald <digit> competition by answering <digit> questions correctly while also being able to answer queries on distributed sources . we study the runtime of sina in its mono core and parallel implementations and draw preliminary conclusions on the scalability of keyword search on linked data .
generalized median string computation by means of string embedding in vector spaces . <eos> in structural pattern recognition the median string has been established as a useful tool to represent a set of strings . however , its exact computation is complex and of high computational burden . in this paper we propose a new approach for the computation of median string based on string embedding . strings are embedded into a vector space and the median is computed in the vector domain . we apply three different inverse transformations to go from the vector domain back to the string domain in order to obtain a final approximation of the median string . all of them are based on the weighted mean of a pair of strings . experiments show that we succeed to compute good approximations of the median string .
efficient indexing of the historical , present , and future positions of moving objects . <eos> although significant effort has been put into the development of efficient spatio temporal indexing techniques for moving objects , little attention has been given to the development of techniques that efficiently support queries about the past , present , and future positions of objects . the provisioning of such techniques is challenging , both because of the nature of the data , which reflects continuous movement , and because of the types of queries to be supported . this paper proposes the bb x index structure , which indexes the positions of moving objects , given as linear functions of time , at any time . the index stores linearized moving object locations in a forest of b trees . the index supports queries that select objects based on temporal and spatial constraints , such as queries that retrieve all objects whose positions fall within a spatial range during a set of time intervals . empirical experiments are reported that offer insight into the query and update performance of the proposed technique .
towards model driven unit testing . <eos> the model driven architecture ( mda ) approach for constructing software systems advocates a stepwise refinement and transformation process starting from high level models to concrete program code . in contrast to numerous research efforts that try to generate executable function code from models , we propose a novel approach termed model driven monitoring . on the model level the behavior of an operation is specified with a pair of uml composite structure diagrams ( visual contract ) , a visual notation for pre and post conditions . the specified behavior is implemented by a programmer manually . an automatic translation from our visual contracts to jml assertions allows for monitoring the hand coded programs during their execution . in this paper we present how we extend our approach to allow for model driven unit testing , where we utilize the generated jml assertions as test oracles . further , we present an idea how to generate sufficient test cases from our visual contracts with the help of model checking techniques .
sliding window based frequent pattern mining over data streams . <eos> finding frequent patterns in a continuous stream of transactions is critical for many applications such as retail market data analysis , network monitoring , web usage mining , and stock market prediction . even though numerous frequent pattern mining algorithms have been developed over the past decade , new solutions for handling stream data are still required due to the continuous , unbounded , and ordered sequence of data elements generated at a rapid rate in a data stream . therefore , extracting frequent patterns from more recent data can enhance the analysis of stream data . in this paper , we propose an efficient technique to discover the complete set of recent frequent patterns from a high speed data stream over a sliding window . we develop a compact pattern stream tree ( cps tree ) to capture the recent stream data content and efficiently remove the obsolete , old stream data content . we also introduce the concept of dynamic tree restructuring in our cps tree to produce a highly compact frequency descending tree structure at runtime . the complete set of recent frequent patterns is obtained from the cps tree of the current window using an fp growth mining technique . extensive experimental analyses show that our cps tree is highly efficient in terms of memory and time complexity when finding recent frequent patterns from a high speed data stream .
modeling cryptographic properties of voice and voice based entity authentication . <eos> strong and or multi factor entity authentication protocols are of crucial importancein building successful identity management architectures . popular mechanisms to achieve these types of entity authentication are biometrics , and , in particular , voice , for which there are especially interesting business cases in the telecommunication and financial industries , among others . despite several studies on the suitability of voice within entity authentication protocols , there has been little or no formal analysis of any such methods . in this paper we embark into formal modeling of seemingly cryptographic properties of voice . the goal is to define a formal abstraction for voice , in terms of algorithms with certain properties , that are of both combinatorial and cryptographic type . while we certainly do not expect to achieve the perfect mathematical model for a human phenomenon , we do hope that capturing some properties of voice in a formal model would help towards the design and analysis of voice based cryptographic protocols , as for entity authentication . in particular , in this model we design and formally analyze two voice based entity authentication schemes , the first being a voice based analogue of the conventional password transmission entity authentication scheme . we also design and analyze , in the recently introduced bounded retrieval model <digit> , one voice and password based entity authentication scheme that is additionally secure against intrusions and brute force attacks , including dictionary attacks .
inference of finite state transducers from regular languages . <eos> finite state transducers are models that are being used in different areas of pattern recognition and computational linguistics . one of these areas is machine translation , where the approaches that are based on building models automatically from training examples are becoming more and more attractive . finite state transducers are very adequate to be used in constrained tasks where training samples of pairs of sentences are available . a technique to infer finite state transducers is proposed in this work . this technique is based on formal relations between finite state transducers and finite state grammars . given a training corpus of inputoutput pairs of sentences , the proposed approach uses statistical alignment methods to produce a set of conventional strings from which a stochastic finite state grammar is inferred . this grammar is finally transformed into a resulting finite state transducer . the proposed methods are assessed through series of machine translation experiments within the framework of the eutrans project .
particle swarm optimization with preference order ranking for multi objective optimization . <eos> a new optimality criterion based on preference order ( po ) scheme is used to identify the best compromise in multi objective particle swarm optimization ( mopso ) . this scheme is more efficient than pareto ranking scheme , especially when the number of objectives is very large . meanwhile , a novel updating formula for the particles velocity is introduced to improve the search ability of the algorithm . the proposed algorithm has been compared with nsga ii and other two mopso algorithms . the experimental results indicate that the proposed approach is effective on the highly complex multi objective optimization problems .
real time deformation using modal analysis on graphics hardware . <eos> this paper presents an approach for fast simulating deformable objects that is suitable for interactive applications in computer graphics . linear modal analysis is often used to simulate small amplitude deformation . compared to traditional linear modal analysis where the cpu has been used to calculate the nodal displacements , the vertex program of gpu has been found widely adopted in the current applications . however the calculation suffers from great errors due to the limitation of the number of the input registers on gpu vertex pipeline . in our approach , we solve this problem by the fragment program . a series of 2d floating point textures are used to hold the model displacement matrix , the fragment program multiplies this matrix with the modal amplitude and sums up the results . experiments show that the proposed technique fully utilizes the parallelism nature of gpu , and runs in real time even for the complex models .
stability analysis of a class of general periodic neural networks with delays and impulses . <eos> based on the inequality analysis , matrix theory and spectral theory , a class of general periodic neural networks with delays and impulses is studied . some sufficient conditions are established for the existence and globally exponential stability of a unique periodic solution . furthermore , the results are applied to some typical impulsive neural network systems as special cases , with a real life example to show feasibility of our results .
neuroprotective properties of resveratrol and derivatives . <eos> stilbenoid compounds consist of a family of resveratrol derivatives . they have demonstrated promising activities in vitro and in vivo that indicate they may be useful in the prevention of a wide range of pathologies , such as cardiovascular diseases and cancers , as well have anti aging effects . more recently stilbenoid compounds have shown promise in the treatment and prevention of neurodegenerative disorders , such as huntingtons , parkinsons , and alzheimer 's diseases . this paper primarily focuses on the impact of stilbenoids in alzheimer 's disease and more specifically on the inhibition of amyloid peptide aggregation .
a new fuzzy multicriteria decision making method and its application in diversion of water . <eos> taking account of uncertainty in multicriteria decision making problems is crucial due to the fact that depending on how it is done , ranking of alternatives can be completely different . this paper utilizes linguistic values to evaluate the performance of qualitative criteria and proposes using appropriate shapes of fuzzy numbers to evaluate the performance of quantitative criteria for each problem with respect to its particular conditions . in addition , a process to determine the weights of criteria using fuzzy numbers , which considers their competition to gain greater weights and their influence on each other is described . a new fuzzy methodology is proposed to solve such a problem that utilizes parametric form of fuzzy numbers . the case study of diversion of water into lake urmia watershed , which is defined using triangular , trapezoidal , and bell shape fuzzy numbers demonstrates the utility of the proposed method . ( c ) <digit> elsevier ltd. all rights reserved .
scheduling divisible workloads on heterogeneous platforms . <eos> in this paper , we discuss several algorithms for scheduling divisible workloads on heterogeneous systems . our main contributions are ( i ) new optimality results for single round algorithms and ( ii ) the design of an asymptotically optimal multi round algorithm . this multi round algorithm automatically performs resource selection , a difficult task that was previously left to the user . because it is periodic , it is simpler to implement , and more robust to changes in the speeds of the processors and or communication links . on the theoretical side , to the best of our knowledge , this is the first published result assessing the absolute performance of a multi round algorithm . on the practical side , extensive simulations reveal that our multi round algorithm outperforms existing solutions on a large variety of platforms , especially when the communication to computation ratio is not very high ( the difficult case ) .
a connective ethnography of peer knowledge sharing and diffusion in a tween virtual world . <eos> prior studies have shown how knowledge diffusion occurs in classrooms and structured small groups around assigned tasks yet have not begun to account for widespread knowledge sharing in more native , unstructured group settings found in online games and virtual worlds . in this paper , we describe and analyze how an insider gaming practice spread across a group of tween players ages <digit> years in an after school gaming club that simultaneously participated in a virtual world called whyville.net . in order to understand how this practice proliferated , we followed the club members as they interacted with each other and members of the virtual world at large . employing connective ethnography to trace the movements in learning and teaching this practice , we coordinated data records from videos , tracking data , field notes , and interviews . we found that club members took advantage of the different spaces , people , and times available to them across whyville , the club , and even home and classroom spaces . by using an insider gaming practice , namely teleporting , rather than the more traditional individual person as our analytical lens , we were able to examine knowledge sharing and diffusion across the gaming spaces , including events in local small groups as well as encounters in the virtual world . in the discussion , we address methodological issues and design implications of our findings .
unsupervised classification of sar images using normalized gamma process mixtures . <eos> we propose an image prior for the model based nonparametric classification of synthetic aperture radar ( sar ) images that allows working with infinite number of mixture components . in order to enclose the spatial interactions of the pixel labels , the prior is derived by incorporating a conditional multinomial auto logistic random field into the normalized gamma process prior . in this way , we obtain an image classification prior that is free from the limitation on the number of classes and includes the smoothing constraint into classification problem . in this model , we introduced a hyper parameter that can control the preservation of the important classes and the extinction of the weak ones . the recall rates reported on the synthetic and the real terrasar x images show that the proposed model is capable of accurately classifying the pixels . unlike the existing methods , it applies a simple iterative update scheme without performing a hierarchical clustering strategy . we demonstrate that the estimation accuracy of the proposed method in number of classes outperforms the conventional finite mixture models .
two couple resolution blocking protocols on adaptive query splitting for rfid tag identification . <eos> how to accelerate tag identification is an important issue in radio frequency identification ( rfid ) systems . in some cases , the rfid reader repeatedly identifies the same tags since these tags always stay in its communication range . an anticollision protocol , called the adaptive query splitting protocol ( aqs ) , was proposed to handle these cases . this protocol reserves information obtained from the last process of tag identification so that the reader can quickly identify these staying tags again . this paper proposes two blocking protocols , a couple resolution blocking protocol ( crb ) and an enhanced couple resolution blocking protocol ( ecrb ) , based on aqs . crb and ecrb not only have the above mentioned capability as aqs but also use the blocking technique , which prohibits unrecognized tags from colliding with staying tags , to reduce the number of collisions . moreover , crb adopts a couple resolution technique to couple staying tags by simultaneously transmitting two id prefixes from the reader , while ecrb allows the reader to send only one id prefix to interrogate a couple of staying tags . thus , they only need half time to identify staying tags . we formally analyze the identification delay of crb and ecrb in the worst and average cases . our analytic and simulation results show that they obviously outperform aqs , and ecrb needs less transmitted bits than crb .
an active measurement system for shared environments . <eos> testbeds composed of end hosts deployed across the internet enable researchers to simultaneously conduct a wide variety of experiments . active measurement studies of internet path properties that require precisely crafted probe streams can be problematic in these environments . the reason is that load on the host systems from concurrently executing experiments ( as is typical in planetlab ) can significantly alter probe stream timings . in this paper we measure and characterize how packet streams from our local planetlab nodes are affected by experimental concurrency . we find that the effects can be extreme . we then set up a simple planetlab deployment in a laboratory testbed to evaluate these effects in a controlled fashion . we find that even relatively low load levels can cause serious problems in probe streams . based on these results , we develop a novel system called mad that can operate as a linux kernel module or as a stand alone daemon to support real time scheduling of probe streams . mad coordinates probe packet emission for all active measurement experiments on a node . we demonstrate the capabilities of mad , showing that it performs effectively even under very high levels of multiplexing and host system load .
policy based inconsistency management in relational databases . <eos> we define inconsistency management policies ( imps ) for real world applications . we show how imps relate to belief revision postulates , cqa , and relational algebra operators . we present several approaches to efficiently implement an imp based framework .
a new delay dependent stability criterion for linear neutral systems with norm bounded uncertainties in all system matrices . <eos> this paper deals with the problem of robust stability for a class of uncertain linear neutral systems . the uncertainties under consideration are of norm bounded type and appear in all system matrices . a new delay dependent stability criterion is obtained and formulated in the form of linear matrix inequalities ( lmis ) . neither model transformation nor bounding technique for cross terms is involved through derivation of the stability criterion . numerical examples show that the results obtained in this paper significantly improve the estimate of the stability limit over some existing results in the literature .
neutralization new insights into the problem of employee information systems security policy violations . <eos> employees ' failure to comply with information systems security policies is a major concern for information technology security managers . in efforts to understand this problem , is security researchers have traditionally viewed violations of is security policies through the lens of deterrence theory . in this article , we show that neutralization theory . a theory prominent in criminology but not yet applied in the context of is , provides a compelling explanation for is security policy violations and offers new insight into how employees rationalize this behavior . in doing so , we propose a theoretical model in which the effects of neutralization techniques are tested alongside those of sanctions described by deterrence theory . our empirical results highlight neutralization as an important factor to take into account with regard to developing and implementing organizational security policies and practices .
simplifying complex environments using incremental textured depth meshes . <eos> we present an incremental algorithm to compute image based simplifications of a large environment . we use an optimization based approach to generate samples based on scene visibility , and from each viewpoint create textured depth meshes ( tdms ) using sampled range panoramas of the environment . the optimization function minimizes artifacts such as skins and cracks in the reconstruction . we also present an encoding scheme for multiple tdms that exploits spatial coherence among different viewpoints . the resulting simplifications , incremental textured depth meshes ( itdms ) , reduce preprocessing , storage , rendering costs and visible artifacts . our algorithm has been applied to large , complex synthetic environments comprising millions of primitives . it is able to render them at <digit> <digit> frames a second on a pc with little loss in visual fidelity .
a neural approach to the underdetermined order recursive least squares adaptive filtering . <eos> the incorporation of the neural architectures in adaptive filtering applications has been addressed in detail . in particular , the underdetermined order recursive least squares ( urls ) algorithm , which lies between the well known normalized least mean square and recursive least squares algorithms , is reformulated via a neural architecture . the response of the neural network is seen to be identical to that of the algorithmic approach . together with the advantage of simple circuit realization , this neural network avoids the drawbacks of digital computation such as error propagation and matrix inversion , which is ill conditioned in most cases . it is numerically attractive because the quadratic optimization problem performs an implicit matrix inversion . also , the neural network offers the flexibility of easy alteration of the prediction order of the urls algorithm which may be crucial in some applications . it is rather difficult to achieve in the digital implementation , as one would have to use levinson recursions . the neural network can easily be integrated into a digital system through appropriate digital to analog and analog to digital converters .
bottleneck flows in unit capacity networks . <eos> the bottleneck network flow problem ( bnfp ) is a generalization of several well studied bottleneck problems such as the bottleneck transportation problem ( btp ) , bottleneck assignment problem ( bap ) , bottleneck path problem ( bpp ) , and so on . the bnfp can easily be solved as a sequence of o ( log n ) maximum flow problems on almost unit capacity networks . we observe that this algorithm runs in o ( min m ( <digit> <digit> ) . n ( <digit> <digit> ) m log n ) time by showing that the maximum flow problem on an almost unit capacity graph can be solved in o ( min m ( <digit> <digit> ) . n ( <digit> <digit> ) m ) time . we then propose a faster algorithm to solve the unit capacity bnfp in o ( min m ( n log n ) ( <digit> <digit> ) . m ( <digit> <digit> ) root log n ) time , an improvement by a factor of at least <digit> root log n. for dense graphs , the improvement is by a factor of root log n. on unit capacity simple graphs , we show that bnfp can be solved in o root n log n ) time , an improvement by a factor of root log n. as a consequence we have an o ( m root n log n ) algorithm for the btp with unit arc capacities . ( c ) <digit> elsevier b.v. all rights reserved .
taylor 's decomposition on four points for solving third order linear time varying systems . <eos> in the present paper , the use of three step difference schemes generated by taylor 's decomposition on four points for the numerical solutions of third order time varying linear dynamical systems is presented . the method is illustrated for the numerical analysis of an up converter used in communication systems .
bem formulation for von krmn plates . <eos> this work deals with nonlinear geometric plates in the context of von krmn 's theory . the formulation is written such that only the boundary in plane displacement and deflection integral equations for boundary collocations are required . at internal points , only out of plane rotation , curvature and in plane internal force representations are used . thus , only integral representations of these values are derived . the nonlinear system of equations is derived by approximating all densities in the domain integrals as single values , which therefore reduces the computational effort needed to evaluate the domain value influences . hyper singular equations are avoided by approximating the domain values using only internal nodes . the solution is obtained using a newton scheme for which a consistent tangent operator was derived .
on x variable filling and flipping for capture power reduction in linear decompressor based test compression environment . <eos> excessive test power consumption and growing test data volume are both serious concerns for the semiconductor industry . various low power x filling techniques and test data compression schemes were developed accordingly to address the above problems . these methods , however , often exploit the very same do n't care bits in the test cubes to achieve different objectives and hence may contradict each other . in this paper , we propose novel techniques to reduce scan capture power in linear decompressor based test compression environment , by employing algorithmic solutions to fill and flip x variables supplied to the linear decompressor . experimental results on benchmark circuits demonstrate that our proposed techniques significantly outperform existing solutions .
www based access to object oriented clinical databases the khospad project . <eos> khospad is a project aiming at improving the quality of the process of patient care concerning general practitionerpatienthospital relationships , using current information and networking technologies . the studied application field is a cardiology division , with hemodynamic laboratory and the population of ptca patients . data related to ptca patients are managed by arcadia , an object oriented database management system developed for the considered clinical setting . we defined a remotely accessible view of arcadia medical record , suitable for general practitioners ( gps ) caring patients after ptca , during the follow up period . using a pc , a modem and internet , an authorized gp can consult remotely the medical records of his ptca patients . main features of the application are related to the management and display of complex data , specifically characterized by multimedia and temporal features , based on an object oriented temporal data model .
fuzzy r subgroups with thresholds of near rings and implication operators . <eos> using the belongs to relation ( q ) and quasi coincidence with relation ( q ) between fuzzy points and fuzzy sets , the concept of ( alpha , beta ) fuzzy r subgroup of a near ring where alpha , beta are any two of epsilon , q , epsilon boolean and q , epsilon boolean or q with alpha not equal epsilon boolean and q is introduced and related properties are investigated . we also introduce the notion of a fuzzy r subgroup with thresholds which is a generalization of an ordinary fuzzy r subgroup and an ( epsilon , epsilon boolean or q ) fuzzy r subgroup . finally , we give the definition of an implication based fuzzy r subgroup .
a time accurate pseudo wavelet scheme for two dimensional turbulence . <eos> in this paper , we propose a wavelet taylor galerkin method for solving the two dimensional navier stokes equations . the discretization in time is performed before the spatial discretization by introducing second order generalization of the standard time stepping schemes with the help of taylor series expansion in time step . wavelet taylor galerkin schemes taking advantage of the wavelet bases capabilities to compress both functions and operators are presented . results for two dimensional turbulence are shown .
audio augmented paper for therapy and educational intervention for children with autistic spectrum disorder . <eos> physical tokens are artifacts which sustain cooperation between the children and therapists . therapists anchor children 's attention through physical tokens . therapists controlled children 's attention through physical tokens . the environment provides to the therapists the control of the flow of the therapeutic activity . the environment provides a good mean to stimulate fun and consequently to help children 's attention on listening tasks .
treating epilepsy via adaptive neurostimulation a reinforcement learning approach . <eos> this paper presents a now methodology for automatically learning an optimal neurostimulation strategy for the treatment of epilepsy . the technical challenge is to automatically modulate neurostimulation parameters , as a function of the observed eeg signal , so as to minimize the frequency and duration of seizures . the methodology leverages recent techniques from the machine learning literature , in particular the reinforcement learning paradigm , to formalize this optimization problem . we present an algorithm which is able to automatically learn an adaptive neurostimulation strategy directly from labeled training data acquired from animal brain tissues . our results suggest that this methodology can be used to automatically find a stimulation strategy which effectively reduces the incidence of seizures , while also minimizing the amount of stimulation applied . this work highlights the crucial role that modern machine learning techniques can play in the optimization of treatment strategies for patients with chronic disorders such as epilepsy .
load balanced parallel streamline generation on large scale vector fields . <eos> because of the ever increasing size of output data from scientific simulations , supercomputers are increasingly relied upon to generate visualizations . one use of supercomputers is to generate field lines from large scale flow fields . when generating field lines in parallel , the vector field is generally decomposed into blocks , which are then assigned to processors . since various regions of the vector field can have different flow complexity , processors will require varying amounts of computation time to trace their particles , causing load imbalance , and thus limiting the performance speedup . to achieve load balanced streamline generation , we propose a workload aware partitioning algorithm to decompose the vector field into partitions with near equal workloads . since actual workloads are unknown beforehand , we propose a workload estimation algorithm to predict the workload in the local vector field . a graph based representation of the vector field is employed to generate these estimates . once the workloads have been estimated , our partitioning algorithm is hierarchically applied to distribute the workload to all partitions . we examine the performance of our workload estimation and workload aware partitioning algorithm in several timings studies , which demonstrates that by employing these methods , better scalability can be achieved with little overhead .
an integrated research tool for x ray imaging simulation . <eos> this paper presents a software simulation package of the entire x ray projection radiography process including beam generation , absorber structure and composition , irradiation set up , radiation transport through the absorbing medium , image formation and dose calculation . phantoms are created as composite objects from geometrical or voxelized primitives and can be subjected to simulated irradiation process . the acquired projection images represent the two dimensional spatial distribution of the energy absorbed in the detector and are formed at any geometry , taking into account energy spectrum , beam geometry and detector response . this software tool is the evolution of a previously presented system , with new functionalities , user interface and an expanded range of applications . this has been achieved mainly by the use of combinatorial geometry for phantom design and the implementation of a monte carlo code for the simulation of the radiation interaction at the absorber and the detector .
requirements and solutions to software encapsulation and engineering in next generation manufacturing systems oooneida approach . <eos> this paper addresses the solutions enabling agile development , deployment and reconfiguration of software intensive automation systems both in discrete manufacturing and process technologies . as the key enabler for reaching the required level of flexibility of such systems , the paper discusses the issues of encapsulation , integration and re use of the automation intellectual property ( ip ) . the goals can be fulfilled by the use of a vendor independent concept of a reusable portable and scalable software module ( function block ) , as well as by a vendor independent automation device model . this paper also discusses the requirements of the methodology for the application of such modules in the time and cost effective specification , design , validation , realization and deployment of intelligent mechatronic components in distributed industrial automation and control systems . a new global initiative oooneida is presented , that targets these goals through the development of the automation object concept based on the recognized industrial standards iec61131 , iec61499 , iec61804 and unified modelling language ( uml ) and through the creation of the technological infrastructure for a new , open knowledge economy for automation components and automated industrial products . in particular , a web based repository for standardized automation solutions will be developed to serve as an electronic commerce facility in industrial automation businesses .
robust camera pose and scene structure analysis for service robotics . <eos> successful path planning and object manipulation in service robotics applications rely both on a good estimation of the robot 's position and orientation ( pose ) in the environment , as well as on a reliable understanding of the visualized scene . in this paper a robust real time camera pose and a scene structure estimation system is proposed . first , the pose of the camera is estimated through the analysis of the so called tracks . the tracks include key features from the imaged scene and geometric constraints which are used to solve the pose estimation problem . second , based on the calculated pose of the camera , i.e. robot , the scene is analyzed via a robust depth segmentation and object classification approach . in order to reliably segment the object 's depth , a feedback control technique at an image processing level has been used with the purpose of improving the robustness of the robotic vision system with respect to external influences , such as cluttered scenes and variable illumination conditions . the control strategy detailed in this paper is based on the traditional open loop mathematical model of the depth estimation process . in order to control a robotic system , the obtained visual information is classified into objects of interest and obstacles . the proposed scene analysis architecture is evaluated through experimental results within a robotic collision avoidance system . ( c ) <digit> elsevier b.v. all rights reserved .
nml , a schematic extension of f. esteva and l. godo 's logic mtl . <eos> a schematic extension nml of f.esteva and l.godo 's logic mtl is introduced in this paper . based on a new left continuous but discontinuous t norm , which was proposed by s.jenei and can be regarded as a kind of distorted nilpotent minimum , the semantics of nml is interpreted and the standard completeness theorem of nml is proved . the fact that the maximum and the minimum are definable from the negation and implication in nml and nm is discovered , which also leads to a modification of the nm axiom system . ( c ) <digit> elsevier b.v. all rights reserved .
verifying safety properties of concurrent java programs using <digit> valued logic . <eos> we provide a parametric framework for verifying safety properties of concurrent java programs . the framework combines thread scheduling information with information about the shape of the heap . this leads to error detection algorithms that are more precise than existing techniques . the framework also provides the most precise shape analysis algorithm for concurrent programs . in contrast to existing verification techniques , we do not put a bound on the number of allocated objects . the framework even produces interesting results when analyzing java programs with an unbounded number of threads . the framework is applied to successfully verify the following properties of a concurrent program concurrent manipulation of linked list based adt preserves the adt datatype invariant <digit> . the program does not perform inconsistent updates due to interference . the program does not reach a deadlock . the program does not produce run time errors due to illegal thread interactions . we also find bugs in erroneous versions of such implementations . a prototype of our framework has been implemented .
automatic discovery of theorems in elementary geometry . <eos> we present here a further development of the well known approach to automatic theorem proving in elementary geometry via algorithmic commutative algebra and algebraic geometry . rather than confirming refuting geometric statements ( automatic proving ) or finding geometric formulae holding among prescribed geometric magnitudes ( automatic derivation ) , in this paper we consider ( following kapur and mundy ) the problem of dealing automatically with arbitrary geometric statements ( i.e. , theses that do not follow , in general , from the given hypotheses ) aiming to find complementary hypotheses for the statements to become true . first we introduce some standard algebraic geometry notions in automatic proving , both for self containment and in order to focus our own contribution . then we present a rather successful but noncomplete method for automatic discovery that , roughly , proceeds adding the given conjectural thesis to the collection of hypotheses and then derives some special consequences from this new set of conditions . several examples are discussed in detail .
using support vector machines with a novel hybrid feature selection method for diagnosis of erythemato squamous diseases . <eos> in this paper , we developed a diagnosis model based on support vector machines ( svm ) with a novel hybrid feature selection method to diagnose erythemato squamous diseases . our proposed hybrid feature selection method , named improved f score and sequential forward search ( ifsfs ) , combines the advantages of filter and wrapper methods to select the optimal feature subset from the original feature set . in our ifsfs , we improved the original f score from measuring the discrimination of two sets of real numbers to measuring the discrimination between more than two sets of real numbers . the improved f score and sequential forward search ( sfs ) are combined to find the optimal feature subset in the process of feature selection , where , the improved f score is an evaluation criterion of filter method , and sfs is an evaluation system of wrapper method . the best parameters of kernel function of svm are found out by grid search technique . experiments have been conducted on different training test partitions of the erythemato squamous diseases dataset taken from uci ( university of california irvine ) machine learning database . our experimental results show that the proposed svm based model with ifsfs achieves 98.61 % classification accuracy and contains <digit> features . with these results , we conclude our method is very promising compared to the previously reported results . ( c ) <digit> elsevier ltd. all rights reserved .
domain specific languages from design to implementation application to video device drivers generation . <eos> domain specific languages ( dsl ) have many potential advantages in terms of software engineering ranging from increased productivity to the application of formal methods . although they have been used in practice for decades , there has been little study of methodology or implementation tools for the dsl approach . in this paper , we present our dsl approach and its application to a realistic domain the generation of video display device drivers . the presentation focuses on the validation of our proposed framework for domain specific languages , from design to implementation . the framework leads to a flexible design and structure , and provides automatic generation of efficient implementations of dsl programs . additionally , we describe an example of a complete dsl for video display adaptors and the benefits of the dsl approach for this application . this demonstrates some of the generally claimed benefits of using dsls increased productivity , higher level abstraction , and easier verification . this dsl has been fully implemented with our approach and is available . compose project url http www.irisa.fr compose gal .
mapping visual notations to mof compliant models with qvt relations . <eos> model centric methodologies rely on the definition of domain specific modeling languages for being able to create domain specific models . with mof the omg adopted a standard which provides the essential constructs for the definition of semantic language constructs ( abstract syntax ) . however , there are no specifications on how to define the notations ( concrete syntax ) for abstract syntax elements . usually , the concrete syntax of mof compliant languages is described informally . we propose to define mof based metamodels for abstract syntax and concrete syntax and to connect them by model transformations specified with qvt relations in a flexible , declarative way . using a qvt based transformation engine one can easily implement a model view controller architecture by integrating modeling tools and metadata repositories
financial early warning system model and data mining application for risk detection . <eos> one of the biggest problems of smes is their tendencies to financial distress because of insufficient finance background . in this study , an early warning system ( ews ) model based on data mining for financial risk detection is presented . chaid algorithm has been used for development of the ews . developed ews can be served like a tailor made financial advisor in decision making process of the firms with its automated nature to the ones who have inadequate financial background . besides , an application of the model implemented which covered <digit> smes based on turkish central bank ( tcb ) <digit> data . by using ews model , <digit> risk profiles , <digit> risk indicators , <digit> early warning signals , and <digit> financial road maps has been determined for financial risk mitigation .
a new wavelet algorithm to enhance and detect microcalcifications . <eos> we have proposed a new thresholding technique applied over wavelet coefficients for mammogram enhancement . we have utilized shannon entropy to find the best t in the wavelet domain . we have utilized tsallis entropy to find the best t in the wavelet domain . the proposed technique has better froc test with 96.5 % true positives and 0.36 false positives .
an overview of the bioasq large scale biomedical semantic indexing and question answering competition . <eos> this article provides an overview of the first bioasq challenge , a competition on large scale biomedical semantic indexing and question answering ( qa ) , which took place between march and september <digit> . bioasq assesses the ability of systems to semantically index very large numbers of biomedical scientific articles , and to return concise and user understandable answers to given natural language questions by combining information from biomedical articles and ontologies .
regreening the metropolis pathways to more ecological cities keynote address . <eos> eighty percent of the american population now lives in metropolitan regions whose geographic extent continues to expand even as many core cities and inner tier suburbs lose middle class populations , jobs , and tax base . urban sprawl and the socioeconomic polarizing of metropolitan america have been fostered by public policies including ( <digit> ) federal subsidies for new infrastructure on the urban fringe ( <digit> ) tax policies that favor home ownership over rental properties ( <digit> ) local zoning codes and ( <digit> ) federal and state neglect of older urban neighborhoods . in the face of diminished access to nature outside of metropolitan areas , locally based efforts to protect and restore greenspaces within urban areas seek to make older communities more habitable and more ecological . some pathways to more ecological cities include the following
a modified runs test for symmetry . <eos> we propose a modification of a modarresgastwirth test for the hypothesis of symmetry about a known center . by means of a monte carlo study we show that the modified test overtakes the original modarresgastwirth test for a wide spectrum of asymmetrical alternatives coming from the lambda family and for all assayed sample sizes . we also show that our test is the best runs test among the runs tests we have compared .
probability based approaches to vlsi circuit partitioning . <eos> iterative improvement two way min cut partitioning is an important phase in most circuit placement tools , and finds use in many other computer aided design ( cad ) applications . most iterative improvement techniques for circuit netlists like the fiduccia mattheyses ( fm ) method compute the gains of nodes using local netlist information that is only concerned with the immediate improvement in the cutset , this can lead to misleading gain information . krishnamurthy suggested a lookahead ( la ) gain calculation method to ameliorate this situation however , as we show , it leaves room for improvement . we present here a probabilistic gain computation approach called probabilistic partitioner ( prop ) that is capable of capturing the future implications of moving a node at the current time . we also propose an extended algorithm shrink prop that increases the provability of removing recently perturbed nets ( nets whose nodes have been moved for the first time ) from the cutset , experimental results on medium to large size acm sigda benchmark circuits show that prop and shrink prop outperform previous iterative improvement methods like fm ( bq . about <digit> % and <digit> % , respectively ) and la ( by about <digit> % and <digit> % , respectively ) . both prop and shrink prop also obtain much better cutsizes than many recent state of the art partitioners like eig1 , window melo , paraboli , gfm and cmetis ( by 4.5 % to <digit> % ) . our empirical timing results reveal that prop is appreciably faster than most recent techniques , we also obtain results on the more recent ispd <digit> benchmark suite that show similar substantial mincut improvements by prop and shrink prop over fm ( <digit> % and <digit> % , respectively ) . it is also noteworthy that shrink prop 's results are within 2.5 % of those obtained by hmetis . one of the best multilevel partitioners . however . the multilevel paradigm is orthogonal to shrink prop . further , since it is a flat partitioner , it has advantages over hmetis in partition driven placement applications .
shengbte a solver of the boltzmann transport equation for phonons . <eos> shengbte is a software package for computing the lattice thermal conductivity of crystalline bulk materials and nanowires with diffusive boundary conditions . it is based on a full iterative solution to the boltzmann transport equation . its main inputs are sets of second and third order interatomic force constants , which can be calculated using third party ab initio packages . dirac delta distributions arising from conservation of energy are approximated by gaussian functions . a locally adaptive algorithm is used to determine each process specific broadening parameter , which renders the method fully parameter free . the code is free software , written in fortran and parallelized using mpi . a complementary python script to help compute third order interatomic force constants from a minimum number of ab initio calculations , using a real space finite difference approach , is also publicly available for download . here we discuss the design and implementation of both pieces of software and present results for three example systems si , inas and lonsdaleite . program title shengbte catalogue identifier aesl_v1_0 program summary url http cpc.cs.qub.ac.uk summaries aesl_v1_0.html program obtainable from cpc program library , queens university , belfast , n. ireland licensing provisions gnu general public license , version <digit> no . of lines in distributed program , including test data , etc. <digit> no . of bytes in distributed program , including test data , etc. <digit> distribution format tar.gz programming language fortran <digit> , mpi . computer non specific . operating system unix linux . has the code been vectorized or parallelized yes , parallelized using mpi . ram up to several gb classification 7.9 . external routines lapack , mpi , spglib ( http spglib.sourceforge.net ) nature of problem calculation of thermal conductivity and related quantities , determination of scattering rates for allowed three phonon processes solution method iterative solution , locally adaptive gaussian broadening running time up to several hours on several tens of processors
tracing impact in a usability improvement process . <eos> analyzing usability improvement processes as they take place in real life organizations is necessary to understand the practice of usability work . this paper describes a case study where the usability of an information system is improved and a relationship between the improvements and the evaluation efforts is established . results show that evaluation techniques complemented each other by suggesting different kinds of usability improvement . among the techniques applied , a combination of questionnaires and metaphors of human thinking ( mot ) showed the largest mean impact and mot produced the largest number of impacts . logging of real life use of the system over <digit> months indicated six aspects of improved usability , where significant differences among evaluation techniques were found . concerning five of the six aspects think aloud evaluations and the above mentioned combination of questionnaire and mot performed equally well , and better than mot . based on the evaluations <digit> redesign proposals were developed and <digit> of these were implemented . four of the implemented redesigns where considered especially important . these evolved with inspiration from multiple evaluations and were informed by stakeholders with different kinds of expertise . our results suggest that practitioners should not rely on isolated evaluations . instead complementing techniques should be combined , and people with different expertise should be involved . ( c ) <digit> elsevier b.v. all rights reserved .
homan , a learning based negotiation method for holonic multi agent systems . <eos> holonic multi agent systems are a special category of multi agent systems that best fit to environments with numerous agents and high complexity . like in general multi agent systems , the agents in the holonic system may negotiate with each other . these systems have their own characteristics and structure , for which a specific negotiation mechanism is required . this mechanism should be simple , fast and operable in real world applications . it would be better to equip negotiators with a learning method which can efficiently use the available information . the learning method should itself be fast , too . additionally , this mechanism should match the special characteristics of the holonic multi agent systems . in this paper , we introduce such a negotiation method . experimental results demonstrate the efficiency of this new approach .
the portable common runtime approach to interoperability . <eos> operating system abstractions do not always reach high enough for direct use by a language or applications designer . the gap is filled by language specific runtime environments , which become more complex for richer languages ( commonlisp needs more than c , which needs more than c ) . but language specific environments inhibit integrated multi lingual programming , and also make porting hard ( for instance , because of operating system dependencies ) . to help solve these problems , we have built the portable common runtime ( pcr ) , a language independent and operating system independent base for modern languages . pcr offers four interrelated facilities storage management ( including universal garbage collection ) , symbol binding ( including static and dynamic linking and loading ) , threads ( lightweight processes ) , and low level i o ( including network sockets ) . pcr is common because these facilities simultaneously support programs in several languages . pcr supports c. cedar , scheme , and commonlisp intercalling and runs pre existing c and commonlisp ( kyoto ) binaries . pcr is portable because it uses only a small set of operating system features . the pcr source code is available for use by other researchers and developers .
efficient keyword search over virtual xml views . <eos> emerging applications such as personalized portals , enterprise search , and web integration systems often require keyword search over semi structured views . however , traditional information retrieval techniques are likely to be expensive in this context because they rely on the assumption that the set of documents being searched is materialized . in this paper , we present a system architecture and algorithm that can efficiently evaluate keyword search queries over virtual ( unmaterialized ) xml views . an interesting aspect of our approach is that it exploits indices present on the base data and thereby avoids materializing large parts of the view that are not relevant to the query results . another feature of the algorithm is that by solely using indices , we can still score the results of queries over the virtual view , and the resulting scores are the same as if the view was materialized . our performance evaluation using the inex data set in the quark ( bhaskar et al. in quark an efficient xquery full text implementation . in sigmod , <digit> ) open source xml database system indicates that the proposed approach is scalable and efficient .
a novel clustering method on time series data . <eos> time series is a very popular type of data which exists in many domains . clustering time series data has a wide range of applications and has attracted researchers from a wide range of discipline . in this paper a novel algorithm for shape based time series clustering is proposed . it can reduce the size of data , improve the efficiency and not reduce the effects by using the principle of complex network . firstly , one nearest neighbor network is built based on the similarity of time series objects . in this step , triangle distance is used to measure the similarity . of the neighbor network each node represents one time series object and each link denotes neighbor relationship between nodes . secondly , the nodes with high degrees are chosen and used to cluster . in clustering process , dynamic time warping distance function and hierarchical clustering algorithm are applied . thirdly , some experiments are executed on synthetic and real data . the results show that the proposed algorithm has good performance on efficiency and effectiveness . ( c ) <digit> elsevier ltd. all rights reserved .
an improved floating to fixed point conversion scheme for dct quantization algorithm . <eos> conventional fixed point implementation of the dct coefficients quantization algorithm in video compression may result in deteriorated image quality . the paper investigates this problem and proposes an improved floating to fixed point conversion scheme . with a proper scaling factor and a new established look up table , the proposed fixed point scheme can obtain bit wise consistence to the floating point realization . experimental results verify the validity of the proposed method .
a radial basis function network approach for the computation of inverse continuous time variant functions . <eos> this paper presents an efficient approach for the fast computation of inverse continuous time variant functions with the proper use of radial basis function networks ( rbfns ) . the approach is based on implementing rbfns for computing inverse continuous time variant functions via an overall damped least squares solution that includes a novel null space vector for singularities prevention . the singularities avoidance null space vector is derived from developing a sufficiency condition for singularities prevention that conduces to establish some characterizing matrices and an associated performance index .
cryptography on smart cards . <eos> this article presents an overview of the cryptographic primitives that are commonly implemented on smart cards . we also discuss attacks that can be mounted on smart cards as well as countermeasures against such attacks .
the antecedents of customer satisfaction and its link to complaint intentions in online shopping an integration of justice , technology , and trust . <eos> complaint behaviors are critical to maintaining customer loyalty in an online market . they provide insight into the customer 's experience of service failure and help to redress the failures . previous studies have shown the importance of customer satisfaction as a mediator for complaint intentions . it is important to examine the antecedents of customer satisfaction and its link to complaint intentions . online shoppers are both buyers of products services and users of web based systems . trust also plays a critical role in forming a psychological state with positive or negative feelings toward e vendors . in this context , there are three major concerns justice , technology and trust . this study proposes a research model to combine these issues , in order to investigate complaint intentions . data were collected from an online survey wherein subjects were encouraged to reflect on recent service failure experiences . the results from testing a structural equation model indicate that distributive and interactional justice contribute significantly to customer satisfaction and , in turn , to complaint intentions , but procedural justice does not . technology based features and trust are also important in determining the two target variables . the implications for managers and scholars are also discussed .
to divide and conquer search ranking by learning query difficulty . <eos> learning to rank plays an important role in information retrieval . in most of the existing solutions for learning to rank , all the queries with their returned search results are learnt and ranked with a single model . in this paper , we demonstrate that it is highly beneficial to divide queries into multiple groups and conquer search ranking based on query difficulty . to this end , we propose a method which first characterizes a query using a variety of features extracted from user search behavior , such as the click entropy , the query reformulation probability . next , a classification model is built on these extracted features to assign a score to represent how difficult a query is . based on this score , our method automatically divides queries into groups , and trains a specific ranking model for each group to conquer search ranking . experimental results on ranksvm and ranknet with a large scale evaluation dataset show that the proposed method can achieve significant improvement in the task of web search ranking .
defect reduction in pcb contract manufacturing operations . <eos> this study addresses the identification and improvement of a defect reducing process step in plated through hole ( pth ) technology of printed circuit board ( pcb ) assemblies . the process step discussed is a step in which the substrates are baked prior to assembly . while this step is developed to address defect problems faced by both oems and contract manufacturers alike , this paper discusses an experiment designed to improve the effect of the baking step that was performed at a pcb contract manufacturing facility . furthermore , due to the tremendous variations in product complexity , a relatively new statistical process control chart which tracks defects per millions of opportunities ( dpmo ) , was used to help evaluate the results . ( c ) <digit> elsevier science ltd .
a new form of dos attack in a cloud and its avoidance mechanism . <eos> data center networks are typically grossly under provisioned . this is not a problem in a corporate data center , but it could be a problem in a shared infrastructure , such as a co location facility or a cloud infrastructure . if an application is deployed in such an infrastructure , the application owners need to take into account the infrastructure limitations . they need to build in counter measures to ensure that the application is secure and it meets its performance requirements . in this paper , we describe a new form of dos attack , which exploits the network under provisioning in a cloud infrastructure . we have verified that such an attack could be carried out in practice in one cloud infrastructure . we also describe a mechanism to detect and avoid this new form of attack .
interdisciplinary applications of mathematical modeling . <eos> we demonstrate applications of numerical integration and visualization algorithms in diverse fields including psychological modeling ( biometrics ) in high energy physics for the study of collisions of elementary particles and in medical physics for regulating the dosage of proton beam radiation therapy . we discuss the problems and solution methods , as supported by numerical results .
the probability ranking principle revisited . <eos> a theoretic framework for multimedia information retrieval is introduced which guarantees optimal retrieval effectiveness . in particular . a ranking principle for distributed multimedia documents ( rpdm ) is described together with an algorithm that satisfies this principle . finally , the rpdm is shown to be a generalization of the probability ranking principle ( prp ) which guarantees optimal retrieval effectiveness in the case of text document retrieval . the prp justifies theoretically the relevance ranking adopted by modern search engines . in contrast to the classical prp . the new rpdm takes into account transmission and inspection time , and most importantly , aspectual recall rather than simple recall .
how users associate wireless devices . <eos> in a wireless world , users can establish connections between devices spontaneously , and unhampered by cables . however , in the absence of cables , what is the natural interaction to connect one device with another a wide range of device association techniques have been demonstrated , but it has remained an open question what actions users would spontaneously choose for device association . we contribute a study eliciting device association actions from non technical users without premeditation . over <digit> user defined actions were collected for <digit> different device combinations . we present a classification of user defined actions , and observations of the users ' rationale . our findings indicate that there is no single most spontaneous action instead five prominent categories of user defined actions were found .
analyticity of weighted central paths and error bounds for semidefinite programming . <eos> the purpose of this paper is two fold . firstly , we show that every cholesky based weighted central path for semidefinite programming is analytic under strict complementarity . this result is applied to homogeneous cone programming to show that the central paths defined by the known class of optimal self concordant barriers are analytic in the presence of strictly complementary solutions . secondly , we consider a sequence of primal dual solutions that lies within a prescribed neighborhood of the central path of a pair of primal dual semidefinite programming problems , and converges to the respective optimal faces . under the additional assumption of strict complementarity , we derive two necessary and sufficient conditions for the sequence of primal dual solutions to converge linearly with their duality gaps .
multi class blue noise sampling . <eos> sampling is a core process for a variety of graphics applications . among existing sampling methods , blue noise sampling remains popular thanks to its spatial uniformity and absence of aliasing artifacts . however , research so far has been mainly focused on blue noise sampling with a single class of samples . this could be insufficient for common natural as well as man made phenomena requiring multiple classes of samples , such as object placement , imaging sensors , and stippling patterns .
a note on the inventory models for deteriorating items with ramp type demand rate . <eos> in this research we study the inventory models for deteriorating items with ramp type demand rate . we first clearly point out some questionable results that appeared in ( mandal , b. , pal , a.k. , <digit> . order level inventory system with ramp type demand rate for deteriorating items . journal of interdisciplinary mathematics <digit> , <digit> and wu , k.s. , ouyang , l.y. , <digit> . a replenishment policy for deteriorating items with ramp type demand rate ( short communication ) . proceedings of national science council roc ( a ) <digit> , <digit> ) . and then resolve the similar problem by offering a rigorous and efficient method to derive the optimal solution . in addition , we also propose an extended inventory model with ramp type demand rate and its optimal feasible solution to amend the incompleteness in the previous work . moreover , we also proposed a very good inventory replenishment policy for this kind of inventory model . we believe that our work will provide a solid foundation for the further study of this sort of important inventory models with ramp type demand rate .
efficient multiple faces tracking based on relevance vector machine and boosting learning . <eos> a multiple faces tracking system was presented based on relevance vector machine ( rvm ) and boosting learning . in this system , a face detector based on boosting learning is used to detect faces at the first frame , and the face motion model and color model are created . the face motion model consists of a set of rvms that learn the relationship between the motion of the face and its appearance , and the face color model is the 2d histogram of the face region in crcb color space . in the tracking process different tracking methods ( rvm tracking , local search , giving up tracking ) are used according to different states of faces , and the states are changed according to the tracking results . when the full image search condition is satisfied , a full image search is started in order to find new coming faces and former occluded faces . in the full image search and local search , the similarity matrix is introduced to help matching faces efficiently . experimental results demonstrate that this system can ( a ) automatically find new coming faces ( b ) recover from occlusion , for example , if the faces are occluded by others and reappear or leave the scene and return ( c ) run with a high computation efficiency , run at about <digit> frames s. ( c ) <digit> elsevier inc. all rights reserved .
a design flow for application specific networks on chip with guaranteed performance to accelerate soc design and verification . <eos> systems on chip ( soc ) are composed of intellectual property blocks ( ip ) and interconnect . while mature tooling exists to design the former , tooling for interconnect design is still a research area . in this paper we describe an operational design flow that generates and configures application specific network on chip ( noc ) instances , given application communication requirements . the noc can be simulated in systemc and rtl vhdl . an independent performance verification tool verifies analytically that the noc instance ( hardware ) and its configuration ( software ) together meet the application performance requirements . the thereal noc 's guaranteed performance is essential to replace time consuming simulation by fast analytical performance validation . as a result , application specific nocs that are guaranteed to meet the application 's communication requirements are generated and verified in minutes , reducing the number of design iterations . a realistic mpeg soc example substantiates our claims .
homing pigeon based messaging multiple pigeon assisted delivery in delay tolerant networks . <eos> in this paper , we consider the applications of delay tolerant networks ( dtns ) , where the nodes in a network are located in separated areas , and in each separated area , there exists ( at least ) an anchor node that provides regional network coverage for the nearby nodes . the anchor nodes are responsible for collecting and distributing messages for the nodes in the vicinity . this work proposes to use a set of messengers ( named pigeons ) that move around the network to deliver messages among multiple anchor nodes . each source node ( anchor node or internet access point ) owns multiple dedicated pigeons , and each pigeon takes a round trip starting from its home ( i.e. , the source ) through the destination anchor nodes and then returns home , disseminating the messages on its way . we named this as a homing pigeon based messaging ( hopm ) scheme . the hopm scheme is different from the prior schemes in that each messenger is completely dedicated to its home node for providing messaging service . we obtained the average message delay of hopm scheme in dtn through theoretical analysis with three different pigeon scheduling schemes . the analytical model was validated by simulations . we also studied the effects of several key parameters on the system performance and compared the results with previous solutions . the results allowed us to better understand the impacts of different scheduling schemes on the system performance of hopm and demonstrated that our proposed scheme outperforms the previous ones . copyright ( c ) <digit> john wiley sons , ltd .
the west nile virus encephalitis outbreak in the united states ( <digit> <digit> ) . <eos> viruses cause most forms of encephalitis . the two main types responsible for epidemic encephalitis are enteroviruses and arboviruses . the city of new york reports about <digit> cases of encephalitis yearly . establishing a diagnosis is often difficult . in august <digit> , a cluster of five patients with fever , confusion , and weakness were admitted to a community hospital in flushing , new york . flaccid paralysis developed in four of the five patients , and they required ventilatory support . three , less severe , cases presented later in the same month . an investigation was conducted by the new york city ( nyc ) and new york state ( nys ) health departments and the national centers for disease control and prevention ( cdc ) . the west nile virus ( wnv ) was identified as the etiologic agent . wnv is an arthropod borne flavivirus , with a geographic distribution in africa , the middle east , and southwestern asia . it has also been isolated in australia and sporadically in europe but never in the americas . the majority of people infected have no symptoms . fever , severe myalgias , headache , conjunctivitis , lymphadenopathy , and a roseolar rash can occur . rarely , encephalitis or meningitis is seen . the nyc outbreak resulted in the first cases of wnv infection in the western hemisphere and the first arboviral infection in nyc since yellow fever in the nineteenth century . the wnv is now a public health concern in the united states .
existence results for impulsive neutral second order stochastic evolution equations with nonlocal conditions . <eos> in this paper we consider a class of impulsive neutral second order stochastic evolution equations with nonlocal initial conditions in a real separable hilbert space . sufficient conditions for the existence of mild solutions are established by operator theory and the sadovskii fixed point theorem . an example is provided to illustrate the theory . ( c ) <digit> elsevier ltd. all rights reserved .
distribution network design new problems and related models . <eos> we study some complex distribution network design problems , which involve facility location , warehousing , transportation and inventory decisions . several realistic scenarios are investigated . two kinds of mathematical programming formulations are proposed for all the introduced problems , together with a proof of their correctness . some formulations extend models proposed by perl and daskin ( <digit> ) for some warehouse location routing problems other formulations are based on flow variables and constraints .
lyapunov based nonlinear controllers for obstacle avoidance with a planar n link doubly nonholonomic manipulator . <eos> a mobile manipulator is a robotic system made up of two components a mobile platform and a manipulator mounted on the platform equipped with non deformable wheels . such a combined system requires complex design and control . this paper considers the autonomous navigation problem of a nonholonomic mobile platform and an n link nonholonomic manipulator fixed to the platform . for this planar n link doubly nonholonomic manipulator , we present the first ever set of nonlinear continuous controllers for obstacle avoidance . the controllers provide a collision free trajectory within a constrained workspace cluttered with fixed obstacles of different shapes and sizes whilst satisfying the nonholonomic and kinodynamic constraints associated with the robotic system . an advantage of the proposed method is the ease at which the acceleration based control laws can be derived from the lyapunov function . the effectiveness of the nonholonomic planner is demonstrated via computer simulations . ( c ) <digit> elsevier b.v. all rights reserved .
automatic analysis of trabecular bone structure from knee mri . <eos> we investigated the feasibility of quantifying osteoarthritis ( oa ) by analysis of the trabecular bone structure in low field knee mri . generic texture features were extracted from the images and subsequently selected by sequential floating forward selection ( sffs ) , following a fully automatic , uncommitted machine learning based framework . six different classifiers were evaluated in cross validation schemes and the results showed that the presence of oa can be quantified by a bone structure marker . the performance of the developed marker reached a generalization area under the roc ( auc ) of 0.82 , which is higher than the established cartilage markers known to relate to the oa diagnosis .
an application of fuzzy sets theory to the eoq model with imperfect quality items . <eos> this article investigates the inventory problem for items received with imperfect quality , where , upon the arrival of order lot , <digit> % screening process is performed and the items of imperfect quality are sold as a single batch at a discounted price , prior to receiving the next shipment . the objective is to determine the optimal order lot size to maximize the total profit . we first propose a model with fuzzy defective rate . then , the model with fuzzy defective rate and fuzzy annual demand is presented . for each case , we employ the signed distance , a ranking method for fuzzy numbers , to find the estimate of total profit per unit time in the fuzzy sense , and then derive the corresponding optimal lot size . numerical examples are provided to illustrate the results of proposed models .
maximin performance of binary input channels with uncertain noise distributions . <eos> we consider uncertainty classes of noise distributions defined by a bound on the divergence with respect to a nominal noise distribution . the noise that maximizes the minimum error probability for binary input channels is found . the effect of the reduction in uncertainty brought about by knowledge of the signal to noise ratio is also studied . the particular class of gaussian nominal distributions provides an analysis tool for near gaussian channels . asymptotic behavior of the least favorable noise distribution and resulting error probability are studied in a variety of scenarios , namely asymptotically small divergence with and without power constraint asymptotically large divergence with and without power constraint and asymptotically large signal to noise ratio .
alleviating the problem of local minima in backpropagation through competitive learning . <eos> the backpropagation ( bp ) algorithm is widely recognized as a powerful tool for training feedforward neural networks ( fnns ) . however , since the algorithm employs the steepest descent technique to adjust the network weights , it suffers from a slow convergence rate and often produces suboptimal solutions , which are the two major drawbacks of bp . this paper proposes a modified bp algorithm which can remarkably alleviate the problem of local minima confronted with by the standard bp ( sbp ) . as one output of the modified training procedure , a bucket of all the possible solutions of weights matrices found during training is acquired , among which the best solution is chosen competitively based upon their performances on a validation dataset . simulations are conducted on four benchmark classification tasks to compare and evaluate the classification performances and generalization capabilities of the proposed modified bp and sbp .
towards categorical models for fairness fully abstract presheaf semantics of sccs with finite delay . <eos> we present a presheaf model for the observation of infinite as well as finite computations . we give a concrete representation of the presheaf model as a category of generalised synchronisation trees and show that it is coreflective in a category of generalised transition systems , which are a special case of the general transition systems of hennessy and stirling . this can be viewed as a first step towards representing fairness in categorical models for concurrency . the open map bisimulation is shown to coincide with extended bisimulation of hennessy and stirling , which is essentially fair ctl bisimulation . we give a denotational semantics of milner 's sccs with finite delay in the presheaf model , which differs from previous semantics by giving the meanings of recursion by final coalgebras and meanings of finite delay by initial algebras of the process equations for delay . finally , we formulate milner 's operational semantics of sccs with finite delay in terms of generalised transition systems and prove that the presheaf semantics is fully abstract with respect to extended bisimulation . ( c ) <digit> published by elsevier science b.v.
determining efficient temperature sets for the simulated tempering method . <eos> in statistical physics , the efficiency of tempering approaches strongly depends on ingredients such as the number of replicas r r , reliable determination of weight factors and the set of used temperatures , tr t1 , t2 , , tr t r t <digit> , t <digit> , , t r . for the simulated tempering ( st ) in particularuseful due to its generality and conceptual simplicitythe latter aspect ( closely related to the actual r r ) may be a key issue in problems displaying metastability and trapping in certain regions of the phase space . to determine tr t r s leading to accurate thermodynamics estimates and still trying to minimize the simulation computational time , here a fixed exchange frequency scheme is considered for the st. from the temperature of interest t1 t <digit> , successive t t s are chosen so that the exchange frequency between any adjacent pair tr t r and tr <digit> t r <digit> has a same value f f . by varying the f f s and analyzing the tr t r s through relatively inexpensive tests ( e.g. , time decay towards the steady regime ) , an optimal situation in which the simulations visit much faster and more uniformly the relevant portions of the phase space is determined . as illustrations , the proposal is applied to three lattice models , beg , belllavis , and potts , in the hard case of extreme first order phase transitions , always giving very good results , even for r <digit> r <digit> . also , comparisons with other protocols ( constant entropy and arithmetic progression ) to choose the set tr t r are undertaken . the fixed exchange frequency method is found to be consistently superior , specially for small r r s. finally , distinct instances where the prescription could be helpful ( in second order transitions and for the parallel tempering approach ) are briefly discussed .
a numerical method for solving variable coefficient elliptic equation with interfaces . <eos> a new 2nd order accurate numerical method on non body fitting grids is proposed for solving the variable coefficient elliptic equation in disjoint subdomains separated by interfaces . the variable coefficients , the source term , and hence the solution itself and its derivatives may be discontinuous across the interfaces . jump conditions in solution and its co normal derivative at interface are prescribed . instead of smooth , the interfaces are only required to be lipschitz continuous as submanifold . a weak formulation is developed , the existence , uniqueness and regularity of the solutions are studied . the numerical method is derived by discretizing the weak formulation . the method is different from traditional finite element methods . extensive numerical experiments are presented and show that the method is 2nd order accurate in solution and 1st order accurate in its gradient in l norm if the interface is c2 and solutions are c2 on the closures of the subdomains . the method can handle the problems when the solutions and or the interfaces are weaker than c2 . for example , u h2 ( ) , is lipschitz continuous and their singularities coincide , see example <digit> in section <digit> . the accuracies of the method under various circumstances are listed in table <digit> .
attitudes of community pharmacists , university based pharmacists , and students toward on line information resources . <eos> the study sought to explore the attitudes of community pharmacists , university based pharmacists , and pharmacy students before and after exposure to computerized systems of on line information services . a <digit> item attitudinal survey was administered to <digit> community pharmacists , <digit> university clinical pharmacist faculty , and <digit> senior pharmacy students , prior to and at the end of a year of access to grateful med ( r ) and brs colleague ( r ) . few significant differences were noted among the participants at baseline . no significant interaction effect differences for type of participant or system used were found . participants were generally positive about computers in general , the accuracy of on line information services , their impact on knowledge and confidence , and their usefulness for pharmacists .
comparison of several approaches to the linear approximation of the yield condition and application to the robust design of plane frames for the case of uncertainty . <eos> since the yield condition for frame structures is non linear , piecewise linear approximations are needed in order to apply linear optimization methods . four approaches are presented and compared . after the theoretical consideration and comparison of the different approximation methods , they are applied to the robust design of an <digit> bar frame in case of uncertainty . here , the less restrictive methods yield the cheapest design , as expected . it will be shown , that the approximation from inside of first level does not cause much higher costs than the other methods . but since its constraints are sufficient in contrast to other approximations , it is recommended .

a class of differential vector variational inequalities in finite dimensional spaces . <eos> in this paper , we introduce and study a class of differential vector variational inequalities in finite dimensional euclidean spaces . we establish a relationship between differential vector variational inequalities and differential scalar variational inequalities . under various conditions , we obtain the existence and linear growth of solutions to the scalar variational inequalities . in particular we prove existence theorems for carathodory weak solutions of the differential vector variational inequalities . furthermore , we give a convergence result on euler time dependent procedure for solving the initial value differential vector variational inequalities .
adaptive hypermedia . <eos> adaptive hypermedia is a relatively new direction of research on the crossroads of hypermedia and user modeling . adaptive hypermedia systems build a model of the goals , preferences and knowledge of each individual user , and use this model throughout the interaction with the user , in order to adapt to the needs of that user . the goal of this paper is to present the state of the art in adaptive hypermedia at the eve of the year <digit> , and to highlight some prospects for the future . this paper attempts to serve both the newcomers and the experts in the area of adaptive hypermedia by building on an earlier comprehensive review ( brusilovsky , <digit> brusilovsky , <digit> ) .
framing design in the third paradigm . <eos> this paper develops vocabulary to discuss the phenomena related to the new design paradigm , which considers designing as a situated and constructive activity of meaning making rather than as problem solving . the paper studies how design projects proceed from the fuzzy early phases towards the issues of central relevance to designing . a central concept is framing , and it is elaborated with examples from two case studies . several aspects of framing are explicated , exploratory , anticipatory and social framing , and related concepts of ' focusing ' , ' priming ' , and ' grounding ' are explained . the paper concludes that understanding designing as a situated and constructive making of meaning has bearings on how designing needs to be supported .
interval evaluations in the analytic hierarchy process by possibility analysis . <eos> since a pairwise comparison matrix in the analytic hierarchy process ( ahp ) is based on human intuition , the given matrix will always include inconsistent elements violating the transitivity property . we propose the interval ai ip by which interval weights can be obtained . the widths of the estimated interval weights represent inconsistency in judging data . since interval weights can be obtained from inconsistent data , the proposed interval ai ip is more appropriate to human judgment . assuming crisp values in a pairwise comparison matrix , the interval comparisons including the given crisp comparisons can be obtained by applying the linear programming ( lp ) approach . using an interval preference relation , the interval ahp for crisp data can be extended to an approach for interval data allowing to express the uncertainty of human judgment in pairwise comparisons .
a model for real time failure prognosis based on hidden markov model and belief rule base . <eos> as one of most important aspects of condition based maintenance ( cbm ) , failure prognosis has attracted an increasing attention with the growing demand for higher operational efficiency and safety in industrial systems . currently there are no effective methods which can predict a hidden failure of a system real time when there exist influences from the changes of environmental factors and there is no such an accurate mathematical model for the system prognosis due to its intrinsic complexity and operating in potentially uncertain environment . therefore , this paper focuses on developing a new hidden markov model ( hmm ) based method which can deal with the problem . although an accurate model between environmental factors and a failure process is difficult to obtain , some expert knowledge can be collected and represented by a belief rule base ( brb ) which is an expert system in fact . as such , combining the hmm with the brb , a new prognosis model is proposed to predict the hidden failure real time even when there are influences from the changes of environmental factors . in the proposed model , the hmm is used to capture the relationships between the hidden failure and monitored observations of a system . the brb is used to model the relationships between the environmental factors and the transition probabilities among the hidden states of the system including the hidden failure , which is the main contribution of this paper . moreover , a recursive algorithm for online updating the prognosis model is developed . an experimental case study is examined to demonstrate the implementation and potential applications of the proposed real time failure prognosis method .
computing the volume of a union of balls a certified algorithm . <eos> balls and spheres are amongst the simplest 3d modeling primitives , and computing the volume of a union of balls is an elementary problem . although a number of strategies addressing this problem have been investigated in several communities , we are not aware of any robust algorithm , and present the first such algorithm . our calculation relies on the decomposition of the volume of the union into convex regions , namely the restrictions of the balls to their regions in the power diagram . theoretically , we establish a formula for the volume of a restriction , based on gauss ' divergence theorem . the proof being constructive , we develop the associated algorithm . on the implementation side , we carefully analyse the predicates and constructions involved in the volume calculation , and present a certified implementation relying on interval arithmetic . the result is certified in the sense that the exact volume belongs to the interval computed . experimental results are presented on hand crafted models illustrating various difficulties , as well as on the 58,898 models found in the tenth of july <digit> release of the protein data bank .
simple polynomial multiplication algorithms for exact conditional tests of linearity in a logistic model . <eos> the linear logistic model is often employed in the analysis of binary response data . the well known asymptotic chi square and likelihood ratio tests are usually used to detect the assumption of linearity in such a model . for small , sparse , or skewed data , the asymptotic theory is however dubious and exact conditional chi square and likelihood ratio tests may provide reliable alternatives . in this article , we propose efficient polynomial multiplication algorithms to compute exact significance levels as well as exact powers of these tests . two options , namely the cell and stage wise approaches , in implementing these algorithms will be discussed . when sample sizes are large , we propose an efficient monte carlo method for estimating the exact significance levels and exact powers . real data are used to demonstrate the performance with an application of the proposed algorithms .
on the definitions of anonymity for ring signatures . <eos> this paper studies the relations among several definitions of anonymity for ring signature schemes in the same attack environment . it is shown that one intuitive and two technical definitions we consider are asymptotically equivalent , and the indistinguishability based technical definition is the strongest , i.e. , the most secure when achieved . when the exact reduction cost is taken into account . we then extend our result to the threshold case where a subset of members cooperate to create a signature . the threshold setting makes the notion of anonymity more complex and yields a greater variety of definitions . we explore several notions and observe certain relation does not seem hold unlike the simple single signer case . nevertheless , we see that an indistinguishability based definition is the most favorable in the threshold case . we also study the notion of linkability and present a simple scheme that achieves both anonymity and linkability .
scalable proximity estimation and link prediction in online social networks . <eos> proximity measures quantify the closeness or similarity between nodes in a social network and form the basis of a range of applications in social sciences , business , information technology , computer networks , and cyber security . it is challenging to estimate proximity measures in online social networks due to their massive scale ( with millions of users ) and dynamic nature ( with hundreds of thousands of new nodes and millions of edges added daily ) . to address this challenge , we develop two novel methods to efficiently and accurately approximate a large family of proximity measures . we also propose a novel incremental update algorithm to enable near real time proximity estimation in highly dynamic social networks . evaluation based on a large amount of real data collected in five popular online social networks shows that our methods are accurate and can easily scale to networks with millions of nodes . to demonstrate the practical values of our techniques , we consider a significant application of proximity estimation link prediction , i.e. , predicting which new edges will be added in the near future based on past snapshots of a social network . our results reveal that ( i ) the effectiveness of different proximity measures for link prediction varies significantly across different online social networks and depends heavily on the fraction of edges contributed by the highest degree nodes , and ( ii ) combining multiple proximity measures consistently yields the best link prediction accuracy .
applications of regional strain energy in compliant structure design for energy absorption . <eos> topology optimization of regional strain energy is studied in this paper . unlike the conventional mean compliance formulation , this paper considers two main functions of structure rigidity and compliance . for normal usages , rigidity is chosen as the design objective . for compliant design , a portion of the structure absorbs energy , while another part maintains the structural integrity . therefore , we implemented a regional strain energy formulation for topology optimization . sensitivity to regional strain energy is derived from the adjoint method . numerical results from the proposed formulation are presented .
conversion of control dependence to data dependence . <eos> program analysis methods , especially those which support automatic vectorization , are based on the concept of interstatement dependence where a dependence holds between two statements when one of the statements computes values needed by the other . powerful program transformation systems that convert sequential programs to a form more suitable for vector or parallel machines have been developed using this concept allk <digit> , kklw <digit> . the dependence analysis in these systems is based on data dependence . in the presence of complex control flow , data dependence is not sufficient to transform programs because of the introduction of control dependences . a control dependence exists between two statements when the execution of one statement can prevent the execution of the other . control dependences do not fit conveniently into dependence based program translators.one solution is to convert all control dependences to data dependences by eliminating goto statements and introducing logical variables to control the execution of statements in the program . in this scheme , action statements are converted to if statements . the variables in the conditional expression of an if statement can be viewed as inputs to the statement being controlled . the result is that control dependences between statements become explicit data dependences expressed through the definitions and uses of the controlling logical variables.this paper presents a method for systematically converting control dependences to data dependences in this fashion . the algorithms presented here have been implemented in pfc , an experimental vectorizer written at rice university .
an integration scheme for electromagnetic scattering using plane wave edge elements . <eos> finite element techniques for the simulation of electromagnetic wave propagation are , like all conventional element based approaches for wave problems , limited by the ability of the polynomial basis to capture the sinusoidal nature of the solution . the partition of unity method ( pum ) has recently been applied successfully , in finite and boundary element algorithms , to wave propagation . in this paper , we apply the pum approach to the edge finite elements in the solution of maxwells equations . the electric field is expanded in a set of plane waves , the amplitudes of which become the unknowns , allowing each element to span a region containing multiple wavelengths . however , it is well known that , with pum enrichment , the burden of computation shifts from the solver to the evaluation of oscillatory integrals during matrix assembly . a full electromagnetic scattering problem is not simulated or solved in this paper . this paper is an addition to the work of ledger and concentrates on efficient methods of evaluating the oscillatory integrals that arise . a semi analytical scheme of the filon character is presented .
search based metamodel matching with structural and syntactic measures . <eos> metamodel matching using search based software engineering . the use of syntactic measures improve the results of metamodel matching we compared our approach to four ontology based approaches . our results show that our search based approach was significantly better than state of the art matching tools .
a recursion based broadcast paradigm in wormhole routed networks . <eos> a novel broadcast technique for wormhole routed parallel computers based on recursion is presented in this paper . it works by partitioning the interconnection graph into a number of higher level subgraphs . then , we identify the transmission subgraph ( tsg ) in each subgraph . both the higher level subgraphs and the tsgs are recursively defined , i.e. , we split each level i subgraph into several level i <digit> subgraphs and identify level i <digit> tsgs accordingly . we first split and scatter the source message into the tsg of the original graph . next , in each recursive round message transmissions are from lower level tsgs to higher level tsgs and all transmissions at the same level happen concurrently . the algorithm proceeds recursively from lower level subgraphs to higher level subgraphs until each highest level subgraph ( a single node ) gets the complete message . we have applied this general paradigm to a number of topologies including two or higher dimension mesh torus and hypercube . our results show considerable improvements over all other algorithms for a wide range of message sizes under both one port and all port models .
stress analysis of three dimensional contact problems using the boundary element method . <eos> this paper presents a technique based on the boundary element method <digit> to analyse three dimensional contact problems . the formulation is implemented for the frictionless and infinite friction conditions . following a review of the basic nature of contact problems , the analytical basis of the direct formulation of the boundary element method is described . the numerical implementation employs linear triangular elements for the representation of the boundary and variables of the bodies in contact . opposite nodal points in similar element pairs are defined on the two surfaces in the area which are expected to come into contact under the increasing load . the use of appropriate contact conditions enables the integral equations for the two bodies to be coupled together . following an iteration procedure , the size of the contact zone is determined by finding a boundary solution compatible with the contact conditions . different examples have been analysed in order to verify the applicability of the proposed method to various contact situations . the results have been compared with those obtained using the finite element method in conjunction with the abaqus <digit> and ideas <digit> packages which are shown to be in good agreement .
co evolving application code and design models by exploiting meta data . <eos> evolvability and adaptability are intrinsic properties of today 's software applications . unfortunately , the urgency of evolving adapting a system often drives the developer to directly modify the application code neglecting to update its design models . even , most of the development environments support the code refactoring without supporting the refactoring of the design information . refactoring , evolution and in general every change to the code should be reflected into the design models , so that these models consistently represent the application and can be used as documentation in the successive maintenance steps . the code evolution should not evolve only the application code but also its design models . unfortunately , to co evolve the application code and its design is a hard job to be carried out automatically , since there is an evident and notorious gap between these two representations . we propose a new approach to code evolution ( in particular to code refactoring ) that supports the automatic co evolution of the design models . the approach relies on a set of predefined metadata that the developer should use to annotate the application code and to highlight the refactoring performed on the code . then , these meta data are retrieved through reflection and used to automatically and coherently update the application design models .
differential effects of donepezil on methamphetamine and cocaine dependencies . <eos> donepezil , a choline esterase inhibitor , has been widely used as a medicine for alzheimer 's disease . recently , a study showed that donepezil inhibited addictive behaviors induced by cocaine , including cocaine conditioned place preference ( cpp ) and locomotor sensitization to cocaine . in the present study , we investigated the effects of donepezil on methamphetamine ( meth ) induced behavioral changes in mice . in counterbalanced cpp tests , the intraperitoneal ( i.p. ) administration of <digit> mg kg donepezil prior to <digit> mg kg meth i.p. failed to inhibit meth cpp , whereas pretreatment with <digit> mg kg donepezil abolished the cpp for cocaine ( <digit> mg kg , i.p. ) . similarly , in locomotor sensitization experiments , i.p. administration of <digit> mg kg donepezil prior to <digit> mg kg meth i.p. failed to inhibit locomotor sensitivity to meth , whereas pretreatment with <digit> mg kg donepezil significantly inhibited locomotor sensitivity to cocaine ( <digit> mg kg , i.p. ) . these results suggest that donepezil may be a useful tool for treating cocaine dependence but not for treating meth dependence . the differences in the donepezil effects on addictive behaviors induced by meth and cocaine might be due to differences in the involvement of acetylcholine in the mechanisms of meth and cocaine dependencies

a gaussian function model for simulation of complex environmental sensing . <eos> sensors can be used to sense not only simple behavior but also complex ones . previous work has demonstrated how agent based modeling can be used to model sensing of complex behavior in complex environments .
an integer programming based search technique for error prone structures of ldpc codes . <eos> in this paper , an efficient , general framework is presented for finding common , devastating error prone structures ( eps ) of any finite length low density parity check ( ldpc ) code . the smallest stopping set for the binary erasure channel ( bec ) , the smallest fully absorbing set , the smallest absorbing set , and the smallest elementary trapping set for the binary symmetric channel ( bsc ) are found and the dominant eps are enumerated . the method involves integer programming optimization techniques , which guarantees that the results are provably optimal .
chemosensitization of tumors by resveratrol . <eos> because tumors develop resistance to chemotherapeutic agents , the cancer research community continues to search for effective chemosensitizers . one promising possibility is to use dietary agents that sensitize tumors to the chemotherapeutics . in this review , we discuss that the use of resveratrol can sensitize tumor cells to chemotherapeutic agents . the tumors shown to be sensitized by resveratrol include lung carcinoma , acute myeloid leukemia , promyelocytic leukemia , multiple myeloma , prostate cancer , oral epidermoid carcinoma , and pancreatic cancer . the chemotherapeutic agents include vincristine , adriamycin , paclitaxel , doxorubicin , cisplatin , gefitinib , <digit> fluorouracil , velcade , and gemcitabine . the chemosensitization of tumor cells by resveratrol appears to be mediated through its ability to modulate multiple cell signaling molecules , including drug transporters , cell survival proteins , cell proliferative proteins , and members of the nf b and stat3 signaling pathways . interestingly , this nutraceutical has also been reported to suppress apoptosis induced by paclitaxel , vincristine , and daunorubicin in some tumor cells . the potential mechanisms underlying this dual effect are discussed . overall , studies suggest that resveratrol can be used to sensitize tumors to standard cancer chemotherapeutics .
towards scalable summarization of consumer videos via sparse dictionary selection . <eos> the rapid growth of consumer videos requires an effective and efficient content summarization method to provide a user friendly way to manage and browse the huge amount of video data . compared with most previous methods that focus on sports and news videos , the summarization of personal videos is more challenging because of its unconstrained content and the lack of any pre imposed video structures . we formulate video summarization as a novel dictionary selection problem using sparsity consistency , where a dictionary of key frames is selected such that the original video can be best reconstructed from this representative dictionary . an efficient global optimization algorithm is introduced to solve the dictionary selection model with the convergence rates as o ( <digit> root k <digit> ) ( where k is the iteration counter ) , in contrast to traditional sub gradient descent methods of o ( <digit> root k ) . our method provides a scalable solution for both key frame extraction and video skim generation , because one can select an arbitrary number of key frames to represent the original videos . experiments on a human labeled benchmark dataset and comparisons to the state of the art methods demonstrate the advantages of our algorithm .
sufficient completeness verification for conditional and constrained trs . <eos> we present a procedure for checking sufficient completeness of conditional and constrained term rewriting systems containing axioms for constructors which may be constrained ( by e.g. equalities , disequalities , ordering , membership , ... ) . such axioms allow to specify complex data structures like e.g. sets , sorted lists or powerlists . our approach is integrated into a framework for inductive theorem proving based on tree grammars with constraints , a formalism which permits an exact representation of languages of ground constructor terms in normal form . the procedure is presented by an inference system which is shown sound and complete . a precondition of one inference of this system refers to a ( undecidable ) property called strong ground reducibility which is discharged to the above inductive theorem proving system . we have successfully applied our method to several examples , yielding readable proofs and , in case of negative answer , a counter example suggesting how to complete the specification . moreover , we show that it is a decision procedure when the trs is unconditional but constrained , for an expressive class of constrained constructor axioms . ( c ) <digit> elsevier b.v. all rights reserved .
scheduling parallel programs by work stealing with private deques . <eos> work stealing has proven to be an effective method for scheduling parallel programs on multicore computers . to achieve high performance , work stealing distributes tasks between concurrent queues , called deques , which are assigned to each processor . each processor operates on its deque locally except when performing load balancing via steals . unfortunately , concurrent deques suffer from two limitations <digit> ) local deque operations require expensive memory fences in modern weak memory architectures , <digit> ) they can be very difficult to extend to support various optimizations and flexible forms of task distribution strategies needed many applications , e. g. , those that do not fit nicely into the divide and conquer , nested data parallel paradigm . for these reasons , there has been a lot recent interest in implementations of work stealing with non concurrent deques , where deques remain entirely private to each processor and load balancing is performed via message passing . private deques eliminate the need for memory fences from local operations and enable the design and implementation of efficient techniques for reducing task creation overheads and improving task distribution . these advantages , however , come at the cost of communication . it is not known whether work stealing with private deques enjoys the theoretical guarantees of concurrent deques and whether they can be effective in practice . in this paper , we propose two work stealing algorithms with private deques and prove that the algorithms guarantee similar theoretical bounds as work stealing with concurrent deques . for the analysis , we use a probabilistic model and consider a new parameter , the branching depth of the computation . we present an implementation of the algorithm as a c library and show that it compares well to cilk on a range of benchmarks . since our approach relies on private deques , it enables implementing flexible task creation and distribution strategies . as a specific example , we show how to implement task coalescing and steal half strategies , which can be important in fine grain , non divide and conquer algorithms such as graph algorithms , and apply them to the depth first search problem .
multilevel huffman coding an efficient test data compression method for ip cores . <eos> a new test data compression method suitable for cores of unknown structure is introduced in this paper . the proposed method encodes the test data provided by the core vendor using a new , very effective compression scheme based on multilevel huffman coding . each huffman codeword corresponds to three different kinds of information , and thus , significant compression improvements compared to the already known techniques are achieved . a simple architecture is proposed for decoding the compressed data on chip . its hardware overhead is very low and comparable to that of the most efficient methods in the literature . moreover , the major part of the decompressor can be shared among different cores , which reduces the hardware overhead of the proposed architecture considerably . additionally , the proposed technique offers increased probability of detection of unmodeled faults since the majority of the unknown values of the test sets are replaced by pseudorandom data generated by a linear feedback shift register .
variable selection in regression models using nonstandard optimisation of information criteria . <eos> the question of variable selection in a regression model is a major open research topic in econometrics . traditionally two broad classes of methods have been used . one is sequential testing and the other is information criteria . the advent of large datasets used by institutions such as central banks has exacerbated this model selection problem . a solution in the context of information criteria is provided in this paper . the solution rests on the judicious selection of a subset of models for consideration using nonstandard optimisation algorithms for information criterion minimisation . in particular , simulated annealing and genetic algorithms are considered . both a monte carlo study and an empirical forecasting application to uk cpi inflation suggest that the proposed methods are worthy of further consideration .
highly nonlinear photonic crystal fiber with ultrahigh birefringence using a nano scale slot core . <eos> a new type of slot photonic crystal fiber is proposed . ultrahigh nonlinear coefficient up to 3.5739104 w <digit> km <digit> can be achieved for the quasi tm mode . the modal birefringence at 1.55 m is up to 0.5015 . the proposed pcf is suitable for all optical signal processing .
what will system level design be when it grows up . <eos> we have seen a growing new interest in electronic system level ( esl ) architectures , design methods , tools and implementation fabrics in the last few years . but the picture of what types and approaches to building embedded systems will become the most widely accepted norms in the future remains fuzzy at best . everyone want to know where systems and system design is going when it grows up , if it ever grows up . some of the key questions that need to be answered include which applications will be key system drivers , what sw hw architectures will suit best , how programmable and configurable will they be , will systems designers need to deal with physical implementation issues or will that be hidden behind fabric abstractions and programming models , and what will those abstractions and models be moreover , will these abstractions stabilize and be still useful as the underlying technology keeps developing at high speed.this panel consists of proponents of a number of alternative visions for where we will end up , and how we will get there .
the effectiveness of bootstrap methods in evaluating skewed auditing populations a simulation study . <eos> this article describes a comparison among four bootstrap methods the percentile , reflective , bootstrap t , and variance stabilized bootstrap t using a simple new stabilization procedure . the four methods are employed in constructing upper confidence bounds for the mean error in a wide variety of audit populations . the simulation results indicate that the variance stabilized bootstrap t bound is to be preferred . it exhibits reliable coverage while maintaining reasonable tightness .
evaluation of arctic multibeam sonar data quality using nadir crossover error analysis and compilation of a full resolution data product . <eos> characterize uncertainty in multi source multibeam data sets . highest spatial resolution compilation for the canada basin and chukchi borderland . fully resolvable pdf for interpretation of arctic seafloor morphology .
software trace cache for commercial applications . <eos> in this paper we address the important problem of instruction fetch for future wide issue superscalar processors . our approach focuses on understanding the interaction between software and hardware techniques targeting an increase in the instruction fetch bandwidth . that is the objective , for instance , of the hardware trace cache ( htc ) . we design a profile based code reordering technique which targets a maximization of the sequentiality of instructions , while still trying to minimize instruction cache misses . we call our software approach , software trace cache ( stc ) . we evaluate our software approach , and then compare it with the htc and the combination of both techniques . our results on postgresql show that for large codes with few loops and deterministic execution sequences the stc offers better results than a htc . also , both the software and hardware approaches combine well to obtain improved results .
goal state optimization algorithm considering computational resource constraints and uncertainty in task execution time . <eos> a search methodology with goal state optimization considering computational resource constraints is proposed . the combination of an extended graph search methodology and parallelization of task execution and online planning makes it possible to solve the problem . the uncertainty of the task execution time is also considered . the problem can be solved by utilizing a random based and or a greedy based graph searching methodology . the proposed method is evaluated using a rearrangement problem of <digit> movable objects with uncertainty in the task execution time , and the effectiveness is shown with simulation results .
a <digit> mhz cmos quadrature modulator for a gsm transmitter . <eos> this paper describes a <digit> mhz cmos quadrature modulator ( qmod ) fur a global system for mobile communications ( gsm ) transmitter . qmod consists of two attenuators and two doubly balanced modulators ( dbm 's ) and fabricated by using 0.35 mu m cmos process . the carrier leakage level of 35.7 dbc and the image rejection level of 45.1 dbc are achieved . it 's total chip area is <digit> mu m x <digit> mu m and it consumes 1.0 ma with 3.0 v power supply .
mining multi tag association for image tagging . <eos> automatic media tagging plays a critical role in modern tag based media retrieval systems . existing tagging schemes mostly perform tag assignment based on community contributed media resources , where the tags are provided by users interactively . however , such social resources usually contain dirty and incomplete tags , which severely limit the performance of these tagging methods . in this paper , we propose a novel automatic image tagging method aiming to automatically discover more complete tags associated with information importance for test images . given an image dataset , all the near duplicate clusters are discovered . for each near duplicate cluster , all the tags occurring in the cluster form the cluster 's document . given a test image , we firstly initialize the candidate tag set from its near duplicate cluster 's document . the candidate tag set is then expanded by considering the implicit multi tag associations mined from all the clusters ' documents , where each cluster 's document is regarded as a transaction . to further reduce noisy tags , a visual relevance score is also computed for each candidate tag to the test image based on a new tag model . tags with very low scores can be removed from the final tag set . extensive experiments conducted on a real world web image dataset nus wide , demonstrate the promising effectiveness of our approach .
improve the performance of co training by committee with refinement of class probability estimations . <eos> semi supervised learning is a popular machine learning technique where only a small number of labeled examples are available and a large pool of unlabeled examples can be obtained easily . in co training by committee , a paradigm of semi supervised learning , it is necessary to pick out a fixed number of most confident examples according to the ranking of class probability values at each iteration . unfortunately , the class probability values may repeat , which results in the problem that some unlabeled instances share the same probability and will be picked out randomly . this brings a negative effect on the improvement of the performance of classifiers . in this paper , we propose a simple method to deal with this problem under the intuition that different probabilities are crucial . the distance metric between unlabeled instances and labeled instances can be combined with the probabilities of class membership of committee . two distance metrics are considered to assign each unlabeled example a unique probability value . in order to prove that our method can get higher quality examples and reduce the introduction of noise , a data editing technique is used to compare with our method . experimental results verify the effectiveness of our method and the data editing technique , and also confirm that the method for the first distance metric is generally better than the data editing technique .
a unified ransles model computational development , accuracy and cost . <eos> large eddy simulation ( les ) is computationally extremely expensive for the investigation of wall bounded turbulent flows at high reynolds numbers . a way to reduce the computational cost of les by orders of magnitude is to combine les equations with reynolds averaged navierstokes ( rans ) equations used in the near wall region . a large variety of such hybrid ransles methods are currently in use such that there is the question of which hybrid rans les method represents the optimal approach . the properties of an optimal hybrid ransles model are formulated here by taking reference to fundamental properties of fluid flow equations . it is shown that unified ransles models derived from an underlying stochastic turbulence model have the properties of optimal hybrid ransles models . the rest of the paper is organized in two parts . first , a priori and a posteriori analyses of channel flow data are used to find the optimal computational formulation of the theoretically derived unified ransles model and to show that this computational model , which is referred to as linear unified model ( lum ) , does also have all the properties of an optimal hybrid ransles model . second , a posteriori analyses of channel flow data are used to study the accuracy and cost features of the lum . the following conclusions are obtained . ( i ) compared to rans , which require evidence for their predictions , the lum has the significant advantage that the quality of predictions is relatively independent of the rans model applied . ( ii ) compared to les , the significant advantage of the lum is a cost reduction of high reynolds number simulations by a factor of 0.07 re 0.46 . for coarse grids , the lum has a significant accuracy advantage over corresponding les . ( iii ) compared to other usually applied hybrid ransles models , it is shown that the lum provides significantly improved predictions .
on kelly networks with shuffling . <eos> we consider kelly networks with shuffling of customers within each queue . specifically , each arrival , departure or movement of a customer from one queue to another triggers a shuffle of the other customers at each queue . the shuffle distribution may depend on the network state and on the customer that triggers the shuffle . we prove that the stationary distribution of the network state remains the same as without shuffling . in particular , kelly networks with shuffling have the product form . moreover , the insensitivity property is preserved for symmetric queues .
log based receiver reliable multicast for distributed interactive simulation . <eos> reliable multicast communication is important in large scale distributed applications . for example , reliable multicast is used to transmit terrain and environmental updates in distributed simulations . to date , proposed protocols have not supported these applications ' requirements , which include wide area data distribution , low latency packet loss detection and recovery , and minimal data and management over head within fine grained multicast groups , each containing a single data source.in this paper , we introduce the notion of log based receiver reliable multicast ( lbrm ) communication , and we describe and evaluate a collection of log based receiver reliable multicast optimizations that provide an efficient , scalable protocol for high performance simulation applications . we argue that these techniques provide value to a broader range of applications and that the receiver reliable model is an appropriate one for communication in general .
design of relational views over network schemas . <eos> an algorithm is presented for designing relational views over network schemas to ( <digit> ) support general query and update capability , ( <digit> ) preserve the information content of the data base and ( <digit> ) provide independence from its physical organization . the proposed solution is applicable to many existing codasyl databases without data or schema conversion . the particular declarations of a codasyl schema which supply sources of logical data definition are first identified . then the view design algorithm is derived on the basis of a formal analysis of the semantic constraints established by these declarations . a new form of data structure diagram is also introduced to visualize these constraints .
on the coverings by tolerance classes . <eos> a tolerance is a reflexive and symmetric , but not necessarily transitive , binary relation . contrary to what happens with equivalence relations , when dealing with tolerances one must distinguish between blocks ( maximal subsets where the tolerance is a total relation ) and classes ( the class of an element is the set of those elements tolerable with it ) . both blocks and classes of a tolerance on a set define coverings of this set , but not every covering of a set is defined in this way . the characterization of those coverings that are families of blocks of some tolerance has been known for more than a decade now . in this paper we give a characterization of those coverings of a finite set that are families of classes of some tolerance .
boundary conditions control for a shallow water model . <eos> a variational data assimilation technique was used to estimate optimal discretization of interpolation operators and derivatives in the nodes adjacent to the rigid boundary . assimilation of artificially generated observational data in the shallow water model in a square box and assimilation of real observations in the model of the black sea are discussed . it is shown in both experiments that controlling the discretization of operators near a rigid boundary can bring the model solution closer to observations as in the assimilation window and beyond the window . this type of control also allows to improve climatic variability of the model . copyright ( c ) <digit> john wiley sons , ltd .
a simple local smoothing scheme in strongly singular boundary integral representation of potential gradient . <eos> a new approach for computation of potential gradient at and near boundary is introduced . a strongly singular boundary integral representation of potential gradient , whose integral density is the potential gradient , is derived and analysed . applying the concept of the osculating circle , a local smoothing procedure which computes a continuous approximation of potential gradient from the results of a 2d boundary element method ( bem ) analysis using linear elements is proposed and evaluated . this approximation is used in the integral representation derived as an integral density which fulfills the continuity requirements . numerical experiments demonstrate , for quasiuniform meshes , an o ( h2 ) accuracy of potential gradient computed by both the local smoothing procedure on smooth parts of the boundary and by the integral representation on smooth boundary parts and near smooth boundary parts for points inside the domain . a consequence of the latter result is that no significant increase in the error appears near the boundary , boundary layer effect thus being eliminated in this approach .
functional modularity for genetic programming . <eos> in this paper we introduce , formalize , and experimentally validate a novel concept of functional modularity for genetic programming ( gp ) . we rely on module definition that is most natural for gp a piece of program code ( subtree ) . however , as opposed to syntax based approaches that abstract from the actual computation performed by a module , we analyze also its semantic using a set of fitness cases . in particular , the central notion of this approach is subgoal , an entity that embodies module 's desired semantic and is used to evaluate module candidates . as the cardinality of the space of all subgoals is exponential with respect to the number of fitness cases , we introduce monotonicity to assess subgoals ' potential utility for searching for good modules . for a given subgoal and a sample of modules , monotonicity measures the correlation of subgoal 's distance from module 's semantics and the fitness of the solution the module is part of . in the experimental part we demonstrate how these concepts may be used to describe and quantify the modularity of two simple problems of boolean function synthesis . in particular , we conclude that monotonicity usefully differentiates two problems with different nature of modularity , allows us to tell apart the useful subgoals from the other ones , and may be potentially used for problem decomposition and enhance the efficiency of evolutionary search .

answering approximate queries over autonomous web databases . <eos> to deal with the problem of empty or too little answers returned from a web database in response to a user query , this paper proposes a novel approach to provide relevant and ranked query results . based on the user original query , we speculate how much the user cares about each specified attribute and assign a corresponding weight to it . this original query is then rewritten as an approximate query by relaxing the query criteria range . the relaxation order of all specified attributes and the relaxed degree on each specified attribute are varied with the attribute weights . for the approximate query results , we generate users ' contextual preferences from database workload and use them to create a priori orders of tuples in an off line preprocessing step . only a few representative orders are saved , each corresponding to a set of contexts . then , these orders and associated contexts are used at query time to expeditiously provide ranked answers . results of a preliminary user study demonstrate that our query relaxation and results ranking methods can capture the user 's preferences effectively . the efficiency and effectiveness of our approach is also demonstrated by experimental result .
stochastic finite learning of the pattern languages . <eos> the present paper proposes a new learning model called stochastic finite learning and shows the whole class of pattern languages to be learnable within this model . this main result is achieved by providing a new and improved average case analysis of the lange wiehagen ( new generation computing , <digit> , <digit> <digit> ) algorithm learning the class of all pattern languages in the limit from positive data . the complexity measure chosen is the total learning time , i.e. , the overall time taken by the algorithm until convergence . the expectation of the total learning time is carefully analyzed and exponentially shrinking tail bounds for it are established for a large class of probability distributions . for every pattern pi containing k different variables it is shown that lange and wiehagen 's algorithm possesses an expected total learning time of o ( ) over cap > alpha ( k ) e lambda log ( <digit> beta ) ( k ) ) , where ) over cap > and beta are two easily computable parameters arising naturally from the underlying probability distributions , and e lambda is the expected example string length . finally , assuming a bit of domain knowledge concerning the underlying class of probability distributions , it is shown how to convert learning in the limit into stochastic finite learning .

probabilistic quantum key distribution . <eos> this work presents a new concept in quantum key distribution called the probabilistic quantum key distribution ( pqkd ) protocol , which is based on the measurement uncertainty in quantum phenomena . it allows two mutually untrusted communicants to negotiate an unpredictable key that has a randomness guaranteed by the laws of quantum mechanics . in contrast to conventional qkd ( e.g. , bb84 ) in which one communicant has to trust the other for key distribution or quantum key agreement ( qka ) in which the communicants have to artificially contribute subkeys to a negotiating key , pqkd is a natural and simple method for distributing a secure random key . the communicants in the illustrated pqkd take einstein podolsky rosen ( epr ) pairs as quantum resources and then use entanglement swapping and bell measurements to negotiate an unpredictable key .
an experiment with reflective middleware to support grid based flood monitoring . <eos> flooding is a growing problem , which affects more than <digit> % of the u.k. population . the cost of damage caused by flooding correlates closely with the warning time given before a flood event , making flood monitoring and prediction critical to minimizing the cost of flood damage . this paper describes a wireless sensor network ( wsn ) for flood warning , which is capable of not only integrating with remote fixed network grids for computationally intensive flood modelling purposes but also performing on site grid computation . this functionality is supported by the reflective and component based gridkit middleware , which provides support for both wsn and grid application domains . copyright ( c ) <digit> john wiley sons , ltd .
rate control for delay sensitive traffic in multihop wireless networks . <eos> we propose two multipath rate control algorithms that guarantee bounded end to end delay in multihop wireless networks . our work extends the previous research on optimal rate control and scheduling in multihop wireless networks , to support inelastic delay requirements . using the relationship between dual variables and packet delay , we develop two alternative solutions that are independent from any queuing model assumption , contrary to the previous research . in the first solution , we derive lower bounds on source rates that achieve the required delay bounds . we then develop a distributed algorithm comprising scheduling and rate control functions , which requires each source to primarily check the feasibility of its qos before initiating its session . in the second solution we eliminate the admission control phase by developing an algorithm that converges to the utility function weights that ensure the required delay bounds for all flows . both solutions carry out scheduling at slower timescale than rate control , and consequently are more efficient than previous cross layer algorithms . we show through numerical examples that even when there are no delay constraints , the proposed algorithms significantly reduce the delay compared to the previous solutions .
optimality of klt for high rate transform coding of gaussian vector scale mixtures application to reconstruction , estimation , and classification . <eos> the karhunen loeve transform ( klt ) is known to be optimal for high rate transform coding of gaussian vectors for both fixed rate and variable rate encoding . the klt is also known to be suboptimal for some non gaussian models . this paper proves high rate optimality of the klt for variable rate encoding of a broad class of non gaussian vectors gaussian vector scale mixtures ( gvsm ) , which extend the gaussian scale mixture ( gsm ) model of natural signals . a key concavity property of the scalar gsm ( same as the scalar gvsm ) is derived to complete the proof . optimality holds under a broad class of quadratic criteria , which include mean squared error ( mse ) as well as generalized f divergence loss in estimation and binary classification systems . finally , the theory is illustrated using two applications signal estimation in multiplicative noise and joint optimization of classification reconstruction systems .
the norepinephrine transporter and pheochromocytoma . <eos> pheochromocytomas are rare neuroendocrine tumors of chromaffin cell origin that synthesize and secrete excess quantities of catecholamines and other vasoactive peptides . pheochromocytomas also express the norepinephrine transporter ( net ) , a molecule that is used clinically as a means of incorporating radiolabelled substrates such as 131i mibg ( iodo metaiodobenzylguanidine ) into pheochromocytoma tumor cells . this allows the diagnostic localization of these tumors and , more recently , 131i mibg has been used in trials in the treatment of pheochromocytoma , potentially giving rise to net as a therapeutic target . however , because of varying levels or activities of the transporter , the ability of 131i mibg to be consistently incorporated into tumor cells is limited , and therefore various strategies to increase net functional activity are being investigated , including the use of traditional chemotherapeutic agents such as cisplatin or doxorubicin . other aspects of net discussed in this short review include the regulation of the transporter and how novel proteinprotein interactions between net and structures such as syntaxin 1a may hold the key to innovative ways to increase the therapeutic value of 131i mibg
metaeasy a meta analysis add in for microsoft excel . <eos> meta analysis is a statistical methodology that combines or integrates the results of several independent clinical trials considered by the analyst to be ' combinable ' ( huque <digit> ) . however , completeness and user friendliness are uncommon both in specialised meta analysis software packages and in mainstream statistical packages that have to rely on user written commands . we implemented the meta analysis methodology in a microsoft excel add in which is freely available and incorporates more meta analysis models ( including the iterative maximum likelihood and profile likelihood ) than are usually available , while paying particular attention to the user friendliness of the package .
adaptive data collection strategies for lifetime constrained wireless sensor networks . <eos> communication is a primary source of energy consumption in wireless sensor networks . due to resource constraints , the sensor nodes may not have enough energy to report every reading to the base station over a required network lifetime . this paper investigates data collection strategies in lifetime constrained wireless sensor networks . our objective is to maximize the accuracy of data collected by the base station over the network lifetime . instead of sending sensor readings periodically , the relative importance of the readings is considered in data collection the sensor nodes send data updates to the base station when the new readings differ more substantially from the previous ones . we analyze the optimal update strategy and develop adaptive update strategies for both individual and aggregate data collections . we also present two methods to cope with message losses in wireless transmission . to make full use of the energy budgets , we design an algorithm to allocate the numbers of updates allowed to be sent by the sensor nodes based on their topological relations . experimental results using real data traces show that , compared with the periodic strategy , adaptive strategies significantly improve the accuracy of data collected by the base station .
image fusion based contrast enhancement . <eos> the goal of contrast enhancement is to improve visibility of image details without introducing unrealistic visual appearances and or unwanted artefacts . while global contrast enhancement techniques enhance the overall contrast , their dependences on the global content of the image limit their ability to enhance local details . they also result in significant change in image brightness and introduce saturation artefacts . local enhancement methods , on the other hand , improve image details but can produce block discontinuities , noise amplification and unnatural image modifications . to remedy these shortcomings , this article presents a fusion based contrast enhancement technique which integrates information to overcome the limitations of different contrast enhancement algorithms . the proposed method balances the requirement of local and global contrast enhancements and a faithful representation of the original image appearance , an objective that is difficult to achieve using traditional enhancement methods . fusion is performed in a multi resolution fashion using laplacian pyramid decomposition to account for the multi channel properties of the human visual system . for this purpose , metrics are defined for contrast , image brightness and saturation . the performance of the proposed method is evaluated using visual assessment and quantitative measures for contrast , luminance and saturation . the results show the efficiency of the method in enhancing details without affecting the colour balance or introducing saturation artefacts and illustrate the usefulness of fusion techniques for image enhancement applications .
reachability analysis for uncertain ssps . <eos> stochastic shortest path problems ( ssps ) can be efficiently dealt with by the real time dynamic programming algorithm ( rtdp ) . yet , rtdp requires that a goal state is always reachable . this article presents an algorithm checking for goal reachability , especially in the complex case of an uncertain ssp where only a possible interval is known for each transition probability . this gives an analysis method for determining if ssp algorithms such as rtdp are applicable , even if the exact model is not known . as this is a time consuming algorithm , we also present a simple process that often speeds it up dramatically . yet , the main improvement still needed is to turn to a symbolic analysis in order to avoid a complete state space enumeration .
embodiment in brain computer interaction . <eos> with emerging opportunities for using brain computer interaction ( bci ) in gaming applications , there is a need to understand the opportunities and constraints of this interaction paradigm . to complement existing laboratory based studies , there is also a call for the study of bci in real world contexts . in this paper we present such a real world study of a simple bci game called mindflex , played as a social activity in the home . in particular , drawing on the philosophical traditions of embodied interaction , we highlight the importance of considering the body in bci and not simply what is going on in the head . the study shows how people use bodily actions to facilitate control of brain activity but also to make their actions and intentions visible to , and interpretable by , others playing and watching the game . it is the public availability of these bodily actions during bci that allows action to be socially organised , understood and coordinated with others and through which social relationships can be played out . we discuss the implications of this perspective and findings for bci .
formally measuring agreement and disagreement in ontologies . <eos> ontologies are conceptual models of particular domains , and domains can be modeled differently , representing different opinions , beliefs or perspectives . in other terms , ontologies may disagree with some particular pieces of information and among themselves . assessing such agreements and disagreements is very useful in a variety of scenarios , in particular when integrating external elements of information into existing ones . in this paper , we present a set of measures to evaluate the agreement and disagreement of an ontology with a statement or with other ontologies . while our work goes beyond the naive approach of checking for logical inconsistencies , it relies on a complete formal framework based on the semantics of the considered ontologies . the experiments realized on several concrete scenarios show the validity of our approach and the usefulness of measuring agreement and disagreement in ontologies .
minimal realizations of linear systems the shortest basis approach . <eos> given a discrete time linear system c , a shortest basis for is a set of linearly independent generators for c with the least possible lengths . a basis b is a shortest basis if and only if it has the predictable span property ( i.e. , has the predictable delay and degree properties , and is non catastrophic ) , or alternatively if and only if it has the subsystem basis property ( for any interval j , the generators in b whose span is in j is a basis for the subsystem c ( j ) ) . the dimensions of the minimal state spaces and minimal transition spaces of c are simply the numbers of generators in a shortest basis b that are active at any given state or symbol time , respectively . a minimal linear realization for c in controller canonical form follows directly from a shortest basis for c , and a minimal linear realization for c in observer canonical form follows directly from a shortest basis for the orthogonal system c ( perpendicular to ) . this approach seems conceptually simpler than that of classical minimal realization theory .
a low latency multi layer prefix grouping technique for parallel huffman decoding of multimedia standards . <eos> huffman coding is a popular and important lossless compression scheme for various multimedia applications . this paper presents a low latency parallel huffman decoding technique with efficient memory usage for multimedia standards . first , the multi layer prefix grouping technique is proposed for sub group partition . it exploits the prefix characteristic in huffman codewords to solve the problem of table size explosion . second , a two level table lookup approach is introduced which can promptly branch to the correct sub group by level <digit> table lookup and decode the symbols by level <digit> table lookup . third , two optimization approaches are developed one is to reduce the branch cycles and the other is parallel processing between two level table lookup and direct table lookup approaches to fully utilize the advantage of vliw parallel processing . an aac huffman decoding example is realized on the parallel architecture core dsp ( pac dsp ) processor . the simulation results show that the proposed method can further improve about <digit> % of decoding cycles and <digit> % of table size comparing to the linear search method .
analysis and numerical simulation of strong discontinuities in finite strain poroplasticity . <eos> this paper presents an analysis of strong discontinuities in coupled poroplastic media in the finite deformation range . a multi scale framework is developed for the characterization of these solutions involving a discontinuous deformation ( or displacement ) field in this coupled setting . the strong discontinuities are used as a tool for the modeling of the localized dissipative effects characteristic of the localized failures of typical poroplastic systems . this is accomplished through the inclusion of a cohesive frictional law relating the resolved stresses on the discontinuity and the accumulated fluid content on it with the displacement and fluid flow jumps across the discontinuity surface . the formulation considers the limit of vanishing small scales , hence recovering a problem in the large scale involving the usual regular displacement and pore pressure variables , while capturing correctly these localized dissipative mechanisms . all the couplings between the mechanical and fluid problems , from the modeling of the solid 's response through effective stresses and tractions to the geometric coupling consequence of the assumed finite deformation setting , are taken into account in these considerations . the multi scale structure of the theoretical formulation is fully employed in the development of new enhanced strain finite elements to capture these discontinuous solutions with no regularization of the singular fields appearing in the formulation . several numerical simulations are presented showing the properties and performance of the proposed localized models and the enhanced finite elements used in their numerical implementation .
toward real noon state sources . <eos> path entangled n photon systems described by noon states are the main ingredient of many quantum information and quantum imaging protocols . our analysis aims to lead the way toward the implementation of both noon state sources and their applications . to this end , we study the functionality of real noon state sources by quantifying the effect real experimental apparatuses have on the actual generation of the desired noon state . in particular , since the conditional generation of noon states strongly relies on photon counters , we evaluate the dependence of both the reliability and the signal to noise ratio of real noon state sources on detection losses . we find a surprising result noon state sources relying on nondetection are much more reliable than noon state sources relying on single photon detection . also the comparison of the resources required to implement these two protocols comes out to be in favor of noon state sources based on nondetection . a scheme to improve the performances of real noon state sources based on single photon detection is also proposed and analyzed .
using tpack as a framework to understand teacher candidates ' technology integration decisions . <eos> this research uses the technological pedagogical and content knowledge ( tpack ) framework as a lens for understanding how teacher candidates make decisions about the use of information and communication technology in their teaching . pre and post treatment assessments required elementary teacher candidates at brigham young university to articulate how and why they would integrate technology in three content teaching design tasks . researchers identified themes from student rationales that mapped to the tpack constructs . rationales simultaneously supported subcategories of knowledge that could be helpful to other researchers trying to understand and measure tpack . the research showed significant student growth in the use of rationales grounded in content specific knowledge and general pedagogical knowledge , while rationales related to general technological knowledge remained constant .
exploiting temporal coherence in global illumination . <eos> producing high quality animations featuring rich object appearance and compelling lighting effects is very time consuming using traditional frame by frame rendering systems . in this paper we present a number of global illumination and rendering solutions that exploit temporal coherence in lighting distribution for subsequent frames to improve the computation performance and overall animation quality . our strategy relies on extending into temporal domain well known global illumination techniques such as density estimation photon tracing , photon mapping , and bi directional path tracing , which were originally designed to handle static scenes only .
effectiveness of cognitive load based adaptive instruction in genetics education . <eos> research addressing the issue of instructional control in computer assisted instruction has revealed mixed results . prior knowledge level seems to play a mediating role in the students ability to effectively use given instructional control . this study examined the effects of three types of instructional control ( non adaptive program control , learner control , adaptive program control ) and prior knowledge ( high school , 1st year and 2nd year college students ) on effectiveness and efficiency of learning in a genetics training program . the results revealed that adaptive program control led to highest training performance but not to superior post test or far transfer performance . furthermore , adaptive program control proved to be more efficient in terms of learning outcomes of the test phase than the other two instructional control types . college students outperformed the high school students on all aspects of the study thereby strengthening the importance of prior knowledge in learning effectiveness and efficiency . lastly , the interaction effects showed that for each prior knowledge level different levels of support were beneficial to learning .
sub pixel mapping based on artificial immune systems for remote sensing imagery . <eos> we propose an artificial immune sub pixel mapping framework for remote sensing imagery . the sub pixel mapping problem is transformed to an optimization problem . the proposed algorithm can obtain better sub pixel mapping results by immune operators . experimental results demonstrate that the proposed approach outperforms the previous methods .
modeling of the quenching of blast products from energetic materials by expansion into vacuum . <eos> condensed phase energetic materials include propellants and explosives . their detonation or burning products generate dense , high pressure states that are often adjacent to regions that are at vacuum or near vacuum conditions . an important chemical diagnostic experiment is the time of flight mass spectroscopy experiment that initiates an energetic material sample via an impact from a flyer plate , whose products expand into a vacuum . the rapid expansion quenches the reaction in the products so that the products can be differentiated by molecular weight detection as they stream past a detector . analysis of this experiment requires a gas dynamic simulation of the products of a reacting multi component gas that flows into a vacuum region . extreme computational difficulties can arise if flow near the vacuum interface is not carefully and accurately computed . we modify an algorithm proposed by munz <digit> , that computed the fluxes appropriate to a gasvacuum interface for an inert ideal gas , and extend it to a multi component mixture of reacting chemical components reactions with general , non ideal equations of state . we illustrate how to incorporate that extension in the context of a complete set of algorithms for a general , cell based flow solver . a key step is to use the local exact solution for an isentropic expansion fan , for the mixture that connects the computed flow states to the vacuum . regularity conditions ( i.e. the liusmoller conditions ) are necessary conditions that must be imposed on the equation of state of the multicomponent fluid in the limit of a vacuum state . we show that the jones , wilkins , lee ( jwl ) equation of state meets these requirements .
modeling multiple event situations across news articles . <eos> readers interested in the context of an event covered in the news such as the dismissal of a lawsuit can benefit from easily finding out about the overall news situation , the legal trial , of which the event is a part . guided by abstract models of news situation types such as legal trials , corporate acquisitions , and kidnappings , brussell is a system that presents situation instances it creates by reading multiple articles about the specific events that comprise them . we discuss how these situation models are structured and how they drive the creation of particular instances .
model selection for least squares support vector regressions based on small world strategy . <eos> model selection plays a key role in the application of support vector machine ( svm ) . in this paper , a method of model selection based on the small world strategy is proposed for least squares support vector regression ( ls svr ) . in this method , the model selection is treated as a single objective global optimization problem in which generalization performance measure performs as fitness function . to get better optimization performance , the main idea of depending more heavily on dense local connections in small world phenomenon is considered , and a new small world optimization algorithm based on tabu search , called the tabu based small world optimization ( tswo ) , is proposed by employing tabu search to construct local search operator . therefore , the hyper parameters with best generalization performance can be chosen as the global optimum based on the powerful search ability of tswo . experiments on six complex multimodal functions are conducted , demonstrating that tswo performs better in avoiding premature of the population in comparison with the genetic algorithm ( ga ) and particle swarm optimization ( pso ) . moreover , the effectiveness of leave one out bound of ls svm on regression problems is tested on noisy sinc function and benchmark data sets , and the numerical results show that the model selection using tswo can almost obtain smaller generalization errors than using ga and pso with three generalization performance measures adopted .
a model of seepage field in the tailings dam considering the chemical clogging process . <eos> the radial collector well , an important water drainage construction , has been widely applied to the tailings dam . chemical clogging frequently occurs around the vertical shaft in radial collector well due to enough dissolved oxygen and some heavy metals in groundwater flow of tailings dam . considering the contribution of water discharge from both vertical shaft and horizontal screen laterals and chemical clogging occurring around vertical shaft well , a new model was developed on the basis of multi node well ( mnw2 ) package of modflow . moreover , two cases were calculated by the newly developed model . the results indicate that the model considering chemical clogging occurring around the vertical shaft well is reasonable . owing to the decrease in hydraulic conductivity caused by chemical clogging , the groundwater level in dam body increases constantly and water discharge of radial collector well declines by <digit> % . for ordinary vertical well , it decreases by <digit> % . therefore , chemical clogging occurring around radial collector well can arouse increases of groundwater level , and influence dambody safety .
a symmetrisation method for non associated unified hardening model . <eos> this paper presents a simple method for symmetrising the asymmetric elastoplastic matrix arising from non associated flow rules . the symmetrisation is based on mathematical transformation and does not alter the incremental stressstrain relationship . the resulting stress increment is identical to that obtained using the original asymmetrized elastoplastic matrix . the symmetrisation method is applied to integrate the unified hardening ( uh ) model where the elastoplastic matrix is asymmetric due to stress transformation . the performance of the method is verified through finite element analysis ( fea ) of boundary value problems such as triaxial extension tests and bearing capacity of foundations . it is found that the symmetrisation method can improve the convergence of the fea and reduce computational time significantly for non associated elastoplastic models .
rofl routing on flat labels . <eos> it is accepted wisdom that the current internet architecture conflates network locations and host identities , but there is no agreement on how a future architecture should distinguish the two . one could sidestep this quandary by routing directly on host identities themselves , and eliminating the need for network layer protocols to include any mention of network location . the key to achieving this is the ability to route on flat labels . in this paper we take an initial stab at this challenge , proposing and analyzing our rofl routing algorithm . while its scaling and efficiency properties are far from ideal , our results suggest that the idea of routing on flat labels can not be immediately dismissed .
a branch and cut approach for a generic multiple product , assembly system design problem . <eos> this paper presents two new models to deal with different tooling requirements in the generic multiple product assembly system design ( mpasd ) problem and proposes a new branch and cut solution approach , which adds cuts at each node in the search tree . it employs the facet generation procedure ( fgp ) to generate facets of underlying knapsack polytopes . in addition , it uses the fgp in a new way to generate additional cuts and incorporates two new methods that exploit special structures of the mpasd problem to generate cuts . one new method is based on a principle that can be applied to solve generic <digit> <digit> problems by exploiting embedded integral polytopes . the approach includes new heuristic and pre processing methods , which are applied at the root node to manage the size of each instance . this paper establishes benchmarks for mpasd through an experiment in which the approach outperformed ibm 's optimization subroutine library ( osl ) , a commercially available solver .
managing cognitive and mixed motive conflicts in concurrent engineering . <eos> in collaborative activities such as concurrent engineering ( ge ) , conflicts arise due to differences in goals , information available , and the understanding of the task . such conflicts can be categorized into two types mixed motive and cognitive . mixed motive conflicts are essentially due to interest differentials among stakeholders . cognitive conflicts can occur even when the stakeholders do not differ in their respective utilities , but simply because they offer multiple cognitive perspectives on the problem . because conflicts in ce occur under a wider context of cooperative problem solving , the imperative for solving conflicts in such situations is strong . this paper argues that mechanisms for managing conflicts in ce should bear a strong conceptual mapping with the nature of the underlying conflict . moreover , since ce activities are performed in collaborative settings , such mechanisms should accommodate information processing at multiple referent levels . we discuss the nature of both types of conflicts and the requirements of mechanisms for managing them , the functionalities of an implementation that addresses these requirements are illustrated through an example of a ce task
designing robust emergency medical service via stochastic programming . <eos> this paper addresses the problem of designing robust emergency medical services . under this respect , the main issue to consider is the inherent uncertainty which characterizes real life situations . several approaches can be used to design robust mathematical models which are able to hedge uncertain conditions . we are using here the stochastic programming framework and , in particular , the probabilistic paradigm . more specifically , we develop a stochastic programming model with probabilistic constraints aimed to solve both the location and the dimensioning problems , i.e. where service sites must be located and how many emergency vehicles must be assigned to each site , in order to achieve a reliable level of service and minimize the overall costs . in doing so , we consider the randomness of the system as far as the demand of emergency service is concerned . the numerical results , which have been collected on a large set of test problems , demonstrate the validity of the proposed model , particularly in dealing with the trade off between quality of service and costs management .
recommendation of optimized information seeking process based on the similarity of user access behavior patterns . <eos> differing from many studies of recommendation that provided the final results directly , our study focuses on providing an optimized process of information seeking to users . based on process mining , we propose an integrated adaptive framework to support and facilitate individualized recommendation based on the gradual adaptation model that gradually adapts to a target users transition of needs and behaviors of information access , including various search related activities , over different time spans . in detail , successful information seeking processes are extracted from the information seeking histories of users . furthermore , these successful information seeking processes are optimized as a series of action units to support the target users whose information access behavior patterns are similar to the reference users . based on these , the optimized information seeking processes are navigated to the target users according to their transitions of interest focus . in addition to describing some definitions and measures introduced , we go further to present an optimized process recommendation model and show the system architecture . finally , we discuss the simulation and scenario for the proposed system .
analytical mechanics solution for mechanism motion and elastic deformation hybrid problem of beam system . <eos> based on the dynamics of flexible multi body systems and finite element method , a beam system dynamics model is built for solving motiondeformation mixed problem and tracing the whole process of mechanism motion . kinetic control equation and constraint equation in which , mechanism motion and elastic deformation is described using hybrid coordinates , and the spatial position matrix of the element is described using euler quaternion are derived . numerical examples show that the method can trace and solve the track and internal force of the system .
availability analysis of shared backup path protection under multiple link failure scenario in wdm networks . <eos> dedicated protection and shared protection are the main protection schemes in optical wavelength division multiplexing ( wdm ) networks . shared protection techniques surpass the dedicated protection techniques by providing the same level of availability as dedicated protection with reduced spare capacity . satisfying the service availability levels defined by the users service level agreement ( sla ) in a cost effective and resource efficient way is a major challenge for networks operators . hence , evaluating the availability of the shared protection scheme has a great interest . we recently developed an analytical model to estimate network availability of a wdm network with shared link connections under multiple link failures . however , this model requires the information of all possible combinations of the unshared protection paths , which is somehow troublesome . in this paper , we propose a more practical analytical model for evaluating the availability of a wdm network with shared link connections under multiple link failures . the proposed model requires only an estimate of the set of shared paths of each protection path . the estimated availability of the proposed model accurately matched with that of the previous model . finally , we compare the previous model with the proposed model to demonstrate the merits and demerits of both models illustrating the threshold at which each model performs better based on the computational complexity . the proposed model significantly contributes to the related areas by providing network operators with a practical tool to evaluate quantitatively the system availability and , thus , the expected survivability degree of wdm optical networks with shared connections under multiple link failures .
evolving rbf neural networks for time series forecasting with evrbf . <eos> this paper is focused on determining the parameters of radial basis function neural networks ( number of neurons , and their respective centers and radii ) automatically . while this task is often done by hand , or based in hillclimbing methods which are highly dependent on initial values , in this work , evolutionary algorithms are used to automatically build a radial basis function neural networks ( rbf nn ) that solves a specified problem , in this case related to currency exchange rates forecasting . the evolutionary algorithm evrbf has been implemented using the evolutionary computation framework evolving object , which allows direct evolution of problem solutions . thus no internal representation is needed , and specific solution domain knowledge can be used to construct specific evolutionary operators , as well as cost or fitness functions . results obtained are compared with existent bibliography , showing an improvement over the published methods .
modified centralized rocof based load shedding scheme in an islanded distribution network . <eos> two new centralized adaptive under frequency load shedding methods are proposed . dg units operation and loads willing to pay ( wtp ) are considered . the objective is to minimize the resulting penalties of the load shedding .
horn ok please . <eos> road congestion is a common problem worldwide . existing intelligent transport systems ( its ) are mostly inapplicable in developing regions due to high cost and assumptions of orderly traffic . in this work , we develop a low cost technique to estimate vehicular speed , based on vehicular honks . honks are a characteristic feature of the chaotic road conditions common in many developing regions like india and south east asia . we envision a system where dynamic road traffic information is learnt using inexpensive , wireless enabled on road sensors . subsequent analyzed information can then be sent to mobile road users this would fit well with the burgeoning mobile market in developing regions . the core of our technique comprises a pair of road side acoustic sensors , separated by a distance . if a moving vehicle honks between the two sensors , its speed can be estimated from the doppler shift of the honk frequency . in this context , we have developed algorithms for honk detection , honk matching across sensors , and speed estimation . based on the speed estimates , we subsequently detect road congestion . we have done extensive experiments in semi controlled settings as well as real road scenarios under different traffic conditions . using over <digit> hours of road side recordings , we show that our speed estimation technique is effective in real conditions . further , we use our data to characterize traffic state as free flowing versus congested using a variety of metrics the vehicle speed distribution , the number and duration of honks . our results show clear statistical divergence of congested versus free flowing traffic states , and a threshold based classification accuracy of <digit> <digit> % in most situations .
triangular mesh offset for generalized cutter . <eos> in <digit> axis nc ( numerical control ) machining , various cutters are used and the offset compensation for these cutters is important for a gouge free tool path generation . this paper introduces triangular mesh offset method for a generalized cutter defined based on the apt ( automatically programmed tools ) definition or parametric curve . an offset vector is computed according to the geometry of a cutter and the normal vector of a part surface . a triangular mesh is offset to the cl ( cutter location ) surface by multiple normal vectors of a vertex and the offset vector computation method . a tool path for a generalized cutter is generated on the cl surface , and the machining test shows that the proposed offset method is useful for the nc machining .
the new fifa rules are hard complexity aspects of sports competitions . <eos> consider a soccer competition among various teams playing against each other in pairs ( matches ) according to a previously determined schedule . at some stage of the competition one may ask whether a particular team still has a ( theoretical ) chance to win the competition . the complexity of this question depends on the way scores are allocated according to the outcome of a match . for example , the problem is polynomially solvable for the ancient fifa rules ( resp . ) but becomes np hard if the new rules ( resp . ) are applied . we determine the complexity of the above problem for all possible score allocation rules .
a new segmentation method for phase change thermography sequence . <eos> a new segmentation method for image sequence is proposed in order to get the isotherm from phase change thermography sequence ( pcts ) . firstly , the pcts is transformed into a series of synthesized images by compression and conversion , so the isotherm extraction can be transformed into the segmentation of a series of synthesized images . secondly , a virtual illumination model is constructed to eliminate the glisten of the aerocraft model . in order to get the parameters of virtual illumination model , a coordination optimization method is employed and all parameters are obtained according to the similarity constraint . finally , the proving isotherms are gained after the threshold coefficients are compensated . the eventual results demonstrate the efficiency of the proposed segmentation method .
multi color continuous variable entangled optical beams generated by nopos . <eos> we propose an alternative scalable way to generate multi color entangled optical beams efficiently utilizing the tripartite entanglement existent between three fieldssignal , idler , and pumpfrom a nondegenerate optical parametric oscillator ( nopo ) operating above the threshold . the special case of two cascaded nopos is studied , as it is shown that the five beams with very different frequencies are generated by nopoa ( one of the retained signal and idler beams , and the reflected pump beam ) and nopob ( the output signal and idler beams , and the reflected pump beam ) . these beams are theoretically demonstrated to be continuous variable ( cv ) entangled with each other by applying the positivity of the partially transposed criterion for the inseparability of multipartite cv entanglement . the symplectic eigenvalues of the partial transposition covariance matrix of the obtained optical entangled state are numerically calculated in terms of experimentally reachable system parameters . the optimal operation conditions to achieve high five color entanglement are presented . as the cavity parameters and the nonlinear crystals of the two nopos can be chosen freely , the frequencies of the submodes in the entangled state thus are adjustable to match the transition frequencies of atoms or low loss fiber optic communication window . the calculated results provide direct references for future experiment to generate multi color entangled optical beams efficiently by means of nopos operating above the threshold .
bched energy balanced sub round local topology management for wireless sensor network . <eos> topology controlling based on cluster structure is an important method to improve the energy efficiency of wireless sensor network ( wsn ) systems . frequent clustering process of classical controlling methods , such as leach , is apt to cause serious energy consuming . some improved methods reduced re clustering frequency , but these methods sometimes lead to energy imbalance in the stable communication period . in this paper , a hierarchical topology controlling method bched is proposed . with double round clustering mechanism , bched activates a local re clustering process between two rounds of data transmission , and with optional cluster head exchanging mechanism , bched reorganize the node clusters according to their residual energy distribution . experimental results show that , with bched , the energy balance performance of wsn system is significantly improved , and the system lifetime can be effectively extended .
hierarchical reconstruction for discontinuous galerkin methods on unstructured grids with a weno type linear reconstruction and partial neighboring cells . <eos> the hierarchical reconstruction ( hr ) y. j. liu , c. w. shu , e. tadmor , m. p. zhang , central discontinuous galerkin methods on overlapping cells with a non oscillatory hierarchical reconstruction , siam j. numer . anal . <digit> ( <digit> ) <digit> <digit> is applied to the piecewise quadratic discontinuous galerkin method on two dimensional unstructured triangular grids . a variety of limiter functions have been explored in the construction of piecewise linear polynomials in every hierarchical reconstruction stage . we show that on triangular grids , the use of center biased limiter functions is essential in order to recover the desired order of accuracy . several new techniques have been developed in the paper ( a ) we develop a weno type linear reconstruction in each hierarchical level , which solves the accuracy degeneracy problem of previous limiter functions and is essentially independent of the local mesh structure ( b ) we find that hr using partial neighboring cells significantly reduces over under shoots , and further improves the resolution of the numerical solutions . the method is compact and therefore easy to implement . numerical computations for scalar and systems of nonlinear hyperbolic equations are performed . we demonstrate that the procedure can generate essentially non oscillatory solutions while keeping the resolution and desired order of accuracy for smooth solutions .
spatialtemporal model for demand and allocation of waste landfills in growing urban regions . <eos> shortage of land for waste disposal is a serious and growing potential problem in most large urban regions . however , no practical studies have been reported in the literature that incorporate the process of consumption and depletion of landfill space in urban regions over time and analyse its implications for the management of waste . an evaluation of existing models of waste management indicates that they can provide significant insights into the design of solid waste management activities . however , these models do not integrate spatial and temporal aspects of waste disposal that are essential to understand and measure the problem of shortage of land . the lack of adequate models is caused in part due to limitations of the methodologies the existing models are based upon , such as limitations of geographic information systems ( gis ) in handling dynamic processes , and the limitations of systems analysis in incorporating spatial physical properties . this indicates that new methods need to be introduced in waste management modelling . moreover , existing models generally do not link waste management to the process of urban growth . this paper presents a model to spatially and dynamically model the demand for and allocation of facilities for urban solid waste disposal in growing urban regions . the model developed here consists of a loose coupled system that integrates gis ( geographic information systems ) and cellular automata ( ca ) in order to give it spatial and dynamic capabilities . the model combines three sub systems ( <digit> ) a ca based model to simulate spatial urban growth over the future ( <digit> ) a spread sheet calculation for designing waste disposal options and hence evaluating demand for landfill space over time and ( <digit> ) a model developed within a gis to evaluate the availability and suitability of land for landfill over time and then simulate allocation of landfills in the available land . the proposed model has been tested and set up with data from a real source ( porto alegre city , brazil ) , and has successfully assessed the demand for landfills and their allocation over time under a range of scenarios of decision making regarding waste disposal systems , urban growth patterns and land evaluation criteria .
dynamic delamination modelling using interface elements . <eos> existing techniques in explicit dynamic finite element ( fe ) codes for the analysis of delamination in composite structures and components can be simplistic , using simple stress based failure function to initiate and propagate delaminations . this paper presents an interface modelling technique for explicit fe codes . the formulation is based on damage mechanics and uses only two constants for each delamination mode firstly , a stress threshold for damage to commence , and secondly , the critical energy release rate for the particular delamination mode . the model has been implemented into the llnl dyna3d finite element ( fe ) code and the ls dyna3d commercial fe code . the interface element modelling technique is applied to a series of common fracture toughness based delamination problems , namely the dcb , enf and mmb tests . the tests are modelled using a simple dynamic relaxation technique , and serves to validate the methodology before application to more complex problems . explicit finite elements codes , such as dyna3d , are commonly used to solve impact type problems . a modified boeing impact test at two energy levels is used to illustrate the application of the interface element technique , and its coupling to existing in plane failure models . simulations are also performed without interface elements to demonstrate the need to include the interface when modelling impact on composite components .
a new steiner patch based file format for additive manufacturing processes . <eos> a new steiner patch based additive manufacturing file format has been developed . steiner format uses triangular rational bezier representation of steiner patches . steiner format has high geometric fidelity and low approximation error . the steiner patches can be easily sliced and closed form solutions can be obtained . am parts manufactured using steiner format has very low profile and form errors .
empirical challenges and solutions in constructing a high performance metasearch engine . <eos> purpose this paper seeks to disclose the important role of missing documents , broken links and duplicate items in the results merging process of a metasearch engine in detail . it aims to investigate some related practical challenges and proposes some solutions . the study also aims to employ these solutions to improve an existing model for results aggregation . design methodology approach this research measures the amount of an increase in retrieval effectiveness of an existing results merging model that is obtained as a result of the proposed improvements . the <digit> queries of the <digit> trec web track were employed as a standard test collection based on a snapshot of the worldwide web to explore and evaluate the retrieval effectiveness of the suggested method . three popular web search engines ( ask , bing and google ) as the underlying resources of metasearch engines were selected . each of the <digit> queries was passed to all three search engines . for each query the top ten non sponsored results of each search engine were retrieved . the returned result lists of the search engines were aggregated using a proposed algorithm that takes the practical issues of the process into consideration . the effectiveness of the result lists generated was measured using a well known performance indicator called tsap ( trec style average precision ) . findings experimental results demonstrate that the proposed model increases the performance of an existing results merging system by 14.39 percent on average . practical implications the findings of this research would be helpful for metasearch engine designers as well as providing motivation to the vendors of web search engines to improve their technology . originality value this study provides some valuable concepts , practical challenges , solutions and experimental results in the field of web metasearching that have not been previously investigated .
maintaining awareness using policies enabling agents to identify relevance of information . <eos> the field of computer supported cooperative work aims at providing information technology models , methods , and tools that assist individuals to cooperate . the presented paper is based on three main observations from literature . first , one of the problems in utilizing information technology for cooperation is to identify the relevance of information , called awareness . second , research in computer supported cooperative work proposes the use of agent technologies to aid individuals to maintain their awareness . third , literature lacks the formalized methods on how software agents can identify awareness . this paper addresses the problem of awareness identification . the main contribution of this paper is to propose and evaluate a formalized structure , called policy based awareness management ( pam ) . pam extends the logic of general awareness in order to identify relevance of information . pam formalizes existing policies into directory enabled networks next generation structure and uses them as a source for awareness identification . the formalism is demonstrated by applying pam to the space shuttle columbia disaster occurred in <digit> . the paper also argues that efficacy and cost efficiency of the logic of general awareness will be increased by pam . this is evaluated by simulation of hypothetical scenarios as well as a case study . ( c ) <digit> elsevier inc. all rights reserved .
extension headers for ipv6 anycast . <eos> anycast is a new communication paradigm defined in ipv6 . different from unicast and multicast routing , routers on the internetwork deliver an anycast datagrant to the nearest available node . by shifting the task of resolving destinations from source node to internetwork , anycasting is highly flexible and cost effective on routing process and inherently load balanced and robust on server selection . to achieve these objectives , not only distance but also other metrics , such as load balance , reliability , qos , can and should be taken into account in anycast routing . the ipv6 basic header is designed in a simple and fixed length format for the purpose of efficient forwarding . extra data and options needed for packet processing are encoded into extension headers . such a design makes possible the adding of extension headers for special purposes . in this paper , we define routing extension headers for ipv6 anycasting to enable various types of anycast routing mechanism . scenarios are also provided to demonstrate how to apply them . ( c ) <digit> elsevier b.v. all rights reserved .
the calculus of constructions as a framework for proof search with set variable instantiation . <eos> we show how a procedure developed by bledsoe for automatically finding substitution instances for set variables in higher order logic can be adapted to provide increased automation in proof search in the calculus of constructions ( cc ) . bledsoe 's procedure operates on an extension of first order logic that allows existential quantification over set variables . this class of variables can also be identified in cc . the existence of a correspondence between higher order logic and higher order type theories such as cc is well known . cc can be viewed as an extension of higher order logic where the basic terms of the language , the simply typed lambda terms , are replaced with terms containing dependent types . we show how bledsoe 's techniques can be incorporated into a reformulation of a search procedure for cc given by dowek and extended to handle terms with dependent types . we introduce a notion of search context for cc which allows us to separate the operations of assumption introduction and backchaining . search contexts allow a smooth integration of the step which finds solutions to set variables . we discuss how the procedure can be restricted to obtain procedures for set variable instantiation in sublanguages of cc such as the logical framework ( lf ) and higher order hereditary harrop formulas ( hohh ) . the latter serves as the logical foundation of the lambda prolog logic programming language . ( c ) <digit> elsevier science b.v. all rights reserved .
learning temporal nodes bayesian networks . <eos> temporal nodes bayesian networks ( tnbns ) are an alternative to dynamic bayesian networks for temporal reasoning with much simpler and efficient models in some domains . tnbns are composed of temporal nodes , temporal intervals , and probabilistic dependencies . however , methods for learning this type of models from data have not yet been developed . in this paper , we propose a learning algorithm to obtain the structure and temporal intervals for tnbns from data . the method consists of three phases ( i ) obtain an initial approximation of the intervals , ( ii ) obtain a structure using a standard algorithm and ( iii ) refine the intervals for each temporal node based on a clustering algorithm . we evaluated the method with synthetic data from three different tnbns of different sizes . our method obtains the best score using a combined measure of interval quality and prediction accuracy , and a competitive structural quality with lower running times , compared to other related algorithms . we also present a real world application of the algorithm with data obtained from a combined cycle power plant in order to diagnose temporal faults . ( c ) <digit> elsevier inc. all rights reserved .
solving bilevel programs with the kkt approach . <eos> bilevel programs ( bl ) form a special class of optimization problems . they appear in many models in economics , game theory and mathematical physics . bl programs show a more complicated structure than standard finite problems . we study the so called kkt approach for solving bilevel problems , where the lower level minimality condition is replaced by the kkt or the fj condition . this leads to a special structured mathematical program with complementarity constraints . we analyze the kkt approach from a generic viewpoint and reveal the advantages and possible drawbacks of this approach for solving bl problems numerically .
modeling and evaluating of typical advanced peer to peer botnet . <eos> in this paper , we present a general model for an advanced peer to peer ( p2p ) botnet , in which the performance of the botnet can be systematically studied . from the model , we can derive five performance metrics to describe the robustness , security and efficiency of the botnet . additionally , we analyze the relationship between the performance metrics and the model feature metrics of the botnet , and it is helpful to study the botnet under different model feature metrics . furthermore , the proposed model can be easily applied to other types of botnets . finally , taking the robustness and security into consideration , an optimization scheme for designing an optimal p2p botnet is proposed .
learning a coverage set of maximally general fuzzy rules by rough sets . <eos> expert systems have been widely used in domains where mathematical models can not be easily built , human experts are not available or the cost of querying an expert is high . machine learning or data mining can extract desirable knowledge or interesting patterns from existing databases and ease the development bottleneck in building expert systems . in the past we proposed a method hong , t.p. , wang , t.t. , wang , s.l. ( <digit> ) . knowledge acquisition from quantitative data using the rough set theory . intelligent data analysis ( in press ) . , which combined the rough set theory and the fuzzy set theory to produce all possible fuzzy rules from quantitative data . in this paper , we propose a new algorithm to deal with the problem of producing a set of maximally general fuzzy rules for coverage of training examples from quantitative data . a rule is maximally general if no other rule exists that is both more general and with larger confidence than it . the proposed method first transforms each quantitative value into a fuzzy set of linguistic terms using membership functions and then calculates the fuzzy lower approximations and the fuzzy upper approximations . the maximally general fuzzy rules are then generated based on these fuzzy approximations by an iterative induction process . the rules derived can then be used to build a prototype knowledge base in a fuzzy expert system .
an ontological conceptualization approach for awareness in domain independent collaborative modeling systems application to a model driven development method . <eos> one of the most important aspects of collaborative systems is the concept of awareness , which refers to the perception and knowledge of the group and its activities . support for the design and automatic development of awareness mechanisms within collaborative systems is hard to find . furthermore , awareness conceptualizations are usually partial and differ greatly between the proposals of different authors . in response to these problems , we propose an awareness ontology that conceptualizes some of the most important aspects of awareness in a specific kind of system collaborative systems for carrying out modeling activities . the awareness ontology brings together and extends a series of ontologies we have developed in the past . the ontology is prepared to better meet the specific implementation needs of a model driven development approach . in order to validate the usefulness of this ontology , we relate its concepts to the awareness dimensions set out in gutwin and greenbergs framework , and we apply the ontology to two systems presently in use .
approximating k node connected subgraphs via critical graphs . <eos> we present two new approximation algorithms for the problem of finding a k node connected spanning subgraph ( directed or undirected ) of minimum cost . the best known approximation guarantees for this problem were o ( min k , n root ( n k ) ) for both directed and undirected graphs , and o ( ln k ) for undirected graphs with n > 6k ( <digit> ) , where n is the number of nodes in the input graph . our first algorithm has approximation ratio o ( n n k ln ( <digit> ) k ) , which is o ( ln ( <digit> ) k ) except for very large values of k , namely , k n o ( n ) . this algorithm is based on a new result on l connected p critical graphs , which is of independent interest in the context of graph theory . our second algorithm uses the primal dual method and has approximation ratio o ( v n ln k ) for all values of n , k. combining these two gives an algorithm with approximation ratio o ( ln k center dot min root k , n n k ln k ) , which asymptotically improves the best known approximation guarantee for directed graphs for all values of n , k , and for undirected graphs for k > root n <digit> . moreover , this is the first algorithm that has an approximation guarantee better than t ( k ) for all values of n , k. our approximation ratio also provides an upper bound on the integrality gap of the standard lp relaxation .
development of an autowep distributed hydrological model and its application to the upstream catchment of the miyun reservoir . <eos> based on the physically characterized distributed hydrological modeling scheme wep l a more generalized and expandable method autowep has been developed that is equipped with updated modules for pre processing and automatic parameter identification . sub basin scale classifications of land use and soil are undertaken by incorporating remote sensing data and geographic information system techniques . in the process of developing the autowep modeling scheme , a new concept of parameter partitioning is proposed and an automatic delineation of parameter partitions is achieved through programming . the sensitivity analysis algorithm , lh oat , and the parameter optimization algorithm , sce ua , are embedded in the model . its application to the upstream watershed of the miyun reservoir shows that autowep features time savings , improved efficiency and suitable generalizations , that result in a long series of acceptable simulations .
towards insider threat detection using web server logs . <eos> malicious insiders represent one of the most difficult categories of threats an organization must consider when mitigating operational risk . insiders by definition possess elevated privileges have knowledge about control measures and may be able to bypass security measures designed to prevent , detect , or react to unauthorized access . in this paper , we discuss our initial research efforts focused on the detection of malicious insiders who exploit internal organizational web servers . the objective of the research is to apply lessons learned in network monitoring domains and enterprise log management to investigate various approaches for detecting insider threat activities using standardized tools and a common event expression framework .
weight similarity measurement model based , object oriented approach for bug databases mining to detect similar and duplicate bugs . <eos> in this paper data mining is applied on bug database to discover the similar and duplicate bugs . whenever a new bug will be entered in the bug database through bug tracking system , it will be matched against the existing bugs and duplicate and similar bugs will be mined from the bug database . similar kind of bugs are resolved in almost in same manners . so if a bug is found somewhere similar to other existing bug which is already resolved then its resolution will take less time , since some of the bug analysis part is similar to existing one , hence it will save time . in the existing tradition developers must have to manually identify duplicate bug reports , but this identification process is time consuming and exacerbates the already high cost of software maintenance . so if the similar and duplicate bugs can be found out using some approach it will be a cost and time saving activity . based on this concept a weight similarity measurement model based object orinted approach is described here in this paper to discover similar and duplicate bugs in the bug database .
recursive modeling for completed code generation . <eos> model driven development is promising to software development because it can reduce the complexity and cost of developing large software systems . the basic idea is the use of different kinds of models during the software development process , transformations between them , and automatic code generation at the end of the development . but unlike the structural parts , fully automated code generation from the behavior parts is still hard , if it works at all , restricted to specific application areas using a domain specific language , dsl . this paper proposes an approach to model the behavior parts of a system and to embed them into the structural models . the underlying idea is recursive refinements of activity elements in an activity diagram . with this , the detail generated code depends on the depth at which the refinements are done , i.e. if the lowest level of activities is mapped into activities executors , the completed code can be obtained .
on the theoretical comparison of low bias steady state estimators . <eos> the time average estimator is typically biased in the context of steady state simulation , and its bias is of order <digit> t , where t represents simulated time . several low bias estimators have been developed that have a lower order bias , and , to first order , the same variance of the time average . we argue that this kind of first order comparison is insufficient , and that a second order asymptotic expansion of the mean square error ( mse ) of the estimators is needed . we provide such an expansion for the time average estimator in both the markov and regenerative settings . additionally , we provide a full bias expansion and a second order mse expansion for the meketon heidelberger low bias estimator , and show that its mse can be asymptotically higher or lower than that of the time average depending on the problem . the situation is different in the context of parallel steady state simulation , where a reduction in bias that leaves the first order variance unaffected is arguably an improvement in performance .
the multisymplectic numerical method for grosspitaevskii equation . <eos> for a boseeinstein condensate placed in a rotating trap and confined in the z axis , a multisymplectic difference scheme was constructed to investigate the evolution of vortices in this paper . first , we look for a steady state solution of the imaginary time g p equation . then , we numerically study the vortices 's development in real time , starting with the solution in imaginary time as initial value .
system design , data collection and evaluation of a speech dialog system . <eos> this paper describes design issues of a speech dialogue system , the evaluation of the system , and the data collection of spontaneous speech in a transportation guidance domain . as it is difficult to collect spontaneous speech and to use a real system for the collection and evaluation , the phenomena related with dialogues have not been quantitatively clarified yet . the authors constructed a speech dialogue system which operates in almost real time , with acceptable recognition accuracy and flexible dialogue control . the system was used for spontaneous speech collection in a transportation guidance domain . the system performance evaluated in the domain is the understanding rate of 84.2 % for the utterances within the predefined grammar and the lexicon . also some statistics of the spontaneous speech collected are given .
abstract convex evolutionary search . <eos> geometric crossover is a formal class of crossovers which includes many well known recombination operators across representations . in this paper , we present a general result showing that all evolutionary algorithms using geometric crossover with no mutation perform the same form of convex search regardless of the underlying representation , the specific selection mechanism , the specific offspring distribution , the specific search space , and the problem at hand . we then start investigating a few representation space independent geometric conditions on the fitness landscape various forms of generalized concavity that when matched with the convex evolutionary search guarantee , to different extents , improvement of offspring over parents for any choice of parents . this is a first step towards showing that the convexity relation between search and landscape may play an important role towards explaining the performance of evolutionary algorithms in a general setting across representations .
area measurement of large closed regions with a mobile robot . <eos> how can a mobile robot measure the area of a closed region that is beyond its immediate sensing range this problem , which we name as blind area measurement , is inspired from scout worker ants who assess potential nest cavities . we first review the insect studies that have shown that these scouts , who work in dark , seem to assess arbitrary closed spaces and reliably reject nest sites that are small for the colony . we briefly describe the hypothesis that these scouts use buffon 's needle method to measure the area of the nest . then we evaluate and analyze this method for mobile robots to measure large closed regions . we use a simulated mobile robot system to evaluate the performance of the method through systematic experiments . the results showed that the method can reliably measure the area of large and rather open , closed regions regardless of their shape and compactness . moreover , the method 's performance seems to be undisturbed by the existence of objects and by partial barriers placed inside these regions . finally , at a smaller scale , we partially verified some of these results on a real mobile robot platform .
minimum pilot power for service coverage in wcdma networks . <eos> pilot power management is an important issue for efficient resource utilization in wcdma networks . in this paper , we consider the problem of minimizing pilot power subject to a coverage constraint . the constraint can be used to model various levels of coverage requirement , among which full coverage is a special case . the pilot power minimization problem is np hard , as it generalizes the set covering problem . our solution approach for this problem consists of mathematical programming models and methods . we present a linear integer mathematical formulation for the problem . to solve the problem for large scale networks , we propose a column generation method embedded into an iterative rounding procedure . we apply the proposed method to a range of test networks originated from realistic network planning scenarios , and compare the results to those obtained by two ad hoc approaches . the numerical experiments show that our algorithm is able to find near optimal solutions with a reasonable amount of computing effort for large networks . moreover , optimized pilot power considerably outperforms the ad hoc approaches , demonstrating that efficient pilot power management is an important component of radio resource optimization . as another part of our numerical study , we examine the trade off between service coverage and pilot power consumption .
asymptotically stable multi valued many to many associative memory neural network and its application in image retrieval . <eos> as an important artificial neural network , associative memory model can be employed to mimic human thinking and machine intelligence . in this paper , first , a multi valued many to many gaussian associative memory model ( m ( <digit> ) gam ) is proposed by introducing the gaussian unidirectional associative memory model ( guam ) and gaussian bidirectional associative memory model ( gbam ) into hattori et al 's multi module associative memory model ( ( mma ) ( <digit> ) ) . second , the m ( <digit> ) gam 's asymptotical stability is proved theoretically in both synchronous and asynchronous update modes , which ensures that the stored patterns become the m ( <digit> ) gam 's stable points . third , by substituting the general similarity metric for the negative squared euclidean distance in m ( <digit> ) gam , the generalized multivalued many to many gaussian associative memory model ( gm ( <digit> ) gam ) is presented , which makes the m ( <digit> ) gam become its special case . finally , we investigate the m ( <digit> ) gam 's application in association based image retrieval , and the computer simulation results verify the m ( <digit> ) gam 's robust performance .
identity based threshold proxy signature from bilinear pairings . <eos> delegation of rights is a common practice in the real world . we present two identity based threshold proxy signature schemes , which allow an original signer to delegate her signing capability to a group of n proxy signers , and it requires a consensus of t or more proxy signers in order to generate a valid signature . in addition to identity based scheme , privacy protection for proxy singers and security assurance are two distinct features of this work . our first scheme provides partial privacy protection to proxy signers such that all signers ' identities are revealed , whereas none of those t participating signers is specified . on the other hand , all proxy signers remain anonymous in the second scheme . this provides a full privacy protection to all proxy signers however , each valid signature contains a tag that allows one to trace all the participating proxy signers . both our proposed schemes are secure against unforgeability under chosen message attack , and satisfy many other necessary conditions for proxy signature .
semi autonomous navigation of a robotic wheelchair . <eos> the present work considers the development of a wheelchair for people with special needs , which is capable of navigating semi autonomously within its workspace . this system is expected to prove useful to people with impaired mobility and limited fine motor control of the upper extremities . among the implemented behaviors of this robotic system are the avoidance of obstacles , the motion in the middle of the free space and the following of a moving target specified by the user ( e.g. , a person walking in front of the wheelchair ) . the wheelchair is equipped with sonars , which are used for distance measurement in preselected critical directions , and with a panoramic camera with a <digit> degree field of view , which is used for following a moving target . after suitably processing the color sequence of the panoramic images using the color histogram of the desired target , the orientation of the target with respect to the wheelchair is determined , while its distance is determined by the sonars . the motion control laws developed for the system use the sensory data and take into account the non holonomic kinematic constraints of the wheelchair , in order to guarantee certain desired features of the closed loop system , such as stability . moreover , they are as simplified as possible to minimize implementation requirements . an experimental prototype has been developed at ics forth , based on a commercially available wheelchair . the sensors , the computing power and the electronics needed for the implementation of the navigation behaviors and of the user interfaces ( touch screen , voice commands ) were developed as add on modules and integrated with the wheelchair .
learning protein secondary structure from sequential and relational data . <eos> we propose a method for sequential supervised learning that exploits explicit knowledge of short and long range dependencies . the architecture consists of a recursive and bi directional neural network that takes as input a sequence along with an associated interaction graph . the interaction graph models ( partial ) knowledge about long range dependency relations . we tested the method on the prediction of protein secondary structure , a task in which relations due to beta strand pairings and other spatial proximities are known to have a significant effect on the prediction accuracy . in this particular task , interactions can be derived from knowledge of protein contact maps at the residue level . our results show that prediction accuracy can be significantly boosted by the integration of interaction graphs .
document replication strategies for geographically distributed web search engines . <eos> large scale web search engines are composed of multiple data centers that are geographically distant to each other . typically , a user query is processed in a data center that is geographically close to the origin of the query , over a replica of the entire web index . compared to a centralized , single center search engine , this architecture offers lower query response times as the network latencies between the users and data centers are reduced . however , it does not scale well with increasing index sizes and query traffic volumes because queries are evaluated on the entire web index , which has to be replicated and maintained in all data centers . as a remedy to this scalability problem , we propose a document replication framework in which documents are selectively replicated on data centers based on regional user interests . within this framework , we propose three different document replication strategies , each optimizing a different objective reducing the potential search quality loss , the average query response time , or the total query workload of the search system . for all three strategies , we consider two alternative types of capacity constraints on index sizes of data centers . moreover , we investigate the performance impact of query forwarding and result caching . we evaluate our strategies via detailed simulations , using a large query log and a document collection obtained from the yahoo web search engine .
collision correction using a cross layer design architecture for dedicated short range communications vehicle safety messaging . <eos> this paper presents a new physical ( phy ) and medium access control ( mac ) cross layer design frame collision correction ( cc ) architecture for correction of dedicated short range communications ( dsrcs ) safety messages . conditions suitable for the use of this design are presented , which can be used for optimization . at its basic level , the cc at the phy uses a new decision making block that uses information from the mac layer for the channel estimator and equalizer . this requires a cache of previously received frames , and pre announcing frame repetitions from the mac . we present the theoretical equations behind cc mechanism , and describe the components required to implement the cross layer cc using deployment and sequence diagrams . simulation results show that especially under high user load , reception reliability of the dsrc safety messages increases and per decreases .
a general model of unit testing efficacy . <eos> much of software engineering is targeted towards identifying and removing existing defects while preventing the injection of new ones . defect management is therefore one important software development process whose principal aim is to ensure that the software produced reaches the required quality standard before it is shipped into the market place . in this paper , we report on the results of research conducted to develop a predictive model of the efficacy of one important defect management technique , that of unit testing . we have taken an empirical approach . we commence with a number of assumptions that led to a theoretical model which describes the relationship between effort expended and the number of defects remaining in a software code module tested ( the latter measure being termed correctness ) . this model is general enough to capture the possibility that debugging of a software defect is not perfect and could lead to new defects being injected . the model is examined empirically against actual data and validated as a good predictive model under specific conditions . the work has been done in such a way that models are derived not only for the case of overall correctness but also for specific types of correctness such as correctness arising from the removal of defects contributing to shortcoming in reliability ( r type ) , functionality ( f type ) , usability ( u type ) and maintainability ( m type ) aspects of the program subject to defect management .
constitutive modeling of materials and contacts using the disturbed state concept part <digit> background and analysis . <eos> computer methods have opened a new era for accurate and economic analysis and design of engineering problems . they account for many significant factors such as arbitrary geometries , nonhomogeneities in material composition , complex boundary conditions , nonlinear material behavior ( constitutive modeling ) and complex loading conditions , which were difficult to include in conventional and closed form solution procedures . constitutive modeling characterizes the mechanical behavior of solids and contacts ( e.g. interfaces and joints ) , and plays perhaps the most important role for realistic solutions from procedures in computational mechanics . a great number of constitutive models , from simple to the advanced , have been proposed . most of them account for specific characteristics of the material . however , a deforming material may experience , simultaneously , many characteristics such as elastic , plastic and creep strains , different loading ( stress ) paths , volume change under shear stress , microcracking leading to fracture and failure , strain softening or degradation , and healing or strengthening . hence , there is a need for developing unified models that account for these characteristics . the main objective of these two papers is to present a brief review of the available constitutive models , and identify their capabilities and limitations then a novel and unified approach , called the disturbed state concept ( dsc ) with hierarchical single surface ( hiss ) plasticity , is presented including its theoretical background , constitutive parameters and their determination , and validation at the test specimen and boundary value problem levels . the general capabilities of the dsc hiss approach are emphasized by its application for a wide range of materials and contacts ( interfaces and joints ) . because of its generality , the dsc contains many previous models as special cases . the presentation is divided in two papers . this paper ( part <digit> ) contains the review of various models , and then description of the dsc hiss model and its analysis for issues such as mesh dependence and localization . part <digit> also contains the capability of the dsc hiss model to define the behavior of both solids and contacts . validations of the dsc hiss model at the specimen and boundary value problem levels for a wide range of materials and contacts are included in the compendium paper , part <digit> . the idea of the dsc is considered to be relatively simple , and it can be easily implemented in computer procedures . it is believed that the dsc can provide a realistic and unified approach for constitutive modeling for a wide range of materials and contacts .
an efficient neumann series based algorithm for thermoacoustic and photoacoustic tomography with variable sound speed . <eos> we present an efficient algorithm for reconstructing an unknown source in thermoacoustic and photoacoustic tomography based on the recent advances in understanding the theoretical nature of the problem . we work with variable sound speeds that also might be discontinuous across some surface . the latter problem arises in brain imaging . the algorithmic development is based on an explicit formula in the form of a neumann series . we present numerical examples with nontrapping , trapping , and piecewise smooth speeds , as well as examples with data on a part of the boundary . these numerical examples demonstrate the robust performance of the neumann series based algorithm .
documentary genre and digital recordkeeping red herring or a way forward . <eos> the purpose of this paper is to provide a preliminary assessment of the utility of the genre concept for digital recordkeeping . the exponential growth in the volume of records created since the 1940s has been a key motivator for the development of strategies that do not involve the review or processing of individual documents or files . automation now allows processes at a level of granularity that is rarely , if at all , possible in the case of manual processes , without loss of cognisance of context . for this reason , it is timely to revisit concepts that may have been disregarded because of a perceived limited effectiveness in contributing anything to theory or practice . in this paper , the genre concept and its employability in the management of current and archival digital records are considered , as a form of social contextualisation of a document and as an attractive entry point of granularity at which to implement automation of appraisal processes . particular attention is paid to the structurational view of genre and its connections with recordkeeping theory .
existence and multiplicity of positive periodic solutions for a class of higher dimension functional differential equations with impulses . <eos> this paper deals with the existence of multiple periodic solutions for n dimensional functional differential equations with impulses . by employing the krasnoselskii fixed point theorem , we obtain some easily verifiable sufficient criteria which extend previous results . ( c ) <digit> elsevier ltd. all rights reserved .
modelling and querying geographical data warehouses . <eos> a number of proposals for integrating geographical ( geographical information systems gis ) and multidimensional ( data warehouse dw and online analytical processing olap ) processing are found in the database literature . however , most of the current approaches do not take into account the use of a cow ( geographical data warehouse ) metamodel or query language to make available the simultaneous specification of multidimensional and spatial operators . to address this , this paper discusses the uml class diagram of a gdw metamodel and proposes its formal specifications . we then present a formal metamodel for a geographical data cube and propose the geographical multidimensional query language ( geomdql ) as well . geomdql is based on well known standards such as the multidimensional expressions ( mdx ) language and ogc simple features specification for sql and has been specifically defined for spatial olap environments based on a gdw . we also present the geomdql syntax and a discussion regarding the taxonomy of geomdql query types . additionally , aspects related to the geomdql architecture implementation are described , along with a case study involving the brazilian public healthcare system in order to illustrate the proposed query language . ( c ) <digit> elsevier b.v. all rights reserved .
ddas distance and direction awareness system for intelligent vehicles . <eos> wireless technology has been widely used for applications of wireless internet access . with the matured wireless transmission technology , the new demand on wireless applications is toward the concept of deploying wireless devices on transportation systems such as buses , trains and vehicles . statistics of car accident cases show that car accidents are often caused from drivers unnoticing other approaching cars during driving . without the assistants of automotive personal computer system ( also called as auto pc ) , during high speed moving , driver always counts on himself herself to look for all vehicles around him her via limited vision and acoustic recognition . in case that the auto pc is able to provide useful surrounding information , such as the directions and distances to nearby vehicles , to drivers , unnecessary collisions could be obviously avoided , especially in cases of changing lane , crossing intersection and making a turn . in this paper , we will introduce the concept of automatic distance and direction awareness system ( ddas ) and describe the designed embedded ddas integrated with three wheel and four wheel robot cars .
investigating models for preservice teachers ' use of technology to support student centered learning . <eos> the study addressed two limitations of previous research on factors related to teachers integration of technology in their teaching . it attempted to test a structural equation model ( sem ) of the relationships among a set of variables influencing preservice teachers ' use of technology specifically to support student centered learning a review of literature led to a path model that provided the design and analysis for the study , which involved <digit> preservice teachers in the united states . the results show that the proposed model had a moderate fit to the observed data , and a more parsimonious model was found to have a better fit . in addition , preservice teachers ' self efficacy of teaching with technology had the strongest influence on technology use , which was mediated by their perceived value of teaching and learning with technology . school 's contextual factors had moderate influence on technology use . moreover , the effect of preservice teachers ' training on student centered technology use was mediated by both perceived value and self efficacy of technology . the implications for teacher preparation include close collaboration between teacher education program and field experience , focusing on specific technology uses ( c ) <digit> elsevier ltd all rights reserved
a risc approach to process groups . <eos> isis <digit> , developed at cornell university , is a system for building applications consisting of cooperating , distributed processes . group management and group communication are two basic building blocks provided by isis . isis has been very successful , and there is currently a demand for a version that will run on many different environments and transport protocols , and will scale to many process groups . furthermore , performance is an important issue . for this purpose , isis is being redesigned and rebuilt from scratch <digit> . of particular importance to us is getting the new isis system to run well on modern microkernel technology , notably mach <digit> and chorus <digit> . the basic reasoning behind these plans is that microkernels appear to offer satisfactory support for memory management and communication between processes on the same machine , but that support for applications that run on multiple machines is weak . the current ipc mechanisms are adequate only for the simpler distributed applications , as they do not address any of the internal management issues of distribution.the new isis system has several well defined layers . the lowest layers , which implement multicast transport and failure detection , are near completion and currently run on sun os using sun lwp threads , on mach using c threads , and on the x kernel <digit> . this system can use several different network protocols at the same time , such as ip , udp ( with or without multicast support ) , and raw ethernet . this enables processes on sun os , mach , and chorus to multicast among each other , even though the environments are very dissimilar . the system makes use of available hardware multicast if possible . it also queues messages if a backlog appears , so that multiple messages may be packed together in a single packet . using this strategy , the number of messages per second can become very large , and in the current ( simple ) implementation about 10,000 per second can be sent between distributed sun os user processes , a figure that approaches the speed of local light weight remote procedure call mechanisms . ( the current round trip time on sun os over ethernet is about <digit> milliseconds . )
on affine scaling algorithms for nonconvex quadratic programming . <eos> we investigate the use of interior algorithms , especially the affine scaling algorithm , to solve nonconvex indefinite or negative definite quadratic programming ( qp ) problems . although the nonconvex qp with a polytope constraint is a hard problem , we show that the problem with an ellipsoidal constraint is easy . when the hard qp is solved by successively solving the easy qp , the sequence of points monotonically converge to a feasible point satisfying both the first and the second order optimality conditions .
multi agent simulation of group behavior in e government policy decision . <eos> to research complex group behavior in e government policy decision , this study proposes a multi agent qualitative simulation approach using eggbm ( e government group behavior model ) . causal reasoning is employed to analyze it from the perspective of system . then , a multi agent simulation decision system based on java repast is developed . moreover , three validation experiments are designed to prove that eggbm can exactly represent the actual situation . at last , an example of application is given to show that this method can help policy makers choose appropriate policies to improve the level of accepting information technology ( lait ) of groups . it is shown that this approach could be a new attempt for the research of group behavior in governmental organization .
independent component analysis for unaveraged single trial meg data decomposition and single dipole source localization . <eos> this paper presents a novel method for decomposing and localizing unaveraged single trial magnetoencephalographic data based on the independent component analysis ( ica ) approach associated with pre and post processing techniques . in the pre processing stage , recorded single trial raw data are first decomposed into uncorrelated signals with the reduction of high power additive noise . in the stage of source separation , the decorrelated source signals are further decomposed into independent source components . in the post processing stage , we perform a source localization procedure to seek a single dipole map of decomposed individual source components , e.g. , evoked responses . the first results of applying the proposed robust ica approach to single trial data with phantom and auditory evoked field tasks indicate the following . ( <digit> ) a source signal is successfully extracted from unaveraged single trial phantom data . the accuracy of dipole estimation for the decomposed source is even better than that of taking the average of total trials . ( <digit> ) not only the behavior and location of individual neuronal sources can be obtained but also the activity strength ( amplitude ) of evoked responses corresponding to a stimulation trial can be obtained and visualized . moreover , the dynamics of individual neuronal sources , such as the trial by trial variations of the amplitude and location , can be observed .
the effects of learning style and hypermedia prior experience on behavioral disorders knowledge and time on task a case based hypermedia environment . <eos> this study involved <digit> graduate students enrolled in a behavioral disorders course . as a part of the course , they engaged in an extensive case based hypermedia program designed to enhance their ability to solve student emotional and behavioral problems . results include ( <digit> ) students increased their knowledge about behavioral disorders ( <digit> ) those students with more hypermedia experience spent more time using the hypermedia program ( <digit> ) those students who acquired greater knowledge also wrote better student reports and ( <digit> ) students , regardless of learning style ( as measured by kolb 's learning style inventory ) , benefited equally from using the hypermedia program .
rough sets , coverings and incomplete information . <eos> rough sets are often induced by descriptions of objects based on the precise observations of an insufficient number of attributes . in this paper , we study generalizations of rough sets to incomplete information systems , involving imprecise observations of attributes . the precise role of covering based approximations of sets that extend the standard rough sets in the presence of incomplete information about attribute values is described . in this setting , a covering encodes a set of possible partitions of the set of objects . a natural semantics of two possible generalisations of rough sets to the case of a covering ( or a non transitive tolerance relation ) is laid bare . it is shown that uncertainty due to granularity of the description of sets by attributes and uncertainty due to incomplete information are superposed , whereby upper and lower approximations themselves ( in pawlak 's sense ) become ill known , each being bracketed by two nested sets . the notion of measure of accuracy is extended to the incomplete information setting , and the generalization of this construct to fuzzy attribute mappings is outlined .
exploiting power budgeting in thermal aware dynamic placement for reconfigurable systems . <eos> in this paper , a novel thermal aware dynamic placement planner for reconfigurable systems is presented , which targets transient temperature reduction . rather than solving time consuming differential equations to obtain the hotspots , we propose a fast and accurate heuristic model based on power budgeting to plan the dynamic placements of the design statically , while considering the boundary conditions . based on our heuristic model , we have developed a fast optimization technique to plan the dynamic placements at design time . our results indicate that our technique is two orders of magnitude faster while the quality of the placements generated in terms of temperature and interconnection overhead is the same , if not better , compared to the thermal aware placement techniques which perform thermal simulations inside the search engine .
optimized independent components for parameter regression . <eos> in this paper , a modified icr algorithm is proposed for quality prediction purpose . the disadvantage of original independent component regression ( icr ) is that the extracted independent components ( ics ) are not informative for quality prediction and interpretation . in the proposed method , to enhance the causal relationship between the extracted ics and quality variables , a dual objective optimization which combines the cost function w ( t ) x ( t ) yv in partial least squares ( pls ) and the approximations of negentropy in independent component analysis ( ica ) is constructed in the first step for feature extraction . it simultaneously considers both the quality correlation and the independence , and then the icr mlr ( multiple linear regression ) method is used to obtain the regression coefficients . the proposed method is applied to the quality prediction in continuous annealing process and tennessee eastman process . applications indicate that the proposed approach effectively captures the relations in the process variables and use of proposed method instead of original pls and icr improves the regression matching and prediction ability . ( c ) <digit> elsevier b.v. all rights reserved .
discriminant bag of words based representation for human action recognition . <eos> human action recognition based on bag of words representation . discriminant codebook learning for better action class discrimination . unified framework for the determination of both the optimized codebook and linear data projections .
unsupervised connectionist algorithms for clustering an environmental data set a comparison . <eos> various unsupervised algorithms for vector quantization can be found in the literature . being based on different assumptions , they do not all yield exactly the same results on the same problem . to better understand these differences , this article presents an evaluation of some unsupervised neural networks , considered among the most useful for quantization , in the context of a real world problem radioelectric wave propagation . radio wave propagation is highly dependent upon environmental characteristics ( e.g. those of the city , country , mountains , etc. ) . within the framework of a cell net planning its radiocommunication strategy , we are interested in determining a set of environmental classes , sufficiently homogeneous , to which a specific prediction model of radio electrical field can be applied . of particular interest are techniques that allow improved analysis of results . firstly , mahalanobis distance , taking data correlation into account , is used to make assignments . secondly , studies of class dispersion and homogeneity , using both a data structure mapping representation and statistical analysis , emphasize the importance of the global properties of each algorithm . in conclusion , we discuss the advantages and disadvantages of each method on real problems .
preference based multi objective evolutionary algorithms for power aware application mapping on noc platforms . <eos> network on chip ( noc ) are considered the next generation of communication infrastructure in embedded systems . in the platform based design methodology , an application is implemented by a set of collaborative intellectual property ( ip ) blocks . the selection of the most suited set of ips as well as their physical mapping onto the noc infrastructure to implement efficiently the application at hand are two hard combinatorial problems that occur during the synthesis process of noc based embedded system implementation . in this paper , we propose an innovative preference based multi objective evolutionary methodology to perform the assignment and mapping stages . we use one of the well known and efficient multi objective evolutionary algorithms nsga ii and microga as a kernel . the optimization processes of assignment and mapping are both driven by the minimization of the required silicon area and imposed execution time of the application , considering that the decision makers preference is a pre specified value of the overall power consumption of the implementation .
2d dry granular free surface transient flow over complex topography with obstacles . part ii numerical predictions of fluid structures and benchmarking . <eos> dense granular flows are present in geophysics and in several industrial processes , which has lead to an increasing interest for the knowledge and understanding of the physics which govern their propagation . for this reason , a wide range of laboratory experiments on gravity driven flows have been carried out during the last two decades . the present work is focused on geomorphological processes and , following previous work , a series of laboratory studies which constitute a further step in mimicking natural phenomena are described and simulated . three situations are considered with some common properties a two dimensional configuration , variable slope of the topography and the presence of obstacles . the setup and measurement technique employed during the development of these experiments are deeply explained in the companion work . the first experiment is based on a single obstacle , the second one is performed against multiple obstacles and the third one studies the influence of a dike on which overtopping occurs . due to the impact of the flow against the obstacles , fast moving shocks appear , and a variety of secondary waves emerge . in order to delve into the physics of these types of phenomena , a shock capturing numerical scheme is used to simulate the cases . the suitability of the mathematical models employed in this work has been previously validated . comparisons between computed and experimental data are presented for the three cases . the computed results show that the numerical tool is able to predict faithfully the overall behavior of this type of complex dense granular flow .
context sharing in a real world ubicomp deployment . <eos> while the application of ubicomp systems to explore context sharing has received a large amount of interest , only a very small number of studies have been carried out which involve real world use outside of the lab . this article presents an in depth analysis of context sharing behaviours that built up around use of the hermes interactive office door display system received during deployment . the hermes system provided a groupware application supporting asynchronous messaging facilities , analogous to a digital form of post it notes , in order to explore the use of situated display systems to support awareness and coordination in an office environment . from this analysis we distil a set of issues relating to context sharing ranging from privacy concerns to ease of use each supported through qualitative data from user interviews and questionnaires .
holding time aware dynamic traffic grooming algorithms based on multipath routing for wdm optical networks . <eos> this paper investigates approaches for the traffic grooming problem that consider connection holding times and bandwidth availability . moreover , solutions can indicate the splitting of connections into two or more sub streams by multipath routing and fine tuned by traffic grooming to utilize network resources better . algorithms are proposed and the results of simulations using a variety of realistic scenarios indicate that the proposed algorithms significantly reduce the blocking of connection requests yet promote a fair distribution of the network resources in relation to the state of the art solutions .
three classes of maximal hyperclones . <eos> in this paper , we present three classes of maximal hyperclones . they are determined by three classes of rosenberg 's relations nontrivial equivalence relations , central relations and h regular relations .
the knowledge acquisition workshops a remarkable convergence of ideas . <eos> intense interest in knowledge acquisition research began <digit> years ago , stimulated by the excitement about knowledge based systems that emerged in the 1970s followed by the realities of the ai winter that arrived in the 1980s . the knowledge acquisition workshops that responded to this interest led to the formation of a vibrant research community that has achieved remarkable consensus on a number of issues . these viewpoints include ( <digit> ) the rejection of the notion of knowledge as a commodity to be transferred from one locus to another , ( <digit> ) an acceptance of the situated nature of human expertise , ( <digit> ) emphasis on knowledge acquisition as the modeling of problem solving , and ( <digit> ) the pursuit of reusable patterns in problem solving and in domain descriptions that can facilitate both modeling and system implementation . the semantic web community will benefit greatly by incorporating these perspectives in its work .
topological persistence for medium access control . <eos> the primary function of the medium access control ( mac ) protocol is managing access to the shared communication channel . from the viewpoint of the transmitters , the mac protocol determines each transmitter 's channel occupancy , the fraction of time that it spends transmitting over the channel . in this paper , we define a set of topological persistences that conform to both network topology and traffic load . we employ these persistences as target occupancies for the mac layer protocol . a centralized algorithm is developed for calculating topological persistences and its correctness is established . a distributed algorithm and implementation are developed that can operate within scheduled and contention based mac protocols . in the distributed algorithm , network resources are allocated through auctions at each receiver in which transmitters participate as bidders to converge on the topological allocation . very low overhead is achieved by piggybacking auction and bidder communication on existing data packets . the practicality of the distributed algorithm is demonstrated in a wireless network via simulation using the ns <digit> network simulator . simulation results show fast convergence to the topological solution and , once operating with topological persistences , improved performance compared to ieee 802.11 in delay , throughput , and drop rate .
plate on layered foundation analyzed by a semi analytical and semi numerical method . <eos> a semi analytical and semi numerical method is developed for the analysis of plate layered soil systems . applying a hankel transform , an expression relating the surface settlement and the reaction of the layered soil is derived . such a reaction can be treated as a load acting on the plate in addition to the applied external load . having the plate modeled by eight noded isoparametric elements , the governing equations of the plate can be formed and solved . numerical examples , including square , trapezoidal and circular plates resting on elastic layered soil , are given to demonstrate the advantages , accuracy and versatility of this method .
semi divisible triangular norms . <eos> semi divisibility of left continuous triangular norms is a weakening of the divisibility ( i.e. , continuity ) axiom for t norms . in this contribution we focus on the class of semi divisible t norms and show the following properties each semi divisible t norm with ran ( n ( t ) ) <digit> , <digit> is nilpotent . semi divisibility of an ordinal sum t norm is determined by the corresponding property of its first component ( which can be a proper t subnorm , too ) . finally , negations with finite range derived from semi divisible t norms are studied .
expert system for remnant life prediction of defected components under fatigue and creep fatigue loadings . <eos> life prediction and management of cracked high temperature structures is a matter of great importance for both economical and safe reasons . to implement such a task , many fields such as material science , structure engineering and mechanics science etc. are involved and expertise is generally required . in terms of the methodology of advanced time dependent fracture mechanics , this paper developed an expert system to realize an appropriate combination of material database , condition database and knowledge database . many assessment criteria including the multi defects interaction and combination , invalidation criterion and creep fatigue interaction are employed in the inference engine of expert system . the over conservativeness of life prediction from traditional method is reduced reasonably and therefore the accuracy of predicted life is improved . consequently , the intelligent and expert life management of cracked high temperature structures is realized which provides a powerful tool in practice . ( c ) <digit> elsevier ltd. all rights reserved .
packet mode scheduling in input queued cell based switches . <eos> we consider input queued switch architectures dealing at their interfaces with variable size packets , but internally operating on fixed size cells . packets are segmented into cells at input ports , transferred through the switching fabric , and reassembled at output ports . cell transfers are controlled by a scheduling algorithm , which operates in packet mode all cells belonging to the same packet are transferred from inputs to outputs without interruption . we prove that input queued switches using packet mode scheduling can achieve <digit> % throughput , and we show by simulation that , depending on the packet size distribution , packet mode scheduling may provide advantages over cell mode scheduling .
walkneta biologically inspired network to control six legged walking . <eos> to investigate walking we perform experimental studies on animals in parallel with software and hardware simulations of the control structures and the body to be controlled . therefore , the primary goal of our simulation studies is not so much to develop a technical device , but to develop a system which can be used as a scientific tool to study insect walking . to this end , the animat should copy essential properties of the animals . in this review , we will first describe the basic behavioral properties of hexapod walking , as the are known from stick insects . then we describe a simple neural network called walknet which exemplifies these properties and also shows some interesting emergent properties . the latter arise mainly from the use of the physical properties to simplify explicit calculations . the model is simple too , because it uses only static neuronal units . finally , we present some new behavioral results .
modelling the scatter of en curves using a serial hybrid neural network . <eos> if structural reliability is estimated by following a strain based approach , a materials strength should be represented by the scatter of the n ( en ) curves that link the strain amplitude with the corresponding statistical distribution of the number of cycles to failure . the basic shape of the n curve is usually modelled by the coffinmanson relationship . if a loading mean level also needs to be considered , the original coffinmanson relationship is modified to account for the non zero mean level of the loading , which can be achieved by using a smithwatsontopper modification of the original coffinmanson relationship . in this paper , a methodology for estimating the dependence of the statistical distribution of the number of cycles to failure on the smithwatsontopper modification is presented . the statistical distribution of the number of cycles to failure was modelled with a two parametric weibull probability density function . the core of the presented methodology is represented by a multilayer perceptron neural network combined with the weibull probability density function using a size parameter that follows the smithwatsontopper analytical model . the article presents the theoretical background of the methodology and its application in the case of experimental fatigue data . the results show that it is possible to model n curves and their scatter for different influential parameters , such as the specimens diameter and the testing temperature .
rate distortion problem for physics based distributed sensing . <eos> we consider the rate distortion problem for sensing the continuous space time physical temperature in a circular ring on which a heat source is applied over space and time , and which is also allowed to cool by radiation or convection to its surrounding medium . the heat source is modelled as a continuous space time stochastic process which is bandlimited over space and time . the temperature field is the result of a circular convolution over space and a continuous time causal filtering over time of the heat source with the green 's function corresponding to the heat equation , which is space and time invariant . the temperature field is sampled at uniform spatial locations by a set of sensors and it has to be reconstructed at a base station . the goal is to minimize the mean square error per second , for a given number of nats per second , assuming ideal communication channels between sensors and base station . we find a ) the centralized r c ( d ) function of the temperature field , where all the space time samples can be observed and encoded jointly . then , we obtain b ) the r s i ( d ) function , where each sensor , independently , encodes its samples optimally over time and c ) the r st i ( d ) function , where each sensor is constrained to encode also independently over time . we also study two distributed prediction based approaches a ) with perfect feedback from the base station , where temporal prediction is performed at the base station and each sensor performs differential encoding , and b ) without feedback , where each sensor locally performs temporal prediction .
developing a media space for remote synchronous parent child interaction . <eos> while supporting family communication has traditionally been a domain of interest for interaction designers , few research initiatives have explicitly investigated remote synchronous communication between children and parents . we discuss the design of the sharetable , a media space that supports synchronous interaction with children by augmenting videoconferencing with a camera projector system to allow for shared viewing of physical artifacts . we present an exploratory evaluation of this system , highlighting how such a media space may be used by families for learning and play activities . the sharetable was positively received by our participants and preferred over standard videoconferencing . informed by the results of our exploratory evaluation , we discuss the next design iteration of the sharetable and directions for future investigations in this area .
cross noise coupled architecture of complex bandpass delta sigma ad modulator . <eos> complex bandpass delta sigma ad modulators can provide superior performance to a pair of real bandpass delta sigma ad modulators of the same order . they process just input i and q signals , not image signals , and ad conversion can be realized with low power dissipation , so that they are desirable for such low if receiver applications . this paper proposes a new architecture for complex bandpass delta sigma ad modulators with cross noise coupled topology , which effectively raises the order of the complex modulator and achieves higher sqndr ( signal to quantization noise and distortion ratio ) with low power dissipation . by providing the cross coupled quantization noise injection to internal i and q paths , noise coupling between two quantizers can be realized in complex form , which enhances the order of noise shaping in complex domain , and provides a higher order ntf using a lower order loop filter in the complex delta sigma ad modulator . proposed higher order modulator can be realized just by adding some passive capacitors and switches , the additional integrator circuit composed of an operational amplifier is not necessary , and the performance of the complex modulator can be effectively raised without more power dissipation . we have performed simulation with matlab to verify the effectiveness of the proposed architecture . the simulation results show that the proposed architecture can achieve the realization of higher order enhancement , an improve sqndr of the complex bandpass delta sigma ad modulator .
facial motion cloning . <eos> we propose a method for automatically copying facial motion from one 3d face model to another , while preserving the compliance of the motion to the mpeg <digit> face and body animation ( fba ) standard . despite the enormous progress in the field of facial animation , producing a new animatable face from scratch is still a tremendous task for an artist . although many methods exist to animate a face automatically based on procedural methods , these methods still need to be initialized by defining facial regions or similar , and they lack flexibility because the artist can only obtain the facial motion that a particular algorithm offers . therefore a very common approach is interpolation between key facial expressions , usually called morph targets , containing either speech elements ( visemes ) or emotional expressions . following the same approach , the mpeg <digit> facial animation specification offers a method for interpolation of facial motion from key positions , called facial animation tables , which are essentially morph targets corresponding to all possible motions specified in mpeg <digit> . the problem of this approach is that the artist needs to create a new set of morph targets for each new face model . in case of mpeg <digit> there are <digit> morph targets , which is a lot of work to create manually . our method solves this problem by cloning the morph targets , i.e. by automatically copying the motion of vertices , as well as geometry transforms , from source face to target face while maintaining the regional correspondences and the correct scale of motion . it requires the user only to identify a subset of the mpeg <digit> feature points in the source and target faces . the scale of the movement is normalized with respect to mpeg <digit> normalization units ( fapus ) , meaning that the mpeg <digit> fba compliance of the copied motion is preserved . our method is therefore suitable not only for cloning of free facial expressions , but also of mpeg <digit> compatible facial motion , in particular the facial animation tables . we believe that facial motion cloning offers dramatic time saving to artists producing morph targets for facial animation or mpeg <digit> facial animation tables .
an analysis of the intel 80x86 security architecture and implementations . <eos> an in depth analysis of the 80x86 processor families identifies architectural properties that may have unexpected , and undesirable , results in secure computer systems . in addition , reported implementation errors in some processor versions render them undesirable for secure systems because of potential security and reliability problems . in this paper , we discuss the imbalance in scrutiny for hardware protection mechanisms relative to software , and why this imbalance is increasingly difficult to justify as hardware complexity increases . we illustrate this difficulty with examples of architectural subtleties and reported implementation errors .
realtime concatenation technique for skeletal motion in humanoid animation . <eos> in this paper , we propose a realtime concatenation technique between basic skeletal motions obtained ly the motion capture technique and etc. to generate a lifelike behavior for a humanoid character ( avatar ) . we execute several experiments to show the advantage and the property of our technique and also report the results . finally , we describe our applied system called wonderspace which leads participants to the exciting and attractive virtual worlds with humanoid characters in cyberspace . our concatenation technique has the following features ( <digit> ) based on a blending method between a preceding motion and a succeeding motion by a transition function , ( <digit> ) realizing smooth transition , monotone transition , and equivalent transition by the transition function called paste function , ( <digit> ) generating a connecting interval by making the backward and forward predictions for the preceding and succeeding motions , ( <digit> ) executing the prediction under the hypothesis of the smooth stopping state or the state of connecting motion , ( <digit> ) controlling the prediction intervals by the parameter indicating the importance of the motion , and ( <digit> ) realizing realtime calculation .
call by value is dual to call by name . <eos> the rules of classical logic may be formulated in pairs corresponding to de morgan duals rules about are dual to rules about v. a line of work , including that of filinski ( <digit> ) , griffin ( <digit> ) , parigot ( <digit> ) , danos , joinet , and schellinx ( <digit> ) , selinger ( 1998,2001 ) , and curien and herbelin ( <digit> ) , has led to the startling conclusion that call by value is the de morgan dual of call by name . this paper presents a dual calculus that corresponds to the classical sequent calculus of gentzen ( <digit> ) in the same way that the lambda calculus of church ( 1932,1940 ) corresponds to the intuitionistic natural deduction of gentzen ( <digit> ) . the paper includes crisp formulations of call by value and call by name that are obviously dual no similar formulations appear in the literature . the paper gives a cps translation and its inverse , and shows that the translation is both sound and complete , strengthening a result in curien and herbelin ( <digit> ) . note . this paper uses color to clarify the relation of types and terms , and of source and target calculi . if the url below is not in blue , please download the color version , which can be found in the acm digital library archive for icfp <digit> , at http portal.acm.org proceedings icfp archive , or by googling ' wadler dual ' .
universal automata and nfa learning . <eos> the aim of this paper is to develop a new algorithm that , with a complete sample as input , identifies the family of regular languages by means of nondeterministic finite automata . it is a state merging algorithm . one of its main features is that the convergence ( which is proved ) is achieved independently from the order in which the states are merged , that is , the merging of states may be done randomly . ( c ) <digit> elsevier b.v. all rights reserved .
effect of load models on assessment of energy losses in distributed generation planning . <eos> distributed generation ( dg ) is gaining in significance due to the keen public awareness of the environmental impacts of electric power generation and significant advances in several generation technologies which are much more environmentally friendly ( wind power generation , micro turbines , fuel cells , and photovoltaic ) than conventional coal , oil and gas fired plants . accurate assessment of energy losses when dg is connected is gaining in significance due to the developments in the electricity market place , such as increasing competition , real time pricing and spot pricing . however , inappropriate modelling can give rise to misleading results . this paper presents an investigation into the effect of load models on the predicted energy losses in dg planning . following a brief introduction the paper proposes a detailed voltage dependent load model , for dg planning use , which considers three categories of loads residential , industrial and commercial . the paper proposes a methodology to study the effect of load models on the assessment of energy losses based on time series simulations to take into account both the variations of renewable generation and load demand . a comparative study of energy losses between the use of a traditional constant load model and the voltage dependent load model and at various load levels is carried out using a <digit> node example power system . simulations presented in the paper indicate that the load model to be adopted can significantly affect the results of dg planning .
generational stack collection and profile driven pretenuring . <eos> this paper presents two techniques for improving garbage collection performance generational stack collection and profile driven pretenuring . the first is applicable to stack based implementations of functional languages while the second is useful for any generational collector . we have implemented both techniques in a generational collector used by the til compiler ( tarditi , morrisett , cheng , stone , harper , and lee <digit> ) , and have observed decreases in garbage collection times of as much as <digit> % and <digit> % , respectively.functional languages encourage the use of recursion which can lead to a long chain of activation records . when a collection occurs , these activation records must be scanned for roots . we show that scanning many activation records can take so long as to become the dominant cost of garbage collection . however , most deep stacks unwind very infrequently , so most of the root information obtained from the stack remains unchanged across successive garbage collections . generational stack collection greatly reduces the stack scan cost by reusing information from previous scans.generational techniques have been successful in reducing the cost of garbage collection ( ungar <digit> ) . various complex heap arrangements and tenuring policies have been proposed to increase the effectiveness of generational techniques by reducing the cost and frequency of scanning and copying . in contrast , we show that by using profile information to make lifetime predictions , pretenuring can avoid copying data altogether . in essence , this technique uses a refinement of the generational hypothesis ( most data die young ) with a locality principle concerning the age of data most allocations sites produce data that immediately dies , while a few allocation sites consistently produce data that survives many collections .
regulated secretion in chromaffin cells . <eos> arfs constitute a family of structurally related proteins that forms a subset of the ras gtpases . in chromaffin cells , secretagogue evoked stimulation triggers the rapid translocation of arf6 from secretory granules to the plasma membrane and the concomitant activation of pld in the plasma membrane . both pld activation and catecholamine secretion are strongly inhibited by a synthetic peptide corresponding to the n terminal domain of arf6 . arno , a potential guanine nucleotide exchange factor for arf6 , is expressed and localized in the plasma membrane of chromaffin cells . using permeabilized cells , we found that the introduction of anti arno antibodies into the cytosol inhibits both pld activation and catecholamine secretion . chromaffin cells express pld1 at the plasma membrane . we found that microinjection of the catalytically inactive pld1 ( k898r ) dramatically reduces catecholamine secretion monitored by amperometry , most likely by interfering with a late postdocking step of calcium regulated exocytosis . we propose that arno arf6 participate in the exocytotic reaction by controlling the plasma membrane bound pld1 . by generating fusogenic lipids at the exocytotic sites , pld1 may represent an essential component of the fusion machinery in neuroendocrine cells .
analysis of elastic wave propagation in a functionally graded thick hollow cylinder using a hybrid mesh free method . <eos> in this paper , a hybrid mesh free method based on generalized finite difference ( gfd ) and newmark finite difference ( nfd ) methods is presented to calculate the velocity of elastic wave propagation in functionally graded materials ( fgms ) . the physical domain to be considered is a thick hollow cylinder made of functionally graded material in which mechanical properties are graded in the radial direction only . a power law variation of the volume fractions of the two constituents is assumed for mechanical property variation . the cylinder is excited by shock loading to obtain the time history of the radial displacement . the velocity of elastic wave propagation in functionally graded cylinder is calculated from periodic behavior of the radial displacement in time domain . the effects of various grading patterns and various constitutive mechanical properties on the velocity of elastic wave propagation in functionally graded cylinders are studied in detail . numerical results demonstrate the efficiency of the proposed method in simulating the wave propagation in fgms .
a nonparametric methodology for evaluating convergence in a multi input multi output setting . <eos> the paper presents a novel nonparametric methodology to evaluate convergence . we develop two new indexes to evaluate convergence and convergence . the indexes developed allow evaluations using multiple inputs and outputs . the methodology complements productivity assessments based on the malmquist index . the methodology is applied to portuguese construction companies operating in <digit> .
the effects of interaction frequency on the optimization performance of cooperative coevolution . <eos> cooperative coevolution is often used to solve difficult optimization problems by means of problem decomposition . its performance on this task is influenced by many design decisions . it would be useful to have some knowledge of the performance effects of these decisions , in order to make the more beneficial ones . in this paper we study the effects on performance of the frequency of interaction between populations . we show them to be problem dependent and use dynamics analysis to explain this dependency .
brain inspired method for solving fuzzy multi criteria decision making problems ( bifmcdm ) . <eos> we propose a brain inspired method for solving fuzzy decision making problems . we study a websites ranking problem for an e alliance . processing fuzzy information as just abstract element could lead to wrong decision .
environmental model access and interoperability the geo model web initiative . <eos> the group on earth observation ( geo ) model web initiative utilizes a model as a service approach to increase model access and sharing . it relies on gradual , organic growth leading towards dynamic webs of interacting models , analogous to the world wide web . the long term vision is for a consultative infrastructure that can help address what if and other questions that decision makers and other users have . four basic principles underlie the model web open access , minimal barriers to entry , service driven , and scalability any implementation approach meeting these principles will be a step towards the long term vision . implementing a model web encounters a number of technical challenges , including information modelling , minimizing interoperability agreements , performance , and long term access , each of which has its own implications . for example , a clear information model is essential for accommodating the different resources published in the model web ( model engines , model services , etc. ) , and a flexible architecture , capable of integrating different existing distributed computing infrastructures , is required to address the performance requirements . architectural solutions , in keeping with the model web principles , exist for each of these technical challenges . there are also a variety of other key challenges , including difficulties in making models interoperable calibration and validation and social , cultural , and institutional constraints . although the long term vision of a consultative infrastructure is clearly an ambitious goal , even small steps towards that vision provide immediate benefits . a variety of activities are now in progress that are beginning to take those steps . ( c ) <digit> elsevier ltd. all rights reserved .
non uniform micro channel design for stacked 3d ics . <eos> micro channel cooling shows great potential in removing high density heat in 3d circuits . the current micro channel heat sink designs spread the entire surface to be cooled with micro channels . this approach , though might provide sufficient cooling , requires quite high pumping power . in this paper , we investigate the non uniform allocation of micro channels to provide sufficient cooling with less pumping power . specifically , we decide the count , location and pumping pressure drop flow rate of micro channels such that acceptable cooling is achieved at minimum pumping power . thermal wake effect and runtime pressure drop flow rate control are also considered . the experiments showed that , compared with the conventional design which spreads micro channels all over the chip , our non uniform microchannel design achieves <digit> <digit> % pumping power saving .
neurocomputing techniques to dynamically forecast spatiotemporal air pollution data . <eos> real time monitoring , forecasting and modeling air pollutants concentrations in major urban centers is one of the top priorities of all local and national authorities globally . this paper studies and analyzes the parameters related to the problem , aiming in the design and development of an effective machine learning model and its corresponding system , capable of forecasting dangerous levels of ozone ( o3 ) concentrations in the city center of athens and more specifically in the athinas air quality monitoring station . this is a multi parametric case , so an effort has been made to combine a vast number of data vectors from several operational nearby measurements stations . the final result was the design and construction of a group of artificial neural networks capable of estimating o3 concentrations in real time mode and also having the capacity of forecasting the same values for future time intervals of <digit> , <digit> , <digit> and 6h , respectively .
revisiting rational bubbles in the g <digit> stock markets using the fourier unit root test and the nonparametric rank test for cointegration . <eos> this paper re investigates whether rational bubbles existed in the g <digit> stock markets during the period of january <digit> june <digit> using the newly developed fourier unit root test and a nonparametric rank test for cointegration . the empirical results from our fourier unit test indicate that the null hypothesis of j ( <digit> ) unit root in stock prices can be rejected for canada , france , italy and the uk . however , the empirical results from the rank test reveal that rational bubbles did not exist in the g <digit> stock markets during the sample period . ( c ) <digit> imacs . published by elsevier b.v. all rights reserved .
orchestrating stream graphs using model checking . <eos> in this article we use model checking to statically distribute and schedule synchronous dataflow ( sdf ) graphs on heterogeneous execution architectures . we show that model checking is capable of providing an optimal solution and it arrives at these solutions faster ( in terms of algorithm runtime ) than equivalent ilp formulations . furthermore , we also show how different types of optimizations such as task parallelism , data parallelism , and state sharing can be included within our framework . finally , comparison of our approach with the current state of the art heuristic techniques show the pitfalls of these techniques and gives a glimpse of how these heuristic techniques can be improved .
model averaged wald confidence intervals . <eos> the process of model averaging has become increasingly popular as a method for performing inference in the presence of model uncertainty . in the frequentist setting , a model averaged estimate of a parameter is calculated as the weighted sum of single model estimates , often using weights derived from an information criterion such as aic or bic . a standard method for calculating a model averaged confidence interval is to use a wald interval centered around the model averaged estimate . we propose a new method for construction of a model averaged wald confidence interval , based on the idea of model averaging tail areas of the sampling distributions of the single model estimates . we use simulation to compare the performance of the new method and existing methods , in terms of coverage rate and interval width . the new method consistently outperforms existing methods in terms of coverage , often for little increase in the interval width . we also consider choice of model weights , and find that aic weights are preferable to either aicc or bic weights in terms of coverage .
dynamics of the difference equation x n <digit> x n p x n k x n q . <eos> we study the invariant interval , the character of semicycles , the global stability , and the boundedness of the difference equation
cross validation based single response adaptive design of experiments for kriging metamodeling of deterministic computer simulations . <eos> a new approach for single response adaptive design of deterministic computer experiments is presented . the approach is called sfcvt , for space filling cross validation tradeoff . sfcvt uses metamodeling to obtain an estimate of cross validation errors , which are maximized subject to a constraint on space filling to determine sample points in the design space . the proposed method is compared , using a test suite of forty four numerical examples , with three doe methods from the literature . the numerical test examples can be classified into symmetric and asymmetric functions . symmetric examples refer to functions for which the extreme points are located symmetrically in the design space and asymmetric examples are those for which the extreme regions are not located in a symmetric fashion in the design space . based upon the comparison results for the numerical examples , it is shown that sfcvt performs better than an existing adaptive and a non adaptive doe method for asymmetric multimodal functions with high nonlinearity near the boundary , and is comparable for symmetric multimodal functions and other test problems . the proposed approach is integrated with a multi scale heat exchanger optimization tool to reduce the computational effort involved in the design of novel air to water heat exchangers . the resulting designs are shown to be significantly more compact than mainstream heat exchanger designs .
bottlenecks and hubs in inferred networks are important for virulence in salmonella typhimurium . <eos> recent advances in experimental methods have provided sufficient data to consider systems as large networks of interconnected components . high throughput determination of protein protein interaction networks has led to the observation that topological bottlenecks , proteins defined by high centrality in the network , are enriched in proteins with systems level phenotypes such as essentiality . global transcriptional profiling by microarray analysis has been used extensively to characterize systems , for example , examining cellular response to environmental conditions and effects of genetic mutations . these transcriptomic datasets have been used to infer regulatory and functional relationship networks based on co regulation . we use the context likelihood of relatedness ( clr ) method to infer networks from two datasets gathered from the pathogen salmonella typhimurium one under a range of environmental culture conditions and the other from deletions of <digit> regulators found to be essential in virulence . bottleneck and hub genes were identified from these inferred networks , and we show for the first time that these genes are significantly more likely to be essential for virulence than their non bottleneck or non hub counterparts . networks generated using simple similarity metrics ( correlation and mutual information ) did not display this behavior . overall , this study demonstrates that topology of networks inferred from global transcriptional profiles provides information about the systems level roles of bottleneck genes . analysis of the differences between the two clr derived networks suggests that the bottleneck nodes are either mediators of transitions between system states or sentinels that reflect the dynamics of these transitions .
semantic manipulation of users queries and modeling the health and nutrition preferences . <eos> people depend on popular search engines to look for the desired health and nutrition information . many search engines can not semantically interpret , enrich the users natural language queries easily and hence do not retrieve the personalized information that fits the users needs . one reason for retrieving irrelevant information is the fact that people have different preferences where each one likes and dislikes certain types of food . in addition , some people have specific health conditions that restrict their food choices and encourage them to take other foods . moreover , the cultures , where people live in , influence food choices while the search engines are not aware of these cultural habits . therefore , it will be helpful to develop a system that semantically manipulates users queries and models the users preferences to retrieve personalized health and food information . in this paper , we harness semantic web technology to capture users preferences , construct a nutritional and health oriented users profile , model the users preferences and use them to organize the related knowledge so that users can retrieve personalized health and food information . we present an approach that uses the personalization techniques based on integrated domain ontologies , pre constructed by domain experts , to retrieve relevant food and health information that is consistent with peoples needs . we implemented the system , and the empirical results show high precision and recall with a superior users satisfaction .
swift and stable polygon growth and broken line offset . <eos> the problem of object growing ( offsetting the object boundary by a certain distance ) is an important and widely studied problem . in this paper we propose a new approach for offsetting the boundary of an object described by segments which are not necessarily connected . this approach avoids many destructive special cases that arise in some heuristic based approaches . moreover , the method developed in this paper is stable in that it does not fail because of missing segments . also , the time required for the computation of the offset is relatively short and therefore inexpensive , i.e. it is expected to be o ( n log n ) .
ilp based multistage placement of pmus with dynamic monitoring constraints . <eos> a multistage planning of pmus placement for power systems is proposed . the methodology takes into account network expansion plans . system observability is maximized over time . the methodology identifies nodes to locate pmus based on security criteria . the stepwise approach allows the utilities to develop a path for pmu placement .
indoor solar energy harvesting for sensor network router nodes . <eos> a unique method has been developed to scavenge energy from monocrystaline solar cells to power wireless router nodes used in indoor applications . the systems energy harvesting module consists of solar cells connected in series parallel combination to scavenge energy from 34w fluorescent lights . a set of ultracapacitors were used as the energy storage device . two router nodes were used as a router pair at each route point to minimize power consumption . test results show that the harvesting circuit which acted as a plug in to the router nodes manages energy harvesting and storage , and enables near perpetual , harvesting aware operation of the router node .
detecting data records in semi structured web sites based on text token clustering . <eos> this paper describes a new approach to the use of clustering for automatic data detection in semi structured web pages . unlike most exiting web information extraction approaches that usually apply wrapper induction techniques to manually labelled web pages , this approach avoids the pattern induction process by using clustering techniques on unlabelled pages . in this approach , a variant hierarchical agglomerative clustering ( hac ) algorithm called k neighbours hac is developed which uses the similarities of the data format ( html tags ) and the data content ( text string values ) to group similar text tokens into clusters . we also develop a new method to label text tokens to capture the hierarchical structure of html pages and an algorithm for mapping labelled text tokens to xml . the new approach is tested and compared with several common existing wrapper induction systems on three different sets of web pages . the results suggest that the new approach is effective for data record detection and that it outperforms these common existing approaches examined on these web sites . compared with the existing approaches , the new approach does not require training and successfully avoids the explicit pattern induction process , and accordingly the entire data detection process is simpler .
the design of gsc fieldlog ontology based software for computer aided geological field mapping . <eos> databases containing geological field information are increasingly being constructed directly in the field . the design of such databases is often challenged by opposing needs ( <digit> ) the individual need to maintain flexibility of database structure and contents , to accommodate unexpected field situations and ( <digit> ) the corporate need to retain compatibility between distinct field databases , to accommodate their interoperability . the fieldlog mapping software balances these needs by exploiting a domain ontology developed for field information , one that enables field database flexibility and facilitates compatibility . the ontology consists of cartographic , geospatial , geological and metadata objects that form a common basis for interoperability and that can be instantiated by users into customized field databases . the design of the fieldlog software , its foundation on this ontology , and the resulting benefits to usability are presented in this paper . the discussion concentrates on satisfying the flexibility requirement by implementing the ontology as a generic data model within an object relational database environment issues of interoperability are not considered in detail . benefits of this ontologic driven approach are also developed within a description of the fieldlog application , including ( <digit> ) improved usability due to an user interface based on the geological components of the ontology , and ( <digit> ) diminished technical prerequisites as users are shielded from the many database and gis technicalities handled by the ontology .
systemic disease sequelae in chronic inflammatory diseases and chronic psychological stress comparison and pathophysiological model . <eos> in chronic inflammatory diseases ( cids ) , the neuroendocrineimmune crosstalk is important to allocate energy rich substrates to the activated immune system . since the immune system can request energy rich substrates independent of the rest of the body , i refer to it as the selfish immune system , an expression that was taken from the theory of the selfish brain , giving the brain a similar position . in cids , the theory predicts the appearance of long term disease sequelae , such as metabolic syndrome . since long standing energy requirements of the immune system determine disease sequelae , the question arose as to whether chronic psychological stress due to chronic activation of the brain causes similar sequelae . indeed , there are many similarities however , there are also differences . a major difference is the behavior of body weight ( constant in cids versus loss or gain in stress ) . to explain this discrepancy , a new pathophysiological theory is presented that places inflammation and stress axes in the middle .
setting parameters by example . <eos> we introduce a class of inverse parametric optimization problems , in which one is given both a parametric optimization problem and a desired optimal solution the task is to determine parameter values that lead to the given solution . we describe algorithms for solving such problems for minimum spanning trees , shortest paths , and other optimal subgraph problems and discuss applications in multicast routing , vehicle path planning , resource allocation , and board game programming .
pedvis a structured , space efficient technique for pedigree visualization . <eos> public genealogical databases are becoming increasingly populated with historical data and records of the current population 's ancestors . as this increasing amount of available information is used to link individuals to their ancestors , the resulting trees become deeper and more dense , which justifies the need for using organized , space efficient layouts to display the data . existing layouts are often only able to show a small subset of the data at a time . as a result , it is easy to become lost when navigating through the data or to lose sight of the overall tree structure . on the contrary , leaving space for unknown ancestors allows one to better understand the tree 's structure , but leaving this space becomes expensive and allows fewer generations to be displayed at a time . in this work , we propose that the h tree based layout be used in genealogical software to display ancestral trees . we will show that this layout presents an increase in the number of displayable generations , provides a nicely arranged , symmetrical , intuitive and organized fractal structure , increases the user 's ability to understand and navigate through the data , and accounts for the visualization requirements necessary for displaying such trees . finally , user study results indicate potential for user acceptance of the new layout .
the evolution of mobile communications in europe the transition from the second to the third generation . <eos> this paper analyses the evolution of the mobile communications industry in the european union . the research focuses its interest on the different roles played by the regulator in europe and in other regions of the world ( mainly the us ) . the diffusion of gsm was extraordinarily fast in europe , mainly due to the adoption of a unified standard from inception . this rapid diffusion has resulted in an important competitive advantage for european operators . interestingly , while the regulator acted similarly in the case of umts , the development of the latter has faced many problems and , presently , its diffusion is still low ( about <digit> % in the eu ) . the paper also offers basic information on market structure that may be useful for extracting some preliminary conclusions about the degree of rivalry within the industry and the differences that can be observed between european countries .
a decision support system for design of transmission system of low power tractors . <eos> a decision support system ( dss ) was developed in visual basic 6.0 programming language to design transmission system of low horsepower agricultural tractors , which involved the design of clutch and gearbox . the dss provided graphical user interface by linking databases to support decision on design of transmission system for low horsepower tractors on the basis of modified asabe draft model . the developed program for design of tractor transmission system calculated clutch size , gear ratios , number of teeth on each gear , and various gear design parameters . related deviation was computed for design of transmission system of tractors based on measured and predicted values ( simulated ) . the related deviation was less than <digit> % for design of clutch plate outer diameter and less than <digit> % for inner diameter . there was less than <digit> % variation between the predicted results by the developed dss and those obtained from actual measurement for design of gear ratio . the dss program was user friendly and efficient for predicting the design of transmission system for different tractor models to meet requirements of research institutions and industry . <digit> wiley periodicals , inc. comput . appl . eng . educ . comput appl eng educ 23 760770 , <digit> view this article online at wileyonlinelibrary.com journal cae doi 10.1002 cae .21648
distributed automation pabadis versus hms . <eos> distributed control systems ( dcs ) have gained huge interest in the automation business . several approaches have been made which aim at the design and application of dcs to improve system flexibility and robustness . important approaches are ( among others ) the holonic manufacturing systems ( hms ) and the plant automation based on distributed systems ( pabadis ) approach . pabadis deals with plant automation systems in a distributed way using generic mobile and stationary agents and plug and participate facilities within a flat structure as key points of the developed control architecture . hms deals with a similar structure , but aims more at a control hierarchy of special agents . this paper gives a description of the pabadis project and makes comparisons between the two concepts , showing advantages and disadvantages of both systems . based on this paper , it will be possible to observe the abilities and drawbacks of distributed agent based control systems .
a new supervised learning algorithm for multiple spiking neural networks with application in epilepsy and seizure detection . <eos> a new multi spiking neural network ( muspinn ) model is presented in which information from one neuron is transmitted to the next in the form of multiple spikes via multiple synapses . a new supervised learning algorithm , dubbed multi spikeprop , is developed for training muspinn . the model and learning algorithm employ the heuristic rules and optimum parameter values presented by the authors in a recent paper that improved the efficiency of the original single spiking spiking neural network ( snn ) model by two orders of magnitude . the classification accuracies of muspinn and multi spikeprop are evaluated using three increasingly more complicated problems the xor problem , the fisher iris classification problem , and the epilepsy and seizure detection ( eeg classification ) problem . it is observed that muspinn learns the xor problem in twice the number of epochs compared with the single spiking snn model but requires only one fourth the number of synapses . for the iris and eeg classification problems , a modular architecture is employed to reduce each <digit> class classification problem to three <digit> class classification problems and improve the classification accuracy . for the complicated eeg classification problem a classification accuracy in the range of 90.7 .8 % was achieved , which is significantly higher than the <digit> % classification accuracy obtained using the single spiking snn with spikeprop .
investigations about replication of empirical studies in software engineering a systematic mapping study . <eos> two recent mapping studies which were intended to verify the current state of replication of empirical studies in software engineering ( se ) identified two sets of studies empirical studies actually reporting replications ( published between <digit> and <digit> ) and a second group of studies that are concerned with definitions , classifications , processes , guidelines , and other research topics or themes about replication work in empirical software engineering research ( published between <digit> and <digit> ) . in this current article , our goal is to analyze and discuss the contents of the second set of studies about replications to increase our understanding of the current state of the work on replication in empirical software engineering research . we applied the systematic literature review method to build a systematic mapping study , in which the primary studies were collected by two previous mapping studies covering the period <digit> complemented by manual and automatic search procedures that collected articles published in <digit> . we analyzed <digit> papers reporting studies about replication published in the last <digit> years . these papers explore different topics related to concepts and classifications , presented guidelines , and discuss theoretical issues that are relevant for our understanding of replication in our field . we also investigated how these <digit> papers have been cited in the <digit> replication papers published between <digit> and <digit> . replication in se still lacks a set of standardized concepts and terminology , which has a negative impact on the replication work in our field . to improve this situation , it is important that the se research community engage on an effort to create and evaluate taxonomy , frameworks , guidelines , and methodologies to fully support the development of replications .
worst case analysis of memory allocation algorithms . <eos> various memory allocation problems can be modeled by the following abstract problem . given a list a equil ( agr <digit> , agr <digit> , ... agr n , ) of real numbers in the range ( <digit> , <digit> , place these in a minimum number of bins so that no bin holds numbers summing to more than <digit> . we let a be the smallest number of bins into which the numbers of list a may be placed . since a general placement algorithm for attaining a appears to be impractical , it is important to determine good heuristic methods for assigning numbers of bins . we consider four such simple methods and analyze the worst case performance of each , closely bounding the maximum of the ratio of the number of bins used by each method applied to list a to the optimal quantity a .
building a financial diagnosis system based on fuzzy logic production system . <eos> the purpose of this study is to build a financial expert system based on fuzzy theory and fuzzy logic production system ( flops ) , which is an expert tool for processing the ambiguity . the study consists if four parts . for the first part , the basic features of expert systems are presented . for the second part , fuzzy concepts and the evaluation of classical expert systems to fuzzy expert systems will be presented . for the third part , the expert system shell ( flops ) used in this study will be described . for the last part , it will be presented the financial diagnosis system , developed by using the wall 's seven ratios , traditional seven ratios and also <digit> ratios selected by a financial expert . alter analyzing and investigating these three kinds of methods , financial diagnosis system will be developed as a fuzzy expert system which used a membership function bared on averages and standard deviation . at the last step , the new approach will be tried by increasing the fuzzy sets far five membership functions . some practical examples will be given . throughout the paper , the way of budding i financial diagnosis system based on fuzzy expert system is stressed .
a hybrid boundary element wave based method for an efficient solution of bounded acoustic problems with inclusions . <eos> this paper presents a novel hybrid approach for the efficient solution of bounded acoustic problems with arbitrarily shaped inclusions . the hybrid method couples the wave based method ( wbm ) and the boundary element method ( bem ) in order to benefit from the prominent advantages of both . the wbm is based on an indirect trefftz approach as such , it uses exact solutions of the governing equations to approximate the field variables . it has a high computational advantage as compared to conventional element based methods , when applied on moderately complex geometries . the bem , on the other hand , can tackle complex geometries with ease . however , it can be computationally expensive . the hybrid boundary element wave based method ( be wbm ) combines the best properties of the two it makes use of the efficient wbm for the moderately complex bounded domains and utilizes the flexibility of the bem for the complex objects that reside in the bounded domains . the accuracy and the efficiency of the method is demonstrated with three numerical examples , where the hybrid be wbm is shown to be more efficient than a quadratic finite element method ( fem ) . while the hybrid method provides efficient solution for the bounded problems with inclusions , it also brings certain conceptual advantages over the fem . the fact that it is a boundary type method with an easy refinement concept reduces the modeling effort on the preprocessing step . moreover , for certain optimization scenarios such as optimization of the position of inclusions , the fem becomes disadvantageous because of its domain discretization requirements for each iteration . on the other hand , the hybrid method allows reusing of the fixed geometries and only needs recalculation of the coupling matrices without a further need of preprocessing . as such , the hybrid method combines efficiency with versatility .
a fuzzy clustering based binary threshold bispectrum estimation approach . <eos> a fuzzy clustering bispectrum estimation approach is proposed in this paper and applied on the rolling element bearing fault recognition . the method combines the basic higher order spectrum theory and fuzzy clustering technique in data mining . at first , all the bispectrum estimation results of the training samples and test samples are taken binarization threshold processing and turned into binary feature images . then , the binary feature images of the training samples are used to construct object templates including kernel images and domain images . every fault category has one object templates . at last , by calculating the distances between test samples binary feature images and the different object templates , the object classification and pattern recognition can be effectively accomplished . bearing is the most important and much easier to be damaged component in rotating machinery . furthermore , there exist large amounts of noise jamming and nonlinear coupling components in bearing vibration signals . higher order cumulants , which can quantitatively describe the nonlinear characteristic signals with close relationship between the mechanical faults , are introduced in this paper to de noise the raw bearing vibration signals and obtain the bispectrum estimation pictures . at last , the rolling bearing fault diagnosis experiment results showed that the classification was completely correct .
scalability of write ahead logging on multicore and multisocket hardware . <eos> the shift to multi core and multi socket hardware brings new challenges to database systems , as the software parallelism determines performance . even though database systems traditionally accommodate simultaneous requests , a multitude of synchronization barriers serialize execution . write ahead logging is a fundamental , omnipresent component in aries style concurrency and recovery , and one of the most important yet to be addressed potential bottlenecks , especially in oltp workloads making frequent small changes to data . in this paper , we identify four logging related impediments to database system scalability . each issue challenges different level in the software architecture ( a ) the high volume of small sized i o requests may saturate the disk , ( b ) transactions hold locks while waiting for the log flush , ( c ) extensive context switching overwhelms the os scheduler with threads executing log i os , and ( d ) contention appears as transactions serialize accesses to in memory log data structures . we demonstrate these problems and address them with techniques that , when combined , comprise a holistic , scalable approach to logging . our solution achieves a <digit> <digit> % speedup over a modern database system when running log intensive workloads , such as the tpc b and tatp benchmarks , in a single socket multiprocessor server . moreover , it achieves log insert throughput over 2.2 gb s for small log records on the single socket server , roughly <digit> times higher than the traditional way of accessing the log using a single mutex . furthermore , we investigate techniques on scaling the performance of logging to multi socket servers . we present a set of optimizations which partly ameliorate the latency penalty that comes with multi socket hardware , and then we investigate the feasibility of applying a distributed log buffer design at the socket level .
time driven priority router implementation analysis and experiments . <eos> low complexity solutions to provide deterministic quality over packet switched networks while achieving high resource utilization have been an open research issue for many years . service differentiation combined with resource overprovisioning has been considered an acceptable compromise and widely deployed given that the amount of traffic requiring quality guarantees has been limited . this approach is not viable , though , as new bandwidth hungry applications , such as video on demand , telepresence , and virtual reality , populate networks invalidating the rationale that made it acceptable so far . time driven priority represents a potentially interesting solution . however , the fact that the network operation is based on a time reference shared by all nodes raises concerns on the complexity of the nodes , from the point of view of both their hardware and software architecture . this work analyzes the implications that the timing requirements of time driven priority have on network nodes and shows how proper operation can be ensured even when system components introduce timing uncertainties . experimental results on a time driven priority router implementation based on a personal computer both validate the analysis and demonstrate the feasibility of the technology even on an architecture that is not designed for operating under timing constraints .
computationally sound symbolic security reduction analysis of the group key exchange protocols using bilinear pairings . <eos> the security of the group key exchange protocols has been widely studied in the cryptographic community in recent years . current work usually applies either the computational approach or the symbolic approach for security analysis . the symbolic approach is more efficient than the computational approach , because it can be easily automated . however , compared with the computational approach , it has to overcome three challenges ( <digit> ) the computational soundness is unclear ( <digit> ) the number of participants must be fixed and ( <digit> ) the advantage of efficiency disappears , if the number of participants is large . this paper proposes a computationally sound symbolic security reduction approach to resolve these three issues . on one hand , combined with the properties of the bilinear pairings , the universally composable symbolic analysis ( ucsa ) approach is extended from the two party protocols to the group key exchange protocols . meanwhile , the computational soundness of the symbolic approach is guaranteed . on the other hand , for the group key exchange protocols which satisfy the syntax of the simple protocols proposed in this paper , the security is proved to be unrelated with the number of participants . as a result , the symbolic approach just needs to deal with the protocols among three participants . this makes the symbolic approach has the ability to handle arbitrary number of participants . therefore , the advantage of efficiency is still guaranteed . the proposed approach can also be applied to other types of cryptographic primitives besides bilinear pairing for computationally sound and efficient symbolic analysis of group key exchange protocols .
dynamic gradient method for pebs detection in power system transient stability assessment . <eos> in methods for assessing the critical clearing time based on the transient energy function , the dominant procedures in use for detecting the exit point across the potential energy boundary surface ( pebs ) are the ray and the gradient methods . because both methods rely on the geometrical characteristics of the post fault potential energy surface , they may yield erroneous results . in this paper , a more reliable method for pebs detection is proposed . it is called the dynamic gradient method to indicate that from a given system state , a small portion of the trajectory of the gradient system is approximated and tested for convergence toward the post fault stable equilibrium point . it is shown that a trade off between computing time and reliability can be found as the number of machines in the system becomes greater . the method is illustrated on <digit> machine and <digit> machine systems .
exact matrix completion via convex optimization . <eos> we consider a problem of considerable practical interest the recovery of a data matrix from a sampling of its entries . suppose that we observe m entries selected uniformly at random from a matrix m. can we complete the matrix and recover the entries that we have not seen we show that one can perfectly recover most low rank matrices from what appears to be an incomplete set of entries . we prove that if the number m of sampled entries obeys m > cn ( 1.2 ) r log n for some positive numerical constant c , then with very high probability , most n x n matrices of rank r can be perfectly recovered by solving a simple convex optimization program . this program finds the matrix with minimum nuclear norm that fits the data . the condition above assumes that the rank is not too large . however , if one replaces the 1.2 exponent with 1.25 , then the result holds for all values of the rank . similar results hold for arbitrary rectangular matrices as well . our results are connected with the recent literature on compressed sensing , and show that objects other than signals and images can be perfectly reconstructed from very limited information .
a general framework for expressing preferences in causal reasoning and planning . <eos> we consider the problem of representing arbitrary preferences in causal reasoning and planning systems . in planning , a preference may be seen as a goal or constraint that is desirable , but not necessary , to satisfy . to begin , we define a very general query language for histories , or interleaved sequences of world states and actions . based on this , we specify a second language in which preferences are defined . a single preference defines a binary relation on histories , indicating that one history is preferred to the other . from this , one can define global preference orderings on the set of histories , the maximal elements of which are the preferred histories . the approach is very general and flexible thus it constitutes a base language in terms of which higher level preferences may be defined . to this end , we investigate two fundamental types of preferences that we call choice and temporal preferences . we consider concrete strategies for these types of preferences and encode them in terms of our framework . we suggest how to express aggregates in the approach , allowing , e.g. the expression of a preference for histories with lowest total action costs . last , our approach can be used to express other approaches and so serves as a common framework in which such approaches can be expressed and compared . we illustrate this by indicating how an approach due to son and pontelli can be encoded in our approach , as well as the language pddl3 .
a cost sensitive decision tree algorithm with two adaptive mechanisms . <eos> an adaptive selecting cut point mechanism is designed to build a classifier . adaptive removing attribute mechanism will remove the redundant attributes . we adopt two mechanisms to design algorithm which for classifier construction . experimental results show the effectiveness and feasibility of our algorithm .
solving the buckley leverett equation with gravity in a heterogeneous porous medium . <eos> immiscible two phase flow in porous media can be described by the fractional flow model . if capillary forces are neglected , then the saturation equation is a non linear hyperbolic conservation law , known as the buckley leverett equation . this equation can be numerically solved by the method of godunov , in which the saturation is computed from the solution of riemann problems at cell interfaces . at a discontinuity of permeability this solution has to be constructed from two flux functions . in order to determine a unique solution an entropy inequality is needed . in this article an entropy inequality is derived from a regularisation procedure , where the physical capillary pressure term is added to the buckley leverett equation . this entropy inequality determines unique solutions of riemann problems for all initial conditions . it leads to a simple recipe for the computation of interface fluxes for the method of godunov .
an exploratory study of enterprise resource planning adoption in greek companies . <eos> purpose to examine enterprise resource planning ( erp ) adoption in greek companies , and explore the effects of uncertainty on the performance of these systems and the methods used to cope with uncertainty . design methodology approach this research was exploratory and six case studies were generated . this work was part of a larger project on the adoption , implementation and integration of erp systems in greek enterprises . a taxonomy of erp adoption research was developed from the literature review and used to underpin the issues investigated in these cases . the results were compared with the literature on erp adoption in the usa and uk . findings there were major differences between erp adoption in greek companies and companies in other countries . the adoption , implementation and integration of erp systems were fragmented in greek companies . this fragmentation demonstrated that the internal enterprise 's culture , resources available , skills of employees , and the way erp systems are perceived , treated and integrated within the business and in the supply chain , play critical roles in determining the success failure of erp systems adoption . a warehouse management system was adopted by some greek enterprises to cope with uncertainty . research limitations implications a comparison of erp adoption was made between the usa , uk and greece , and may limit its usefulness elsewhere . practical implications practical advice is offered to managers contemplating adopting erp . originality value a new taxonomy of erp adoption research was developed , which refocused the erp implementation and integration into related critical success failure factors and total integration issues , thus providing a more holistic erp adoption framework .
classification of newborn eeg maturity with bayesian averaging over decision trees . <eos> eeg experts can assess a newborns brain maturity by visual analysis of age related patterns in sleep eeg . it is highly desirable to make the results of assessment most accurate and reliable . however , the expert analysis is limited in capability to provide the estimate of uncertainty in assessments . bayesian inference has been shown providing the most accurate estimates of uncertainty by using markov chain monte carlo ( mcmc ) integration over the posterior distribution . the use of mcmc enables to approximate the desired distribution by sampling the areas of interests in which the density of distribution is high . in practice , the posterior distribution can be multimodal , and so that the existing mcmc techniques can not provide the proportional sampling from the areas of interest . the lack of prior information makes mcmc integration more difficult when a model parameter space is large and can not be explored in detail within a reasonable time . in particular , the lack of information about eeg feature importance can affect the results of bayesian assessment of eeg maturity . in this paper we explore how the posterior information about eeg feature importance can be used to reduce a negative influence of disproportional sampling on the results of bayesian assessment . we found that the mcmc integration tends to oversample the areas in which a model parameter space includes one or more features , the importance of which counted in terms of their posterior use is low . using this finding , we proposed to cure the results of mcmc integration and then described the results of testing the proposed method on a set of sleep eeg recordings .
parametric estimation of the continuous non stationary spectrum and its dynamics in surface emg studies . <eos> frequency spectrum of surface electromyographic signals ( semgs ) exhibit a non stationary nature even in the case of constant level isometric muscle contractions due to changes related to muscle fatigue processes . these changes can be evaluated by methods for estimation of time varying ( tv ) spectrum . the most widely adopted non parametric approach is a short time fourier transform ( stft ) , from which changes of mean frequency ( mf ) as well as other parameters for qualitative description of spectrum variation can be calculated . similar idea of a sliding window generalisation can also be used in case of parametric spectrum analysis methods . we applied such approach to obtain tv linear models of semgs , although its large variance due to independence of estimations in consequent windows represents a major drawback . this variance causes unrealistic abrupt changes in the curve of overall spectrum dynamics , calculated either as the second derivative of the mf or , as we propose , autoregressive moving average ( arma ) distance between subsequent linear models forming the tv parametric spectrum . a smoother estimation is therefore sought and another method shows to be superior over a simple sliding window technique . it supposes that trajectories of tv linear model coefficients can be described as linear combinations of known basis functions . we demonstrate that the later method is very appropriate for description of slowly changing spectra of semgs and that dynamics measures obtained from such estimations can be used as an additional indication of the fatigue process .
a multiple case design methodology for studying mrp success and csfs . <eos> we used a multiple case design to study materials requirements planning ( mrp ) implementation outcome in <digit> manufacturing companies in singapore . using a two phased data collection approach ( pre interview questionnaires and personal interviews ) , we sought to develop a comprehensive and operationally acceptable measure of mrp success . our measure consists of two linked components . they are a satisfaction score ( a quantitative measure ) and a complementary measure based on comments from the interviewees regarding the level of usage and acceptance of the system . we also extended and consolidated a seven factor critical success factor ( csf ) framework using this methodology . csfs are important , but knowing the linkages between them is even more important , because these linkages tell us which csfs to emphasize at various stages of the project .
hybrid heuristic waterfilling game theory approach in mc cdma resource allocation . <eos> this paper discusses the power allocation with fixed rate constraint problem in multi carrier code division multiple access ( mc cdma ) networks , that has been solved through game theoretic perspective by the use of an iterative water filling algorithm ( iwfa ) . the problem is analyzed under various interference density configurations , and its reliability is studied in terms of solution existence and uniqueness . moreover , numerical results reveal the approach shortcoming , thus a new method combining swarm intelligence and iwfa is proposed to make practicable the use of game theoretic approaches in realistic mc cdma systems scenarios . the contribution of this paper is twofold ( i ) provide a complete analysis for the existence and uniqueness of the game solution , from simple to more realist and complex interference scenarios ( ii ) propose a hybrid power allocation optimization method combining swarm intelligence , game theory and iwfa . to corroborate the effectiveness of the proposed method , an outage probability analysis in realistic interference scenarios , and a complexity comparison with the classical iwfa are presented . ( c ) <digit> elsevier b.v. all rights reserved .
direct type specific conic fitting and eigenvalue bias correction . <eos> a new method to fit specific types of conics to scattered data points is introduced . direct , specific fitting of ellipses and hyperbolae is achieved by imposing a quadratic constraint on the conic coefficients , whereby an improved partitioning of the design matrix is devised so as to improve computational efficiency and numerical stability by eliminating redundant aspects of the fitting procedure . fitting of parabolas is achieved by determining an orthogonal basis vector set in the grassmannian space of the quadratic terms coefficients . the linear combination of the basis vectors that fulfills the parabolic condition and has a minimum residual norm is determined using lagrange multipliers . this is the first known direct solution for parabola specific fitting . furthermore , the inherent bias of a linear conic fit is addressed . we propose a linear method of correcting this bias , producing better geometric fits which are still constrained to specific conic type .
communication with www in czech . <eos> this paper describes uio , a multi domain question answering system for the czech language that looks for answers on the web . uio exploits two fields , namely natural language interface to databases and question answering . in its current version , uio can be used for asking questions about train and coach timetables , cinema and theatre performances , about currency exchange rates , name days and on the diderot encyclopaedia . much effort have been made into making addition of a new domain very easy . no limits concerning words or the form of a question need to be set in uio . users can ask syntactically correct as well as incorrect questions , or use keywords . a czech morphological analyser and a bottom up chart parser are employed for analysis of the question . the database of multi word expressions is automatically updated when a new item has been found on the web . for all domains uio has an accuracy rate about <digit> % .
efficient coloring of a large spectrum of graphs . <eos> we have developed a new algorithm and software for graph coloring by systematically combining several algorithm and software development ideas that had crucial impact on the algorithm 's performance . the algorithm explores the divide and conquer paradigm , global search for constrained independent sets using a computationally inexpensive objective function , assignment of most constrained vertices to least constraining colors , reuse and locality exploration of intermediate solutions , search time management , post processing lottery scheduling iterative improvement , and statistical parameter determination and validation . the algorithm was tested on a set of real life examples . we found that hard to color real life examples are common especially in domains where problem modeling results in denser graphs . systematic experimentations demonstrated that for numerous instances the algorithm outperformed all other implementations reported in literature in solution quality and run time .
boolean equations and boolean inequations . <eos> in this paper we consider boolean inequations of the form f ( x ) not equal <digit> . we also consider the system of boolean inequation and boolean equation f ( x ) not equal <digit> boolean and g ( x ) <digit> and we describe all the solutions of this system .
time efficient centralized gossiping in radio networks . <eos> in this paper we study the gossiping problem ( all to all communication ) in radio networks where all nodes are aware of the network topology . we start our presentation with a deterministic gossiping algorithm that works in at most n units of time in any radio network of size n. this algorithm is optimal in the worst case scenario since there exist radio network topologies , such as lines , stars and complete graphs in which radio gossiping can not be completed in less than n communication rounds . furthermore , we show that there does not exist any radio network topology in which the gossiping task can be solved in less than log ( n <digit> ) <digit> rounds . we also show that this lower bound can be matched from above for a fraction of all possible integer values of n , and for all other values of n we propose a solution which accomplishes gossiping in log ( n <digit> ) <digit> rounds . then we show an almost optimal radio gossiping algorithm in trees , which misses the optimal time complexity by a single round . finally , we study asymptotically optimal o ( d ) time gossiping ( where d is the diameter of the network ) in graphs with the maximum degree delta o ( d1 <digit> ( i <digit> ) log ( i ) n ) , for any integer constant i > <digit> and d large enough . ( c ) <digit> elsevier b.v. all rights reserved .
tree kernel based protein protein interaction extraction from biomedical literature . <eos> there is a surge of research interest in protein protein interaction ( ppi ) extraction from biomedical literature . while most of the state of the art ppi extraction systems focus on dependency based structured information , the rich structured information inherent in constituent parse trees has not been extensively explored for ppi extraction . in this paper , we propose a novel approach to tree kernel baled ppi extraction , where the tree representation generated from a constituent syntactic parser is further refined using the shortest dependency path between two proteins derived from a dependency parser . specifically , all the constituent tree nodes associated with the nodes on the shortest dependency path are kept intact , while other nodes are removed safely to make the constituent tree concise and precise for ppi extraction . compared with previously used constituent tree setups , our dependency motivated constituent tree setup achieves the best results across five commonly used ppi corpora . moreover , our tree kernel based method outperforms other single kernel based ones and performs comparably with some multiple kernel ones on the most commonly tested aimed corpus . ( c ) <digit> elsevier inc. all rights reserved .
a review of content based image retrieval systems in medical applications clinical benefits and future directions . <eos> content based visual information retrieval ( cbvir ) or content based image retrieval ( cbir ) has been one on the most vivid research areas in the field of computer vision over the last <digit> years . the availability of large and steadily growing amounts of visual and multimedia data , and the development of the internet underline the need to create thematic access methods that offer more than simple text based queries or requests based on matching exact database fields . many programs and toots have been developed to formulate and execute queries based on the visual or audio content and to help browsing large multimedia repositories . still , no general breakthrough has been achieved with respect to large varied databases with documents of differing sorts and with varying characteristics . answers to many questions with respect to speed , semantic descriptors or objective image interpretations are still unanswered . in the medical field , images , and especially digital images , are produced in ever increasing quantities and used for diagnostics and therapy . the radiology department of the university hospital of geneva alone produced more than 12,000 images a day in <digit> . the cardiology is currently the second largest producer of digital images , especially with videos of cardiac catheterization ( similar to1800 exams per year containing almost <digit> images each ) . the total amount of cardiologic image data produced in the geneva university hospital was around <digit> tb in <digit> . endoscopic videos can equally produce enormous amounts of data . with digital imaging and communications in medicine ( dicom ) , a standard for image communication has been set and patient information can be stored with the actual image ( s ) , although stilt a few problems prevail with respect to the standardization . in several articles , content based access to medical images for supporting clinical decision making has been proposed that would ease the management of clinical data and scenarios for the integration of content based access methods into picture archiving and communication systems ( pacs ) have been created . this article gives an overview of available literature in the field of content based access to medical image data and on the technologies used in the field . section <digit> gives an introduction into generic content based image retrieval and the technologies used . section <digit> explains the propositions for the use of image retrieval in medical practice and the various approaches . example systems and application areas are described . section <digit> describes the techniques used in the implemented systems , their datasets and evaluations . section <digit> identifies possible clinical benefits of image retrieval systems in clinical practice as well as in research and education . new research directions are being defined that can prove to be useful . this article also identifies explanations to some of the outlined problems in the field as it looks like many propositions for systems are made from the medical domain and research prototypes are developed in computer science departments using medical datasets . still , there are very few systems that seem to be used in clinical practice . it needs to be stated as well that the goal is not , in general , to replace text based retrieval methods as they exist at the moment but to complement them with visual search tools . ( c ) <digit> elsevier ireland ltd. all rights reserved .
infinitesimal plane based pose estimation . <eos> estimating the pose of a plane given a set of point correspondences is a core problem in computer vision with many applications including augmented reality ( ar ) , camera calibration and 3d scene reconstruction and interpretation . despite much progress over recent years there is still the need for a more efficient and more accurate solution , particularly in mobile applications where the run time budget is critical . we present a new analytic solution to the problem which is far faster than current methods based on solving pose from ( n ) points ( pnp ) and is in most cases more accurate . our approach involves a new way to exploit redundancy in the homography coefficients . this uses the fact that when the homography is noisy it will estimate the true transform between the model plane and the image better at some regions on the plane than at others . our method is based on locating a point where the transform is best estimated , and using only the local transformation at that point to constrain pose . this involves solving pose with a local non redundant 1st order pde . we call this framework infinitesimal plane based pose estimation ( ippe ) , because one can think of it as solving pose using the transform about an infinitesimally small region on the surface . we show experimentally that ippe leads to very accurate pose estimates . because ippe is analytic it is both extremely fast and allows us to fully characterise the method in terms of degeneracies , number of returned solutions , and the geometric relationship of these solutions . this characterisation is not possible with state of the art pnp methods .
evolutionary learning of spiking neural networks towards quantification of 3d mri brain tumor tissues . <eos> this paper presents a new classification technique for 3d mr images , based on a third generation network of spiking neurons . implementation of multi dimensional co occurrence matrices for the identification of pathological tumor tissue and normal brain tissue features are assessed . the results show the ability of spiking classifier with iterative training using genetic algorithm to automatically and simultaneously recover tissue specific structural patterns and achieve segmentation of tumor part . the spiking network classifier has been validated and tested for various real time and harvard benchmark datasets , where appreciable performance in terms of mean square error , accuracy and computational time is obtained . the spiking network employed izhikevich neurons as nodes in a multi layered structure . the classifier has been compared with computational power of multi layer neural networks with sigmoidal neurons . the results on misclassified tumors are analyzed and suggestions for future work are discussed .
inferential queueing and speculative push . <eos> communication latencies within critical sections constitute a major bottleneck in some classes of emerging parallel workloads . in this paper , we argue for the use of two mechanisms to reduce these communication latencies inferentially queued locks ( iqls ) and speculative push ( sp ) . with iqls , the processor infers the existence , and limits , of a critical section from the use of synchronization instructions and joins a queue of lock requestors , reducing synchronization delay . the sp mechanism extracts information about program structure by observing iqls . sp allows the cache controller , responding to a request for a cache line that likely includes a lock variable , to predict the data sets the requestor will modify within the associated critical section . the controller then pushes these lines from its own cache to the target cache , as well as writing them to memory . overlapping the protected data transfer with that of the lock can substantially reduce the communication latencies within critical sections . by pushing data in exclusive state , the mechanism can collapse a read modify write sequences within a critical section into a single local cache access . the write back to memory allows the receiving cache to ignore the push . neither mechanism requires any programmer or compiler support nor any instruction set changes . our experiments demonstrate that iqls and sp can improve performance of applications employing frequent synchronization .
the thermal failure process of the quantum cascade laser . <eos> we report the thermal failure process of the quantum cascade laser . firstly , high temperature and strain in the active region are verified by raman spectra , and the conspicuous catastrophically failed characteristics are observed by scanning electron microscope . secondly , the defects generate serious structure disorder due to the high temperature of the active region and the resulted strain relaxation . thirdly , the abundant atomic diffusion in the active region and substrate are observed . the structure disorder and the change of element composition in the active region directly lead to the quantum cascade laser failure . the theoretical analysis fits well with the results of experimental studies .
admira atomic decomposition for minimum rank approximation ( vol <digit> , pg <digit> , <digit> ) . <eos> in this correspondence , a corrected version of the convergence analysis given by lee and bresler is presented .
stable advection reaction diffusion with arbitrary anisotropy . <eos> turing first theorized that many biological patterns arise through the processes of reaction and diffusion . subsequently , reaction diffusion systems have been studied in many fields , including computer graphics . we first show that for visual simulation purposes , reaction diffusion equations can be made unconditionally stable using a variety of straightforward methods . second , we propose an anisotropy embedding that significantly expands the space of possible patterns that can be generated . third , we show that by adding an advection term , the simulation can be coupled to a fluid simulation to produce visually appealing flows . fourth , we couple fast marching methods to our anisotropy embedding to create a painting interface to the simulation . unconditional stability is maintained throughout , and our system runs at interactive rates . finally , we show that on the cell processor , it is possible to implement reaction diffusion on top of an existing fluid solver with no significant performance impact . copyright ( c ) <digit> john wiley sons , ltd .
alexander duality and moments in reliability modelling . <eos> there are strong connections between coherent systems in reliability for systems which have components with a finite number of states and certain algebraic structures . a special case is binary systems where there are two states fail and not fail . the connection relies on an order property in the system and a way of coding states alpha ( alpha ( <digit> ) , ... , alpha ( d ) ) with monomials x ( alpha ) ( x ( <digit> ) ( alpha1 ) , ... , x ( d ) ( alphad ) ) . the algebraic entities are the scarf complex and the associated alexander duality . the failure '' event '' can be studied using these ideas and identities and bounds derived when the algebra is combined with probability distributions on the states . the x ( alpha ) coding aids the use of moments mu ( alpha ) e ( x alpha ) with respect to the underlying distribution .
failure identification for linear repetitive processes . <eos> this paper investigates the fault detection and isolation ( fdi ) problem for discrete time linear repetitive processes using a geometric approach , starting from a <digit> d model for these processes that incorporates a representation of the failure . based on this model , the fdi problem is formulated in the geometric setting and sufficient conditions for solvability of this problem are given . moreover , the processess behaviour in the presence of noise is considered , leading to the development of a statistical approach for determining a decision threshold . finally , a fdi procedure is developed based on an asymptotic observer reconstruction of the state vector .
determinants of web site information by spanish city councils . <eos> purpose the purpose of this research is to analyse the web sites of large spanish city councils with the objective of assessing the extent of information disseminated on the internet and determining what factors are affecting the observed levels of information disclosure . design methodology approach the study takes as its reference point the existing literature on the examination of the quality of web sites , in particular the provisions of the web quality model ( wqm ) and the importance of content as a key variable in determining web site quality . in order to quantify the information on city council web sites , a disclosure index has been designed which takes into account the content , navigability and presentation of the web sites . in order to contrast which variables determine the information provided on the web sites , our investigation bases itself on the studies about voluntary disclosure in the public sector , and six lineal regressions models have been performed . findings the empirical evidence obtained reveals low disclosure levels among spanish city council web sites . in spite of this , almost <digit> per cent of the city councils have reached the approved level and of these , around a quarter obtained good marks . our results show that disclosure levels depend on political competition , public media visibility and the access to technology and educational levels of the citizens . practical implications the strategy of communication on the internet by local spanish authorities is limited in general to an ornamental web presence but one that does not respond efficiently to the requirements of the digital society . during the coming years , local spanish politicians will have to strive to take advantage of the opportunities that the internet offers to increase both the relational and informational capacity of municipal web sites as well as the digital information transparency of their public management . originality value the internet is a potent channel of communication that is modifying the way in which people access and relate to information and each other . the public sector is not unaware of these changes and is incorporating itself gradually into the new network society . this study systematises the analysis of local administration web sites , showing the lack of digital transparency , and orients politicians in the direction to follow in order to introduce improvements in their electronic relationships with the public .
the development of regional collaboration for resource efficiency a network perspective on industrial symbiosis . <eos> three development patterns of industrial symbiosis systems are proposed and empirically examined . industrial symbiosis networks build on and strengthen the disparity of firms capability on building symbiotic relations . due to this disparity , self organized industrial symbiosis networks favor the most capable firms and grow preferentially . coordinating agencies improve disadvantaged firms capabilities , and change the preferential growth to a homogeneous one . strong government engagement helps disadvantaged firms and facilitates non preferential symbiosis development in a region .
effective diagnosis of heart disease through neural networks ensembles . <eos> in the last decades , several tools and various methodologies have been proposed by the researchers for developing effective medical decision support systems . moreover , new methodologies and new tools are continued to develop and represent day by day . diagnosing of the heart disease is one of the important issue and many researchers investigated to develop intelligent medical decision support systems to improve the ability of the physicians . in this paper , we introduce a methodology which uses sas base software 9.1.3 for diagnosing of the heart disease . a neural networks ensemble method is in the centre of the proposed system . this ensemble based methods creates new models by combining the posterior probabilities or the predicted values from multiple predecessor models . so , more effective models can be created . we performed experiments with the proposed tool . we obtained 89.01 % classification accuracy from the experiments made on the data taken from cleveland heart disease database . we also obtained 80.95 % and 95.91 % sensitivity and specificity values , respectively , in heart disease diagnosis .
soil carbon model yasso07 graphical user interface . <eos> in this article , we present a graphical user interface software for the litter decomposition and soil carbon model yasso07 and an overview of the principles and formulae it is based on . the software can be used to test the model and use it in simple applications . yasso07 is applicable to upland soils of different ecosystems worldwide , because it has been developed using data covering the global climate conditions and representing various ecosystem types . as input information , yasso07 requires data on litter input to soil , climate conditions , and land use change if any . the model predictions are given as probability densities representing the uncertainties in the parameter values of the model and those in the input data the user interface calculates these densities using a built in monte carlo simulation .
time synchronization attacks in sensor networks . <eos> time synchronization is a critical building block in distributed wireless sensor networks . because sensor nodes may be severely resource constrained , traditional time synchronization protocols can not be used in sensor networks . various time synchronization protocols tailored for such networks have been proposed to solve this problem . however , none of these protocols have been designed with security in mind . if an adversary were able to compromise a node , he might prevent a network from effectively executing certain applications , such as sensing or tracking an object , or he might even disable the network by disrupting a fundamental service such as a tdma based channel sharing scheme . in this paper we give a survey of the most common time synchronization protocols and outline the possible attacks on each protocol . in addition , we discuss how different sensor network applications that are affected by time synchronization attacks , and we propose some countermeasures for these attack .
enhanced privacy id a direct anonymous attestation scheme with enhanced revocation capabilities . <eos> direct anonymous attestation ( daa ) is a scheme that enables the remote authentication of a trusted platform module ( tpm ) while preserving the user 's privacy . a tpm can prove to a remote party that it is a valid tpm without revealing its identity and without linkability . in the daa scheme , a tpm can be revoked only if the daa private key in the hardware has been extracted and published widely so that verifiers obtain the corrupted private key . if the unlinkability requirement is relaxed , a tpm suspected of being compromised can be revoked even if the private key is not known . however , with the full unlinkability requirement intact , if a tpm has been compromised but its private key has not been distributed to verifiers , the tpm can not be revoked . furthermore , a tpm can not be revoked from the issuer , if the tpm is found to be compromised after the daa issuing has occurred . in this paper , we present a new daa scheme called enhanced privacy id ( epid ) scheme that addresses the above limitations . while still providing unlinkability , our scheme provides a method to revoke a tpm even if the tpm private key is unknown . this expanded revocation property makes the scheme useful for other applications such as for driver 's license . our epid scheme is efficient and provably secure in the same security model as daa , i.e. , in the random oracle model under the strong rsa assumption and the decisional diffie hellman assumption .
using predicate path information in hardware to determine true dependences . <eos> predicated execution has been put forth as a method for improving processor performance by removing hard to predict branches . as part of the process of turning a set of basic blocks into a predicated region , both paths of a branch are combined into a single path . there can be multiple definitions from disjoint paths that reach a use . waiting to find out the correct definition that actually reaches the use can cause pipeline stalls.in this paper we examine a hardware optimization that dynamically collects and analyzes path information to determine valid dependences for predicated regions of code . we then use this information for an in order vliw predicated processor , so that instructions can continue towards execution without having to wait on operands from false dependences . our results show that using our disjoint path analysis system provides speedups over <digit> % and elimination of false raw dependences of up to <digit> % due to the detection of erroneous dependences in if converted regions of code .
simulation of dna damage clustering after proton irradiation using an adapted dbscan algorithm . <eos> in this work the density based spatial clustering of applications with noise ( dbscan ) algorithm was adapted to early stage dna damage clustering calculations . the resulting algorithm takes into account the distribution of energy deposit induced by ionising particles and a damage probability function that depends on the total energy deposit amount . proton track simulations were carried out in small micrometric volumes representing small dna containments . the algorithm was used to determine the damage concentration clusters and thus to deduce the dsb ssb ratios created by protons between 500kev and 50mev . the obtained results are compared to other calculations and to available experimental data of fibroblast and plasmid cells irradiations , both extracted from literature .
building the academic strategy program . <eos> purpose to present the application of a theory and best practice of a balanced scorecard bsc method , to create bsc strategic program for academic education decision makers , and to present framework for strategy program about research and education in faculty . methodology approach based on the investigation of number of successful project on this topic and on the authors exercise in balanced scorecard approach about educational strategy the program is created and modelled . findings the balanced scorecard strategic program is developed and it allows enhancing the leadership capability across university . practical implications the program can facilitate faculty and staff to formulate and measure strategic management decisions and to create competitive educational and research environment at the university . originality value the value of the program is in integrating competences , experience , best practices and tools within one new program design . the paper shows how to translate the academic strategy into different strategic objectives and goals , how to model them and how to communicate academic research and education processes to realize important improvements in cost and quality of academic services .
nonlinear transport in quantum point contact structures . <eos> we have investigated the magnetotransport properties under nonlinear conditions in quantum point contact structures fabricated on high mobility algaas gaas two dimensional electron gas ( 2deg ) layers . nonlinearities in the iv characteristics are observed at the threshold for conduction when biased initially from the tunneling regime as observed previously . we observe that this non ideality is enhanced by a magnetic field normal to the plane of the 2deg . this behavior is interpreted in terms of corrections to the landauer model extended to nonequilibrium conditions .
mrm a matrix representation and mapping approach for knowledge acquisition . <eos> knowledge acquisition plays a critical role in constructing a knowledge based system ( kbs ) . it is the most time consuming phase and has been recognized as the bottleneck of kbs development . this paper presents a matrix representation and mapping ( mrm ) approach to facilitate the effectiveness of knowledge acquisition in building a kbs . the proposed mrm approach , which is based on matrix representation and mapping operations , comprises six consecutive steps for generating rules . the procedure in each step is elaborated . a case study on primarily diagnosing an automotive system is employed to illustrate how the mrm approach works .
integrated obstacle avoidance and path following through a feedback control law . <eos> the article proposes a novel approach to path following in the presence of obstacles with unique characteristics . first , the approach proposes an integrated method for obstacle avoidance and path following based on a single feedback control law , which produces commands to actuators directly executable by a robot with unicycle kinematics . second , the approach offers a new solution to the well known dilemma that one has to face when dealing with multiple sensor readings , i.e. , whether it is better , to summarize a huge amount of sensor data , to consider only the closest sensor reading , to consider all sensor readings separately to compute the resulting force vector , or to build a local map . the approach requires very little memory and computational resources , thus being implementable even on simpler robots moving in unknown environments .
a review of learning vector quantization classifiers . <eos> in this work , we present a review of the state of the art of learning vector quantization ( lvq ) classifiers . a taxonomy is proposed which integrates the most relevant lvq approaches to date . the main concepts associated with modern lvq approaches are defined . a comparison is made among eleven lvq classifiers using one real world and two artificial datasets .
sketching concurrent data structures . <eos> we describe psketch , a program synthesizer that helps programmers implement concurrent data structures . the system is based on the concept of sketching , a form of synthesis that allows programmers to express their insight about an implementation as a partial program a sketch . the synthesizer automatically completes the sketch to produce an implementation that matches a given correctness criteria . psketch is based on a new counterexample guided inductive synthesis algorithm ( cegis ) that generalizes the original sketch synthesis algorithm from solar lezama et.al . to cope efficiently with concurrent programs . the new algorithm produces a correct implementation by iteratively generating candidate implementations , running them through a verifier , and if they fail , learning from the counterexample traces to produce a better candidate converging to a solution in a handful of iterations . psketch also extends sketch with higher level sketching constructs that allow the programmer to express her insight as a soup of ingredients from which complicated code fragments must be assembled . such sketches can be viewed as syntactic descriptions of huge spaces of candidate programs ( over <digit> <digit> candidates for some sketches we resolved ) . we have used the psketch system to implement several classes of concurrent data structures , including lock free queues and concurrent sets with fine grained locking . we have also sketched some other concurrent objects including a sense reversing barrier and a protocol for the dining philosophers problem all these sketches resolved in under an hour .
evaluating change in user error when using ruggedized handheld devices . <eos> there are no significant differences between user error and age . lack of corrective software may not impact user error as much as expected . keypad devices had more character errors while touchscreen devices had more word .
support for situation awareness in trustworthy ubiquitous computing application software . <eos> due to the dynamic and ephemeral nature of ubiquitous computing ( ubicomp ) environments , it is especially important that the application software in ubicomp environments is trustworthy . in order to have trustworthy application software in ubicomp environments , situation awareness ( saw ) in the application software is needed to enforce flexible security policies and detect violations of security policies . in this paper , an approach is presented to provide development and runtime support to incorporate saw in trustworthy ubicomp , application software . the development support is to provide saw requirement specification and automated code generation to achieve saw in trustworthy ubicomp application software , and the runtime support is for context acquisition , situation analysis and situation aware communication . to realize our approach , the improved reconfigurable context sensitive middleware ( rcsm ) is developed to provide the above development and runtime support . copyright ( c ) <digit> john wiley sons , ltd .
validation of temperature perturbation and cfd based modelling for the prediction of the thermal urban environment the lecce ( it ) case study . <eos> two modelling approaches for air temperature prediction in cities are evaluated . daily trends of air temperature are well captured . envi met requires an ad hoc tuning of surface boundary conditions . adms th model performance depends on the accuracy of energy balance terms . adms th shows an overall better performance than envi met .
comparison of software for computing the action of the matrix exponential . <eos> the implementation of exponential integrators requires the action of the matrix exponential and related functions of a possibly large matrix . there are various methods in the literature for carrying out this task . in this paper we describe a new implementation of a method based on interpolation at leja points . we numerically compare this method with other codes from the literature . as we are interested in applications to exponential integrators we choose the test examples from spatial discretization of time dependent partial differential equations in two and three space dimensions . the test matrices thus have large eigenvalues and can be nonnormal .
learning while designing . <eos> this paper describes how a computational system for designing can learn useful , reusable , generalized search strategy rules from its own experience of designing . it can then apply this experience to transform the design process from search based ( knowledge lean ) to knowledge based ( knowledge rich ) . the domain of application is the design of spatial layouts for architectural design . the processes of designing and learning are tightly coupled .
an algebraic construction of codes for slepian wolf source networks . <eos> this correspondence proposes an explicit construction of fixed length codes for slepian wolf ( sw ) source networks . the proposed code is linear , and has two step encoding and decoding procedures similar to the concatenated code used for channel coding . encoding and decoding of the code can be done in a polynomial order of the block length . the proposed code can achieve arbitrary small probability of error for ergodic sources with finite alphabets , if the pair of encoding rates is in the achievable region . further , if the sources are memoryless , the proposed code can be modified to become universal and the probability of error vanishes exponentially as the block length tends to infinity .
ann and anfis models for performance evaluation of a vertical ground source heat pump system . <eos> the aim of this study is to demonstrate the comparison of an artificial neural network ( ann ) and an adaptive neuro fuzzy inference system ( anfis ) for the prediction performance of a vertical ground source heat pump ( vgshp ) system . the vgshp system using r <digit> as refrigerant has a three single u tube ground heat exchanger ( ghe ) made of polyethylene pipe with a <digit> mm outside diameter . the ghes were placed in a vertical boreholes ( vbs ) with <digit> ( vb1 ) , <digit> ( vb2 ) and <digit> ( vb3 ) m depths and <digit> mm diameters . the monthly mean values of cop for vb1 , vb2 and vb3 are obtained to be 3.37 1.93 , 3.85 2.37 , and 4.33 3.03 , respectively , in cooling heating seasons . experimental performances were performed to verify the results from the ann and anfis approaches . ann model , multi layered perceptron back propagation with three different learning algorithms ( the levenbergmarquardt ( lm ) , scaled conjugate gradient ( scg ) and pola ribiere conjugate gradient ( cgp ) algorithms and the anfis model were developed using the same input variables . finally , the statistical values are given in as tables . this paper shows the appropriateness of anfis for the quantitative modeling of gshp systems .
effects of agent heterogeneity in the presence of a land market a systematic test in an agent based laboratory . <eos> representing agent heterogeneity is one of the main reasons that agent based models become increasingly popular in simulating the emergence of land use , land cover change and socioeconomic phenomena . however , the relationship between heterogeneous economic agents and the resultant landscape patterns and socioeconomic dynamics has not been systematically explored . in this paper , we present a stylized agent based land market model , land use in exurban environments ( luxe ) , to study the effects of multidimensional agents heterogeneity on the spatial and socioeconomic patterns of urban land use change under various market representations . we examined two sources of agent heterogeneity budget heterogeneity , which imposes constraints on the affordability of land , and preference heterogeneity , which determines location choice . the effects of the two dimensions of agents heterogeneity are systematically explored across different market representations by three experiments . agents heterogeneity exhibits a complex interplay with various forms of market institutions as indicated by macro measures ( landscape metrics , segregation index , and socioeconomic metrics ) . in general , budget heterogeneity has pronounced effect on socioeconomic results , while preference heterogeneity is highly pertinent to spatial outcomes . the relationship between agent heterogeneity and macro measures becomes more complex when more land market mechanisms are represented . in other words , appropriately simulating agent heterogeneity plays an important role in guaranteeing the fidelity of replicating empirical land use change process .
feature shaping for linear svm classifiers . <eos> linear classifiers have been shown to be effective for many discrimination tasks . irrespective of the learning algorithm itself , the final classifier has a weight to multiply by each feature . this suggests that ideally each input feature should be linearly correlated with the target variable ( or anti correlated ) , whereas raw features may be highly non linear . in this paper , we attempt to re shape each input feature so that it is appropriate to use with a linear weight and to scale the different features in proportion to their predictive value . we demonstrate that this pre processing is beneficial for linear svm classifiers on a large benchmark of text classification tasks as well as uci datasets .
a service driven development process ( sddp ) model for ultra large scale systems . <eos> achieving ultra large scale software systems will necessarily require new and special development processes . this position paper suggests overall structure of a process model to develop and maintain system of systems similar to ultra large scale ( uls ) systems . the proposed process model will be introduced in details and finally , we will evaluate it considering cmmi acq which has been presented by sei for acquirer organizations .
delineating white matter structure in diffusion tensor mri with anisotropy creases . <eos> geometric models of white matter architecture play an increasing role in neuroscientific applications of diffusion tensor imaging , and the most popular method for building them is fiber tractography . for some analysis tasks , however , a compelling alternative may be found in the first and second derivatives of diffusion anisotropy . we extend to tensor fields the notion from classical computer vision of ridges and valleys , and define anisotropy creases as features of locally extremal tensor anisotropy . mathematically , these are the loci where the gradient of anisotropy is orthogonal to one or more eigenvectors of its hessian . we propose that anisotropy creases provide a basis for extracting a skeleton of the major white matter pathways , in that ridges of anisotropy coincide with interiors of fiber tracts , and valleys of anisotropy coincide with the interfaces between adjacent but distinctly oriented tracts . the crease extraction algorithm we present generates high quality polygonal models of crease surfaces , which are further simplified by connected component analysis . we demonstrate anisotropy creases on measured diffusion mri data , and visualize them in combination with tractography to confirm their anatomic relevance .
a fully parallel method for the singular eigenvalue problem . <eos> in this paper , a fully parallel method for finding some or all finite eigenvalues of a real symmetric matrix pencil ( a , b ) is presented , where a is a symmetric tridiagonal matrix and b is a diagonal matrix with b ( <digit> ) > <digit> and b ( i ) > <digit> , i 2,3 , ... , n. the method is based on the homotopy continuation with rank <digit> perturbation . it is shown that there are exactly m disjoint , smooth homotopy paths connecting the trivial eigenvalues to the desired eigenvalues , where m is the number of finite eigenvalues of ( a , b ) , it is also shown that the homotopy curves are monotonic and easy to follow . ( c ) <digit> elsevier ltd. all rights reserved .
potential and requirements of it for ambient assisted living technologies results of a delphi study . <eos> objectives ambient assisted living ( aal ) technologies are developed to enable elderly to live independently and safely . innovative information technology ( it ) can interconnect personal devices and offer suitable user interfaces . often dedicated solutions are developed for particular projects . the aim of our research was to identify major it challenges for aal to enable generic and sustainable solutions . methods delphi survey . an online questionnaire was sent to <digit> members of the german innovation partnership aal . the first round was qualitative to collect statements . statements were reduced to items by qualitative content analysis . items were assessed in the following two rounds by a <digit> point likert scale . quantitative analyses for second and third round descriptive statistics , factor analysis and anova . results respondents <digit> in first , <digit> in second and <digit> in third round . all items got a rather high assessment . medical issues were rated as having a very high potential . items related to user friendliness were regarded as most important requirements . common requirements to all aal solutions are reliability , robustness , availability , data security , data privacy , legal issues , ethical requirements , easy configuration . the complete list of requirements can be used as framework for customizing future aal projects . conclusions a wide variety of it issues have been assessed important for aal . the extensive list of requirements makes obvious that it is not efficient to develop dedicated solutions for individual projects but to provide generic methods and reusable components . experiences and results from medical informatics research can be used to advance aal solutions ( e.g. ehealth and knowledge based approaches ) .
hybrid harmonic coding of speech at low bit rates . <eos> this paper presents a novel approach to sinusoidal coding of speech which avoids the use of a voicing detector . the proposed model represents the speech signal as a sum of sinusoids and bandpass random signals and it is denoted hybrid harmonic model in this paper . the use of two different sets of basis functions increases the robustness of the model since there is no need to switch between techniques tailored to particular classes of sounds . sinusoidal basis functions with harmonically related frequencies allow an accurate representation of the quasi periodic structure of voiced speech but show difficulties in representing unvoiced sounds . on the other hand , the bandpass random functions are well suited for high quality representation of unvoiced speech sounds , since their bandwidth is larger than the bandwidth of sinusoids . the amplitudes of both sets of basis functions are simultaneously estimated by a least squares algorithm and the output speech signal is synthesized in the time domain by the superposition of all basis functions multiplied by their amplitudes . experimental tests confirm an improved performance of the hybrid model for operation with noise corrupted input speech , relative to classic sinusoidal models which exhibit a strong dependency on voicing decision . finally , the implementation and test of a fully quantized hybrid coder at 4.8 kbit s is described .
semi supervised learning and condition fusion for fault diagnosis . <eos> manifold regularization based semi supervised learning is introduced to fault diagnosis . unlabeled condition data are also utilized to enhance the multi fault detection . a new single conditions and all conditions labeled mode is proposed to feed ssl . this ssl approach outperforms supervised learning in both labeled modes . the manifold fundamental of single conditions labeled mode is analyzed with dimensionality reduction .
a deductive system for proving workflow models from operational procedures . <eos> many modern business environments employ software to automate the delivery of workflows whereas , workflow design and generation remains a laborious technical task for domain specialists . several different approaches have been proposed for deriving workflow models . some approaches rely on process data mining approaches , whereas others have proposed derivations of workflow models from operational structures , domain specific knowledge or workflow model compositions from knowledge bases . many approaches draw on principles from automatic planning , but conceptual in context and lack mathematical justification . in this paper we present a mathematical framework for deducing tasks in workflow models from plans in mechanistic or strongly controlled work environments , with a focus around automatic plan generations . in addition , we prove an associative composition operator that permits crisp hierarchical task compositions for workflow models through a set of mathematical deduction rules . the result is a logical framework that can be used to prove tasks in workflow hierarchies from operational information about work processes and machine configurations in controlled or mechanistic work environments .
graph based signature classes for detecting polymorphic worms via content analysis . <eos> malicious softwares such as trojans , viruses , or worms can cause serious damage for information systems by exploiting operating system and application software vulnerabilities . worms constitute a significant proportion of overall malicious software and infect a large number of systems in very short periods . polymorphic worms combine polymorphism techniques with self replicating and fast spreading characteristics of worms . each copy of a polymorphic worm has a different pattern so it is not effective to use simple signature matching techniques . in this work , we propose a graph based classification framework of content based polymorphic worm signatures . this framework aims to guide researchers to propose new polymorphic worm signature schemes . we also propose a new polymorphic worm signature scheme , conjunction of combinational motifs ( ccm ) , based on the defined framework . ccm utilizes common substrings of polymorphic worm copies and also the relation between those substrings through dependency analysis . ccm is resilient to new versions of a polymorphic worm . ccm also automatically generates signatures for new versions of a polymorphic worm , triggered by partial signature matches . experimental results support that ccm has good flow evaluation time performance with low false positives and low false negatives .
polynomial time theory of matrix groups . <eos> we consider matrix groups , specified by a list of generators , over finite fields . the two most basic questions about such groups are membership in and the order of the group . even in the case of abelian groups it is not known how to answer these questions without solving hard number theoretic problems ( factoring and discrete log ) in fact , constructive membership testing in the case of <digit> <digit> matrices is precisely the discrete log problem . so the reasonable question is whether these problems are solvable in randomized polynomial time using number theory oracles . building on <digit> years of work , including remarkable recent developments by several groups of authors , we are now able to determine the order of a matrix group over a finite field of odd characteristic , and to perform constructive membership testing in such groups , in randomized polynomial time , using oracles for factoring and discrete log . one of the new ingredients of this result is the following . a group is called semisimple if it has no abelian normal subgroups . for matrix groups over finite fields , we show that the order of the largest semisimple quotient can be determined in randomized polynomial time ( no number theory oracles required and no restriction on parity ) . as a by product , we obtain a natural problem that belongs to bpp and is not known to belong either to rp or to corp. no such problem outside the area of matrix groups appears to be known . the problem is the decision version of the above given a list a of nonsingular d d matrices over a finite field and an integer n , does the group generated by a have a semisimple quotient of order > n we also make progress in the area of constructive recognition of simple groups , with the corollary that for a large class of matrix groups , our algorithms become las vegas .
delineation of the genomics field by hybrid citation lexical methods interaction with experts and validation process . <eos> in advanced methods of delineation and mapping of scientific fields , hybrid methods open a promising path to the capitalisation of advantages of approaches based on words and citations . one way to validate the hybrid approaches is to work in cooperation with experts of the fields under scrutiny . we report here an experiment in the field of genomics , where a corpus of documents has been built by a hybrid citation lexical method , and then clustered into research themes . experts of the field were associated in the various stages of the process lexical queries for building the initial set of documents , the seed citation based extension aiming at reducing silence final clustering to identify noise and allow discussion on border areas . the analysis of experts advices show a high level of validation of the process , which combines a high precision and low recall seed , obtained by journal and lexical queries , and a citation based extension enhancing the recall . this findings on the genomics field suggest that hybrid methods can efficiently retrieve a corpus of relevant literature , even in complex and emerging fields .
sampling correlation sources for timing yield analysis of sequential circuits with clock networks . <eos> analyzing timing yield under process variations is difficult because of the presence of correlations . reconvergent fan out nodes ( rfons ) within combinational subcircuits are a major source of topological correlation . we identify two more sources of topological correlation in clocked sequential circuit sequential rfons , which are nodes within a clock network where the clock paths to more than one flip flop branch out and sequential branch points , which are nodes within a combinational block where combinational paths to more than one capturing flip flop branch out . dealing with all sources of correlation is unacceptably complicated , and we therefore show how to sample a handful of correlation sources without sacrificing significant accuracy in the yield . a further reduction in computation time can be obtained by sampling only those nodes that are likely to affect the yield . these techniques are applied to yield analysis using statistical static timing analysis based on discrete random variables and also to yield analysis based on monte carlo simulation the accuracy and efficiency of both methods are assessed using example circuits . the sequential rfons suggest that timing yield may be improved by optimizing the clock network , and we address this possibility .
optimal search and one way trading online algorithms . <eos> this paper is concerned with the time series search and one way trading problems . in the ( time series ) search problem a player is searching for the maximum ( or minimum ) price in a sequence that unfolds sequentially , one price at a time . once during this game the player can decide to accept the current price p in which case the game ends and the player 's payoff is p. in the one way trading problem a trader is given the task of trading dollars to yen . each day , a new exchange rate is announced and the trader must decide how many dollars to convert to yen according to the current rate . the game ends when the trader trades his entire dollar wealth to yen and his payoff is the number of yen acquired . the search and one way trading are intimately related . any ( deterministic or randomized ) one way trading algorithm can be viewed as a randomized search algorithm . using the competitive ratio as a performance measure we determine the optimal competitive performance for several variants of these problems . in particular , we show that a simple threat based strategy is optimal and we determine its competitive ratio which yields , for realistic values of the problem parameters , surprisingly low competitive ratios . we also consider and analyze a one way trading game played against an adversary called nature where the online player knows the probability distribution of the maximum exchange rate and that distribution has been chosen by nature . finally , we consider some applications for a special case of portfolio selection called two way trading in which the trader may trade back and forth between cash and one asset .
interpretation of complex scenes using dynamic tree structure bayesian networks . <eos> this paper addresses the problem of object detection and recognition in complex scenes , where objects are partially occluded . the approach presented herein is based on the hypothesis that a careful analysis of visible object details at various scales is critical for recognition in such settings . in general , however , computational complexity becomes prohibitive when trying to analyze multiple sub parts of multiple objects in an image . to alleviate this problem , we propose a generative model framework namely , dynamic tree structure belief networks ( dtsbns ) . this framework formulates object detection and recognition as inference of dtsbn structure and image class conditional distributions , given an image . the causal ( markovian ) dependencies in dtsbns allow for design of computationally efficient inference , as well as for interpretation of the estimated structure as follows each root represents a whole distinct object , while children nodes down the sub tree represent parts of that object at various scales . therefore , within the dtsbn framework , the treatment and recognition of object parts requires no additional training , but merely a particular interpretation of the tree subtree structure . this property leads to a strategy for recognition of objects as a whole through recognition of their visible parts . our experimental results demonstrate that this approach remarkably outperforms strategies without explicit analysis of object parts . ( c ) <digit> elsevier inc. all rights reserved .
consistent group membership in ad hoc networks . <eos> the design of ad hoc mobile applications often requires the availability of a consistent view of the application state among the participating hosts . such views are important because they simplify both the programming and verification tasks . essential to constructing a consistent view is the ability to know what hosts are within proximity of each other , i.e. , form a group in support of the particular application . in this paper we propose an algorithm that allows hosts within communication range to maintain a consistent view of the group membership despite movement and frequent disconnections . the novel features of this algorithm are its reliance on location information and a conservative notion of logical connectivity that creates the illusion of announced disconnection . movement patterns and delays are factored in the policy that determines which physical connections are susceptible to disconnection .
a real time collision detection algorithm for mobile billiards game . <eos> collision detection is a key technique in game design . however , some algorithms employed in pc game are not suitable for mobile game because of the low performance and small screen size in mobile devices . combining with the features of the mobile devices , this paper proposes a quick and feasible collision detection algorithm . this algorithm makes use of the multi level collision detection and dynamic multi resolution grid subdivision to reduce the computing time for collision detection , which improves the algorithm performance greatly . in the collision response phase , this paper adopts the time step binary search algorithm to ensure both the computing precision and system efficiency . the mobile billiards game designed for the bird company indicates that this algorithm has good performance and real time interaction .
iterative execution feedback model directed gui testing . <eos> current fully automatic model based test case generation techniques for guis employ a static model . therefore they are unable to leverage certain state based relationships between gui events ( e.g. , one enables the other , one alters the others execution ) that are revealed at run time and non trivial to infer statically . we present alt a new technique to generate gui test cases in batches . because of its alternating nature , alt enhances the next batch by using gui run time information from the current batch . an empirical study on four fielded gui based applications demonstrated that alt was able to detect new <digit> and <digit> way gui interaction faults in contrast , previous techniques , due to their requirement of too many test cases , were unable to even test <digit> and <digit> way gui interactions .
a hierarchical refinement algorithm for fully automatic gridding in spotted dna microarray image processing . <eos> gridding , the first step in spotted dna microarray image processing , usually requires human intervention to achieve acceptable accuracy . we present a new algorithm for automatic gridding based on hierarchical refinement to improve the efficiency , robustness and reproducibility of microarray data analysis . this algorithm employs morphological reconstruction along with global and local rotation detection , non parametric optimal thresholding and local fine tuning without any human intervention . using synthetic data and real microarray images of different sizes and with different degrees of rotation of subarrays , we demonstrate that this algorithm can detect and compensate for alignment and rotation problems to obtain reliable and robust results .
genetic algorithms applied in bopp film scheduling problems minimizing total absolute deviation and setup times . <eos> the frequent changeovers in the production processes indicate the importance of setup time in many real world manufacturing activities . the traditional approaches in dealing with setup times are that either to omit or to merge into the processing times so as to simplify the problems . these approaches could reduce the complexity of the problem , but often generated unrealistic outcomes because of the assumed conditions . this situation motivated us to consider sequence dependent setup times in a real world bopp film scheduling problem . first , a setup time based heuristic method was developed to generate the initial solutions for the genetic algorithms ( gas ) . then , genetic algorithms with different mutation methods were applied . extensive experimental results showed that the setup time based heuristic method was relatively efficient . it was also found that a genetic algorithm with the variable mutation rate performed much effectively than one with the fixed mutation rate .
automatic grading of scots pine ( pinus sylvestris l. ) sawlogs using an industrial x ray log scanner . <eos> the successful running of a sawmill is dependent on its ability to achieve the highest possible value recovery from the sawlogs , i.e. to optimize the use of the raw material . such optimization requires information about the properties of every log . one method of measuring these properties is to use an x ray log scanner . the objective of the present study was to determine the accuracy when grading scots pine ( pinus sylvestris l. ) sawlogs using an industrial scanner known as the x ray logscanner . the study was based on <digit> scots pine sawlogs from a sawmill in northern sweden . all logs were scanned in the logscanner at a speed of <digit> m min . the x ray images were analyzed on line with measures of different properties as a result ( e.g. density and density variations ) . the logs were then sawn with a normal sawing pattern ( <digit> mm ) and the logs were graded depending on the result from the manual grading of the center boards . finally , partial least squares ( pls ) regression was used to calibrate statistical models that predict the log grade based on the properties measured by the x ray logscanner . the study showed that <digit> % of the logs were correctly sorted when using the scanner to sort logs into three groups according to the predicted grade of the center boards . after sawing the sorted logs , <digit> % of the boards had the correct grade . when scanning the same logs repeatedly , the relative standard deviation of the predicted grade was <digit> % . the study also showed that it is possible to sort out <digit> and <digit> % , respectively , of the material into two groups with high quality logs , without changing the grade distribution of the rest of the material to any great extent .
learning juntas in the presence of noise . <eos> we investigate the combination of two major challenges in computational learning dealing with huge amounts of irrelevant information and learning from noisy data . it is shown that large classes of boolean concepts that depend only on a small fraction of their variables so called juntas can be learned efficiently from uniformly distributed examples that are corrupted by random attribute and classification noise . we present solutions to cope with the manifold problems that inhibit a straightforward generalization of the noise free case . additionally , we extend our methods to non uniformly distributed examples and derive new results for monotone juntas and for parity juntas in this setting . it is assumed that the attribute noise is generated by a product distribution . without any restrictions of the attribute noise distribution , learning in the presence of noise is in general impossible . this follows from our construction of a noise distribution p and a concept class c such that it is impossible to learn c under p noise . ( c ) <digit> elsevier b.v. all rights reserved .
single allocation ordered median hub location problems . <eos> the discrete ordered median location model is a powerful tool in modeling classic and alternative location problems that have been applied with success to a large variety of discrete location problems . nevertheless , although hub location models have been analyzed from the sum , maximum and coverage point of views , as far as we know , they have never been considered under an alternative unifying point of view . in this paper we consider new formulations , based on the ordered median objective function , for hub location problems with new distribution patterns induced by the different users ' roles within the supply chain network . this approach introduces some penalty factors associated with the position of an allocation cost with respect to the sorted sequence of these costs . first we present basic formulations for this problem , and then develop stronger formulations by exploiting properties of the model . the performance of all these formulations is compared by means of a computational analysis . ( c ) <digit> elsevier ltd. all rights reserved .
how measuring student performances allows for measuring blended extreme apprenticeship for learning bash programming . <eos> many small exercises and few lectures can teach all programming . measuring student behavior in exercises assesses how they learn . the reported study logged student performances in programming exercises . metrics were defined for assessing overall programming performances . data show that all students tend to learn basic programming skills .
application of 3d wavelet statistics to video analysis . <eos> video activity analysis is used in various video applications such as human action recognition , video retrieval , video archiving . in this paper , we propose to apply 3d wavelet transform statistics to natural video signals and employ the resulting statistical attributes for video modeling and analysis . from the 3d wavelet transform , we investigate the marginal and joint statistics as well as the mutual information ( mi ) estimates . we show that marginal histograms are approximated quite well by generalized gaussian density ( ggd ) functions and the mi between coefficients decreases when the activity level increases in videos . joint statistics attributes are applied to scene activity grouping , leading to 87.3 % accurate grouping of videos . also , marginal and joint statistics features extracted from the video are used for human action classification employing support vector machine ( svm ) classifiers and 93.4 % of the human activities are properly classified .
modeling electrokinetic flows in microchannels using coupled lattice boltzmann methods . <eos> we present a numerical framework to solve the dynamic model for electrokinetic flows in microchannels using coupled lattice boltzmann methods . the governing equation for each transport process is solved by a lattice boltzmann model and the entire process is simulated through an iteration procedure . after validation , the present method is used to study the applicability of the poissonboltzmann model for electrokinetic flows in microchannels . our results show that for homogeneously charged long channels , the poissonboltzmann model is applicable for a wide range of electric double layer thickness . for the electric potential distribution , the poissonboltzmann model can provide good predictions until the electric double layers fully overlap , meaning that the thickness of the double layer equals the channel width . for the electroosmotic velocity , the poissonboltzmann model is valid even when the thickness of the double layer is <digit> times of the channel width . for heterogeneously charged microchannels , a higher zeta potential and an enhanced velocity field may cause the poissonboltzmann model to fail to provide accurate predictions . the ionic diffusion coefficients have little effect on the steady flows for either homogeneously or heterogeneously charged channels . however the ionic valence of solvent has remarkable influences on both the electric potential distribution and the flow velocity even in homogeneously charged microchannels . both theoretical analyses and numerical results indicate that the valence and the concentration of the counter ions dominate the debye length , the electrical potential distribution , and the ions transport . the present results may improve the understanding of the electrokinetic transport characteristics in microchannels .
a non iterative continuous model for switching window computation with crosstalk noise . <eos> proper modeling of switching windows leads to a better estimate of the noise induced delay variations . in this paper , we propose a new non iterative continuous switching model . the proposed new model employs an ordering technique combined with the principle of superposition of linear circuits . the principle of superposition considers the impact of aggressors one after the other . the ordering technique avoids convergence and multiple solution issues in many practical cases . our model surpasses the accuracy of the traditional discrete model and the speed of fixed point iteration method .
vibrational analysis of curved single walled carbon nanotube on a pasternak elastic foundation . <eos> continuum mechanics and an elastic beam model were employed in the nonlinear force vibrational analysis of an embedded , curved , single walled carbon nanotube . the analysis considered the effects of the curvature or waviness and midplane stretching of the nanotube on the nonlinear frequency . by utilizing hes energy balance method ( hebm ) , the relationships of the nonlinear amplitude and frequency were expressed for a curved , single walled carbon nanotube . the amplitude frequency response curves of the nonlinear free vibration were obtained for a curved , single walled carbon nanotube embedded in a pasternak elastic foundation . finally , the influence of the amplitude of the waviness , midplane stretching nonlinearity , shear foundation modulus , surrounding elastic medium , radius , and length of the curved carbon nanotube on the amplitude frequency response characteristics are discussed . as a result , the combination effects of waviness and stretching nonlinearity on the nonlinear frequency of the curved swcnt with a small outer radius were larger than the straight one .
fast bokeh effects using low rank linear filters . <eos> we present a method for faster and more flexible approximation of camera defocus effects given a focused image of a virtual scene and depth map . our method leverages the advantages of low rank linear filtering by reducing the problem of 2d convolution to multiple 1d convolutions , which significantly reduces the computational complexity of the filtering operation . in the case of rank <digit> filters ( e.g. , the box filter and gaussian filter ) , the kernel is described as separable since it can be implemented as a horizontal 1d convolution followed by a 1d vertical convolution . while many filter kernels which result in bokeh effects can not be approximated closely by separable kernels , they can be effectively approximated by low rank kernels . we demonstrate the speed and flexibility of low rank filters by applying them to image blurring , tilt shift postprocessing , and depth of field simulation , and also analyze the approximation error for several aperture shapes .
a novel method for cross species gene expression analysis . <eos> analysis of gene expression from different species is a powerful way to identify evolutionarily conserved transcriptional responses . however , due to evolutionary events such as gene duplication , there is no one to one correspondence between genes from different species which makes comparison of their expression profiles complex .
mimo radar signal design to improve the mimo ambiguity function via maximizing its peak . <eos> transmit signals are designed to maximize the ambiguity function s peak of a ws mimo radar . signal design is done for three cases of single target , multi target , and prioritized ambiguity function . it is shown that in spite of increasing the number of antennas of mimo radar , signal design does not provide diversity gain . through simulations , it is shown that better performance can be achieved by the proposed signal design to maximize the af s peak .
optimal design of radial basis function neural networks for fuzzy rule extraction in high dimensional data . <eos> the design of an optimal radial basis function neural network ( rbfnf ) is not a straightforward procedure . in this paper we take advantage of the functional equivalence between rbfn and fuzzy inference systems to propose a novel efficient approach to rbfn design for fuzzy rule extraction . the method is based on advanced fuzzy clustering techniques . solutions to practical problems are proposed . by combining these different solutions , a general methodology is derived . the efficiency of our method is demonstrated on challenging synthetic and real world data sets .
polarization properties of a turnstile antenna in the vicinity of the human body . <eos> polarization of a simple turnstile antenna situated close to the human body , for potential wban applications at 2.45 ghz band , is studied in detail by the use of electromagnetic simulator wipl d pro . circular polarization of the antenna ( when isolated ) is provided by adjusting the dipole impedances . full size , <digit> dimensional simplified homogeneous model of a human body is applied . polarization of both far and near field is studied , with various positions of the antenna and with without metallic reflector . in the far field significant degradation of the circular polarization , due to the vicinity of the body , was observed . in the near field , at points close to the surface of the torso , polarization ( of vector e ) was found to significantly deviate from circular . obtained results can be useful in designing on body sensor networks in which circularly polarized antennas are applied , for both far field communication between sensor nodes and the gateway and near field communication between sensors .
enforcing and defying associativity , commutativity , totality , and strong noninvertibility for worst case one way functions . <eos> rabi and sherman m. rabi , a. sherman , an observation on associative one way functions in complexity theory , information processing letters <digit> ( <digit> ) ( <digit> ) <digit> <digit> m. rabi , a. sherman , associative one way functions a new paradigm for secret key agreement and digital signatures , tech . rep. cs tr <digit> umiacs tr <digit> <digit> , department of computer science , university of maryland , college park , md , <digit> proved that the hardness of factoring is a sufficient condition for there to exist one way functions ( i.e. , p time computable , honest , p time noninvertible functions this paper is in the worst case model , not the average case model ) that are total , commutative , and associative but not strongly noninvertible . in this paper we improve the sufficient condition to p not equal np . more generally , in this paper we completely characterize which types of one way functions stand or fall together with ( plain ) one way functions equivalently , stand or fall together with p <digit> np . we look at the four attributes used in rabi and sherman 's seminal work on algebraic properties of one way functions ( see m. rabi , a. sherman , an observation on associative one way functions in complexity theory , information processing letters <digit> ( <digit> ) ( <digit> ) <digit> <digit> m. rabi , a. sherman , associative one way functions a new paradigm for secret key agreement and digital signatures , tech . rep. cs tr <digit> umiacs tr <digit> <digit> , department of computer science , university of maryland , college park , md , <digit> ) and subsequent papers strongness ( of noninvertibility ) , totality , commutativity , and associativity and for each attribute , we allow it to be required to hold , required to fail , or do n't care . in this categorization there are <digit> ( <digit> ) <digit> potential types of one way functions . we prove that each of these <digit> feature laden types stands or falls together with the existence of ( plain ) one way functions . ( c ) <digit> elsevier b.v. all rights reserved .
an efficient algorithm for constrained global optimization and application to mechanical engineering design league championship algorithm ( lca ) . <eos> the league championship algorithm ( lca ) is a new algorithm originally proposed for unconstrained optimization which tries to metaphorically model a league championship environment wherein artificial teams play in an artificial league for several weeks ( iterations ) . given the league schedule , a number of individuals , as sport teams , play in pairs and their game outcome is determined given known the playing strength ( fitness value ) along with the team formation ( solution ) . modelling an artificial match analysis , each team devises the required changes in its formation ( a new solution ) for the next week contest and the championship goes for a number of seasons . in this paper , we adapt lca for constrained optimization . in particular ( <digit> ) a feasibility criterion to bias the search toward feasible regions is included besides the objective value criterion ( <digit> ) generation of multiple offspring is allowed to increase the probability of an individual to generate a better solution ( <digit> ) a diversity mechanism is adopted , which allows infeasible solutions with a promising objective value precede the feasible solutions . performance of lca is compared with comparator algorithms on benchmark problems where the experimental results indicate that lca is a very competitive algorithm . performance of lca is also evaluated on well studied mechanical design problems and results are compared with the results of <digit> constrained optimization algorithms . computational results signify that with a smaller number of evaluations , lca ensures finding the true optimum of these problems . these results encourage that further developments and applications of lca would be worth investigating in the future studies .
statistical model training technique based on speaker clustering approach for hmm based speech synthesis . <eos> we propose an average voice model training technique using speaker class . the speaker class is obtained on the basis of speaker clustering . the average voice model is trained using the conventional contextual factors and the speaker class . in the speaker adaptation process , the target speakers speaker class is estimated . our proposal can synthesize speech with better similarity and naturalness .
a study of gradual transition detection in historic film material . <eos> the detection of gradual transitions focuses on two types of approaches unified approaches , i.e. one detector for all gradual transition types , and approaches that use specialized detectors for each gradual transition type . we present an overview on existing methods and extend an existing unified approach for the detection of gradual transitions in historic material . in an experimental study we evaluate our approach on complex and low quality historic material as well as on contemporary material from the trecvid evaluation . additionally we investigate different features , feature combinations and fusion strategies . we observe that the historic material requires the use of texture features in contrast to the contemporary material that in most of the cases requires the use of colour and luminance features .
electronic retention what does your mobile phone reveal about you . <eos> the global information rich society is increasingly dependent on mobile phone technology for daily activities . a substantial secondary market in mobile phones has developed as a result of a relatively short life cycle and recent regulatory measures on electronics recycling . these developments are , however , a cause for concern regarding privacy , since it is unclear how much information is retained on a device when it is re sold . the crucial question is what , despite your best efforts , does your mobile phone reveal about you . this research investigates the extent to which personal information continues to reside on mobile phones even when users have attempted to remove the information hence , passing the information into the secondary market . a total of <digit> re sold mobile devices were acquired from two secondary markets a local pawn shop and an online auction site . these devices were examined using three industry standard mobile forensic toolkits . data were extracted from the devices via both physical and logical acquisitions and the resulting information artifacts categorized by type and sensitivity . all mobile devices examined yielded some user information and in total 11,135 artifacts were recovered . the findings confirm that substantial personal information is retained on a typical mobile device when it is re sold . the results highlight several areas of potential future work necessary to ensure the confidentially of personal data stored on mobile devices .
reasoning about digital artifacts with acl2 . <eos> acl2 is both a programming language in which computing systems can be modeled and a tool to help a designer prove properties of such models . acl2 stands for a c omputational l ogic for a pplicative c ommon l isp '' and provides mechanized reasoning support for a first order axiomatization of an extended subset of functional common lisp . most often , acl2 is used to produce operational semantic models of artifacts . such models can be executed as functional lisp programs and so have dual use as both pre fabrication simulation engines and as analyzable mathematical models of intended ( or at least designed ) behavior . this project had its start <digit> years ago in edinburgh with the first boyer moore pure lisp theorem prover and has evolved proofs about list concatenation and reverse to proofs about industrial models . industrial use of theorem provers to answer design questions of critical importance is so surprising to people outside of the theorem proving community that it bears emphasis . in the 1980s , the earlier boyer moore theorem prover , nqthm , was used to verify the computational logic stack '' a hardware software stack starting with the ndl description of the netlist for a microprocessor and ascending through a machine code isa , an assembler , linker , and loader , two compilers ( for subsets of pascal and lisp ) , an operating system , and some simple applications . the system components were proved to compose so that properties proved of high level software were guaranteed by the binary image produced by the composition . at around the same time , nqthm was used to verify <digit> of the <digit> subroutines in the mc68020 binary machine code produced from the berkeley c string library by gcc o , identifying bugs in the library as a result . applications like these convinced us that ( a ) industrial scale formal methods was practical and ( b ) nqthm 's pure lisp produced uncompetitive results compared to c when used for simulation engines . we therefore designed acl2 , which initially was nqthm recoded to support applicative common lisp . the 1990s saw the first industrial application of acl2 , to verify the correspondence between a gate level description of the motorola cap dsp and its microcode engine . the lisp model of the microcode engine was proved to be bit and cycle accurate but operated several times faster than the gate level simulator in c because of the competitive execution speed of lisp and the higher level of trusted abstraction . furthermore , it was used to discover previously unknown microcode hazards . an executable lisp predicate was verified to detect all hazards and subsequently used by microcode programmers to check code . this project and a subsequent one at amd to verify the floating point division operation on the amd k5 microprocessor demonstrated the practicality of acl2 but also highlighted the need to develop better lisp system programming tools wedded to formal methods , formal modeling , proof development , and proof maintenance '' in the face of evolution of the modeled artifacts . much acl2 development in first decade of the 21st century was therefore dedicated to such tools and we have witnessed a cor responding increase in the use of acl2 to construct and reason about commercial artifacts . acl2 has been involved in the design of all amd desktop microprocessors since the athlon specifically , acl2 is used to verify floating point operations on those micro processors . centaur technology ( chipmaker for via technologies ) uses acl2 extensively in verifying its media unit and other parts of its x86 designs . researchers at rockwell collins have shown that acl2 models of microprocessors can run at <digit> % of the speed of c models of those microprocessors . rockwell collins has also used acl2 to do information flow proofs to establish process separation for the aamp7g cryptoprocessor and , on the basis of those proofs , obtained mils certification using formal methods techniques as specified by eal <digit> of the common criteria . ibm has used acl2 to verify floating point operations on the power <digit> and other chips . acl2 was also used to verify key properties of the sun java virtual machine 's class loader . in this talk i will sketch the <digit> year history of this project , showing how the techniques and applications have grown over the years . i will demonstrate acl2 on both some simple prob lems and a complicated one , and i will deal briefly with the question of how and with what tool one verifies a verifier . for scholarly details of some of how to use acl2 and some of its industrial applications see <digit> , <digit> . for source code , lemma li braries , and an online user 's manual , see the acl2 home page , http www.cs.utexas.edu users moore acl2 .
deformation and fracturing using adaptive shape matching with stiffness adjustment . <eos> this paper presents a fast method that computes deformations with fracturing of an object using a hierarchical lattice . our method allows numerically stable computation based on so called shape matching . during the simulation , the deformed shape of the object and the condition of fracturing are used to determine the appropriate detail level in the hierarchy of the lattices . our method modifies the computation of the stiffness of the object in different levels of the hierarchy so that the stiffness is maintained uniform by introducing a stiffness parameter that does not depend on the hierarchy . by merging the subdivided lattices , our method minimizes the increase of computational cost . copyright ( c ) <digit> john wiley sons , ltd .
using bp network for ultrasonic inspection of flip chip solder joints . <eos> flip chip technology has been used extensively in microelectronic packaging , where defect inspection for solder joints plays an extremely important role . in this paper , ultrasonic inspection , one of the non destructive methods , was used for inspection of flip chip solder joints . the image of the flip chip was captured by scanning acoustic microscope and segmented based on the flip chip structure information . then a back propagation network was adopted , and the geometric features extracted from the image were fed to the network for classification and recognition . the results demonstrate the high recognition rate and feasibility of the approach . therefore , this approach has high potentiality for solder joint defect inspection in flip chip packaging .
augmenting reflective middleware with an aspect orientation support layer . <eos> reflective middleware provides an effective way to support adaptation in distributed systems . however , as distributed systems become increasingly complex , certain drawbacks of the reflective middleware approach are becoming evident . in particular , reflective apis are found to impose a steep learning curve , and to place too much expressive power in the hands of developers . recently , researchers in the field of aspect oriented programming ( aop ) have argued that ' dynamic aspects ' show promise in alleviating these drawbacks . in this paper , we report on work that attempts to combine the reflective middleware and aop approaches . we build an aop support layer on top of an underlying reflective middleware substrate in such a way that it can be dynamically deployed undeployed where and when required , and imposes no overhead when it is not used . our aop approach involves aspects that can be dynamically ( un ) weaved across a distributed system on the basis of pointcut expressions that are inherently distributed in nature , and it supports the composition of advice that is remote from the advised joinpoint . an overall goal of the work is to effectively combine reflective middleware and aop in a way that maximises the benefits and minimises the drawbacks of each .
band pass filtering of the time sequences of spectral parameters for robust wireless speech recognition . <eos> in this paper we address the problem of automatic speech recognition when wireless speech communication systems are involved . in this context , three main sources of distortion should be considered acoustic environment , speech coding and transmission errors . whilst the first one has already received a lot of attention , the last two deserve further investigation in our opinion . we have found out that band pass filtering of the recognition features improves asr performance when distortions due to these particular communication systems are present . furthermore , we have evaluated two alternative configurations at different bit error rates ( ber ) typical of these channels band pass filtering the lp mfcc parameters or a modification of the rasta plp using a sharper low pass section perform consistently better than lp mfcc and rasta plp , respectively .
msoar a high throughput ortholog assignment system based on genome rearrangement . <eos> the assignment of orthologous genes between a pair of genomes is a fundamental and challenging problem in comparative genomics , since many computational methods for solving various biological problems critically rely on bona fide orthologs as input . while it is usually done using sequence similarity search , we recently proposed a new combinatorial approach that combines sequence similarity and genome rearrangement . this paper continues the development of the approach and unites genome rearrangement events and ( post speciation ) duplication events in a single framework under the parsimony principle . in this framework , orthologous genes are assumed to correspond to each other in the most parsimonious evolutionary scenario involving both genome rearrangement and ( post speciation ) gene duplication . besides several original algorithmic contributions , the enhanced method allows for the detection of inparalogs . following this approach , we have implemented a high throughput system for ortholog assignment on a genome scale , called msoar , and applied it to human and mouse genomes . as the result will show , msoar is able to find <digit> more true orthologs than the inparanoid program did . in comparison to the iterated exemplar algorithm on simulated data , msoar performed favorably in terms of assignment accuracy . we also validated our predicted main ortholog pairs between human and mouse using public ortholog assignment datasets , synteny information , and gene function classification . these test results indiate that our approach is very promising for genome wide ortholog assignment . supplemental material and msoar program are available at http msoar.cs.ucr.edu .
medlda maximum margin supervised topic models . <eos> a supervised topic model can use side information such as ratings or labels associated with documents or images to discover more predictive low dimensional topical representations of the data . however , existing supervised topic models predominantly employ likelihood driven objective functions for learning and inference , leaving the popular and potentially powerful max margin principle unexploited for seeking predictive representations of data and more discriminative topic bases for the corpus . in this paper , we propose the maximum entropy discrimination latent dirichlet allocation ( medlda ) model , which integrates the mechanism behind the max margin prediction models ( e.g. , svms ) with the mechanism behind the hierarchical bayesian topic models ( e.g. , lda ) under a unified constrained optimization framework , and yields latent topical representations that are more discriminative and more suitable for prediction tasks such as document classification or regression . the principle underlying the medlda formalism is quite general and can be applied for jointly max margin and maximum likelihood learning of directed or undirected topic models when supervising side information is available . efficient variational methods for posterior inference and parameter estimation are derived and extensive empirical studies on several real data sets are also provided . our experimental results demonstrate qualitatively and quantitatively that medlda could <digit> ) discover sparse and highly discriminative topical representations <digit> ) achieve state of the art prediction performance and <digit> ) be more efficient than existing supervised topic models , especially for classification .
ls svm based image segmentation using pixel color texture descriptors . <eos> image segmentation remains an important , but hard to solve , problem since it appears to be application dependent with usually no a priori information available regarding the image structure . moreover , the increasing demands of image analysis tasks in terms of segmentation results quality introduce the necessity of employing multiple cues for improving image segmentation results . in this paper , we present a least squares support vector machine ( ls svm ) based image segmentation using pixel color texture descriptors , in which multiple cues such as edge saliency , color saliency , local maximum energy , and multiresolution texture gradient are incorporated . firstly , the pixel level edge saliency and color saliency are extracted based on the spatial relations between neighboring pixels in hsv color space . secondly , the image pixels texture features , local maximum energy and multiresolution texture gradient , are represented via nonsubsampled contourlet transform . then , both the pixel level edge color saliency and texture features are used as input of ls svm model ( classifier ) , and the ls svm model ( classifier ) is trained by selecting the training samples with arimoto entropy thresholding . finally , the color image is segmented with the trained ls svm model ( classifier ) . this image segmentation not only can fully take advantage of the human visual attention and local texture content of color image , but also the generalization ability of ls svm classifier . experimental results show that our proposed method has very promising segmentation performance compared with the state of the art segmentation approaches recently proposed in the literature .
geometric verification of swirling features in flow fields . <eos> in this paper , we present a verification algorithm for swirling features in flow fields , based on the geometry of streamlines . the features of interest in this case are vortices . without a formal definition , existing detection algorithms lack the ability to accurately identify these features , and the current method for verifying the accuracy of their results is by human visual inspection . our verification algorithm addresses this issue by automating the visual inspection process . it is based on identifying the swirling streamlines that surround the candidate vortex cores . we apply our algorithm to both numerically simulated and procedurally generated datasets to illustrate the efficacy of our approach .
on topology and dynamics of consensus among linear high order agents . <eos> consensus of a group of agents in a multi agent system with and without a leader is considered . all agents are modelled by identical linear n th order dynamical systems while the leader , when it exists , may evolve according to a different linear model of the same order . the interconnection topology between the agents is modelled as a directed weighted graph . we provide answers to the questions of whether the group converges to consensus and what consensus value the group eventually reaches . to that end , we give a detailed analysis of relevant algebraic properties of the graph laplacian . furthermore , we propose an lmi based design for group consensus in the general case .
human cognition in manual assembly theories and applications . <eos> human cognition in production environments is analyzed with respect to various findings and theories in cognitive psychology . this theoretical overview describes effects of task complexity and attentional demands on both mental workload and task performance as well as presents experimental data on these topics . a review of two studies investigating the benefit of augmented reality and spatial cueing in an assembly task is given . results demonstrate an improvement in task performance with attentional guidance while using contact analog highlighting . improvements were obvious in reduced performance times and eye fixations as well as in increased velocity and acceleration of reaching and grasping movements . these results have various implications for the development of an assistive system . future directions in this line of applied research are suggested . the introduced methodology illustrates how the analysis of human information processes and psychological experiments can contribute to the evaluation of engineering applications .
enabling warping on stereoscopic images . <eos> warping is one of the basic image processing techniques . directly applying existing monocular image warping techniques to stereoscopic images is problematic as it often introduces vertical disparities and damages the original disparity distribution . in this paper , we show that these problems can be solved by appropriately warping both the disparity map and the two images of a stereoscopic image . we accordingly develop a technique for extending existing image warping algorithms to stereoscopic images . this technique divides stereoscopic image warping into three steps . our method first applies the user specified warping to one of the two images . our method then computes the target disparity map according to the user specified warping . the target disparity map is optimized to preserve the perceived 3d shape of image content after image warping . our method finally warps the other image using a spatially varying warping method guided by the target disparity map . our experiments show that our technique enables existing warping methods to be effectively applied to stereoscopic images , ranging from parametric global warping to non parametric spatially varying warping .
markov chain modeling of intermittency chaos and its application to hopfield nn . <eos> in this study , a modeling method of the intermittency chaos using the markov chain is proposed . the performances of the intermittency chaos and the markov chain model are investigated when they are injected to the hopfield neural network for a quadratic assignment problem or an associative memory . computer simulated results show that the proposed modeling is good enough to gain similar performance of the intermittency chaos .
splitting integrators for nonlinear schrodinger equations over long times . <eos> conservation properties of a full discretization via a spectral semi discretization in space and a lie trotter splitting in time for cubic schrodinger equations with small initial data ( or small nonlinearity ) are studied . the approximate conservation of the actions of the linear schrodinger equation , energy , and momentum over long times is shown using modulated fourier expansions . the results are valid in arbitrary spatial dimension .
generalized scans and tridiagonal systems . <eos> motivated by the analysis of known parallel techniques for the solution of linear tridiagonal system , we introduce generalized scans , a class of recursively defined length preserving , sequence to sequence transformations that generalize the well known prefix computations ( scans ) . generalized scan functions are described in terms of three algorithmic phases , the reduction phase that saves data for the third of expansion phase and prepares data for the second phase which is a recursive invocation of the same function on one fewer variable . both the reduction and expansion phases operate on bounded number of variables , a key feature for their parallelization . generalized scans enjoy a property , called here protoassociativity , that gives rise to ordinary associativity when generalized scans are specialized to ordinary scans . we show that the solution of positive definite block tridiagonal linear systems can be cast as a generalized scan , thereby shedding light on the underlying structure enabling known parallelization schemes for this problem . we also describe a variety of parallel algorithms including some that are well known for tridiagonal systems and some that are much better suited to distributed computation . ( c ) <digit> elsevier science b.v. all rights reserved .

efficient normal basis multipliers in composite fields . <eos> it is well known that a class of finite fields gf ( <digit> ( n ) ) using an optimal normal basis is most suitable for a hardware implementation of arithmetic in finite fields . in this paper , we introduce composite fields of some hardware applicable properties resulting from the normal basis representation and the optimal condition . we also present a hardware architecture of the proposed composite fields including a hit parallel multiplier .
a stable second order scheme for fluidstructure interaction with strong added mass effects . <eos> in this paper , we present a stable second order time accurate scheme for solving fluidstructure interaction problems . the scheme uses so called combined field with explicit interface ( cfei ) advancing formulation based on the arbitrary lagrangianeulerian approach with finite element procedure . although loosely coupled partitioned schemes are often popular choices for simulating fsi problems , these schemes may suffer from inherent instability at low structure to fluid density ratios . we show that our second order scheme is stable for any mass density ratio and hence is able to handle strong added mass effects . energy based stability proof relies heavily on the connections among extrapolation formula , trapezoidal scheme for second order equation , and backward difference method for first order equation . numerical accuracy and stability of the scheme is assessed with the aid of two dimensional fluidstructure interaction problems of increasing complexity . we confirm second order temporal accuracy by numerical experiments on an elastic semi circular cylinder problem . we verify the accuracy of coupled solutions with respect to the benchmark solutions of a cylinder elastic bar and the navierstokes flow system . to study the stability of the proposed scheme for strong added mass effects , we present new results using the combined field formulation for flexible flapping motion of a thin membrane structure with low mass ratio and strong added mass effects in a uniform axial flow . using a systematic series of fluidstructure simulations , a detailed analysis of the coupled response as a function of mass ratio for the case of very low bending rigidity has been presented .
network information flow . <eos> a formal model for an analysis of an information flow in interconnection networks is presented . it is based on timed process algebra which can express also network properties . the information flow is based on a concept of deducibility on composition . robustness of systems against network timing attacks is defined . a variety of different security properties which reflect different security requirements are defined and investigated .
interactive reduct evolutional computation for aesthetic design . <eos> we propose a method of evolving designs based on the user 's personal preferences . the method works through an interaction between the user and a computer system . the method 's objective is to help the customer to set design parameters via a simple evaluation of displayed samples . an important feature is that the design attributes to which the user pays more attention ( favored features ) are estimated using reducts in rough set theory and reflected when refining the design . new design candidates are generated by the user 's evaluation of design samples generated at random . the values of attributes estimated as favored features are fixed in the refined samples , while other attributes are generated at random . this interaction continues until the samples converge to a satisfactory design . in this manner , the design process efficiently evaluates personal and subjective preferences . the method is applied to design a 3d cylinder model such as a cup or vase . the method is then compared with an interactive ga .
a multiple criteria sorting method where each category is characterized by several reference actions the electre tri nc method . <eos> this paper presents electre tri nc , a new sorting method which takes into account several reference actions for characterizing each category . this new method gives a particular freedom to the decision maker in the co construction decision aiding process with the analyst to characterize the set of categories , while there is no constraint for introducing only one reference action as typical of each category like in electre tri c ( almeida dias et al. , <digit> ) . as in such a sorting method , this new sorting method is composed of two joint rules . electre tri nc also fulfills a certain number of natural requirements . additional results on the behavior of the new method are also provided in this paper , namely the ones with respect to the addition or removal of the reference actions used for characterizing a certain category . a numerical example illustrates the manner in which electre tri nc can be used by a decision maker . a comparison with some related sorting procedures is presented and it allows to conclude that the new method is appropriate to deal with sorting problems .
estimating unique solutions of dc transistor circuits . <eos> for each natural n let f n denote the collection of mappings of r n onto itself defined by f is an element of f u if and only if there exist n strictly monotone increasing functions f ( k ) mapping r onto itself such that for each x x ( <digit> ) , ... , x ( n ) ( t ) is an element of r n , f ( x ) f ( <digit> ) ( x ( <digit> ) ) , ... , f ( n ) ( x ( n ) ) ( t ) . the following new property of the class p <digit> of matices is proved a real n x n matrix a belongs to p <digit> if and only if for every g , h is an element of f n the set s <digit> x is an element of r n g ( x ) less than or equal to ax less than or equal to h ( x ) is bounded . as an illustration of this property a method of estimating the unique solution of the nonlinear equation f ( x ) a ( x ) b describing the large class of dc transistor circuits is developed . this can improve the efficiency of known computation algorithms . numerical examples of transistor circuits illustrate in detail how the method works in practice .
multiple topic identification in human human conversations . <eos> a multiple classification methods for multiple theme hypothesization is proposed . four methods , one of which is new , are initially used and separately evaluated . a new sequential decision strategy for multiple theme hypothesization is introduced . a new hypothesis refinancing component is presented , based on asr word lattice . results show that the strategy makes it possible to obtain reliable service surveys .
towards a documentation maturity model . <eos> this paper presents preliminary work towards a maturity model for system documentation . the documentation maturity model ( dmm ) is specifically targeted towards assessing the quality of documentation used in aiding program understanding . software engineers and technical writers produce such documentation during regular product development lifecycles . the documentation can also be recreated after the fact via reverse engineering . the dmm has both process and product components this paper focuses on the product quality aspects .
numerical representation of product transitive complete fuzzy orderings . <eos> let x be a space of alternatives with a preference relation in the form of product transitive complete fuzzy ordering r. we prove existence of continuous utility functions for r. ( c ) <digit> elsevier ltd. all rights reserved .
design of wdm rof pon based on ofdm and optical heterodyne . <eos> in this paper , we propose a wdm radio over fiber ( rof ) passive optical network ( pon ) based on orthogonal frequency division multiplexing ( ofdm ) and optical heterodyne . with ofdm and coherent receiving technology , the system achieves high , elastic bandwidth allocation and excellent transporting property . using optical heterodyne , the network implements the wireless access without adding a radio source . we evaluate the performance of the system in terms of bit error rate , coverage area , and receiving eye diagram and obtain the network as an excellent wire wireless access property .
image retrieval via isotropic and anisotropic mappings . <eos> this paper presents an approach for content based image retrieval via isotropic and anisotropic mappings . isotropic mappings are defined as mappings invariant to the action of the planar euclidean group on the image spaceinvariant to the translation , rotation and reflection of image data , and hence , invariant to orientation and position . anisotropic mappings , on the other hand , are defined as those mappings that are correspondingly variant . structure extraction ( via a perceptual grouping process ) and color histogram are shown to be representations of isotropic mappings . texture analysis using a channel energy model comprised of even symmetric gabor filters is considered to be a representation of anisotropic mapping . an integration framework for these mappings is developed . results of retrieval of outdoor images by query and by classification using a nearest neighbor classifier are presented .
physical gestures for abstract concepts inclusive design with primary metaphors . <eos> designers in inclusive design are challenged to create interactive products that cater for a wide range of prior experiences and cognitive abilities of their users . but suitable design guidance for this task is rare . this paper proposes the theory of primary metaphor and explores its validity as a source of design guidance . primary metaphor theory describes how basic mental representations of physical sensorimotor experiences are extended to understand abstract domains . as primary metaphors are subconscious mental representations that are highly automated , they should be robustly available to people with differing levels of cognitive ability . their proposed universality should make them accessible to people with differing levels of prior experience with technology . these predictions were tested for <digit> primary metaphors that predict relations between spatial gestures and abstract interactive content . in an empirical study , <digit> participants from two age groups ( young and old ) were asked to produce two dimensional touch and three dimensional free form gestures in response to given abstract keywords and spatial dimensions of movements . the results show that across age groups in <digit> % of all cases users choose gestures that confirmed the predictions of the theory . although the two age groups differed in their cognitive abilities and prior experience with technology , overall they did not differ in the amount of metaphor congruent gestures they made . as predicted , only small or zero correlations of metaphor congruent gestures with prior experience or cognitive ability could be found . the results provide a promising step toward inclusive design guidelines for gesture interaction with abstract content on mobile multitouch devices . ( c ) <digit> elsevier b.v. all rights reserved .
mbs zone configuration schemes for wireless multicast and broadcast service . <eos> the multicast broadcast service ( mbs ) zone technology is proposed to provide mbs with high qos on mobile communications networks ( mcns ) . an mbs zone consists of a group of base stations ( bss ) synchronized to transmit the same mbs content using the same multicasting channel , which potentially reduces the time delay for mobile stations ( mss ) to handoff between different bss in the same mbs zone . however , significant time delay still incurs while mss handoff between different bss belonging to different mbs zones ( i.e. , the inter mbs zone handoff ) . to reduce the possibility for the inter mbs zone handoff , we may increase the size of an mbs zone ( i.e. , more bss contained in an mbs zone ) , which may result in poor multicasting channel utilization . this paper proposes the overlapping scheme ( ols ) and the enhanced overlapping scheme ( eols ) for more flexible mbs zone configuration to get better performance for mbs in terms of qos and radio resource utilization . we propose the analytical models for the original mbs zone technology ( namely the basic scheme ) , and the ols scheme , which are validated against the simulation experiments . based on the simulation results , we investigate the performance for the basic scheme , the ols scheme , and the eols scheme . copyright ( c ) <digit> john wiley sons , ltd .

olsr aware channel access scheduling in wireless mesh networks . <eos> wireless mesh networks ( wmns ) have emerged as a key technology having various advantages , especially in providing cost effective coverage and connectivity solutions in both rural and urban areas . wmns are typically deployed as backbone networks , usually employing spatial tdma ( stdma ) based access schemes which are suitable for the high traffic demands of wmns . this paper aims to achieve higher utilization of the network capacity and thereby aims to increase the application layer throughput of stoma based wmns . the central idea is to use optimized link state routing ( olsr ) specific routing layer information in link layer channel access schedule formation . this paper proposes two stdma based channel access scheduling schemes ( one distributed , one centralized ) that exploit olsr specific information to improve the application layer throughput without introducing any additional messaging overhead . to justify the contribution of using olsr specific information to the throughput , the proposed schemes are compared against one another and against their non olsr aware versions via extensive ns <digit> simulations . our simulation results verify that utilizing olsr specific information significantly improves the overall network performance both in distributed and in centralized schemes . the simulation results further show that olsr aware scheduling algorithms attain higher end to end throughput although their non olsr aware counterparts achieve higher concurrency in slot allocations . ( c ) <digit> elsevier inc. all rights reserved .
privacy preserving indexing of documents on the network . <eos> with the ubiquitous collection of data and creation of large distributed repositories , enabling search over this data while respecting access control is critical . a related problem is that of ensuring privacy of the content owners while still maintaining an efficient index of distributed content . we address the problem of providing privacy preserving search over distributed access controlled content . indexed documents can be easily reconstructed from conventional ( inverted ) indexes used in search . currently , the need to avoid breaches of access control through the index requires the index hosting site to be fully secured and trusted by all participating content providers . this level of trust is impractical in the increasingly common case where multiple competing organizations or individuals wish to selectively share content . we propose a solution that eliminates the need of such a trusted authority . the solution builds a centralized privacy preserving index in conjunction with a distributed access control enforcing search protocol . two alternative methods to build the centralized index are proposed , allowing trade offs of efficiency and security . the new index provides strong and quantifiable privacy guarantees that hold even if the entire index is made public . experiments on a real life dataset validate performance of the scheme . the appeal of our solution is twofold ( a ) content providers maintain complete control in defining access groups and ensuring its compliance , and ( b ) system implementors retain tunable knobs to balance privacy and efficiency concerns for their particular domains .
cross layer optimization for efficient data aggregation in multi hop wireless sensor networks . <eos> wireless sensor networks ( wsn ) is the most promising technological paradigm to support the next generation highly efficient emergency management systems . optimal design of wsn involves all the layers of the protocol stack from the physical ( phy ) , the medium access layer ( mac ) to the application layer . the design problem is conveniently cast in this paper for linear sensor network topologies where the terminals are equidistantly placed on the line between the source and the destination and are monitoring a correlated field . this simple topology can be adopted to provide insights to the performance of multihop networks used in several applications as monitoring systems , acoustic sensor arrays , seismic systems etc. . . the paper provides an analytical tool for performance analysis that takes into account both the statistical properties of the monitored field ( spatial and temporal correlation ) , the phy layer transceiver design ( rf power allocation and modulation ) and the medium access ( duty cycle , routing ) .
rank order polynomial subband decomposition for medical image compression . <eos> in this paper , the problem of progressive lossless image coding is addressed , a nonlinear decomposition for progressive lossless compression is presented . the decomposition into subbands is called rank order polynomial decomposition ( ropd ) according to the polynomial prediction models used . the decomposition method presented here is a further development and generalization of the morphological subband decomposition ( msd ) introduced earlier by the same research group . it is shown that ropd provides similar or slightly better results than the compared coding schemes such as the codec based on set partitioning in hierarchical trees ( spiht ) and the codec based on wavelet trellis coded quantization ( wtcq ) . our proposed method highly outperforms the standard jpeg . the proposed lossless compression scheme has the functionality of having a completely embedded bit stream , which allows for data browsing . it is shown that the ropd has a better lossless rate than the msd but it has also a much better browsing quality when only a part of the bit stream is decompressed . finally , the possibility of hybrid lossy lossless compression is presented using ultrasound images . as with other compression algorithms , considerable gain can be obtained if only the regions of interest are compressed losslessly .
linearn a new approach to nearest neighbour density estimator . <eos> reject the premise that a nn algorithm must find the nn for every instance . the first nn density estimator that has o ( n ) o ( n ) time complexity and o ( <digit> ) o ( <digit> ) space complexity . these complexities are achieved without using any indexing scheme . our asymptotic analysis reveals that it trades off between bias and variance . easily scales up to large data sets in anomaly detection and clustering tasks .
beauty or realism the dimensions of skin from cognitive sciences to computer graphics . <eos> as the most visible interface between the individual and the others , the skin is a key element of visually carried inter individual social information , since skin displays a wide array of information regarding gender , age , or health status . adequate skin perception is central in individual identification and social interactions . this topic elicited marked interest in artists since the first development of visual arts in antiquity . often performed in order to identify the biological correlates of attractiveness , psychological research on skin perception made a jump forward with the development of virtual image synthesis . here , we investigate how advances in both computer graphics and the psychology of skin perception may be turned to use in real time virtual worlds . we propose a model of skin perception based both on purely physical dimensions such as color , texture , and symmetry , and on dimensions carrying socially oriented information , such as perceived youth ( information regarding putative fertility ) , markers of sexual dimorphism ( information regarding hormonal status ) , and level of oxygenation ( information regarding health status ) . it appears that for almost all of the dimensions of skin , maximal attractiveness and realism are the two opposite extremities of a single perceptive continuum .
fault diagnosis by locality preserving discriminant analysis and its kernel variation . <eos> linear discriminant analysis ( lda ) and its nonlinear kernel variation generalized discriminant analysis ( gda ) are the most popular supervised dimensionality reduction methods for fault diagnosis . however , we argue that they probably provide suboptimal results for fault diagnosis due to the fisher 's criterion they use . this paper proposes a new supervised dimensionality reduction method named locality preserving discriminant analysis ( lpda ) and its kernel variation kernel lpda ( klpda ) for fault diagnosis . ( k ) lpda maximizes a new criterion such that local discriminant structure and local geometric structure in data are optimally preserved simultaneously in each dimension of the reduced space . the criterion directly targets at minimizing local overlapping between different classes . extensive simulations on the tennessee eastman ( te ) benchmark simulation process and a waste water treatment plant ( wwtp ) clearly demonstrate the superiority of our methods in terms of misclassification rate and making use of extra training data . ( c ) <digit> elsevier ltd. all rights reserved .
wireless distributed computing in cognitive radio networks . <eos> individual cognitive radio nodes in an ad hoc cognitive radio network ( crn ) have to perform complex data processing operations for several purposes , such as situational awareness and cognitive engine ( ce ) decision making . in an implementation point of view , each cognitive radio ( cr ) may not have the computational and power resources to perform these tasks by itself . in this paper , wireless distributed computing ( wdc ) is presented as a technology that enables multiple resource constrained nodes to collaborate in computing complex tasks in a distributed manner . this approach has several benefits over the traditional approach of local computing , such as reduced energy and power consumption , reduced burden on the resources of individual nodes , and improved robustness . however , the benefits are negated by the communication overhead involved in wdc . this paper demonstrates the application of wdc to crns with the help of an example ce processing task . in addition , the paper analyzes the impact of the wireless environment on wdc scalability in homogeneous and heterogeneous environments . the paper also proposes a workload allocation scheme that utilizes a combination of stochastic optimization and decision tree search approaches . the results show limitations in the scalability of wdc networks , mainly due to the communication overhead involved in sharing raw data pertaining to delegated computational tasks .
advertisement timeout driven bee 's mating approach to maintain fair energy level in sensor networks . <eos> in wireless sensor network , dynamic cluster based routing approach is widely used . such practiced approach , quickly depletes the energy of cluster heads and induces the execution of frequent re election algorithm . this repeated cluster head re election algorithm increases the number of advertisement messages , which in turn depletes the energy of overall sensor network . here , we proposed the advertisement timeout driven bee 's mating approach ( atdbma ) that reduces the cluster set up communication overhead and elects the standby node in advance for current cluster head , which has the capability to withstand for many rounds . our proposed atdbma method uses the honeybee mating behaviour in electing the standby node for current cluster head . this approach really outperforms the other methods in achieving reduced number of re election and maintaining fair energy nodes between the rounds .

protection against soft errors in the space environment a finite impulse response ( fir ) filter case study . <eos> the problem of radiation is a key issue in space applications , since it produces several negative effects on digital circuits . considering the high reliability expected in these systems , many techniques have been proposed to mitigate these effects . however , traditional protection techniques against soft errors , like triple modular redundancy ( tmr ) or edac codes ( for example hamming ) , normally result in a significant area and power overhead . in this paper we propose a specific technique to protect digital finite impulse response ( fir ) filters applying the system knowledge . this means to study and use the singularities in their structure in order to provide effective protection with minimal area and power . the results obtained in the experimental process have been compared with the protection offered by tmr and hamming codes , in order to prove the quality of the proposed solution .
minimizing the dynamic and sub threshold leakage power consumption using least leakage vector assisted technology mapping . <eos> power consumption due to the temperature dependent leakage current becomes a dominant part of the total power dissipation in systems using nanometer scale process technology . to obtain the minimum power consumption for different operating conditions , logic synthesis tools are required to take into consideration the leakage power as well as the operating characteristics during the optimization . conventional logic synthesis flows consider dynamic power only and use an over simplified cost function in modeling the total power consumption of the logic network . in this paper , we propose a complete model of the total power consumption of the logic network , which includes both the active and standby sub threshold leakage power , and the operating duty cycle of the applications . we also propose a least leakage vector ( llv ) assisted technology mapping algorithm to optimize the total power of the final mapped network . instead of finding the llv after the logic network is synthesized and mapped , we use the llv found in the technology decomposed network to help in obtaining the lowest total power match during technology mapping . experimental results on mcnc benchmarks show that on average more than <digit> % reduction in total power consumption is obtained comparing with the conventional low power technology mapping algorithm .
chaos breeds autonomy connectionist design between bias and baby sitting . <eos> in connectionism and its offshoots , models acquire functionality through externally controlled learning schedules . this undermines the claim of these models to autonomy . providing these models with intrinsic biases is not a solution , as it makes their function dependent on design assumptions . between these two alternatives , there is room for approaches based on spontaneous self organization . structural reorganization in adaptation to spontaneous activity is a well known phenomenon in neural development . it is proposed here as a way to prepare connectionist models for learning and enhance the autonomy of these models .
an improvement on the complexity of factoring read once boolean functions . <eos> read once functions have gained recent , renewed interest in the fields of theory and algorithms of boolean functions , computational learning theory and logic design and verification . in an earlier paper m.c. golumbic , a. mintz , u. rotics , factoring and recognition of read once functions using cographs and normality , and the readability of functions associated with partial k trees , discrete appl . math . <digit> ( <digit> ) <digit> , we presented the first polynomial time algorithm for recognizing and factoring read once functions , based on a classical characterization theorem of gurvich which states that a positive boolean function is read once if and only if it is normal and its co occurrence graph is p4 p <digit> free . in this note , we improve the complexity bound by showing that the method can be modified slightly , with two crucial observations , to obtain an o ( n f ) o ( n f ) implementation , where f f denotes the length of the dnf expression of a positive boolean function f , and n is the number of variables in f . the previously stated bound was o ( n2k ) o ( n <digit> k ) , where k is the number of prime implicants of the function . in both cases , f is assumed to be given as a dnf formula consisting entirely of the prime implicants of the function .
a dutch medical language processor part ii evaluation . <eos> this paper provides a preliminary evaluation of a general dutch medical language processor ( dmlp ) . four examples of different potential applications ( based on different linguistic modules ) are presented , each with its own evaluation method . finally , a critical review of the used evaluation methods is offered according to the state of the art in medical language processing .
privacy preserving distributed network troubleshooting bridging the gap between theory and practice . <eos> today , there is a fundamental imbalance in cybersecurity . while attackers act more andmore globally and coordinated , network defense is limited to examine local information only due to privacy concerns . to overcome this privacy barrier , we use secure multiparty computation ( mpc ) for the problem of aggregating network data from multiple domains . we first optimize mpc comparison operations for processing high volume data in near real time by not enforcing protocols to run in a constant number of synchronization rounds . we then implement a complete set of basic mpc primitives in the sepia library . for parallel invocations , sepia 's basic operations are between <digit> and several hundred times faster than those of comparable mpc frameworks . using these operations , we develop four protocols tailored for distributed network monitoring and security applications the entropy , distinct count , event correlation , and top k protocols . extensive evaluation shows that the protocols are suitable for near real time data aggregation . for example , our top k protocol pptks accurately aggregates counts for 180,000 distributed ip addresses in only a few minutes . finally , we use sepia with real traffic data from <digit> customers of a backbone network to collaboratively detect , analyze , and mitigate distributed anomalies . our work follows a path starting from theory , going to system design , performance evaluation , and ending with measurement . along this way , it makes a first effort to bridge two very disparate worlds mpc theory and network monitoring and security practices .
better gp benchmarks community survey results and proposals . <eos> we present the results of a community survey regarding genetic programming benchmark practices . analysis shows broad consensus that improvement is needed in problem selection and experimental rigor . while views expressed in the survey dissuade us from proposing a large scale benchmark suite , we find community support for creating a blacklist of problems which are in common use but have important flaws , and whose use should therefore be discouraged . we propose a set of possible replacement problems .
a tabular steganography scheme for graphical password authentication . <eos> authentication , authorization and auditing are the most important issues of security on data communication . in particular , authentication is the life of every individual essential closest friend . the user authentication security is dependent on the strength of user password . a secure password is usually random , strange , very long and difficult to remember . for most users , remember these irregular passwords are very difficult . to easily remember and security are two sides of one coin . in this paper , we propose a new graphical password authentication protocol to solve this problem . graphical password authentication technology is the use of click on the image to replace input some characters . the graphical user interface can help user easy to create and remember their secure passwords . however , in the graphical password system based on images can provide an alternative password , but too many images will be a large database to store issue . all the information can be steganography to achieve our scheme to solve the problem of database storage . furthermore , tabular steganography technique can achieve our scheme to solve the information eavesdropping problem during data transmission . our modified graphical password system can help user easily and friendly to memorize their password and without loss of any security of authentication . user 's chosen input will be hidden into image using steganography technology , and will be transferred to server security without any hacker problem . and then , our authentication server only needs to store only a secret key for decryption instead of large password database .
a framework for optimal correction of inconsistent linear constraints . <eos> the problem of inconsistency between constraints often arises in practice as the result , among others , of the complexity of real models or due to unrealistic requirements and preferences . to overcome such inconsistency two major actions may be taken removal of constraints or changes in the coefficients of the model . this last approach , that can be generically described as model correction is the problem we address in this paper in the context of linear constraints over the reals . the correction of the right hand side alone , which is very close to a fuzzy constraints approach , was one of the first proposals to deal with inconsistency , as it may be mapped into a linear problem . the correction of both the matrix of coefficients and the right hand side introduces non linearity in the constraints . the degree of difficulty in solving the problem of the optimal correction depends on the objective function , whose purpose is to measure the closeness between the original and corrected model . contrary to other norms , that provide corrections with quite rigid patterns , the optimization of the important frobenius norm was still an open problem . we have analyzed the problem using the kkt conditions and derived necessary and sufficient conditions which enabled us to unequivocally characterize local optima , in terms of the solution of the total least squares and the set of active constraints . these conditions justify a set of pruning rules , which proved , in preliminary experimental results , quite successful in a tree search procedure for determining the global minimizer .
user interface evaluation and empirically based evolution of a prototype experience management tool . <eos> experience management refers to the capture , structuring , analysis , synthesis , and reuse of an organization 's experience in the form of documents , plans , templates , processes , data , etc. the problem of managing experience effectively is not unique to software development , but the field of software engineering has had a high level approach to this problem for some time . the experience factory is an organizational infrastructure whose goal is to produce , store , and reuse experiences gained in a software development organization <digit> , <digit> , <digit> . this paper describes the q labs experience management system ( q labs ems ) , which is based on the experience factory concept and was developed for use in a multinational software engineering consultancy <digit> . a critical aspect of the q labs ems project is its emphasis on empirical evaluation as a major driver of its development and evolution . the initial prototype requirements were grounded in the organizational needs and vision of q labs , as were the goals and evaluation criteria later used to evaluate the prototype . however , the q labs ems architecture , data model , and user interface were designed to evolve , based on evolving user needs . this paper describes this approach , including the evaluation that was conducted of the initial prototype and its implications for the further development of systems to support software experience management .
energy aware performance analysis methodologies for hpc architecturesan exploratory study . <eos> performance analysis is a crucial step in hpc architectures including clouds . traditional performance analysis methodologies that were proposed , implemented , and enacted are functional with the objective of identifying bottlenecks or issues related to memory , programming languages , hardware , and virtualization aspects . however , the need for energy efficient architectures in highly scalable computing environments , such as , grid or cloud , has widened the research thrust on developing performance analysis methodologies that analyze the energy inefficiency of hpc applications or their associated hardware . this paper surveys the performance analysis methodologies that investigates into the available energy monitoring and energy awareness mechanisms for hpc architectures . in addition , the paper validates the existing tools in terms of overhead , portability , and user friendly parameters by conducting experiments at hpccloud research laboratory at our premise . this research work will promote hpc application developers to select an apt monitoring mechanism and hpc tool developers to augment required energy monitoring mechanisms which fit well with their basic monitoring infrastructures .
locating the tightest link of a network path . <eos> the tightest link of a network path is the link where the end to end available bandwidth is limited . we propose a new probe technique , called dual rate periodic streams ( drps ) , for finding the location of the tightest link . a drps probe is a periodic stream with two rates . initially , it goes through the path at a comparatively high rate . when arrived at a particular link , the probe shifts its rate to a lower level and keeps the rate . if proper rates are set to the probe , we can control whether the probe is congested or not by adjusting the shift time . when the point of rate shift is in front of the tightest link , the probe can go through the path without congestion , otherwise congestion occurs . thus , we can find the location of the tightest link by congestion detection at the receiver .
research methodology using online technology for secondary analysis of survey research data act globally , think locally . <eos> the purpose of the this article is to discuss the impact that online technologies are having and will continue to have on the way secondary analysis of survey research is performed . the authors discuss the validity of secondary analysis of survey research studies and the effect that online technology has on such analyses . before reviewing current online public opinion sources , the authors make the argument that online services are becoming increasingly important for secondary analysis . finally , the authors present a model indicating where online services can go in the future given the technology that is available today . ultimately , it is believed that the internet is currently underexploited for its capacity to aid secondary analysis . the authors advocate making survey data more easily available online to all potential users . this entails varying the format and depth of data so that users find sources suitable to their needs . it also entails the use of desktop technology to store and analyze survey research data and making that technology , or the applications that are developed through that technology , available to other users via computer networks , primarily via the internet .
free vibration analysis of multiple stepped beams by using adomian decomposition method . <eos> the adomian decomposition method ( adm ) is employed in this paper to investigate the free vibrations of the euler bernoulli beams with multiple cross section steps . the proposed adm method can be used to analyze the vibration of beams consisting of an arbitrary number of steps through a recursive way . the solution can be obtained by solving a set of algebraic equations with only three unknown parameters . furthermore , the method can be extended to obtain an approximate solution to vibration problems of any type of non uniform beams . several numerical examples are presented and compared to those given in the paper . it is shown that the adm offers an accurate and effective method of free vibration analysis of multiple stepped beams with arbitrary boundary conditions . ( c ) <digit> elsevier ltd. all rights reserved .
modelling and performance evaluation of mobile multimedia systems using qos gspn . <eos> quality of service ( qos ) measurement of multimedia applications is one of the most important issues for call handoff and call admission control in mobile networks . based on the qos measures , we propose a generalized stochastic petri net ( gspn ) based model , called qos gspn , which can express the real time behavior of qos measurement for mobile networks . qos gspn performance analysis methodology includes the formal expression and performance analysis environment . it offers the promise of providing real time behavior predictability for systems characterized by substantial stochastic behavior . with this methodology we model and analyze the call handoff and call admission control schemes in the different multimedia traffic environments of a mobile network . the results of simulation experiments are used to verify the optimal performance achievable for these schemes under the qos constraints in the given setting of design parameters .
watermarking of mpeg <digit> video in compressed domain using vlc mapping . <eos> in this work we propose a new algorithm for fragile , high capacity yet file size preserving watermarking of mpeg <digit> streams . watermarking is done entirely in the compressed domain , with no need for full or even partial decompression . the algorithm is based on a previously developed concept of vlc mapping for compressed domain watermarking . the entropy coded segment of the video is first parsed out and then analyzed in pairs . it is recognized that there are vlc pairs that never appear together in any intra coded block . the list of unused pairs is systematically generated by the intersection of pair trees . one of the trees is generated from the main vlc table given in iso iec <digit> <digit> <digit> standard . the other trees are dynamically generated for each intra coded blocks . forcing one vlc pairs in a block to one of the unused ones generates a watermark block . the change is done while maintaining run level change to a minimum . at the decoder , the main pair tree is created offline using publicly available vlc tables . through a secure key exchange , the indices to unused code pairs are communicated to the receiver . we show that the watermarked video is reasonably resistant to forgery attacks and remains secure to watermark detection attempts .
implementing monads for c plus plus template metaprograms . <eos> c template metaprogramming is used in various application areas , such as expression templates , static interface checking , active libraries , etc. its recognized similarities to pure functional programming languages like haskell make the adoption of advanced functional techniques possible . such a technique is using monads , programming structures representing computations . using them actions implementing domain logic can be chained together and decorated with custom code . c template metaprogramming could benefit from adopting monads in situations like advanced error propagation and parser construction . in this paper we present an approach for implementing monads in c template metaprograms . based on this approach we have built a monadic framework for c template metaprogramming . as real world examples we present a generic error propagation solution for c template metaprograms and a technique for building compile time parser generators . all solutions presented in this paper are implemented and available as an open source library . ( c ) <digit> elsevier b.v. all rights reserved .
non uniform data distribution for communication efficient parallel clustering . <eos> global communication requirements and load imbalance of some parallel data mining algorithms are the major obstacles to exploit the computational power of large scale systems . this work investigates how non uniform data distributions can be exploited to remove the global communication requirement and to reduce the communication cost in parallel data mining algorithms and , in particular , in the k means algorithm for cluster analysis . in the straightforward parallel formulation of the k means algorithm , data and computation loads are uniformly distributed over the processing nodes . this approach has excellent load balancing characteristics that may suggest it could scale up to large and extreme scale parallel computing systems . however , at each iteration step the algorithm requires a global reduction operation which hinders the scalability of the approach . this work studies a different parallel formulation of the algorithm where the requirement of global communication is removed , while maintaining the same deterministic nature of the centralised algorithm . the proposed approach exploits a non uniform data distribution which can be either found in real world distributed applications or can be induced by means of multi dimensional binary search trees . the approach can also be extended to accommodate an approximation error which allows a further reduction of the communication costs . the effectiveness of the exact and approximate methods has been tested in a parallel computing system with <digit> processors and in simulations with <digit> processing elements .
cost effective control of air quality and greenhouse gases in europe modeling and policy applications . <eos> environmental policies in europe have successfully eliminated the most visible and immediate harmful effects of air pollution in the last decades . however , there is ample and robust scientific evidence that even at present rates europes emissions to the atmosphere pose a significant threat to human health , ecosystems and the global climate , though in a less visible and immediate way . as many of the low hanging fruits have been harvested by now , further action will place higher demands on economic resources , especially at a time when resources are strained by an economic crisis . in addition , interactions and interdependencies of the various measures could even lead to counter productive outcomes of strategies if they are ignored . integrated assessment models , such as the gains ( greenhouse gas air pollution interactions and synergies ) model , have been developed to identify portfolios of measures that improve air quality and reduce greenhouse gas emissions at least cost . such models bring together scientific knowledge and quality controlled data on future socio economic driving forces of emissions , on the technical and economic features of the available emission control options , on the chemical transformation and dispersion of pollutants in the atmosphere , and the resulting impacts on human health and the environment . the gains model and its predecessor have been used to inform the key negotiations on air pollution control agreements in europe during the last two decades . this paper describes the methodological approach of the gains model and its components . it presents a recent policy analysis that explores the likely future development of emissions and air quality in europe in the absence of further policy measures , and assesses the potential and costs for further environmental improvements . to inform the forthcoming negotiations on the revision of the gothenburg protocol of the convention on long range transboundary air pollution , the paper discusses the implications of alternative formulations of environmental policy targets on a cost effective allocation of further mitigation measures .
costs assessments of european environmental policies . <eos> the evolution of energy production in the european union ( eu ) is going through a big change in recent years the incidence of traditional fuels is diminishing gradually for increasing renewable energy sources ( res ) , due to international concerns over climate change and for energy security reasons . the aim of this paper is to construct a simulation model that identifies and estimates costs that may arise for a community of negotiating countries from opportunistic behavior of some country when defining environmental policies . in this paper , the model is applied specifically to the new <digit> framework for climate and energy policies ( com ( <digit> ) <digit> ) ( ec , <digit> <digit> ) on the promotion of res that commits eu governments to a common goal to increase the share of res in final consumption to <digit> % by <digit> . costs faced by eu countries to achieve the res target are different due to their endowment heterogeneity , the availability of res , the diffusion process of cost improvements and the different instruments to support the development of the res technologies . given the still undefined participation agreement to reach the new overall res target by <digit> , we want to assess the potential cost penalty induced by free riding behavior . this could stem from some eu country , which avoids complying with the res directive . our policy simulation exercise shows that costs increase more than proportionally with the non participating country size , measured with gdp and co2 emissions . furthermore , we provide a model to analytically assess the likelihood each eu country may have to behave opportunistically within the negotiation process of the new proposal on eu res targets ( com ( <digit> ) <digit> ) .
design methodology for battery powered embedded systems in safety critical application . <eos> battery powered embedded system can be considered as a power aware system for a safety critical application . there is a need of saving the battery power for such power aware system so that it can be used more efficiently , particularly in safety critical applications . present paper describes power optimization procedure using real time scheduling technique having a specific dead line guided by the model based optimum current discharge profile of a battery . in any power aware system ' energy optimization ' is one of the major issues for a faithful operation . ( c ) <digit> elsevier b.v. all rights reserved .
reliability measures for two part partition of states for aggregated markov repairable systems . <eos> three models for the aggregated stochastic processes based on an underlying continuous time markov repairable system are developed in which two part partition of states is used . several availability measures such as interval availability , instantaneous availability and steady state availability are presented . some of these availabilities are derived by using laplace transforms , which are more compact and concise . other reliability distributions for these three models are given as well .
an innovative architecture for context foraging . <eos> nomadic computing is a term for describing computing environments where the nodes are mobile and have only ad hoc interactions with each other . evidently , context aware applications are a key ingredient in such environments . however , nomadic nodes may not always have the capability to sense their environment and infer their exact context . hence , applications carried by the nodes will not be able to execute properly . in this paper , we propose an architecture for collaborative exchange of contextual information in an ad hoc setting . this approach is called context foraging and is used for disseminating contextual information based on a publish subscribe scheme . we present the algorithms required for such architecture along with the dynamic event indexing techniques used by the system . the efficiency of the suggested approach is assessed through simulation results . our proposal is investigated and implemented in the context of the ict ipac project .
on the estimation and correction of bias in local atrophy estimations using example atrophy simulations . <eos> brain atrophy is considered an important marker of disease progression in many chronic neuro degenerative diseases such as multiple sclerosis ( ms ) . a great deal of attention is being paid toward developing tools that manipulate magnetic resonance ( mr ) images for obtaining an accurate estimate of atrophy . nevertheless , artifacts in mr images , inaccuracies of intermediate steps and inadequacies of the mathematical model representing the physical brain volume change , make it rather difficult to obtain a precise and unbiased estimate . this work revolves around the nature and magnitude of bias in atrophy estimations as well as a potential way of correcting them . first , we demonstrate that for different atrophy estimation methods , bias estimates exhibit varying relations to the expected atrophy and these bias estimates are of the order of the expected atrophies for standard algorithms , stressing the need for bias correction procedures . next , a framework for estimating uncertainty in longitudinal brain atrophy by means of constructing confidence intervals is developed . errors arising from mri artifacts and bias in estimations are learned from example atrophy simulations and anatomies . results are discussed for three popular non rigid registration approaches with the help of simulated localized brain atrophy in real mr images .
information technologies and intuitive expertise a method for implementing complex organizational change among new york city transit authoritys bus maintainers . <eos> this paper describes an attempt to implement a complex information technology system with the new york city transit authoritys ( nycta ) bus maintainers intended to help better track and coordinate bus maintenance schedules . it implementation is notorious for high failure rates among so called low level workers . we believe that many it implementation efforts make erroneous assumptions about front line workers expertise , which creates tension between the it implementation effort and the cultures of practice among the front line workers . we designed an aggressive learning intervention to address this issue and called operational simulation . rather than requiring the expected <digit> months for implementation , the hourly staff reached independence with the new system in <digit> weeks and line supervisors ( who do more ) managed in <digit> weeks . additionally , the nycta shifted from a reactive to a proactive maintenance approach , reduced cycle times , and increased the mean distance between failure , resulting in a estimated <digit> million cost savings . implications for cognition , expertise , and training are discussed .
mirrored disk organization reliability analysis . <eos> disk mirroring or raid level <digit> ( raid1 ) is a popular paradigm to achieve fault tolerance and a higher disk access bandwidth for read requests . we consider four raid1 organizations basic mirroring , group rotate declustering , interleaved declustering , and chained declustering , where the last three organizations attain a more balanced load than basic mirroring when disk failures occur . we first obtain the number of configurations , a ( n , i ) , which do not result in data loss when i out of n disks have failed . the probability of no data loss in this case is a ( n , i ) n i the reliability of each raid1 organization is the summation over <digit> < i < n <digit> of a ( n , i ) r ( n <digit> ) ( <digit> r ) ( i ) , where r denotes the reliability of each disk . a closed form expression for a ( n , i ) is obtained easily for the first three organizations . we present a relatively simple derivation of the expression for a ( n , i ) for the chained declustering method , which includes a correctness proof . we also discuss the routing of read requests to balance disk loads , especially when there are disk failures , to maximize the attainable throughput .
data processing in the early cosmic ray experiments in sydney . <eos> the cosmic ray air shower experiment set up at the university of sydney in the late 1950s was one of the first complex experiments in australia to utilize the power of an electronic computer to process and analyse the experimental data . the paper provides a brief overview of the design and construction of the equipment for the experiment and the use of the computer silliac in the processing and analysis of the data . the central role of chris wallace in this latter aspect is given special attention .
the impact of metadata in web resources discovering . <eos> purpose to explore the impact of using metadata in finding and ranking web pages through <digit> december <digit> search engines . design methodology approach the study has been divided into two phases . in phase one , the use of metadata schemes and the impact of overlapped documents have been examined by employing the usability technique . phase two examined the impact of adding metadata elements to web pages in their original rank order , using the experimental method . this study focuses on indexing web pages using metadata . and its impact on search engine 's rankings . findings meta tags are more widely used than dublin core . the overlapped pages tend to include metadata . the second phase shows that by adding metadata . elements to web pages , it raises its rank order . however , this depends on the quality of the description and the metadata schemes . the study shows no great difference in page ranking between adding meta tags and dublin core . practical implications to maximize the impact of metadata , more attention should be given to keyword and descriptive fields . originality value the hypothetical relationship between overlapped pages and the inclusion of metadata and indexing by search engines had not been previously examined .
a new density stiffness interpolation scheme for topology optimization of continuum structures . <eos> in this paper , a new density stiffness interpolation scheme for topology optimization of continuum structures is proposed , based on this new scheme , not only the so caged checkerboard pattern can be eliminated from the final optimal topology , but also the boundary smooth effect associated with the traditional sensitivity averaging approach can also be overcome . a proof of the existence of the solution of the optimization problem is also given , therefore mesh independent optimization results can be obtained numerical examples illustrate the effectiveness and the advantage of the proposed interpolation scheme .
pets and their users a critical review of the potentials and limitations of the privacy as confidentiality paradigm . <eos> privacy as confidentiality has been the dominant paradigm in computer science privacy research . privacy enhancing technologies ( pets ) that guarantee confidentiality of personal data or anonymous communication have resulted from such research . the objective of this paper is to show that such pets are indispensable but are short of being the privacy solutions they sometimes claim to be given current day circumstances . using perspectives from surveillance studies we will argue that the computer scientists conception of privacy through data or communication confidentiality is techno centric and displaces end user perspectives and needs in surveillance societies . we will further show that the perspectives from surveillance studies also demand a critical review for their human centric conception of information systems . last , we rethink the position of pets in a surveillance society and argue for the necessity of multiple paradigms for addressing privacy concerns in information systems design .
an approach to automated decomposition of volumetric mesh . <eos> mesh decomposition is critical for analyzing , understanding , editing and reusing of mesh models . although there are many methods for mesh decomposition , most utilize only triangular meshes . in this paper , we present an automated method for decomposing a volumetric mesh into semantic components . our method consists of three parts . first , the outer surface mesh of the volumetric mesh is decomposed into semantic features by applying existing surface mesh segmentation and feature recognition techniques . then , for each recognized feature , its outer boundary lines are identified , and the corresponding splitter element groups are setup accordingly . the inner volumetric elements of the feature are then obtained based on the established splitter element groups . finally , each splitter element group is decomposed into two parts using the graph cut algorithm each group completely belongs to one feature adjacent to the splitter element group . in our graph cut algorithm , the weights of the edges in the dual graph are calculated based on the electric field , which is generated using the vertices of the boundary lines of the features . experiments on both tetrahedral and hexahedral meshes demonstrate the effectiveness of our method .
algorithms for storytelling . <eos> we formulate a new data mining problem called storytelling as a generalization of redescription mining . in traditional redescription mining , we are given a set of objects and a collection of subsets defined over these objects . the goal is to view the set system as a vocabulary and identify two expressions in this vocabulary that induce the same set of objects . storytelling , on the other hand , aims to explicitly relate object sets that are disjoint ( and , hence , maximally dissimilar ) by finding a chain of ( approximate ) redescriptions between the sets . this problem finds applications in bioinformatics , for instance , where the biologist is trying to relate a set of genes expressed in one experiment to another set , implicated in a different pathway . we outline an efficient storytelling implementation that embeds the cartwheels redescription mining algorithm in an a search procedure , using the former to supply next move operators on search branches to the latter . this approach is practical and effective for mining large data sets and , at the same time , exploits the structure of partitions imposed by the given vocabulary . three application case studies are presented a study of word overlaps in large english dictionaries , exploring connections between gene sets in a bioinformatics data set , and relating publications in the pubmed index of abstracts .
bifurcation study of a neural field competition model with an application to perceptual switching in motion integration . <eos> perceptual multistability is a phenomenon in which alternate interpretations of a fixed stimulus are perceived intermittently . although correlates between activity in specific cortical areas and perception have been found , the complex patterns of activity and the underlying mechanisms that gate multistable perception are little understood . here , we present a neural field competition model in which competing states are represented in a continuous feature space . bifurcation analysis is used to describe the different types of complex spatio temporal dynamics produced by the model in terms of several parameters and for different inputs . the dynamics of the model was then compared to human perception investigated psychophysically during long presentations of an ambiguous , multistable motion pattern known as the barberpole illusion . in order to do this , the model is operated in a parameter range where known physiological response properties are reproduced whilst also working close to bifurcation . the model accounts for characteristic behaviour from the psychophysical experiments in terms of the type of switching observed and changes in the rate of switching with respect to contrast . in this way , the modelling study sheds light on the underlying mechanisms that drive perceptual switching in different contrast regimes . the general approach presented is applicable to a broad range of perceptual competition problems in which spatial interactions play a role .
integration of fuzzy spatial relations in deformable models application to brain mri segmentation . <eos> this paper presents a general framework to integrate a new type of constraints , based on spatial relations , in deformable models . in the proposed approach , spatial relations are represented as fuzzy subsets of the image space and incorporated in the deformable model as a new external force . three methods to construct an external force from a fuzzy set representing a spatial relation are introduced and discussed . this framework is then used to segment brain subcortical structures in magnetic resonance images ( mri ) . a training step is proposed to estimate the main parameters defining the relations . the results demonstrate that the introduction of spatial relations in a deformable model can substantially improve the segmentation of structures with low contrast and ill defined boundaries . ( c ) <digit> pattern recognition society . published by elsevier ltd. all rights reserved .
an efficient animation of wrinkled cloth with approximate implicit integration . <eos> this paper presents an efficient method for creating the animation of flexible objects . the mass spring model was used to represent flexible objects . the easiest approach to creating animation with the mass spring model is the explicit euler method , but the method has a serious weakness in that it suffers from an instability problem . the implicit integration method is a possible solution , but a critical flaw of the implicit method is that it involves a large linear system . this paper presents an approximate implicit method for the mass spring model . the proposed technique updates with stability the state of n mass points in o ( n ) time when the number of total springs is o ( n ) . in order to increase the efficiency of simulation or reduce the numerical errors of the proposed approximate implicit method , the number of mass points must be as small as possible . however , coarse discretization with a small number of mass points generates an unrealistic appearance for a cloth model . by introducing a wrinkled cubic spline curve , we propose a new technique that generates realistic details of the cloth model , even though a small number of mass points are used for simulation .
does computer confidence relate to levels of achievement in ict enriched learning models . <eos> employer expectations have changed university students are expected to graduate with computer competencies appropriate for their field . educators are also harnessing technology as a medium for learning in the belief that information and communication technologies ( icts ) can enliven and motivate learning across a wide range of disciplines . alongside developing students computer skills and introducing them to the use of professional software , educators are also harnessing professional and scientific packages for learning in some disciplines . as the educational use of information and communication technologies increases dramatically , questions arise about the effects on learners . while the use of computers for delivery , support , and communication , is generally easy and unthreatening , higher level use may pose a barrier to learning for those who lack confidence or experience . computer confidence may mediate in how well students perform in learning environments that require interaction with computers . this paper examines the role played by computer confidence ( or computer self efficacy ) in a technology enriched science and engineering mathematics course in an australian university . findings revealed that careful and appropriate use of professional software did indeed enliven learning for the majority of students . however , computer confidence occupied a very different dimension to mathematics confidence and was not a predictor of achievement in the mathematics tasks , not even those requiring use of technology . moreover , despite careful and nurturing support for use of the software , students with low computer confidence levels felt threatened and disadvantaged by computer laboratory tasks . the educational implications of these findings are discussed with regard to teaching and assessment , in particular . the tcat scales used to measure technology attitudes , computer confidence self efficacy and mathematics confidence are included in an appendix . well established , reliable , internally consistent , they may be useful to other researchers . the development of the computer confidence scale is outlined , and guidelines are offered for the design of other discipline specific confidence self efficacy scales appropriate for use alongside the computer confidence scale .
dos protection for udp based protocols . <eos> since ip packet reassembly requires resources , a denial of service attack can be mounted by swamping a receiver with ip fragments . in this paper we argue how this attack need not affect protocols that do not rely on ip fragmentation , and argue how most protocols , e.g. , those that run on top of tcp , can avoid the need for fragmentation . however , protocols such as ipsec 's ike protocol , which both runs on top of udp and requires sending large packets , depend on ip packet reassembly . photuris , an early proposal for ike , introduced the concept of a stateless cookie , intended for dos protection . however , the stateless cookie mechanism can not protect against a dos attack unless the receiver can successfully receive the cookie , which it will not be able to do if reassembly resources are exhausted . thus , without additional design and or implementation defenses , an attacker can successfully , through a fragmentation attack , prevent legitimate ike handshakes from completing . defense against this attack requires both protocol design and implementation defenses . the ikev2 protocol was designed to make it easy to design a defensive implementation . this paper explains the defense strategy designed into the ikev2 protocol , along with the additional needed implementation mechanisms . it also describes and contrasts several other potential strategies that could work for similar udp based protocols .
on the parallel efficiency and scalability of the correntropy coefficient for image analysis . <eos> similarity measures have application in many scenarios of digital image processing . the correntropy is a robust and relatively new similarity measure that recently has been employed in various engineering applications . despite other competitive characteristics , its computational cost is relatively high and may impose hard to cope time restrictions for high dimensional applications , including image analysis and computer vision .
positive solution to a special singular second order boundary value problem . <eos> let lambda be a nonnegative parameter . the existence of a positive solution is studied for a semipositone second order boundary value problem u '' ( t ) lambda q ( t ) f ( t , u ( t ) , u ' ( t ) ) , alpha u ( <digit> ) beta u ' ( <digit> ) d , u ( <digit> ) <digit> , where d > <digit> , alpha > <digit> , beta > <digit> , alpha beta > <digit> , q ( t ) f ( t , u , v ) > <digit> on a suitable subset of <digit> , <digit> x <digit> , infinity ) x ( infinity , infinity ) and f ( t , u , v ) is allowed to be singular at t <digit> , t <digit> and u <digit> . the proofs are based on the leray schauder fixed point theorem and the localization method . ( c ) <digit> published by elsevier ltd .
report of research activities in fuzzy ai and medicine at usfcse . <eos> several projects involving the use of fuzzy and neuro fuzzy methods in medical applications , developed by members of the department of computer science and engineering , university of south florida , tampa , florida , are briefly reviewed . the successful applications are emphasized . ( c ) <digit> elsevier science b.v. all rights reserved .
societally connected multimedia across cultures . <eos> the advance of the internet in the past decade has radically changed the way people communicate and collaborate with each other . physical distance is no more a barrier in online social networks , but cultural differences ( at the individual , community , as well as societal levels ) still govern human human interactions and must be considered and leveraged in the online world . the rapid deployment of high speed internet allows humans to interact using a rich set of multimedia data such as texts , pictures , and videos . this position paper proposes to define a new research area called ' connected multimedia ' , which is the study of a collection of research issues of the super area social media that receive little attention in the literature . by connected multimedia , we mean the study of the social and technical interactions among users , multimedia data , and devices across cultures and explicitly exploiting the cultural differences . we justify why it is necessary to bring attention to this new research area and what benefits of this new research area may bring to the broader scientific research community and the humanity .
multiple object retrieval in image databases using hierarchical segmentation tree . <eos> with the rapid growth of information , efficient and robust information retrieval techniques have become increasingly more important . multiple object retrieval remains challenging due to the complex nature of this problem . the proposed research , unlike most existing works that are designed for single object retrieval or adopt heuristic multiple object matching scheme , aims at contributing to this field through the development of an image retrieval system that adopts a hierarchical region tree representation of image , and enables effective and efficient multiple object retrieval and automatic discovery of the objects of interest through users ' relevance feedback . we believe this is the first systematic attempt to formulate a comprehensive , intelligent , and interactive framework for multiple object retrieval in image databases that makes use of a hierarchical region tree representation .
delay dependent stability analysis for impulsive neural networks with time varying delays . <eos> in this paper , the global exponential stability and global asymptotic stability of the neural networks with impulsive effect and time varying delays is investigated . by using lyapunovkrasovskii type functional , the quality of negative definite matrix and cauchy criterion , we obtain the sufficient conditions for global exponential stability and global asymptotic stability of such model , in terms of linear matrix inequality ( lmi ) , which depend on the delays . two examples are given to illustrate the effectiveness of our theoretical results .
olap over uncertain and imprecise data . <eos> we extend the olap data model to represent data ambiguity , specifically imprecision and uncertainty , and introduce an allocation based approach to the semantics of aggregation queries over such data . we identify three natural query properties and use them to shed light on alternative query semantics . while there is much work on representing and querying ambiguous data , to our knowledge this is the first paper to handle both imprecision and uncertainty in an olap setting .
maximum skew symmetric flows and matchings . <eos> the maximum integer skew symmetric flow problem ( msfp ) generalizes both the maximum flow and maximum matching problems . it was introduced by tutte <digit> in terms of self conjugate flows in antisymmetrical digraphs . he showed that for these objects there are natural analogs of classical theoretical results on usual network flows , such as the flow decomposition , augmenting path , and max flow min cut theorems . we give unified and shorter proofs for those theoretical results . we then extend to msfp the shortest augmenting path method of edmonds and karp <digit> and the blocking flow method of dinits <digit> , obtaining algorithms with similar time bounds in general case . moreover , in the cases of unit arc capacities and unit node capacities our blocking skew symmetric flow algorithm has time bounds similar to those established in <digit> , <digit> for dinits ' algorithm . in particular , this implies an algorithm for finding a maximum matching in a nonbipartite graph in o ( rootnm ) time , which matches the time bound for the algorithm of micali and vazirani <digit> . finally , extending a clique compression technique of feder and motwani <digit> to particular skew symmetric graphs , we speed up the implied maximum matching algorithm to run in o ( rootnm log ( n ( <digit> ) m ) log n ) time , improving the best known bound for dense nonbipartite graphs . also other theoretical and algorithmic results on skew symmetric flows and their applications are presented .
efficient performance estimate for one class support vector machine . <eos> this letter proposes and analyzes a method ( estimate ) to estimate the generalization performance of one class support vector machine ( svm ) for novelty detection . the method is an extended version of the estimate method , which is used to estimate the generalization performance of standard svm for classification . our method is derived from analyzing the connection between one class svm and standard svm . without any computation intensive re sampling , the method is computationally much more efficient than leave one out method , since it can be computed immediately from the decision function of one class svm . using our method to estimate the error rate is more precise than using the fraction of support vectors and a parameter of one class svm . we also propose that the fraction of support vectors characterizes the precision of one class svm . a theoretical analysis and experiments on an artificial data and a widely known handwritten digit recognition set ( mnist ) show that our method can effectively estimate the generalization performance of one class svm for novelty detection .
comparison of recent methods for inference of variable influence in neural networks . <eos> neural networks ( nns ) belong to black box models and therefore suffer from interpretation difficulties . four recent methods inferring variable influence in nns are compared in this paper . the methods assist the interpretation task during different phases of the modeling procedure . they belong to information theory ( itss ) , the bayesian framework ( ard ) , the analysis of the network 's weights ( gim ) , and the sequential omission of the variables ( szw ) . the comparison is based upon artificial and real data sets of differing size , complexity and noise level . the influence of the neural network 's size has also been considered . the results provide useful information about the agreement between the methods under different conditions . generally , szw and gim differ from ard regarding the variable influence , although applied to nns with similar modeling accuracy , even when larger data sets sizes are used . itss produces similar results to szw and gim , although suffering more from the curse of dimensionality .
finite element modelling of reinforced concrete framed structures including catenary action . <eos> in this paper , a 1d discrete element is formulated for analysis of reinforced concrete frames with catenary action . a force based formulation is developed based on the total secant stiffness approach and an associated direct iterative solution scheme is derived . the effect of material nonlinearity as well as softening of concrete under compression is taken into account and a nonlocal averaging technique is employed to maintain the objectivity of displacement and force responses . concerning geometrical nonlinearities , the fibre strains are assumed to be small however , the effect of transverse displacement on the axial strain is large and it is taken into account as well as the effect of shear on the axial force . using a simpson integration scheme , together with a piecewise interpolation of curvature , the deformed shape of the element is consistently updated . the formulation is verified with numerical examples .
fostering a creative interest in computer science . <eos> in this paper , we describe activities undertaken at our university to revise our computer science program to develop an environment and curriculum which encourages creative , hands on learning by our students . our main changes were the development of laboratory space , increased hands on problem solving activities in the introductory course , open ended programming projects in the early courses including a requirement of an open ended project extension for an a grade , and the integration of a seminar into the senior project requirement . our results suggest that these changes have improved student skill and willingness to deal with new problems and technologies . an additional surprising side effect appears to be a dramatic increase in retention over the first two years , despite lower overall grade averages in those courses .
concave piecewise linear service curves and deadline calculations . <eos> the internet is gradually and constantly becoming a multimedia network that needs mechanisms to provide effective quality of service ( qos ) requirements to users . the service curve ( sc ) is an efficient description of qos and the service curve based earliest deadline first policy ( sced ) is a scheduling algorithm to guarantee scs specified by users . in sced , deadline calculation is the core . however , not every sc has a treatable deadline calculation currently the only known treatable sc is the concave piecewise linear sc ( cplsc ) . in this paper , we propose an algorithm to translate all kinds of scs into cplscs . in this way , the whole internet can have improved performance . moreover , a modification of the deadline calculation of the original sced is developed to obtain neat and precise results . the results combining with our proposed algorithm can make the deadline calculation smooth and the multimedia internet possible .
optimal retrial and timeout strategies for accessing network resources . <eos> the notion of timeout ( namely , the maximal time to wait before retrying an action ) turns up in many networking contexts , such as packet transmission , connection establishment , etc. usage of timeouts is encountered especially in large scale networks , where negative acknowledgments ( nacks ) on failures have significantly higher delays than positive acknowledgments ( acks ) and frequently are not employed at all . selection of a proper timeout involves a tradeoff between waiting too long and loading the network needlessly by waiting too little . the common approach is to set the timeout to a large value , such that , unless the action fails , it is acknowledged within the timeout duration with a high probability . this approach is conservative and leads to overly long , far from optimal , timeouts . we take a quantitative approach with the purpose of computing and studying the optimal timeout strategy . the above tradeoff is modeled by introducing a cost per unit time ( until success ) and a cost per repeated attempt . the optimal timeout strategy is then defined as one that a selfish user would follow to minimize its expected cost . we discuss the various practical interpretations that these costs may have . we then derive the formulas for the optimal timeout values and study some of their fundamental properties . in particular , we identify the conditions for making parallel attempts from the outset to be worthwhile . in addition , we demonstrate a striking property of positive feedback . this motivates us to study the interaction resulting when many users selfishly apply the optimal timeout strategy specifically , we use a noncooperative game model and show that it suffers from an inherent instability problem . some implications of these results on network design are discussed .
integrated modeling and analysis of dynamics for electric vehicle powertrains . <eos> this paper builds theoretical models for the entire powertrain of evs to describe ev dynamics with both mechanical and electrical systems . a matlab model of an ev is developed to verify the derived theoretical models for the entire powertrain of evs . a variety of final vehicle driving performances are analyzed and predicted as a function of electrical quantities .
wavelet synopses for general error metrics . <eos> several studies have demonstrated the effectiveness of the wavelet decomposition as a tool for reducing large amounts of data down to compact wavelet synopses that can be used to obtain fast , accurate approximate query answers . conventional wavelet synopses that greedily minimize the overall root mean squared ( i.e. , l <digit> norm ) error in the data approximation can suffer from important problems , including severe bias and wide variance in the quality of the data reconstruction , and lack of nontrivial guarantees for individual approximate answers . thus , probabilistic thresholding schemes have been recently proposed as a means of building wavelet synopses that try to probabilistically control maximum approximation error metrics ( e.g. , maximum relative error ) . a key open problem is whether it is possible to design efficient deterministic wavelet thresholding algorithms for minimizing general , non l <digit> error metrics , that are relevant to approximate query processing systems , such as maximum relative or maximum absolute error . obviously , such algorithms can guarantee better maximum error wavelet synopses and avoid the pitfalls of probabilistic techniques ( e.g. , bad coin flip sequences ) leading to poor solutions in addition , they can be used to directly optimize the synopsis construction process for other useful error metrics , such as the mean relative error in data value reconstruction . in this article , we propose novel , computationally efficient schemes for deterministic wavelet thresholding with the objective of optimizing general approximation error metrics . we first consider the problem of constructing wavelet synopses optimized for maximum error , and introduce an optimal low polynomial time algorithm for one dimensional wavelet thresholding our algorithm is based on a new dynamic programming ( dp ) formulation , and can be employed to minimize the maximum relative or absolute error in the data reconstruction . unfortunately , directly extending our one dimensional dp algorithm to multidimensional wavelets results in a super exponential increase in time complexity with the data dimensionality . thus , we also introduce novel , polynomial time approximation schemes ( with tunable approximation guarantees ) for deterministic wavelet thresholding in multiple dimensions . we then demonstrate how our optimal and approximate thresholding algorithms for maximum error can be extended to handle a broad , natural class of distributive error metrics , which includes several important error measures , such as mean weighted relative error and weighted l p norm error . experimental results on real world and synthetic data sets evaluate our novel optimization algorithms , and demonstrate their effectiveness against earlier wavelet thresholding schemes .
on the correlation between children 's performances on electronic board tasks and nonverbal intelligence test measures . <eos> in this study it was investigated whether a tangible electronic console ( tagtiles ) can be used in principle to address a range of cognitive skills by examining the underlying basic psychometric properties of tagtiles tasks . this is a precursor to an intervention study on the impact of tagtiles on cognitive development or an instrument development study . the tasks implemented on the console consisted of abstract visual patterns , which were intended to target perception , spatial knowledge representation , eye hand coordination , reasoning and problem solving . the results of a pilot study ( n <digit> , children aged <digit> <digit> ) and an experiment ( n <digit> , children aged <digit> <digit> ) are presented . correlations between scores on tagtiles tasks on the one hand and a selection of wisc iiinl performance subtests , raven 's progressive matrices and rakit 's memory span on the other hand , were calculated . the results indicate that the tagtiles tasks cover similar skills as the applied wisc iiinl subtests , demonstrated by the moderate to large correlations between performance scores on sets of tagtiles tasks and sets of wisc iiinl tasks . the combined tagtiles task scores were also significantly correlated with the aggregated wisc iiinl subtest scores . significant correlations were found between the tagtiles tasks and the raven test scores , though for the rakit memory span no significant correlation with tagtiles tasks was found . after further refinement and validation , in particular with a larger sample size , the tasks can be applied to provide an indication of children 's skill levels , offering the benefits of a self motivating testing method to children , and avoiding inconsistencies in administration . as such , the tasks may become an effective tool for the training and assessment of nonverbal skills for children . ( c ) <digit> elsevier ltd. all rights reserved .
ontology design patterns for the semantic business processes . <eos> this paper discusses two research paradigms the first one is based on using the meta object facility ( mof ) to support any kind of metadata , whereas the second one emphasizes the role of the ontology design patterns ( odps ) to support knowledge transformation between the source and the target models . more precisely , in this paper we represent the business process odp , which reflects both the syntax and semantics of business process specification and brings an abstract solution to the typical problem of designing and modelling semantic business processes .
rematerialization based register allocation through reverse computing . <eos> reversible computing aims at keeping all information on input and intermediate values available at any step of the computation . rematerialization in register allocation is an alternate solution to spilling where values are recomputed from available data instead of held in registers . in this paper we present the basic ideas of our algorithm for rematerialization with reverse computing . we use the memory demanding lqcd ( lattice quantum chromodynamics ) application to demonstrate that important gains of up to <digit> % on register pressure can be obtained . this in turn enables an increase in instruction level parallelism or in thread level parallelism . we demonstrate a 16.8 % ( statically timed ) gain over a basic lqcd computation .
a modified fieller interval for the interval estimation of effective doses for a logistic dose response curve . <eos> interval estimation of the gamma % effective dose ( p , , say ) is often based on the asymptotic variance of the maximum likelihood estimator ( delta interval ) or fieller 's theorem ( fieller interval ) . sitter and wu ( j. amer . statist . assoc. <digit> ( <digit> ) <digit> ) compared the delta and fieller intervals for the median effective dose ( <digit> ) assuming a logistic dose response curve . their results indicated that although fieller intervals are generally superior to delta intervals , they appear to be conservative . here an adjusted form of the fieller interval for mu ( gamma ) termed an adjusted fieller ( af ) interval is introduced . a comparison of the af interval with the delta and fieller intervals is provided and the properties of these three interval estimation methods are investigated . ( c ) <digit> elsevier science b.v. all rights reserved .
constructing g ( <digit> ) bezier surfaces over a boundary curve network with t junctions . <eos> at junction occurs in a boundary curve network when one boundary curve ends in the middle of another . we show how to construct g ( <digit> ) bezier surfaces over a boundary curve network with t junctions . by treating the two micro patches which meet at the edge forming the upright of the t as a single macro patch , we reduce the problem to one of achieving continuity between this composite patch and the third patch which has the crossbar of the t as an edge . thus we avoid changes to the boundary network , or to any patches except those that meet at the t junction . also , we analyze the singularity of the g ( <digit> ) continuity system with the t junction , and give the constraint to make a consistent system using free variables of weight functions . this is the first method of surfacing the t junction . we present examples and verify continuity by drawing reflection lines and checking angles . ( c ) <digit> elsevier ltd. all rights reserved .
optimal input output reduction in production processes . <eos> while conventional data envelopment analysis ( dea ) models set targets for each operational unit , this paper considers the problem of input output reduction in a centralized decision making environment . the purpose of this paper is to develop an approach to input output reduction problem that typically occurs in organizations with a centralized decision making environment . this paper shows that dea can make an important contribution to this problem and discusses how dea based model can be used to determine an optimal input output reduction plan . an application in banking sector with limitation in it investment shows the usefulness of the proposed method .
ambiguous grammars and the chemical transactions of life part ii the hierarchy of life 's grammars . <eos> purpose this second part of a companion paper seeks to extend the theory proposed to apply the hierarchy of fuzzy formal language to cope with the three major phenomenon of life replication , control and shuffling of genetic information . design methodology approach in order to cope with the proposal , three new classes of ffg are proposed replicating grammars to formalize proper ties and consequences of dna duplication self controlled grammars to provide the tools to control the grammar ambiguity and to improve adaptability , and recombinant grammars to formalize properties and consequences of the sexual reproduction to life evolution . considering all these facts , ffg are proposed as the key instrument to formalize the basic properties of the chemical transactions supporting life . findings the formalism of the model provides a new way to analyze and interpret the findings of the different genome sequencing projects . originality value the theoretical framework developed here provides a new perspective of understanding the code of life and evolution .
a two pass rate control algorithm for h. <digit> avc high definition video coding . <eos> in this paper , we propose a novel two pass rate control algorithm to achieve constant quality for h. <digit> avc high definition video coding . with the first pass collected rate and distortion information and the built model of scene complexity , the encoder can determine the expected distortion which could be achieved in the second pass encoding under the target bit rate . according to the built linear distortion quantizer ( d q ) model , before encoding one frame , the quantization parameter can be solved to realize constant quality encoding . after encoding one frame , the model parameters will be updated with linear regression method to ensure the prediction accuracy of the quantization parameter of next encoded frame with the same coding type . in order to obtain the expected distortion of each frame under the target bit rate , a gop level bit allocation scheme is also designed to adjust the target bit rate of each gop based on the scene complexity of the gop in the second pass encoding . in addition , the effect of scene change on the updating of d q model is considered . the model will be re initialized at the scene change to minimize modeling error . the experimental results show that compared with the latest two pass rate control algorithm , our proposed algorithm can significantly improve the bit control accuracy at comparable coding performance in terms of constant quality and average psnr . on average , the improvement of bit control accuracy achieved about <digit> % .
verification and validation of a work domain analysis with turing machine task analysis . <eos> work domain models produced by work domain analysis need to be validated and verified . a method based on the turing machine formalism was proposed . an application to two domains allowed us to highlight some required changes . over or underspecification , omission or false inclusion of objects were noticed .
modelling and simulation of photosynthetic microorganism growth random walk vs. finite difference method . <eos> the paper deals with photosynthetic microorganism growth modelling and simulation in a distributed parameter system . main result concerns the development and comparison of two modeling frameworks for photo bioreactor modelling . the first classical approach is based on pde ( reaction turbulent diffusion system ) and finite difference method . the alternative approach is based on random walk model of transport by turbulent diffusion . the complications residing in modelling of multi scale transport and reaction phenomena in microalgae are clarified and the solution is chosen . it consists on phenomenological state description of microbial culture by the lumped parameter model of photosynthetic factory ( psf model ) in the re parametrized form , published recently in this journal by papacek , et al. ( <digit> ) . obviously both approaches lead to the same simulation results , nevertheless they provide different advantages . ( c ) <digit> imacs . published by elsevier b.v. all rights reserved .
recent advances in natural language processing for biomedical applications . <eos> we survey a set a recent advances in natural language processing applied to biomedical applications , which were presented in geneva , switzerland , in <digit> at an international workshop . while text mining applied to molecular biology and biomedical literature can report several interesting achievements , we observe that studies applied to clinical contents are still rare . in general , we argue that clinical corpora , including electronic patient records , must be made available to fill the gap between bioinformatics and medical informatics .
a new probabilistic approach for distribution network reconfiguration applicability to real networks . <eos> power loss reduction can be considered as one of the main purposes for distribution system operators , especially for recent non governmental networks . reconfiguration is an operation process widely used for this optimization by means of changing the status of switches in a distribution network . some major points such as time varying loads and the number of switchings , which are often neglected or not applied simultaneously in most previous studies , are the main motivation behind this study . in this paper , a new probabilistic approach is proposed to perform an optimal reconfiguration in order to reduce the total cost of operation , including the cost of switching and benefit of loss reduction . considering time varying loads , the proposed method can obtain an optimal balance between the number of switchings and the power loss . the effectiveness of the suggested method is demonstrated through several experiments and the results are compared with those of other reliable methods in several cases . ( c ) <digit> elsevier ltd. all rights reserved .
a feasibility study of the classification of alpaca ( lama pacos ) wool samples from different ages , sex and color by means of visible and near infrared reflectance spectroscopy . <eos> the usefulness of classifying the alpaca wool samples according to their color , sex and location is associated with their economic value in the market , hence adequate methods for rapid classification are needed to assess the of wool value . this study evaluated the potential of the visible and near infrared ( visnir ) spectroscopy combined with multivariate statistical analysis to classify alpaca ( lama pacos ) fiber samples according to age ( <digit> and <digit> year old ) , sex ( male and female ) and color ( black , brown , lf and white ) . samples ( n <digit> ) were scanned in reflectance mode in the wavelength range of 4002500nm using a monochromator instrument ( foss nirsystems6500 , inc. , silver spring , md , usa ) . principal component analysis ( pca ) and partial least squares discriminant analysis ( pls da ) were used to classify fiber samples . cross validation was used for validation of classification models developed . results showed that pls da correctly classified <digit> % of fiber samples into ages , intermediate classification rates were obtained for color , while lower classification rates were obtained for the discrimination of wool samples according to sex . the results from this study suggested that visnir spectroscopy in combination with multivariate data analysis can be used as a rapid method to classify alpaca fiber samples according to age , sex and color .
the dissipative structure of variational multiscale methods for incompressible flows . <eos> in this paper , we present a precise definition of the numerical dissipation for the orthogonal projection version of the variational multiscale method for incompressible flows . we show that , only if the space of subscales is taken orthogonal to the finite element space , this definition is physically reasonable as the coarse and fine scales are properly separated . then we compare the diffusion introduced by the numerical discretization of the problem with the diffusion introduced by a large eddy simulation model . results for the flow around a surface mounted obstacle problem show that numerical dissipation is of the same order as the subgrid dissipation introduced by the smagorinsky model . finally , when transient subscales are considered , the model is able to predict backscatter , something that is only possible when dynamic les closures are used . numerical evidence supporting this point is also presented .
an integer linear programming based approach for parallelizing applications in on chip multiprocessors . <eos> with energy consumption becoming one of the first class optimization parameters in computer system design , compilation techniques that consider performance and energy simultaneously are expected to play a central role . in particular , compiling a given application code under performance and energy constraints is becoming an important problem . in this paper , we focus on an on chip multiprocessor architecture and present a parallelization strategy based on integer linear programming . given an array intensive application , our optimization strategy determines the number of processors to be used in executing each nest based on the objective function and additional compilation constraints provided by the user . our initial experience with this strategy shows that it is very successful in optimizing array intensive applications on on chip multiprocessors under energy and performance constraints .
sensorless direct torque and flux controlled ipm synchronous machine fed by matrix converter over a wide speed range . <eos> this paper proposes a new sensorless direct torque and flux controlled interior permanent magnet synchronous machine drive fed by a matrix converter . closed loop control of both torque and stator flux is achieved by using two pi controllers . the input and output voltage vectors are modulated with the indirect space vector modulation technique . additionally , unity power factor on the power supply side of matrix converter is achieved through closed loop compensation of the input displacement angle created by the input filter of matrix converter . the adaptive observer used for joint stator flux and rotor speed estimation is enhanced by hf signal injection scheme for stable operation at low speed including standstill . the stator resistance variation is compensated with the current estimation error . the operating range of the drive is extended into high speed region by incorporating field weakening . the sensorless drive exhibits high dynamic and steady state performances over a wide speed range . the implementation of digital control system for the proposed matrix converter drive is described in this paper . extensive experimental results confirming the effectiveness of the proposed method are also included .
integrated framework for assessing urban water supply security of systems with non traditional sources under climate change . <eos> an integrated framework for planning an urban water supply system is proposed . challenges of including non traditional sources under climate change are documented . desalination , stormwater and rainwater are augmentation options for a case study . rainwater is an expensive augmentation option for little supply security gain . reducing per capita consumption may increase robustness to future uncertainties .
an exploratory project expert system for eliciting correlation coefficient and sequential updating of duration estimation . <eos> this study proposes a framework for updating estimation of project duration in project networks . the first step of building a project expert system is to elicit the correlation coefficient of activity durations from experts ' knowledge and intuition . given the correlation coefficients elicited , the linear bayesian approach is used to update the distribution of activity duration . in particular , by reflecting the newly observed duration of completed activities , we can update the duration of upcoming activities repeatedly throughout the entire project period . this helps keep track of the constantly changing longest duration path within the networks . finally , it is shown that all these learning and updating schemes can be relatively easily implemented on an excel spreadsheet , so that field managers can apply the model into real projects .
structural effects of biologically relevant rhodamines on spectroscopy of fluorescence fluctuations . <eos> exciton coupling in complexes between the indole ring and other systems is known to enhance the efficiency of energy and electron transfer . rhodamines ' xanthylium rings allow the formation of weakly or nonfluorescent complexes with the amino acid tryptophan . thus , because of the short distance of the participating electronic clouds , intrinsic electron transferinduced fluorescence quenching occurs . in solution , the rate constant of electron transfer is known to be limited by collision interactions at the contact distance . by contrast , in protein local environments tryptophan residues can be either exposed or buried in hydrophobic regions . herein , i report on the properties of aromatic derivatized rhodamines , among which is one with a bound phenylalanine amino acid group . encompassed is the spectroscopic and kinetic information in bulk and at the single molecule levels both in free solution and in the presence of human serum albumin . spectroscopic characteristics are focused with special emphasis on enhanced fluorescence that is addressed considering optimized geometries and electronic spectra . the importance of the probes associated with peptides and metal ions both in condensed phase or interfaces and as substrates with proteins is put into perspective .
a novel approach for bit serial ab ( <digit> ) multiplication in finite fields gf ( <digit> ( m ) ) . <eos> this paper presents a new inner product ab ( <digit> ) multiplication algorithm and effective hardware architecture for exponentiation in finite fields cf ( <digit> ( m ) ) . exponentiation is more efficiently implemented by applying ab ( <digit> ) multiplication repeatedly rather than ab multiplication . thus , efficient ab ( <digit> ) multiplication algorithms and simple architectures are the key to implementing exponentiation . accordingly , this paper proposes an efficient inner product multiplication algorithm based on an irreducible all one polynomial ( aop ) and simple architecture , which has the same hardware equipment as fenn 's ab multiplier . the proposed bit serial multiplication algorithm and architecture are highly regular and simpler than those of previous works . ( c ) <digit> elsevier ltd. all rights reserved .
policy teaching through reward function learning . <eos> policy teaching considers a markov decision process setting in which an interested party aims to influence an agent 's decisions by providing limited incentives . in this paper , we consider the specific objective of inducing a pre specified desired policy . we examine both the case in which the agent 's reward function is known and unknown to the interested party , presenting a linear program for the former case and formulating an active , indirect elicitation method for the latter . we provide conditions for logarithmic convergence , and present a polynomial time algorithm that ensures logarithmic convergence with arbitrarily high probability . we also offer practical elicitation heuristics that can be formulated as linear programs , and demonstrate their effectiveness on a policy teaching problem in a simulated ad network setting . we extend our methods to handle partial observations and partial target policies , and provide a game theoretic interpretation of our methods for handling strategic agents .
turning telecommunications call details to churn prediction a data mining approach . <eos> as deregulation , new technologies , and new competitors open up the mobile telecommunications industry , churn prediction and management has become of great concern to mobile service providers . a mobile service provider wishing to retain its subscribers needs to be able to predict which of them may be at risk of changing services and will make those subscribers the focus of customer retention efforts . in response to the limitations of existing churn prediction systems and the unavailability of customer demographics in the mobile telecommunications provider investigated , we propose , design , and experimentally evaluate a churn prediction technique that predicts churning from subscriber contractual information and call pattern changes extracted from call details . this proposed technique is capable of identifying potential churners at the contract level for a specific prediction time period . in addition , the proposed technique incorporates the multi classifier class combiner approach to address the challenge of a highly skewed class distribution between churners and non churners . the empirical evaluation results suggest that the proposed call behavior based churn prediction technique exhibits satisfactory predictive effectiveness when more recent call details are employed for the churn prediction model construction . furthermore , the proposed technique is able to demonstrate satisfactory or reasonable predictive power within the one month interval between model construction and churn prediction . using a previous demographics based churn prediction system as a reference , the lift factors attained by our proposed technique appear largely satisfactory .
design and simulation of manufacturing systems facing imperfectly defined information . <eos> due to the constant evolution of the environment and to the complexity of the needs , the specifications of a manufacturing system are often imperfectly known . the initial design data are uncertain , inaccurate and even vague . we propose to represent the quantifiable needs using fuzzy quantities . the data are propagated during the activity of engineering to lead to the parameters of the target system . in this context , simulation techniques , based on fuzzy parameters , are used to verify the exactness of the design . we choose to use a commercial discrete event simulator and response surface methodology to perform fuzzy simulation .
the skeptical explorer a multiple hypothesis approach to visual modeling and exploration . <eos> the primary intent of this work is to present a method for sequentially associating three dimensional surface measurements acquired by an autonomous exploration agent with models that describe those surfaces . traditional multiple viewpoint registration approaches are concerned only with finding the transformation that maps data points to a chosen global frame . given a parts based object representation , and assuming that the view correspondence can be found , the problem of associating the registered data with the correct part models still needs to be solved . while traditional approaches are content to group segmented data sets that geometrically overlap one another with the same part , there are cases where this causes ambiguous situations . this paper addresses the model data association problem as it applies to three dimensional dynamic object modeling . by tracking the state of part models across subsequent views , we wish to identify possible events that explain model data association ambiguities and represent them in a bayesian framework . the model data association problem is therefore relaxed to allow multiple interpretations of the object 's structure , each being assigned a probability . rather than making a decision at every iteration about an ambiguous mapping , we look to the future for the information needed to disambiguate it . experimental results are presented to illustrate the effectiveness of the approach .
mobility and stability evaluation in wireless multi hop networks using multi player games . <eos> multi hop networks have gained a lot of interest in recent years . a lot of work was contributed in the field of protocol design and performance of multi hop networks . it is generally accepted that mobility has a huge impact on the protocol performance even more for multi hop networks . obtaining realistic measurements of mobility , however , is complex and expensive . thus , we adopt virtual world scenarios to explore the mobility issue , by using the well known multi player game , quake ii . the advantage of the quake ii engine is that users move within virtual worlds under realistic constraints , whereas other mobility models may offer insufficient accuracy or operate under unrealistic assumptions . moreover , it is very easy to create new virtual worlds and to adapt them to specialized needs . in this paper , we propose an analytical framework for mobility measurements in virtual worlds that could be adopted for the design of communication protocols . our framework enables the study of the impact of mobility on connectivity and stability of the network , giving useful insights for improving communication performance . an interesting application of our approach is the analysis of coverage extension of so called hotspots or emergency situations , where the fixed network infrastructure is insufficient or non existent . in these extreme cases , multi hop networks can be used to setup communication quickly . as these situations comprise a plethora of different cases and scenarios , our model is appropriate for their analysis , due to its generality . we use our framework to investigate the performance of multi hop networks based on ieee 802.11 a technology . in contrast to other contributions focusing only on connectivity , the ieee 802.11 a technology also considers multi rate connections . our framework covers the evaluation of simple connectivity as well as link quality stability in the presence of mobility , a combination that has not been considered thus far . therefore we introduce two simple routing schemes and highlight the performance of these protocols in presence of mobility . furthermore we come up with four definitions of stability and investigate protocols for multi hop networks in terms of this parameter . our other contributions are the changes to the quake ii engine and the availability of mobility trace files .
efficient method of achieving agreements between individuals and organizations about rfid privacy . <eos> this work presents novel technical and legal approaches that address privacy concerns for personal data in rfid systems . in recent years , to minimize the conflict between convenience and the privacy risk of rfid systems , organizations have been requested to disclose their policies regarding rfid activities , obtain customer consent , and adopt appropriate mechanisms to enforce these policies . however , current research on rfid typically focuses on enforcement mechanisms to protect personal data stored in rfid tags and prevent organizations from tracking user activity through information emitted by specific rfid tags . a missing piece is how organizations can obtain customers ' consent efficiently and flexibly . this study recommends that organizations obtain licenses automatically or semi automatically before collecting personal data via rfid technologies rather than deal with written consents . such digitalized and standard licenses can be checked automatically to ensure that collection and use of personal data is based on user consent . while individuals can easily control who has licenses and license content , the proposed framework provides an efficient and flexible way to overcome the deficiencies in current privacy protection technologies for rfid systems .
people are doing it for themselves . <eos> to date , the objective of creating pleasurable products has concentrated on designers articulating and interpreting user needs as part of the product creation process . this paper explores approaches to enable users to adapt , modify , specify or create products to match their needs directly . using the potential of new technologies , active consumers can now become product creators , paralleling developments in graphics , music and digital media production . empowered users , self builders , recreational manufacturers , web connected silver surfers ( retired individuals using the web ) and punk manufacturers <digit> all exemplify this new relationship between users and products , and the evolving role of designers <digit> .
low cost networks and gateways for teaching data communications . <eos> the growing importance of communications in computer science has resulted in many undergraduate computer science programmes offering courses in data communications . although data communications courses can be taught in a practical manner , the cost of data communications hardware often restricts the amount of actual hands on experience that students can gain . in this paper we describe the hardware and software requirements of several low cost networks that can be used by students to gain experience in a wide variety of data communication topics including local area networks ( such as bus networks and ring networks ) , wide area networks ( i.e. store and forward networks ) , and gateways .
on the application of genetic programming for software engineering predictive modeling a systematic review . <eos> the objective of this paper is to investigate the evidence for symbolic regression using genetic programming ( gp ) being an effective method for prediction and estimation in software engineering , when compared with regression machine learning models and other comparison groups ( including comparisons with different improvements over the standard gp algorithm ) . we performed a systematic review of literature that compared genetic programming models with comparative techniques based on different independent project variables . a total of <digit> primary studies were obtained after searching different information sources in the time span <digit> <digit> . the results of the review show that symbolic regression using genetic programming has been applied in three domains within software engineering predictive modeling ( i ) software quality classification ( eight primary studies ) . ( ii ) software cost effort size estimation ( seven primary studies ) . ( iii ) software fault prediction software reliability growth modeling ( eight primary studies ) . while there is evidence in support of using genetic programming for software quality classification , software fault prediction and software reliability growth modeling the results are inconclusive for software cost effort size estimation . ( c ) <digit> elsevier ltd. all rights reserved .
a unified framework for dynamic pari mutuel information market design . <eos> recently , coinciding with and perhaps driving the increased popularity of prediction markets , several novel pari mutuel mechanisms have been developed such as the logarithmic market scoring rule ( lmsr ) , the cost function formulation of market makers , and the sequential convex parimutuel mechanism ( scpm ) . in this work , we present a unified convex optimization framework which connects these seemingly unrelated models for centrally organizing contingent claims markets . the existing mechanisms can be expressed in our unified framework using classic utility functions . we also show that this framework is equivalent to a convex risk minimization model for the market maker . this facilitates a better understanding of the risk attitudes adopted by various mechanisms . the utility framework also leads to easy implementation since we can now find the useful cost function of a market maker in polynomial time through the solution of a simple convex optimization problem . in addition to unifying and explaining the existing mechanisms , we use the generalized framework to derive necessary and sufficient conditions for many desirable properties of a prediction market mechanism such as proper scoring , truthful bidding ( in a myopic sense ) , efficient computation , controllable risk measure , and guarantees on the worst case loss . as a result , we develop the first proper , truthful , risk controlled , loss bounded ( in number of states ) mechanism none of the previously proposed mechanisms possessed all these properties simultaneously . thus , our work could provide an effective tool for designing new market mechanisms .
interaction design for supporting communication between chinese sojourners . <eos> in our global village , distance is not a barrier anymore for traveling . people experience new cultures and face accompanying difficulties in order to live anywhere . social support can help these sojourners to cope with difficulties , such as culture shock . in this paper , we investigate how computer mediated communication ( cmc ) tools can facilitate social support when living physically separated from loved ones in different cultures . the goal is to understand the design considerations necessary to design new cmc tools . we studied communication practices of chinese sojourners living in the netherlands and the use of a technology probe with a novel video communication system . these results led to recommendations which can help designers to design interactive communication tools that facilitate communication across cultures . we conclude the paper with an interactive communication device called circadian , which was designed based on these recommendations . we experienced the design recommendations to be abstract enough to leave space for creativity while providing a set of clear requirements which we used to base design decisions upon .
backward penalty schemes for monotone inclusion problems . <eos> in this paper , we are concerned with solving monotone inclusion problems expressed by the sum of a set valued maximally monotone operator with a single valued maximally monotone one and the normal cone to the nonempty set of zeros of another set valued maximally monotone operator . depending on the nature of the single valued operator , we propose two iterative penalty schemes , both addressing the set valued operators via backward steps . the single valued operator is evaluated via a single forward step if it is cocoercive , and via two forward steps if it is monotone and lipschitz continuous . the latter situation represents the starting point for dealing with complexly structured monotone inclusion problems from algorithmic point of view .
3d video and free viewpoint videofrom capture to display . <eos> this paper gives an end to end overview of 3d video and free viewpoint video , which can be regarded as advanced functionalities that expand the capabilities of a 2d video . free viewpoint video can be understood as the functionality to freely navigate within real world visual scenes , as it is known for instance from virtual worlds in computer graphics . 3d video shall be understood as the functionality that provides the user with a 3d depth impression of the observed scene , which is also known as stereo video . in that sense as functionalities , 3d video and free viewpoint video are not mutually exclusive but can very well be combined in a single system . research in this area combines computer graphics , computer vision and visual communications . it spans the whole media processing chain from capture to display and the design of systems has to take all parts into account , which is outlined in different sections of this paper giving an end to end view and mapping of this broad area . the conclusion is that the necessary technology including standard media formats for 3d video and free viewpoint video is available or will be available in the future , and that there is a clear demand from industry and user for such advanced types of visual media . as a consequence we are witnessing these days how such technology enters our everyday life
e a brainiac theorem prover . <eos> we describe the superposition based theorem prover e. e is a sound and complete prover for clausal first order logic with equality . important properties of the prover include strong redundancy elimination criteria , the discount loop proof procedure , a very flexible interface for specifying search control heuristics , and an efficient inference engine . we also discuss strength and weaknesses of the system .
the trefftz method for solving eigenvalue problems . <eos> for laplace 's eigenvalue problems , this paper presents new algorithms of the trefftz method ( i.e. the boundary approximation method ) , which solve the helmholtz equation and then use an iteration process to yield approximate eigenvalues and eigenfunctions . the new iterative method has superlinear convergence rates and gives a better performance in numerical testing , compared with the other popular methods of rootfinding . moreover , piecewise particular solutions are used for a basic model of eigenvalue problems on the unit square with the dirichlet condition . numerical experiments are also conducted for the eigenvalue problems with singularities . our new algorithms using piecewise particular solutions are well suited to seek very accurate solutions of eigenvalue problems , in particular those with multiple singularities , interfaces and those on unbounded domains . using piecewise particular solutions has also the advantage to solve complicated problems because uniform particular solutions may not always exist for the entire solution domain .
exertion interfaces for computer videogames using smartphones as input controllers . <eos> as mobile phones become smarter and include a wider and more powerful array of sensory components , the opportunity to leverage those capabilities in contexts other than telephony grows . we have in particular identified those sensory capabilities as key components for modern user interfaces that can detect movement , actions and intentions to enrich human computer interaction in a natural way . in this work , we present research around using smartphones as input controllers in the context of exertion videogames . we propose a conceptual framework that identifies the core elements of such interfaces , regardless of the underlying technological platforms , and provides a design pattern for their integration into existing videogames without having to change the games source code . we present a proof of concept implementation for the framework , with two smartphone input controllers , which using a soft button and accelerometer data , interface to a target shooting exertion game played while exercising on a stationary bicycle . we present findings from a user experience evaluation .
local feature based multi object recognition scheme for surveillance . <eos> in this paper , we propose an efficient multi object recognition scheme for surveillance based on interest points of objects and their feature descriptors . in this scheme , we first define a set of object types of interest and collect their sample images . for each sample image , we detect interest points and construct their feature descriptors using surf . next , we perform a statistical analysis of the local features to select representative points among them . intuitively , the representative points of an object are the interest points that best characterize the object . finally , we calculate thresholds of each object for object recognition . user query is processed in a similar way . a given query image 's local feature descriptors are extracted and then compared with the representative points of objects in the database . especially , to reduce the number of comparisons required , we propose a method for merging descriptors of similar representative points into a single descriptor . this descriptor is different from typical surf descriptor in that each element represents not a single value but a range . by using this merged descriptor , we can calculate the similarity between input image descriptor and multiple descriptors in database efficiently . in addition , since our scheme treats all the objects independently , it can recognize multiple objects simultaneously .
entertainment modeling through physiology in physical play . <eos> this paper is an extension of previous work on capturing and modeling the affective state of entertainment ( fun ) grounded on children 's physiological state during physical game play . the goal is to construct , using representative statistics computed from children 's physiological signals , an estimator of the degree to which games provided by the playground engage the players . previous studies have identified the difficulties of isolating elements of physical activity attributed to reported entertainment derived ( solely ) from heart rate ( hr ) recordings . in the present article , a survey experiment on a larger scale and a physical activity control experiment for surmounting those difficulties are devised . in these experiments , children 's hr , blood volume pulse ( bvp ) and skin conductance ( sc ) signals , as well as their expressed preferences of how much fun particular game variants are , are obtained using games implemented on the playware physical interactive playground . given effective data collection , a set of numerical features is computed from these measurements of the child 's physiological state . a comprehensive statistical analysis shows that children 's reported entertainment preferences correlate well with specific features of the recorded signals . preference learning techniques combined with feature set selection methods permit the construction of user models that predict reported entertainment preferences given suitable signal features . the most accurate models are obtained through evolving artificial neural networks and are demonstrated and evaluated on a playware game and a control task requiring physical activity . the best network is able to correctly match expressed preferences in 69.64 % of cases on previously unseen data ( p value 0.0022 p value 0.0022 ) and indicates two dissimilar classes of children those that prefer constantly energetic play of low mental emotional load and those that report as fun a dynamic play that involves high mental emotional load independently of physical effort . the generality of the methodology , its limitations , its usability as a real time feedback mechanism for entertainment augmentation and as a validation tool are discussed .
discrete program size dependent software reliability assessment modeling , estimation , and goodness of fit comparisons . <eos> in this paper we propose a discrete program size dependent software reliability growth model flexibly describing the software failure occurrence phenomenon based on a discrete weibull distribution . we also conduct model comparisons of our discrete srgm with existing discrete srgms by using actual data sets . the program size is one of the important metrics of software complexity . it is known that flexible discrete software reliability growth modeling is difficult due to the mathematical manipulation under a conventional modeling framework in which the time dependent behavior of the cumulative number of detected faults is formulated by a difference equation . our discrete srgm is developed under an existing unified modeling framework based on the concept of general order statistics , and can incorporate the effect of the program size into software reliability assessment . further , we discuss the method of parameter estimation , and derive software reliability assessment measures of our discrete srgm . finally , we show numerical examples of discrete software reliability analysis based on our discrete srgm by using actual data .
implications of the fit between organizational structure and erp a structural contingency theory perspective . <eos> despite the tremendous popularity and great potential , the field of enterprise resource planning ( erp ) adoption and implementation is littered with remarkable failures . though many contributing factors have been cited in the literature , we argue that the integrated nature of erp systems , which generally requires an organization to adopt standardized business processes reflected in the design of the software , is a key factor contributing to these failures . we submit that the integration and standardization imposed by most erp systems may not be suitable for all types of organizations and thus the fit between the characteristics of the adopting organization and the standardized business process designs embedded in the adopted erp system affects the likelihood of implementation success or failure . in this paper , we use the structural contingency theory to identify a set of dimensions of organizational structure and erp system characteristics that can be used to gauge the degree of fit , thus providing some insights into successful erp implementations . propositions are developed based on analyses regarding the success of erp implementations in different types of organizations . these propositions also provide directions for future research that might lead to prescriptive guidelines for managers of organizations contemplating implementing erp systems .
a neuroscience based design of intelligent tools for the elderly and disabled . <eos> the author has developed one basic research approach for universal accessibility over a period of <digit> years . as reviewed in this paper , he and his co researchers have designed several intelligent tools for universal accessibility as well as obtained many basic findings concerning neuroscience of human information processing . some of the tools have been manufactured in japan and the technologies as well as the basic findings have been applied to construct human centered computer interfaces such as virtual reality , automatic speech recognition and speech syntheses . moreover , these newly developed computer interface technologies have led to the improvement in the design of models for developing universal accessibility devices . lastly , the author has emphasized that a neuroscience based design of intelligent tools for the elderly and disabled may open a large market .
designing for semantic access a video browsing system . <eos> users of browsing applications often have vague information needs which can only be described in conceptual terms . therefore , a video browsing system must accept conceptual queries for preselection and offer mechanisms for interactive inspection of the result set by the user . in this paper , we describe a mm dbms that we extended with the following components our retrieval engine calculates relevance values for the results of a conceptual query by feature aggregation on video shot granularity to offer conceptual , content based access . to reduce startup delays within sessions , our admission control module admits only complete browsing sessions , if required resources , which are heuristically predicted from query results , are available . in addition , our intelligent client buffer strategy employs the retrieval relevance values to enable flexible user interactions during browsing .
a method for analyzing reading comprehension in computer science courses . <eos> reading has traditionally been seen as an essential component in learning , especially at the university level . however , many instructors in higher education , especially in technical courses , do not emphasize reading or try to evaluate it . in this abstract we present an automated system designed to measure and improve reading comprehension and describe preliminary results using the system .
studies on soluble ectodomain proteins of relaxin ( lgr7 ) and insulin <digit> ( lgr8 ) receptors . <eos> abstract the ectodomains of both the relaxin ( lgr7 ) and the insl3 ( lgr8 ) receptors can be expressed on the cell surface using only a single transmembrane domain . these membrane anchored proteins retain the ability to bind relaxin and can be cleaved from the cell surface . the subsequent lgr7 protein , 7bp , binds relaxin and can act as a functional relaxin antagonist . by contrast , the equivalent lgr8 protein 8bp does not bind relaxin or antagonize lgr8 activity . the 7bp protein has been successfully immobilized onto chemically derivatized surfaces for the capture of relaxin peptides and subsequent identification via seldi ms analysis .
on chip delay measurement for silicon debug . <eos> efficient test and debug techniques are indispensable for performance characterization of large complex integrated circuits in deep submicron and nanometer technologies . performance characterization of such chips requires on chip hardware and efficient debug schemes in order to reduce time to market and ensure shipping of chips with lower defect levels . in this paper we present an on chip scheme for delay fault detection and performance characterization . the proposed technique allows for accurate measurement of delays of speed paths for speed binning and facilitates a systematic and efficient test and debug scheme for delay faults . the area overhead associated with the proposed technique is very low .
surface mooring network in the kuroshio extension . <eos> as a contribution to the global earth observation system of systems , the national oceanic and atmospheric administration ( noaa ) is developing surface moorings that carry a suite of field proven and cost effective sensors to monitor air sea heat , moisture , and momentum fluxes , carbon dioxide uptake , and upper ocean temperature , salinity , and currents . in june <digit> , an noaa surface mooring , referred to as the kuroshio extension observatory ( keo ) , was deployed in the kuroshio extension 's ( ke ) southern recirculation gyre , approximately <digit> nautical miles east of japan . in <digit> , a partnership between noaa and the japan agency for marine earth science and technology was formed that deployed a second mooring ( referred to as jkeo ) north of the ke jet in february <digit> . ke is a region of strong currents , typhoons , and winter storms . designing and maintaining moorings in the ke is a challenging engineering task . all data are publicly available . a subset of the data are telemetered and made available in near real time through the global telecommunications system and web based data distribution systems . data from these time series reference sites serve a wide research and operational community and are being used for assessing numerical weather prediction analyses and reanalyses and for quantifying the air sea interaction in this dynamic region .
a parallel fully coupled algebraic multilevel preconditioner applied to multiphysics pde applications drift diffusion , flow transport reaction , resistive mhd . <eos> this study considers the performance of a fully coupled algebraic multilevel preconditioner for newton krylov solution methods . the performance of the preconditioner is demonstrated on a set of challenging multiphysics partial differential equation ( pde ) applications a drift diffusion approximation for semiconductor devices a low mach number formulation for the simulation of coupled flow , transport and non equilibrium chemical reactions and a low mach number formulation for visco resistive magnetohydrodynamics ( mhd ) systems . these systems contain multiple physical mechanisms that are strongly coupled , highly nonlinear , non symmetric and produce solutions with multiple length and time scales . in the context of this study the governing pdes for these systems are discretized in space by a stabilized finite element ( fe ) method that collocates all unknowns at each node of the fe mesh . the algebraic multilevel preconditioner is based on an aggressive coarsening graph partitioning of the non zero block structure of the jacobian matrix . the performance of the algebraic multilevel preconditioner is compared with a standard variable overlap additive schwarz domain decomposition preconditioner . representative performance and parallel scaling results are presented for a set of direct to steady state and fully implicit transient solutions . the performance studies include parallel weak scaling studies on up to <digit> cores and also includes the solution of systems as large as two billion unknowns carried out on <digit> <digit> cores of a cray xt3 <digit> . in general , the results of this study indicate that on this reasonably diverse set of challenging multiphysics applications the algebraic multilevel preconditioner performs very well . copyright ( c ) <digit> john wiley sons , ltd .
an efficient bi objective personnel assignment algorithm based on a hybrid particle swarm optimization model . <eos> a hybrid particle swarm optimization ( hpso ) algorithm which utilizes random key ( rk ) encoding scheme , individual enhancement ( ie ) scheme , and particle swarm optimization ( pso ) for solving a bi objective personnel assignment problem ( bopap ) is presented . the hpso algorithm which was proposed by kuo et al. , 2007andkuo et al. , 2009b is used to solve the flow shop scheduling problem ( fssp ) . in the research of bopap , the main contribution of the work is to improve the f1_f2 heuristic algorithm which was proposed by huang , chiu , yeh , and chang ( <digit> ) . the objective of the f1_f2 heuristic algorithm is to get the satisfaction level ( sl ) value which is satisfied the bi objective values f1 , and f2 for the personnel assignment problem . in this paper , pso is used to search the solution of the input problem in the bopap space . then , with the rk encoding scheme in the virtual space , we can exploit the global search ability of pso thoroughly . based on the ie scheme , we can enhance the local search ability of particles . the experimental results show that the solution quality of bopap based on the proposed hpso algorithm for the first objective f1 ( i.e. , total score ) , the second objective f2 ( i.e. , standard deviation ) , the coefficient of variance ( cv ) , and the time cost is far better than that of the f1_f2 heuristic algorithm . to the best our knowledge , this presented result of the bopap is the best bi objective algorithm known .
micro droplets generated on a rising bubble through an oppositely charged oil water interface . <eos> the mass transfer between immiscible two liquid phases can be greatly enhanced by bubbling gas through a reactor . numerous micro water droplets breaking out from a ruptured water film around a rising bubble through the oil ( upper phase ) water ( lower phase ) interface were demonstrated in the preceding paper ( uemura et al. in europhys lett <digit> <digit> , <digit> ) . in this study , we attempt to oppositely charge the oil and water layers , taking into account the findings of the preliminary study ( uemura et al. in j vis <digit> <digit> , <digit> ) . as a result , this study successfully produces more and finer water droplets than the preceding experiments .
evolving collaboration networks in scientometrics in <digit> <digit> a micro macro analysis . <eos> this paper reports first results on the interplay of different levels of the science system . specifically , we would like to understand if and how collaborations at the author ( micro ) level impact collaboration patterns among institutions ( meso ) and countries ( macro ) . all 2,541 papers ( articles , proceedings papers , and reviews ) published in the international journal scientometrics from <digit> <digit> are analyzed and visualized across the different levels and the evolving collaboration networks are animated over time . studying the three levels in isolation we gain a number of insights ( <digit> ) usa , belgium , and england dominated the publications in scientometrics throughout the <digit> year period , while the netherlands and spain were the subdominant countries ( <digit> ) the number of institutions and authors increased over time , yet the average number of papers per institution grew slowly and the average number of papers per author decreased in recent years ( <digit> ) a few key institutions , including univ sussex , khbo , katholieke univ leuven , hungarian acad sci , and leiden univ , have a high centrality and betweenness , acting as gatekeepers in the collaboration network ( <digit> ) early key authors ( lancaster fw , braun t , courtial jp , narin f , or vanraan afj ) have been replaced by current prolific authors ( such as rousseau r or moed hf ) . comparing results across the three levels reveals that results from one level might propagate to the next level , e.g. , top rankings of a few key single authors can not only have a major impact on the ranking of their institution but also lead to a dominance of their country at the country level movement of prolific authors among institutions can lead to major structural changes in the institution networks . to our knowledge , this is the most comprehensive and the only multi level study of scientometrics conducted to date .
testing a walkthrough methodology for theory based design of walk up and use interfaces . <eos> the value of theoretical analyses in user interface design has been hotly debated . all sides agree that it is difficult to apply current theoretical models within the constraints of real world development projects . we attack this problem in the context of bringing the theoretical ideas within a model of exploratory learning <digit> to bear on the evaluation of alternative interfaces for walk up and use systems . we derived a cognitive walkthrough procedure for systematically evaluating features of an interface in the context of the theory . four people independently applied this procedure to four alternative interfaces for which we have empirical usability data . consideration of the walkthrough sheds light on the consistency with which such a procedure can be applied as well as the accuracy of the results .
fair flow control for atm abr multipoint connections . <eos> multipoint to multipoint communication can be implemented by combining the point to multipoint and multipoint to point connection algorithms . in an atm multipoint to point connection , multiple sources send data to the same destination on a shared tree . traffic from multiple branches is merged into a single stream after every merge point . it is sometimes impossible for the network to determine any source specific characteristics since all sources in the multipoint connection may use the same connection identifiers . the challenge is to develop a fair rate allocation algorithm without per source accounting as this is inequivalent to per connection or per flow accounting in this case . we define fairness objectives for multipoint connections , and we design and simulate an o ( <digit> ) fair atm abr rate allocation scheme for point to point and multipoint connections sharing the same links . simulation results show that the algorithm performs well and exhibits many desirable properties . we list key modifications necessary for any atm abr rate allocation scheme to fairly accommodate multiple sources . ( c ) <digit> elsevier science b.v. all rights reserved .
geometric algorithms for automated design of rotary platen multi shot molds . <eos> this paper describes algorithms for automated design of rotary platen type of multi shot molds for manufacturing multi material objects . the approach behind our algorithms works in the following manner . first , we classify the given multi material object into several basic types based on the relationships among different components in the object . for every basic type , we find a molding sequence based on the precedence constraints resulting due to accessibility and disassembly requirements . then , starting from the last mold stage , we generate the mold pieces for every mold stage . we expect that algorithms described in this paper will provide the necessary foundations for automating the design of rotary platen molds .
a real time responsiveness measurement method of linux based mobile systems for p2p cloud systems . <eos> linux based mobile computing systems such as robots , electronic control devices , and smart phone are the most important types of p2p cloud systems in recent days . to improve the overall performance of networked systems , each mobile computing system requires real time characteristics . for this reason , mobile computing system developers want to know how well real time responsiveness is supported several real time measurement tools have been proposed . however , those previous tools have their own measurement schemes and we think that the results from those models do not show how responsive those systems are . in this paper , we propose elrm , a new real time measurement method that has clear measurement interval definitions and an accurate measurement method for real time responsiveness . we evaluate elrm on various mobile computing systems and compare it with other existing models . as a result , our method can obtain more accurate and intuitive real time responsiveness measurement results .
automatic lung segmentation method for mri based lung perfusion studies of patients with chronic obstructive pulmonary disease . <eos> a novel fully automatic lung segmentation method for magnetic resonance ( mr ) images of patients with chronic obstructive pulmonary disease ( copd ) is presented . the main goal of this work was to ease the tedious and time consuming task of manual lung segmentation , which is required for region based volumetric analysis of four dimensionalmr perfusion studies which goes beyond the analysis of small regions of interest .
headphones with touch control . <eos> the touch headphones are meant for portable music players and aim to present an improvement to the conventional remote control in the headphone wire , and a solution for controls on wireless in ear type headphones . two capacitive touch sensors per earpiece sense when earpieces are being tapped on , and being put in or out .
an adaptive unsupervised approach toward pixel clustering and color image segmentation . <eos> this paper proposes an adaptive unsupervised scheme that could find diverse applications in pattern recognition as well as in computer vision , particularly in color image segmentation the algorithm , named ant colony fuzzy c means hybrid algorithm ( afha ) , adaptively clusters image pixels viewed as three dimensional data pieces in the rgb color space the ant system ( as ) algorithm is applied for intelligent initialization of cluster centroids . which endows clustering with adaptivity . considering algorithmic efficiency , an ant subsampling step is performed to reduce computational complexity while keeping the clustering performance close to original one . experimental results have demonstrated afha clustering 's advantage of smaller distortion and more balanced cluster centroid distribution over fcm with random and uniform initialization quantitative comparisons with the x means algorithm also show that afha makes a better pre segmentation scheme over x means we further extend its application to natural image segmentation . taking into account the spatial information and conducting merging steps in the image space extensive tests were taken to examine the performance of the proposed scheme results indicate that compared with classical segmentation algorithms such as mean shift and normalized cut , our method could generate reasonably good or better image partitioning , which illustrates the method 's practical value ( c ) <digit> elsevier ltd. all rights reserved
an implementation of the acm siggraph proposed graphics standard in a multisystem environment . <eos> los alamos scientific laboratory ( lasl ) has implemented a graphics system designed to support one user interface for all graphics devices in all operating environments at lasl . the common graphics system ( cgs ) will support level one of the graphics standard proposed by the acm siggraph graphic standards planning committee . cgs is available in six operating environments of two different word lengths and supports four types of graphics devices . it can generate a pseudodevice file that may be postprocessed and edited for a particular graphics device , or it can generate device specific graphics output directly . program overlaying and dynamic buffer sharing are also supported . cgs is structured to isolate operating system dependencies and graphics device dependencies . it is written in the ratfor ( rational fortran ) language , which supports control flow statements and macro expansion . cgs is maintained as a single source program from which each version can be extracted automatically .
can social bookmarking enhance search in the web . <eos> social bookmarking is an emerging type of a web service that helps users share , classify , and discover interesting resources . in this paper , we explore the concept of an enhanced search , in which data from social bookmarking systems is exploited for enhancing search in the web . we propose combining the widely used link based ranking metric with the one derived using social bookmarking data . first , this increases the precision of a standard link based search by incorporating popularity estimates from aggregated data of bookmarking users . second , it provides an opportunity for extending the search capabilities of existing search engines . individual contributions of bookmarking users as well as the general statistics of their activities are used here for a new kind of a complex search where contextual , temporal or sentiment related information is used . we investigate the usefulness of social bookmarking systems for the purpose of enhancing web search through a series of experiments done on datasets obtained from social bookmarking systems . next , we show the prototype system that implements the proposed approach and we present some preliminary results .
a survey on approaches to gridification . <eos> the grid shows itself as a globally distributed computing environment , in which hardware and software resources are virtualized to transparently provide applications with vast capabilities . just like the electrical power grid , the grid aims at offering a powerful yet easy to use computing infrastructure to which applications can be easily ' plugged ' and efficiently executed . unfortunately , it is still very difficult to grid enable applications , since current tools force users to take into account many details when adapting applications to run on the grid . in this paper , we survey some of the recent efforts in providing tools for easy gridification of applications and propose several taxonomies to identify approaches followed in the materialization of such tools . we conclude this paper by describing common features among the proposed approaches , and by pointing out open issues and future directions in the research and development of gridification methods . copyright ( c ) <digit> john wiley sons , ltd .
online sequential extreme learning machine in nonstationary environments . <eos> system identification in nonstationary environments represents a challenging problem to solve and lots of efforts have been put by the scientific community in the last decades to provide adequate solutions on purpose . most of them are targeted to work under the system linearity assumption , but also some have been proposed to deal with the nonlinear case study . in particular the authors have recently advanced a neural architecture , namely time varying neural networks ( tv nn ) , which has shown remarkable identification properties in the presence of nonlinear and nonstationary conditions . tv nn training is an issue due to the high number of free parameters and the extreme learning machine ( elm ) approach has been successfully used on purpose . elm is a fast learning algorithm that has recently caught much attention within the neural networks ( nns ) research community . many variants of elm have been appeared in recent literature , specially for the stationary case study . the reference one for tv nn training is named elm tv and is of batch learning type . in this contribution an online sequential version of elm tv is developed , in response to the need of dealing with applications where sequential arrival or large number of training data occurs . this algorithm generalizes the corresponding counterpart working under stationary conditions . its performances have been evaluated in some nonstationary and nonlinear system identification tasks and related results show that the advanced technique produces comparable generalization performances to elm tv , ensuring at the same time all benefits of an online sequential approach .
accelerating mean time to failure computations . <eos> in this paper we consider the problem of numerical computation of the mean time to failure ( mttf ) in markovian dependability and or performance models . the problem can be cast as a system of linear equations which is solved using an iterative method preserving sparsity of the markov chain matrix . for highly dependable systems , system failure is a rare event and the above system solution can take an extremely large number of iterations . we propose to solve the problem by dividing the computation in two parts . first , by making some of the high probability states absorbing , we compute the mttf of the modified markov chain . in a subsequent step , by solving another system of linear equations , we are able to compute the mttf of the original model . we prove that for a class of highly dependable systems , the resulting method can speed up computation of the mttf by orders of magnitude . experimental results supporting this claim are presented . we also obtain bounds on the convergence rate for computing the mean entrance time of a rare set of states in a class of queueing models .
genetic code an alternative model of translation . <eos> abstract our earlier studies of translation have led us to a specific numeric coding of nucleotides ( a <digit> , c <digit> , g <digit> , and u <digit> ) that is , a quaternary numeric system to ordering of digrams and codons ( read right to left . yx and z.yx ) as ordinal numbers from <digit> to <digit> and to seek hypothetic transformation of mrna to <digit> canonic amino acids . in this work , we show that amino acids match the ordinal numberthat is , follow as transforms of their respective digrams and or mrna codons . sixteen digrams and their respective amino acids appear as a parallel ( discrete ) array . a first approximation of translation in this view is demonstrated by a twisted spiral on the side of phantom codons and by ordering amino acids in the form of a cross on the other side , whereby the transformation of digrams and or phantom codons to amino acids appears to be one to one classification of canonical amino acids derived from our dynamic model clarifies physicochemical criteria , such as purinity , pyrimidinity , and particularly codon rules . the system implies both the rules of siemion and siemion and of davidov , as well as balances of atomic and nucleon numbers within groups of amino acids . formalization in this system offers the possibility of extrapolating backward to the initial organization of heredity .
a two step automatic sleep stage classification method with dubious range detection . <eos> a two step classifier for automatic sleep staging is proposed . the system provides two outputs non dubious and dubious classification . the dubious epochs are tagged and re assigned according to a post processing step . the system indicates to an expert physician which results need revision . the accuracy of non dubious classification for wake and rem is around <digit> % .
smartphone based hierarchical crowdsourcing for weed identification . <eos> a novel hierarchical crowdsourcing based system for weed identification . combines image processing with crowdsourcing weed identification . framework for unsupervised determination of crowd hierarchy . prototype that supports low cost and accurate weed identification .
cooperating with free riders in unstructured p2p networks . <eos> free riding is a common phenomenon in peer to peer ( p2p ) file sharing networks . although several mechanisms have been proposed to handle free ridingmostly to exclude free riders , few of them have been adopted in a practical system . this may be attributed to the fact that the mechanisms are often nontrivial , and that completely eliminating free riders could jeopardize the sheer power of the network arising from the huge volume of its participants . rather than excluding free riders , we incorporate and utilize them to provide global index service to the files shared in the network , as well as to relay messages in the search process . the simulation results indicate that our mechanism not only can shift the query processing load from non free riders to free riders , but can also significantly boost the search efficiency of a plain gnutella . moreover , the mechanism is quite resilient to high free riding ratio .
sharing many secrets with computational provable security . <eos> two new multi secret sharing schemes , with computational provable security . the security proofs are in the standard model . the two schemes generalize schemes previously proposed in the literature . we compare the two schemes in terms of security , efficiency and extendability . the schemes work for general access structures .
joint packet scheduling and radio resource assignment for wimax networks . <eos> the ieee 802.16 standard defines the qos signaling framework and various types of service flows , but left the qos based packet scheduling and radio resource assignment undefined . this paper proposes a novel joint packet scheduling and radio resource assignment algorithm for wimax networks . our algorithms can effectively assign the suitable slots to meet the qos requirements of the different service type flows while taking the throughput and fairness into considerations . the effectiveness of our algorithms have been demonstrated through extensive analysis and simulation data . the results show that our algorithms greatly improve the throughput with relatively low complexity .
multi context photo browsing on mobile devices based on tilt dynamics . <eos> this paper presents a photo browsing system on mobile devices to browse and search photos efficiently by tilting action . it employs tilt dynamics and a multi scale photo screen layout for enhancing the browsing and the search capability respectively . the implementation uses continuous inputs from an accelerometer , and a multimodal ( visual , audio and vibrotactile ) display coupled with the states of this model . the model is based on a simple physical model , with its characteristics shaped to enhance controllability . the multi scale layout holds both local and global view for users to both control photos and look at the surrounding context in a single framework . the experiment on samsung mits pda used seven novice users browsing from <digit> photos . we compare a tilt based interaction method with a button based browser and an ipod wheel by a quantitative usability criteria and subjective experience . the proposed tilt dynamics improves the usability over conventional dynamics . the ipod wheel has mixed performance comparing worse on some metrics than button pushing or tilt interaction , despite its commercial popularity .
session based access control in geographically replicated internet services . <eos> performance critical services over internet often rely on geographically distributed architectures of replicated servers . content delivery networks ( cdn ) are a typical example where service is based on a distributed architecture of replica servers to guarantee resource availability and proximity to final users . in such distributed systems , network links are not dedicated , and may be subject to external traffic . this brings up the need to develop access control policies that adapt to network load changing conditions . further , internet services are mainly session based , thus an access control support must take into account a proper differentiation of requests and perform session based decisions while considering the dynamic availability of resources due to external traffic . in this paper we introduce a distributed architecture with access control capabilities at session aware access points . we consider two types of services characterized by different patterns of resource consumption and priorities . we formulate a markov modulated poisson decision process for access control that captures the heterogeneity of multimedia services and the variable availability of resources due to external traffic . the proposed model is optimized by means of stochastic analysis , showing the impact of external traffic on service quality . the structural properties of the optimal solutions are studied and considered as the basis for the formulation of heuristics . the performance of the proposed heuristics is studied by means of simulations , showing that in some typical scenario they perform close to the optimum .
periods in partial words an algorithm . <eos> partial words are finite sequences over a finite alphabet that may contain some holes . a variant of the celebrated finewilf theorem shows the existence of a bound l l ( h , p , q ) l l ( h , p , q ) such that if a partial word of length at least l with h holes has periods p and q , then it also has period gcd ( p , q ) gcd ( p , q ) . in this paper , we associate a graph with each p and q periodic word , and study two types of vertex connectivity on such a graph modified degree connectivity and r set connectivity where r q mod p . as a result , we give an algorithm for computing l ( h , p , q ) l ( h , p , q ) in the general case and show how to use it to derive the closed formulas .
unsupervised learning of word segmentation rules with genetic algorithms and inductive logic programming . <eos> this article presents a combination of unsupervised and supervised learning techniques for the generation of word segmentation rules from a raw list of words . first , a language bias for word se mentation is introduced and a simple genetic algorithm is used in the search for a segmentation that corresponds to the best bias value . in the second phase , the words segmented by the genetic algorithm are used as an input for the first order decision list learner clog . the result is a set of first order rules which can be used for segmentation of unseen words . when applied on either the training data or unseen data , these rules produce segmentations which are linguistically meaningful , and to a large degree conforming to the annotation provided .
nonexponential evolution equations and operator ordering . <eos> nonexponential evolution equations can be treated using a formalism involving the evolution operator method , which , unlike the ordinary case , is not expressed in terms of exponential operators . the use of this technique requires particular care associated with the operator ordering . in this paper , we will present a first systematic approach to this type of problems . ( c ) <digit> elsevier ltd. all rights reserved .
application of genetic algorithm for unknown parameter estimations in cylindrical fin . <eos> this article deals with the application of the genetic algorithm ( ga ) for optimizing an inverse problem and retrieving unknown parameters in cylindrical fin geometry . parameters such as the thermal conductivity and the heat transfer coefficient are attempted for estimation in order to satisfy a desired temperature field in the medium . the study is done for single parameter and simultaneous two parameter retrievals . the temperature field is calculated from a forward problem using the finite difference method using some known values of the properties . these properties are ultimately retrieved by an inverse approach using the ga. the study is done for different controlling parameters such as the number of generations , measurement errors and number of measurement locations . for two parameter simultaneous estimation , many combination of unknown parameters are observed to satisfy a given temperature field , and their ratio is only found to be successfully estimated . the present work is proposed to be useful for selecting the thermal properties which are required to satisfy a given temperature field .
formulation of pedestrian movement in microscopic models with continuous space representation . <eos> when the microscopic pedestrian models , in which pedestrian space is continuously represented , are used to simulate pedestrian movement in the buildings with internal obstacles , some issues arise and need be dealt with in detail . this paper discusses two of the issues , namely formulating the desired direction of each pedestrian in the buildings and determining the region around each pedestrian , other individuals and obstacles in which affect his or her movement . the methods for computing the desired direction and effect region are proposed , using the algorithms for the potential of pedestrian space . by numerical experiments , the performance results of three proposed formulae for the desired direction are compared , the method for the effect region is tested , and the validity of the method for computing the desired direction as considering the border effect of obstacles is verified . numerical results indicate that the proposed methods can be used to formulate pedestrian movement , especially in the buildings with internal obstacles , in the microscopic models with continuous space representation .
extending clips to support temporal representation and reasoning . <eos> applications using expert systems for monitoring and control problems often require the ability to represent temporal knowledge and to apply reasoning based on that knowledge . incorporating temporal representation and reasoning into expert systems leads to two problems in development dealing with an implied temporal order of events using a non procedural tool and maintaining the large number of temporal relations that can occur among facts in the knowledge base . in this paper we explore these problems by using an expert system shell , clips ( c language integrated production system ) , to create temporal relations using common knowledge based constructs . we also build an extension to clips through a user defined function which generates the temporal relations from those facts . we use the extension to create and maintain temporal relations in a workflow application that monitors and controls an engineering design change review process . we also propose a solution to ensure truth maintenance among temporally related facts that links our temporal extension to the clips facility for truth maintenance .
mpeg <digit> digital items to support integration of heterogeneous multimedia content . <eos> the melisa system is a distributed platform for multi platform sports content broadcasting , providing end users with a wide range of real time interactive services during the sport event , such as statistics , visual aids or enhancements , betting , and user and context specific advertisements . in this paper , we present the revamped design of the complete system and the implementation of a middleware entity utilizing concepts present in the emerging mpeg <digit> framework . more specifically , all multimedia content is packaged in a self contained digital item , containing both the binary information ( video , graphics , etc. ) required for the playback , as well as structured representations of the different entities that can handle this item and the actions they can perform on it . this module essentially stands between the different components of the integrated content preparation system , thereby not disrupting their original functionality at all additional tweaks are performed in the receiver sides , as well , to ensure that the additional information and provisions are respected . the outcome of this design upgrade is that ipr issues are dealt with successfully , both with respect to the content itself and to the functionality of the subscription levels in addition to this , end users can be presented with personalized forms of the final content , e.g. , viewing in play virtual advertisements that match their shopping habits and preferences , thus enhancing the viewing experience and creating more revenue opportunities via targeted advertisements . ( c ) <digit> elsevier b.v. all rights reserved .
linguistic recognition system for identification of some possible genes mediating the development of lung adenocarcinoma . <eos> in the present article , we develop a linguistic recognition system for identification of some possible genes mediating the development of human lung adenocarcinoma . the methodology involves dimensionality reduction , classifying the genes through incorporation of the notion of linguistic fuzzy sets low , medium and high , and finally selection of some possible genes obtained by a rule generation grouping technique . the system has been successfully applied on two microarray gene expression data sets . the results are appropriately validated by some earlier investigations , gene expression profiles and t test . the proposed methodology has been able to find more true positives than an existing one in identifying responsible genes . moreover , we have found some new genes that may have role in mediating the development of lung adenocarcinoma . ( c ) <digit> elsevier b.v. all rights reserved .
the existence and upper bound for two types of restricted connectivity . <eos> in this paper , we study two types of restricted connectivity k ( g ) k ( g ) is the cardinality of a minimum vertex cut s s such that every component of g s g s has at least k k vertices k ( g ) is the cardinality of a minimum vertex cut s s such that there are at least two components in g s g s of order at least k k . in this paper , we give some sufficient conditions for the existence and upper bound of k ( g ) k ( g ) and or k ( g ) , and study some properties of these two parameters .
coloring geometric range spaces . <eos> we study several coloring problems for geometric range spaces . in addition to their theoretical interest , some of these problems arise in sensor networks . given a set of points in r ( <digit> ) or r ( <digit> ) , we want to color them so that every region of a certain family ( e. g. , every disk containing at least a certain number of points ) contains points of many ( say , k ) different colors . in this paper , we think of the number of colors and the number of points as functions of k. obviously , for a fixed k using k colors , it is not always possible to ensure that every region containing k points has all colors present . thus , we introduce two types of relaxations either we allow the number of colors used to increase to c ( k ) , or we require that the number of points in each region increases to p ( k ) . symmetrically , given a finite set of regions in r ( <digit> ) or r ( <digit> ) , we want to color them so that every point covered by a sufficiently large number of regions is contained in regions of k different colors . this requires the number of covering regions or the number of allowed colors to be greater than k. the goal of this paper is to bound these two functions for several types of region families , such as halfplanes , halfspaces , disks , and pseudo disks . this is related to previous results of pach , tardos , and toth on decompositions of coverings .
a methodology for design of scalable architectures in software radio networks a unified device and network perspective . <eos> this paper proposes the tissue methodology as a novel methodology for analysis , design and synthesis of networked embedded systems and subsequent development of distributed architectural frameworks . the proposed method aims at reducing the development time through the use of reconfigurable hw sw components and the application of automatic code generation techniques . we devise the usefulness of the proposed methodology in the context of mobile ad hoc networks ( manet ) which exploit software radio ( sr ) technology for reconfigurability issues . drawbacks of current design and simulation tools and advantages coming from the application of the tm are discussed in the paper .
temporal development methods for agent based systems . <eos> in this paper we overview one specific approach to the formal development of multi agent systems . this approach is based on the use of temporal logics to represent both the behaviour of individual agents , and the macro level behaviour of multi agent systems . we describe how formal specification , veri . cation and refinement can all be developed using this temporal basis , and how implementation can be achieved by directly executing these formal representations . we also show how the basic framework can be extended in various ways to handle the representation and implementation of agents capable of more complex deliberation and reasoning .
lay persons ' and professionals ' nutrition related vocabularies and their matching to a general and a specific thesaurus . <eos> this study examines the differences between expressions used by lay persons and professionals in nutrition related questions and answers , and to what degree general finnish ontology ( gfo ) and a medical thesaurus ( finmesh ) cover these expressions . fifty question answer pairs were collected in an electronic answering service . nutrition related concepts and their expressions with their semantic relations were identified . the vocabularies of lay persons and professionals were found to be quite similar . this hints that a special consumer health vocabulary in the field of nutrition is not needed . gfo covered <digit> % of all expressions in questions and <digit> % of expressions in answers . finmesh covered <digit> % of expressions in both groups . the overlapping match of the thesauri was low , <digit> % in both questions and in answers . gfo and finmesh were found to be poor tools for supporting users in expressing nutrition related information needs . gfo seemed not to form a covering bridge to finmesh .
estimating forest biomass using small footprint lidar data an individual tree based approach that incorporates training data . <eos> a new individual tree based algorithm for determining forest biomass using small footprint lidar data was developed and tested . this algorithm combines computer vision and optimization techniques to become the first training data based algorithm specifically designed for processing forest lidar data . the computer vision portion of the algorithm uses generic properties of trees in small footprint lidar canopy height models ( chms ) to locate trees and find their crown boundaries and heights . the ways in which these generic properties are used for a specific scene and image type is dependent on <digit> parameters , nine of which are set using training data and the neldermead simplex optimization procedure . training data consist of small sections of the lidar data and corresponding ground data . after training , the biomass present in areas without ground measurements is determined by developing a regression equation between properties derived from the lidar data of the training stands and biomass , and then applying the equation to the new areas . a first test of this technique was performed using <digit> plots ( radius <digit> m ) in a loblolly pine plantation in central virginia , usa ( 37.42 n , 78.68 w ) that was not intensively managed , together with corresponding data from a lidar canopy height model ( resolution 0.5 m ) . results show correlations ( r ) between actual and predicted aboveground biomass ranging between 0.59 and 0.82 , and rmses between 13.6 and 140.4 t ha depending on the selection of training and testing plots , and the minimum diameter at breast height ( <digit> or <digit> cm ) of trees included in the biomass estimate . correlations between lidar derived plot density estimates were low ( 0.22 r 0.56 ) but generally significant ( at a <digit> % confidence level in most cases , based on a one tailed test ) , suggesting that the program is able to properly identify trees . based on the results it is concluded that the validation of the first training data based algorithm for determining forest biomass using small footprint lidar data was a success , and future refinement and testing are merited .
the upper layers of the iso osi reference model ( part ii ) ( reprinted from computer standards and interfaces , vol <digit> , pg <digit> <digit> , <digit> ) . <eos> this review is intended as an introduction to the communication concepts and functions associated with the upper layers of the iso osi reference model . it describes , in general terms , the requirements and the benefits of an open communication and defines the fundamental requirements for interworking among distributed computer systems . ( c ) <digit> published by elsevier science b.v. all rights reserved .
adjustable chain trees for proteins . <eos> a chain tree is a data structure for changing protein conformations . it enables very fast detection of clashes and free energy potential calculations . a modified version of chain trees that adjust themselves to the changing conformations of folding proteins is introduced . this results in much tighter bounding volume hierarchies and therefore fewer intersection checks . computational results indicate that the efficiency of the adjustable chain trees is significantly improved compared to the traditional chain trees .
real time point based rendering using visibility map . <eos> because of its simplicity and intuitive approach , point based rendering has been a very popular research area . recent approaches have focused on hardware accelerated techniques . by applying a deferred shading scheme , both high quality images and high performance rendering have been achieved . however , previous methods showed problems related to depth based visibility computation . we propose an extended point based rendering method using a visibility map . in our method we employ a distance based visibility technique ( replacing depth based visibility ) , an averaged position map and an adaptive fragment processing scheme , resulting in more accurate and improved image quality , as well as improved rendering performance .
expression and effect of transforming growth factor and tumor necrosis factor in human pheochromocytoma . <eos> this study observed the expression of transforming growth factor ( tgf ) and tumor necrosis factor ( tnf ) in pheochromocytoma ( pheo ) tissue and examined their effects on the proliferation and apoptosis of human pheo cells . the mrna and protein expressions of tgf and tnf were higher in pheo tissues than in normal adrenal medullary tissues , and their expressions varied with pathological features . tgf and tnf stimulated the proliferation of primary human pheo cells , but had no effect on the cell apoptosis . both tgf and tnf might be involved in the pathogenesis of human pheo . tnf needs to be further investigated before its treatment of pheo can be realized in clinical practice
interactive visualisation of spins and clusters in regular and small world ising models with cuda on gpus . <eos> three dimensional simulation models are hard to visualise for dense lattice systems , even with cutaways and flythrough techniques . we use multiple graphics processing units ( gpus ) , cuda and opengl to increase our understanding of computational simulation models such as the <digit> d and <digit> d ising systems with small world link rewiring by accelerating both the simulation and visualisation into interactive time . we show how interactive model parameter updates , visual overlaying of measurements and graticules , cluster labelling and other visual highlighting cues enhance user intuition of the models meaning and exploit the enhanced simulation speed to handle model systems large enough to explore multi scale phenomena .
efficient and accurate nearest neighbor and closest pair search in high dimensional space . <eos> nearest neighbor ( nn ) search in high dimensional space is an important problem in many applications . from the database perspective , a good solution needs to have two properties ( i ) it can be easily incorporated in a relational database , and ( ii ) its query cost should increase sublinearly with the dataset size , regardless of the data and query distributions . locality sensitive hashing ( lsh ) is a well known methodology fulfilling both requirements , but its current implementations either incur expensive space and query cost , or abandon its theoretical guarantee on the quality of query results . motivated by this , we improve lsh by proposing an access method called the locality sensitive b tree ( lsb tree ) to enable fast , accurate , high dimensional nn search in relational databases . the combination of several lsb trees forms a lsb forest that has strong quality guarantees , but improves dramatically the efficiency of the previous lsh implementation having the same guarantees . in practice , the lsb tree itself is also an effective index which consumes linear space , supports efficient updates , and provides accurate query results . in our experiments , the lsb tree was faster than ( i ) idistance ( a famous technique for exact nn search ) by two orders of magnitude , and ( ii ) medrank ( a recent approximate method with nontrivial quality guarantees ) by one order of magnitude , and meanwhile returned much better results . as a second step , we extend our lsb technique to solve another classic problem , called closest pair ( cp ) search , in high dimensional space . the long term challenge for this problem has been to achieve subquadratic running time at very high dimensionalities , which fails most of the existing solutions . we show that , using a lsb forest , cp search can be accomplished in ( worst case ) time significantly lower than the quadratic complexity , yet still ensuring very good quality . in practice , accurate answers can be found using just two lsb trees , thus giving a substantial reduction in the space and running time . in our experiments , our technique was faster ( i ) than distance browsing ( a well known method for solving the problem exactly ) by several orders of magnitude , and ( ii ) than d shift ( an approximate approach with theoretical guarantees in low dimensional space ) by one order of magnitude , and at the same time , outputs better results .
a performance evaluation of a coverage compensation based algorithm for wireless sensor networks . <eos> recent years , coverage has been widely investigated as one of the fundamental quality measurements of wireless sensor networks . in order to maintaining the coverage while saving energy of networks , algorithms have been developed to keep a minimum cover set of sensors working and turn off the redundant sensors . generally , centralized algorithms can give a better result than distributed algorithms in terms of the number of active sensors . however , the heavy computation requirements and message overhead for collecting geographical location data keep centralized algorithms out of most distributed scenarios . in this article , based on the idea of coverage compensation a distributed node partition algorithm for random deployments is presented to generate a minimum cover set by using the optimal node distributions created by the centralized algorithms such as ga. a genetic algorithm for coverage is proposed too to demonstrate how an optimal coverage node distribution created by ga can be used in a distributed scenario . ours works are simulated on jgap and ns2 . the simulation result shows that our partition algorithm based on coverage compensation can achieve the same performance as ocops in terms of coverage and number of active sensors while using less control messages .
automated detection of microaneurysms using scale adapted blob analysis and semi supervised learning . <eos> despite several attempts , automated detection of microaneurysm ( ma ) from digital fundus images still remains to be an open issue . this is due to the subtle nature of mas against the surrounding tissues . in this paper , the microaneurysm detection problem is modeled as finding interest regions or blobs from an image and an automatic local scale selection technique is presented . several scale adapted region descriptors are introduced to characterize these blob regions . a semi supervised based learning approach , which requires few manually annotated learning examples , is also proposed to train a classifier which can detect true mas . the developed system is built using only few manually labeled and a large number of unlabeled retinal color fundus images . the performance of the overall system is evaluated on retinopathy online challenge ( roc ) competition database . a competition performance measure ( cpm ) of 0.364 shows the competitiveness of the proposed system against state of the art techniques as well as the applicability of the proposed features to analyze fundus images .
some complexity questions related to distributive computing ( preliminary report ) . <eos> let m equil <digit> , <digit> , <digit> , ... , m <digit> , n equil <digit> , <digit> , <digit> , ... , n <digit> , and f m n <digit> , <digit> a boolean valued function . we will be interested in the following problem and its related questions . let i egr m , j egr n be integers known only to two persons p <digit> and p <digit> , respectively . for p <digit> and p <digit> to determine cooperatively the value f ( i , j ) , they send information to each other alternately , one bit at a time , according to some algorithm . the quantity of interest , which measures the information exchange necessary for computing f , is the minimum number of bits exchanged in any algorithm . for example , if f ( i , j ) equil ( i j ) mod <digit> . then <digit> bit of information ( conveying whether i is odd ) sent from p <digit> to p <digit> will enable p <digit> to determine f ( i , j ) , and this is clearly the best possible . the above problem is a variation of a model of abelson <digit> concerning information transfer in distributive computions .
towards flattenable mesh surfaces . <eos> in many industries , products are constructed by assembled surface patches in <digit> <digit> , where each patch is expected to have an isometric map to a corresponding region in <digit> <digit> . the widely investigated developable surfaces in differential geometry show this property . however , the method to model a piecewise linear surface with this characteristic is still under research . to distinguish from the continuous developable surface , we name them as flattenable mesh surfaces since a polygonal mesh has the isometric mapping property if it can be flattened into a two dimensional sheet without stretching . in this paper , a novel flattenable mesh surface ( flattenable laplacian mesh ) is introduced and the relevant modelling tool is formulated . moreover , for a given triangular mesh which is almost flattenable , a local perturbation approach is developed to improve its flattenability . the interference between the meshes under process and their nearby objects has been prevented in this local flattenable perturbation . both the computations of flattenable laplacian meshes and the flattenable perturbation are based on the constrained optimization technology .
identification of critical points for the design and synthesis of flexible processes . <eos> optimization problems for the design and synthesis of flexible chemical processes are often associated with highly discretized models . the ultimate goal of this work is to significantly reduce the set of uncertain parameter points used in these problems . to accomplish the task , an approach was developed for identifying the minimum set of critical points needed for flexible design . critical points in this work represent those values of uncertain parameters that determine optimal overdesign of process , so that feasible operation is assured within the specified domain of uncertain parameters . the proposed approach identifies critical values of uncertain parameters a priori by the separate maximization of each design variable , together with simultaneous optimization of the economic objective function . during this procedure , uncertain parameters are transformed into continuous variables . three alternative methods are proposed within this approach the formulation based on karush kuhn tucker ( kkt ) optimality conditions , the iterative two level method , and the approximate one level method . the identified critical points are then used for the discretization of infinite uncertain problems , in order to obtain the design with the optimum objective function and flexibility index at unity . all three methods can identify vertex or even nonvertex critical points , whose total number is less than or equal to the number of design variables , which represents a significant reduction in the problem 's dimensionality . some examples are presented illustrating the applicability and efficiency of the proposed approach , as well as the role of the critical points in the optimization of design problems under uncertainty . ( c ) <digit> elsevier ltd. all rights reserved .
synthesis of autosymmetric functions in a new three level form . <eos> autosymmetric functions exhibit a special type of regularity that can speed up the minimization process . based on this autosymmetry , we propose a three level form of logic synthesis , called orax ( exor and or ) , to be compared with the standard minimal sop ( sum of products ) form . first we provide a fast orax minimization algorithm for autosymmetric functions . the orax network for a function f has a first level of at most <digit> ( n k ) exor gates , followed by the and or levels , where n is the number of input variables and k is the autosymmetry degree of f. in general a minimal orax form has smaller size than a standard minimal sop form for the same function . we show how the gain in area of orax over sop can be measured without explicitly generating the latter . if preferred , a sop expression can be directly derived from the corresponding orax . a set of experimental results confirms that the orax form is generally more compact than the sop form , and its synthesis is much faster than classical three level logic minimization . indeed orax and sop minimization times are often comparable , and in some cases orax synthesis is even faster .
a new class of multi stable neural networks stability analysis and learning process . <eos> recently , multi stable neural networks ( nn ) with exponential number of attractors have been presented and analyzed theoretically however , the learning process of the parameters of these systems while considering stability conditions and specifications of real world problems has not been studied . in this paper , a new class of multi stable nns using sinusoidal dynamics with exponential number of attractors is introduced . the sufficient conditions for multi stability of the proposed system are posed using lyapunov theorem . in comparison to the other methods in this class of multi stable nns , the proposed method is used as a classifier by applying a learning process with respect to the topological information of data and conditions of lyapunov multi stability . the proposed nn is applied on both synthetic and real world datasets with an accuracy comparable to classical classifiers .
an effective approach to entity resolution problem using quasi clique and its application to digital libraries . <eos> we study how to resolve entities that contain a group of related elements in them ( e.g. , an author entity with a list of citations or an intermediate result by group by sql query ) . such entities , named as grouped entities , frequently occur in many applications . by exploiting contextual information mined from the group of elements per entity in addition to syntactic similarity , we show that our approach , quasi clique , improves precision and recall unto <digit> % when used together with a variety of existing entity resolution solutions , but never worsens them .
a parallel shortest path algorithm based on graph partitioning and iterative correcting . <eos> in this paper , we focus on satisfying the actual demands of quickly finding the shortest paths over real road networks in an intelligent transportation system . a parallel shortest path algorithm based on graph partitioning and iterative correcting is proposed . after evaluating the algorithm using three real road networks , we conclude that our graph partitioning and iterative correcting based parallel algorithm has good performance . in addition , we do the evaluation with two hardware platforms , and the new parallel algorithm achieves more than a <digit> fold speedup on <digit> processes in an ibm cluster ( <digit> cores , <digit> nodes ) , gets about <digit> fold speedup on <digit> processes with dawning 5000a server ( <digit> cores , <digit> nodes ) .
teaching page replacement algorithms with a java based vm simulator . <eos> computer system courses have long benefited from simulators in conveying important concepts to students . we have modified the java source code of the moss virtual memory simulator to allow users to easily switch between different page replacement algorithms including fifo , lru , and optimal replacement algorithms . the simulator clearly demonstrates the behavior of the page replacement algorithms in a virtual memory system , and provides a convenient way to illustrate page faults and their corresponding page fault costs . equipped with a gui for control and page table visualization , it allows the student to visually see how page tables operate and which pages page replacement algorithms evict in case of a page fault . moreover , class projects may be assigned requiring operating system students to code new page replacement algorithms which they want to simulate and integrate them into the moss vm simulator code files thus enhancing the students ' java coding skills . by running various simulations , students can collect page replacement statistics thus comparing the performance of various replacement algorithms .
finite element modeling of multi pass welding and shaped metal deposition processes . <eos> this paper describes the formulation adopted for the numerical simulation of the shaped metal deposition process ( smd ) and the experimental work carried out at itp industry to calibrate and validate the proposed model . the smd process is a novel manufacturing technology , similar to the multi pass welding used for building features such as lugs and flanges on fabricated components ( see fig. 1a and b ) . a fully coupled thermo mechanical solution is adopted including phase change phenomena defined in terms of both latent heat release and shrinkage effects . temperature evolution as well as residual stresses and distortions , due to the successive welding layers deposited , are accurately simulated coupling the heat transfer and the mechanical analysis . the material behavior is characterized by a thermo elasto viscoplastic constitutive model coupled with a metallurgical model . nickel super alloy <digit> is the target material of this work . both heat convection and heat radiation models are introduced to dissipate heat through the boundaries of the component . an in house coupled fe software is used to deal with the numerical simulation and an ad hoc activation methodology is formulated to simulate the deposition of the different layers of filler material . difficulties and simplifying hypotheses are discussed . thermo mechanical results are presented in terms of both temperature evolution and distortions , and compared with the experimental data obtained at the smd laboratory of itp .
a k nearest neighbours method based on imprecise probabilities . <eos> k nearest neighbours algorithms are among the most popular existing classification methods , due to their simplicity and good performances . over the years , several extensions of the initial method have been proposed . in this paper , we propose a k nearest neighbours approach that uses the theory of imprecise probabilities , and more specifically lower previsions . we show that the proposed approach has several assets it can handle uncertain data in a very generic way , and decision rules developed within this theory allow us to deal with conflicting information between neighbours or with the absence of close neighbour to the instance to classify . we show that results of the basic k nn and weighted k nn methods can be retrieved by the proposed approach . we end with some experiments on the classical data sets .
optimizing distortion for real time data gathering in randomly deployed sensor networks . <eos> in several wireless sensor network applications , it is required to perform real time reconstruction of the data field being sensed by the network . this task is generally carried out at a central location , e.g. sink node , using a continuous data gathering phase and relying on the known correlation properties of the underlying data field . estimating the overall spatial and temporal distortion in the reconstructed field is an important step toward deciding the number of sensors to be deployed and the data collection algorithm to be used . however , estimating distortion in arbitrary networks is a challenging task . existing work has focused on regular network deployments such as one and two dimensional girds . such deployments are deemed infeasible in a realistic environment . in this paper , we consider one and two dimensional random networks . for the analysis purposes , we assume that the nodes are randomly deployed following poisson distribution . we determine the total distortion function given the correlation coefficients of the field while assuming a simple data gathering protocol . based on this , we also determine the optimal number of nodes to be deployed in the field that will minimize distortion . copyright ( c ) <digit> john wiley sons , ltd .
variable structure neural networks for adaptive control of nonlinear systems using the stochastic approximation . <eos> this paper is concerned with the adaptive control of continuous time nonlinear dynamical systems using neural networks . referred to as a variable structure neural network , a novel neural network architecture , is proposed and shown to be useful in approximating the unknown nonlinearities of dynamical systems . in the variable structure neural network , the number of basis functions can be either increased or decreased with time according specified design strategies so that the network will not overfit or underfit the data set . based on the gaussian radial basis function ( gbrf ) variable neural network , an adaptive control scheme is presented . the location of the centers and the determination of the widths of the gbrfs are analysed using a new method inspired from the adaptive diffuse element method combined with a pruning algorithm . in the standard problem of a feedback based control , the cost to be minimized is a function of the output derivative . when the cost function depends on the output error , the gradient method can not be applied to adjust the neural network parameters . in this case , the stochastic approximation approach allows the computation of the cost function derivatives . the developed weight adaptive laws use a stochastic approximation algorithm . this algorithm consists of the use of the kieferwolfowitz method .
evaluation of two finite element formulations for a rapid 3d stress analysis of sandwich structures . <eos> for efficiently simulating the impact behavior of sandwich structures made from composite face sheets and a lightweight core a rapid and accurate 3d stress analysis is essential . for that reason , a three layered finite element formulation based on plane stress assumptions was recently developed by krger et al. krger , wa , rolfes r , rohwer k. a three layered sandwich element with improved transverse shear stiffness and stress based on fsdt . comput struct , submitted for publication . it has turned out , however , that under concentrated out of plane loads this element formulation lacks appropriate accuracy of stress results . therefore , an improved finite element formulation is developed , which accounts for the full 3d stress state . in a post processing routine , the transverse stresses are improved by using the extended 2d method , which was developed by rolfes and rohwer rolfes r , rohwer k. improved transverse shear stresses in composite finite elements based on first order shear deformation theory . int j numer meth eng <digit> <digit> <digit> and extended to a three layered sandwich structure by krger et al. both the finite element formulation by krger et al. and the new formulation presented in the present article use pure displacement approaches and require only c0 continuity conditions , which simplifies integration into existing fe codes and allows combined application with other finite elements . two examples demonstrate the accuracy and applicability of the two elements .
low complexity load balancing with a self organized intelligent distributed antenna system . <eos> a high call blocking rate is a consequence of an inefficient utilization of system resources , which is often caused by a load imbalance in the network . load imbalances are common in wireless networks with a large number of cellular users . this paper investigates a load balancing scheme for mobile networks that optimizes cellular performance with constraints of physical resource limits and users quality of service demands . in order to efficiently utilize the system resources , an intelligent distributed antenna system ( idas ) fed by a multi base transceiver station ( bts ) has the ability to distribute the system resources over a given geographic area . to enable load balancing among distributed antenna modules we dynamically allocate the remote antenna modules to the btss using an intelligent algorithm . a self optimizing network for an idas is formulated as an integer based linear constrained optimization problem , which tries to balance the load among the btss . a discrete particle swarm optimization ( dpso ) algorithm as an evolutionary algorithm is proposed to solve the optimization problem . the computational results of the dpso algorithm demonstrate optimum performance for small scale networks and near optimum performance for large scale networks . the dpso algorithm is faster with marginally less complexity than an exhaustive search algorithm .
self assessed changes in mental health and employment status as a result of unemployment training . <eos> the main question addressed in this article is what factors in an unemployment programme serve both the individual and society our research focuses on background variables and process variables and how these can be assumed to affect certain dependent variables in unemployment training . the current focus is on the dependent variable subjective assessment of the effect of the training on mental health , together with the more objective dependent variable of employment status after training . self confidence , well being , faith in the future , level of initiative and personal development have been used as indicators of self assessed mental health . data were collected from an unemployment training programme in sweden and the variables combined to create a hypothetical model . the model was statistically tested and then modified with the aid of lisrel statistics , which helps to adjust the model to statistical acceptance . the findings show that the salient factors directly related to the subjective assessment of the effect of training on mental health are gender , attitude to skills development , perceived training requirements and formal educational background . the latter relationship was negative . of indirect importance are the level of commitment of the teacher , the satisfaction of the trainee with the process , and the level of control . the duration of previous unemployment was the only independent variable , which directly affected the employment status after the training , and this was in the negative direction . of indirect importance for this dependent variable were training requirement , satisfaction with the process , own level of control and attitude to skills development .
sliding mode control of quantized systems against bounded disturbances . <eos> this paper investigates the sliding mode control problem of quantized systems with simultaneous input and output disturbances . in a network environment , the output measurements are supposed to be quantized with a logarithmic strategy before transmitting over the digital channels . the main difficulties in this design are as follows ( <digit> ) there exists input output disturbances and state time delay in the plant under consideration , such that model discretization is difficult to be implemented . the design work is therefore forbidden to be considered in continuous time domain ( <digit> ) the quantized signals ( piecewise constants ) can not be used to synthesize a continuous time sliding mode surface ( <digit> ) traditional observer technique is not effective to handle output disturbances . in this paper , a filtering based technique is proposed to solve these difficulties , based on which a sliding mode observer based control scheme is developed to stabilize the resulting closed loop systems . finally , the effectiveness of the proposed methodology is illustrated via a numerical example .
synchronization of stochastic fuzzy cellular neural networks with leakage delay based on adaptive control . <eos> this paper considers the synchronization problem of coupled chaotic fuzzy cellular neural networks with stochastic noise perturbation and time delay in the leakage term by using adaptive feedback control . motivated by the achievements from both the stability of neural networks with time delay in the leakage term and the synchronization of coupled chaotic fuzzy cellular neural networks with stochastic perturbation , lyapunov stability theory combining with stochastic analysis approaches are employed to derive sufficient criteria ensuring the coupled chaotic fuzzy cellular neural networks to be completely synchronous . this paper also presents an illustrative example and uses simulated results of this example to show the feasibility and effectiveness of the proposed scheme .
designing discriminative spatial filter vectors in motor imagery braincomputer interface . <eos> the problem of a volume conduction effect in electroencephalography is considered one of the challenging issues in braincomputer interface ( bci ) community . in this article , we propose a novel method of designing a class discriminative spatial filter assuming that a combination of spatial pattern vectors , irrespective of the eigenvalues of the common spatial pattern ( csp ) , can produce better performance in terms of classification accuracy . we select discriminative spatial filter vectors that determine features in a pairwise manner , that is , eigenvectors of the k largest eigenvalue and the k smallest eigenvalue . although the pair of the eigenvectors of the k largest and the k smallest eigenvalues helps extract discriminative features , we believe that a different set of eigenvector pairs is more appropriate to extract class discriminative features . in our experimental results using the publicly available dataset of bci competition iv , we show that the proposed method outperformed the conventional csp methods and a filter bank csp . <digit> wiley periodicals , inc. int j imaging syst technol , <digit> , <digit> , <digit>
underspecification for a simple process algebra of recursive processes . <eos> this paper deals with underspecification for process algebras which is relevant in early design stages . we consider a form of underspecification that arises from a situation where at a certain design stage the decision between several options of system behaviour is to be postponed until more information is available . we follow an approach of veglioni and de nicola ( lecture notes in computer science <digit> ( <digit> ) <digit> ) who propose to interpret the choice operator of a simple class of finite process terms as underspecification whenever it combines two processes that have some initial action in common , as e.g. in ( a.p b.q ) ( a.r c.s ) . in particular , we consider recursive processes and discuss several extensions . ( c ) <digit> elsevier science b.v. all rights reserved .
pervasive games for education . <eos> the paper analyzes how pervasive games can be used for an efficient transfer of knowledge at universities . pervasive games provide an innovative game model that combines the real world with the virtual world . in this instance , the game concept is used in conjunction with mobile phones as a means of interaction and communication enabler to support learning . the paper presents the design of a pervasive learning game , which was compared with a conventional case study approach in an empirical study with <digit> students in respect of learning efficiency and motivation to learn . the empirical results reveal that the pervasive game leads to higher energetic activation , more positive emotions , more positive attitudes towards learning content and more efficient knowledge transfer than the conventional case study approach .
optimal numerical parameterization of discontinuous galerkin method applied to wave propagation problems . <eos> this paper deals with the high order discontinuous galerkin ( dg ) method for solving wave propagation problems . first , we develop a one dimensional dg scheme and numerically compute dissipation and dispersion errors for various polynomial orders . an optimal combination of time stepping scheme together with the high order dg spatial scheme is presented . it is shown that using a time stepping scheme with the same formal accuracy as the dg scheme is too expensive for the range of wave numbers that is relevant for practical applications . an efficient implementation of a high order dg method in three dimensions is presented . using id convergence results , we further show how to adequately choose elementary polynomial orders in order to equi distribute a priori the discretization error . we also show a straightforward manner to allow variable polynomial orders in a dg scheme . we finally propose some numerical examples in the field of aero acoustics . ( c ) <digit> elsevier inc. all rights reserved .
combining <digit> d geovisualization with force feedback driven user interaction . <eos> we describe a prototype software system for investigating novel human computer interaction techniques for <digit> d geospatial data . this system , m4 geo ( multi modal mesh manipulation of geospatial data ) , aims to provide a more intuitive interface for directly manipulating <digit> d surface data , such as digital terrain models ( dtm ) . the m4 geo system takes place within a <digit> d environment and uses a phantom haptic force feedback device to enhance <digit> d computer graphics with touch based interactions . the phantom uses a <digit> d force feedback stylus , which acts as a virtual finger tip that allows the user to feel the shape ( morphology ) of the terrain 's surface in great detail . in addition , it acts as a touch sensitive tool for different gis tasks , such as digitizing ( draping ) of lines and polygons directly onto a <digit> d surface and directly deforming surfaces ( by pushing or pulling the stylus in or out ) . the user may adjust the properties of the surface deformation ( e.g. , soft or hard ) locally by painting it with a special material color . the overlap of visual and force representation of <digit> d data aides hand eye coordination for these tasks and helps the user to perceive the <digit> d spatial data in a more holistic , multi sensory way . the use of such a <digit> d force feedback device for direct interaction may thus provide more intuitive and efficient alternatives to the mouse and keyboards driven interactions common today , in particular in areas related to digital landscape design , surface hydrology and geotechnical engineering .
locating head and face boundaries for head shoulder images . <eos> this paper presents a model based approach to locate head and face boundaries in a head shoulder image with plain background . three models are constructed for the images , where the head boundary is divided into left right sub boundaries and the face boundary is divided into left right and top bottom sub boundaries . the left right head boundaries are located from two thresholded images and the final result is the combination of them . after the head boundary is located , the four face sub boundaries are located from the grey edge image . the algorithm is carried out iteratively by detecting low level edges and then organizing verifying them using high level knowledge of the general shape of a head . the experimental results using a database of <digit> images show that this approach is promising . ( c ) <digit> pattern recognition society . published by elsevier science ltd. all rights reserved .
a new perspective to null linear discriminant analysis method and its fast implementation using random matrix multiplication with scatter matrices . <eos> null linear discriminant analysis ( lda ) method is a popular dimensionality reduction method for solving small sample size problem . the implementation of null lda method is , however , computationally very expensive . in this paper , we theoretically derive the null lda method from a different perspective and present a computationally efficient implementation of this method . eigenvalue decomposition ( evd ) of s t s b ( where sb is the between class scatter matrix and s t is the pseudoinverse of the total scatter matrix st ) is shown here to be a sufficient condition for the null lda method . as evd of s t s b is computationally expensive , we show that the utilization of random matrix together with s t s b is also a sufficient condition for null lda method . this condition is used here to derive a computationally fast implementation of the null lda method . we show that the computational complexity of the proposed implementation is significantly lower than the other implementations of the null lda method reported in the literature . this result is also confirmed by conducting classification experiments on several datasets .
a fuzzy coherent rule mining algorithm . <eos> in real world applications , transactions usually consist of quantitative values . many fuzzy data mining approaches have thus been proposed for finding fuzzy association rules with the predefined minimum support from the give quantitative transactions . however , the common problems of those approaches are that an appropriate minimum support is hard to set , and the derived rules usually expose common sense knowledge which may not be interesting in business point of view . in this paper , an algorithm for mining fuzzy coherent rules is proposed for overcoming those problems with the properties of propositional logic . it first transforms quantitative transactions into fuzzy sets . then , those generated fuzzy sets are collected to generate candidate fuzzy coherent rules . finally , contingency tables are calculated and used for checking those candidate fuzzy coherent rules satisfy the four criteria or not . if yes , it is a fuzzy coherent rule . experiments on the foodmart dataset are also made to show the effectiveness of the proposed algorithm . ( c ) <digit> elsevier b.v. all rights reserved .
betti numbers and minimal free resolutions for multi state system reliability bounds . <eos> every coherent system has a monomial ideal associated with it and the knowledge of its multigraded betti numbers provides reliability bounds for the corresponding system , which are the tightest among a certain class of such bounds . some alternative methods for computing the multigraded betti numbers are used in this paper and applied in the study of reliability . we obtain special results for well known examples and show that computational commutative algebra techniques can be used beneficially in the reliability analysis of systems of different types . ( c ) <digit> elsevier ltd. all rights reserved .
new insights into churn prediction in the telecommunication sector a profit driven data mining approach . <eos> customer churn prediction models aim to indicate the customers with the highest propensity to attrite , allowing to improve the efficiency of customer retention campaigns and to reduce the costs associated with churn . although cost reduction is their prime objective , churn prediction models are typically evaluated using statistically based performance measures , resulting in suboptimal model selection . therefore , in the first part of this paper , a novel , profit centric performance measure is developed , by calculating the maximum profit that can be generated by including the optimal fraction of customers with the highest predicted probabilities to attrite in a retention campaign . the novel measure selects the optimal model and fraction of customers to include , yielding a significant increase in profits compared to statistical measures . in the second part an extensive benchmarking experiment is conducted , evaluating various classification techniques applied on eleven real life data sets from telecom operators worldwide by using both the profit centric and statistically based performance measures . the experimental results show that a small number of variables suffices to predict churn with high accuracy , and that oversampling generally does not improve the performance significantly . finally , a large group of classifiers is found to yield comparable performance .
comorbidity study of adhd applying association rule mining ( arm ) to national health insurance database of taiwan . <eos> this paper intends to apply association rule mining ( arm ) to explore the labyrinthian network of adhd comorbidity , and to examine the practicality of arm in comorbidity studies using clinic databases . from clinic records of enrollees of taiwan national health insurance ( nhi ) , 18,321 youngsters aged <digit> or less with diagnosis of adhd in <digit> were recruited as case group in this study . and all their clinic diagnoses made from <digit> to <digit> , as comorbidity , were categorized according to the international classification of disease , 9th revision , clinical modification ( icd <digit> cm ) diagnosis system . for comparison , fourfold non adhd controls were recruited from 2001s nhi enrollees on a random base but matched gender and age of cases . arm was done with apriori algorithm to examine the strengths of associations among those diagnoses . the support and confidence values of arm results were examined . comorbidity rates and relative risk ( rr ) ratios of both groups of each diagnosis were compared one another . adhd case group has apparently higher risk of comorbidity with psychiatric comorbidity than with other physical illnesses . from results of arm , developmental delay ( dd ) appears as an important node between adhd and anxiety disorder ( support 5.12 % , confidence 97.42 % ) , mild mental retardation ( support 4.42 % , confidence 92.09 % ) and autism ( support 6.49 % , confidence 94.93 % ) . the finding of this study , an important role of dd between adhd and other psychiatric comorbidity , supports neurological findings in developmental delay of adhd children 's front cortex , as well as some epidemiology findings . this study also demonstrated the practicality of arm in comorbidity studies using enormous clinic databases like nhird .
a novel hybrid arq scheme using turbo codes and diversity combining . <eos> a novel harq scheme using turbo codes is proposed . the method jointly utilizes diversity combining , partial retransmission , and power scaling . computer simulations and density evolution analysis show that the new method outperforms equal gain diversity combining and soft information combining techniques for a wide eb n0 e b n <digit> range .
providing rapid feedback in generated modular language environments adding error recovery to scannerless generalized lr parsing . <eos> integrated development environments ( ides ) increase programmer productivity , providing rapid , interactive feedback based on the syntax and semantics of a language . a heavy burden lies on developers of new languages to provide adequate ide support . code generation techniques provide a viable , efficient approach to semi automatically produce ide plugins . key components for the realization of plugins are the language 's grammar and parser . for embedded languages and language extensions , constituent ide plugin modules and their grammars can be combined . unlike conventional parsing algorithms , scannerless generalized lr parsing supports the full set of context free grammars , which is closed under composition , and hence can parse language embeddings and extensions composed from separate grammar modules . to apply this algorithm in an interactive environment , this paper introduces a novel error recovery mechanism , which allows it to be used with files with syntax errors common in interactive editing . error recovery is vital for providing rapid feedback in case of syntax errors , as most ide services depend on the parser from syntax highlighting to semantic analysis and cross referencing . we base our approach on the principles of island grammars , and derive permissive grammars with error recovery productions from normal sdf grammars . to cope with the added complexity of these grammars , we adapt the parser to support backtracking . we evaluate the recovery quality and performance of our approach using a set of composed languages , based on java and stratego .
some extensions of optimal hardy 's inequality using estimates of p laplacian . <eos> in this paper we estabilish some extensions of the hardy 's type inequalities using estimates of p laplacian . we use also landau 's inequalities .
vertex pancyclicity of augmented cubes with maximal faulty edges . <eos> the n dimensional augmented cube , denoted as aqn , a variation of the hypercube , possesses some properties superior to those of the hypercube . in this paper , we show that every vertex in aqn lies on a fault free cycle of every length from <digit> to 2n , even if there are up to 2n <digit> link faults . we also show that this result is optimal .
the open source rfortran library for accessing r from fortran , with applications in environmental modelling . <eos> the open source rfortran library is introduced as a convenient tool for accessing the functionality and packages of the r programming language from fortran programs . it significantly enhances fortran programming by providing a set of easy to use functions that enable access to r s very rapidly growing statistical , numerical and visualization capabilities , and support a richer and more interactive model development , debugging and analysis setup . rfortran differs from current approaches that require calling fortran dynamic link libraries ( dll ) from r , and instead enables the fortran program to transfer data to from r and invoke r based procedures via the r command interpreter . more generally , rfortran obviates the need to re organize fortran code into dlls callable from r , or to re write existing r packages in fortran , or to jointly compile their fortran code with the r language itself . code snippets illustrate the basic transfer of data and commmands to and from r using rfortran , while two case studies discuss its advantages and limitations in realistic environmental modelling applications . these case studies include the generation of automated and interactive inference diagnostics in hydrological model calibration , and the integration of r statistical packages into a fortran based numerical quadrature code for joint probability analysis of coastal flooding using numerical hydraulic models . currently , rfortran uses the component object model ( com ) interface for data command transfer and is supported on the microsoft windows operating system and the intel and compaq visual fortran compilers . extending its support to other operating systems and compilers is planned for the future . we hope that rfortran expedites method and software development for scientists and engineers with primary programming expertise in fortran , but who wish to take advantage of r s extensive statistical , mathematical and visualization packages by calling them from their fortran code . further information can be found at www.rfortran.org .
challenges and industry practices for managing software variability in small and medium sized enterprises . <eos> software variability is an ability to change ( configure , customize , extend ) software artefacts ( e.g. code , product , domain requirements , models , design , documentation , test cases ) for a specific context . optimized variability management can lead a software company to <digit> ) shorter development lead time , <digit> ) improved customer and improved user satisfaction , <digit> ) reduced complexity of product management ( more variability , same ) and <digit> ) reduced costs ( same variability , less ) . however , it is not easy for software companies , especially small and medium size of enterprises to deal with variability . in this paper we present variability challenges and used practices collected from five smes . our study indicates that increased product complexity can lead growing smes to the time consuming decision making . many of the analyzed medium size of companies also expect improved tool support to help them to boost their productivity when managing increasingly complex products and increasing amount of variants in fact , in many of the analysed smes , a high level of automation in design , release management and testing are or become a key factor for market success by introducing the challenges and used practices related to variability the paper deepens understanding of this highly relevant but relatively under researched phenomenon and contributes to the literature on software product line engineering .
an efficient hash table based node identification method for bandwidth reservation in hybrid cellular and ad hoc networks . <eos> hybrid cellular and wireless ad hoc network architectures are currently considered to be promising alternative solutions to the stand alone cellular or ad hoc network architectures . in this paper , we propose an efficient hash table based node identification ( htni ) method using which bandwidth for various flows can be reserved in such network environments . bandwidth reservation depends on the type of the traffic and its priorities . we define a bandwidth reservation factor for use in such hybrid network environments . we propose a cross layer based architecture for bandwidth reservation to maintain quality of service ( qos ) . we use a priority re allocation method for flows which starve for long time . the proposed method is useful for finding the position of nodes with low communication cost . ( c ) <digit> elsevier b.v. all rights reserved .
a strategy for minlp synthesis of flexible and operable processes . <eos> this paper presents a sequential two stage strategy for the stochastic synthesis of chemical processes in which flexibility and static operability ( the ability to adjust manipulated variables ) are taken into account . in the first stage , the optimal flexible structure and optimal oversizing of the process units are determined in order to assure feasibility of design for a fixed degree of flexibility . in the second stage , the structural alternatives and additional manipulative variables are included in the mathematical model in order to introduce additional degrees of freedom for efficient control . the expected value of the objective function is approximated in both stages by a novel method , which relies on optimization at the central basic point ( cbp ) . the latter is determined by a simple set up procedure based on calculations of the objective function 's conditional expectations for uncertain parameters . the feasibility is assured by simultaneous consideration of critical vertices . the important feature of the proposed stochastic model is that its size depends mainly on the number of design variables and not on the number of uncertain parameters . the strategy is illustrated by two examples for heat exchanger network synthesis . ( c ) <digit> elsevier ltd. all rights reserved .
finite element analysis of shear critical reinforced concrete walls . <eos> advanced material models for concrete are not widely available in general purpose finite element codes . parameters to define them complicate the implementation because they are case sensitive . in addition to this , their validity under severe shear condition has not been verified . in this article , simple engineering plasticity material models available in a commercial finite element code are used to demonstrate that complicated shear behavior can be calculated with reasonable accuracy . for this purpose dynamic response of a squat shear wall that had been tested on a shaking table as part of an experimental program conducted in japan is analyzed . both the finite element and material aspects of the modeling are examined . a corrective artifice for general engineering plasticity models to account for shear effects in concrete is developed . the results of modifications in modeling the concrete in compression are evaluated and compared with experimental response quantities .
reducing the computational complexity of information theoretic approaches for reconstructing gene regulatory networks . <eos> information theoretic approaches are increasingly being used for reconstructing regulatory networks from microarray data . these approaches start by computing the pairwise mutual information ( mi ) between all gene pairs . the resulting mi matrix is then manipulated to identify regulatory relationships . a barrier to these approaches is the time consuming step of computing the mi matrix . we present a method to reduce this computation time . we apply spectral analysis to re order the genes , so that genes that share regulatory relationships are more likely to be placed close to each other . then , using a sliding window '' approach with appropriate window size and step size , we compute the mi for the genes within the sliding window , and the remainder is assumed to be zero . using both simulated data and microarray data , we demonstrate that our method does not incur performance loss in regions of high precision and low recall , while the computational time is significantly lowered . the proposed method can be used with any method that relies on the mutual information to reconstruct networks .
photorealistic rendering of rain streaks . <eos> photorealistic rendering of rain streaks with lighting and viewpoint effects is a challenging problem . raindrops undergo rapid shape distortions as they fall , a phenomenon referred to as oscillations . due to these oscillations , the reflection of light by , and the refraction of light through , a falling raindrop produce complex brightness patterns within a single motion blurred rain streak captured by a camera or observed by a human . the brightness pattern of a rain streak typically includes speckles , multiple smeared highlights and curved brightness contours . in this work , we propose a new model for rain streak appearance that captures the complex interactions between the lighting direction , the viewing direction and the oscillating shape of the drop . our model builds upon a raindrop oscillation model that has been developed in atmospheric sciences . we have measured rain streak appearances under a wide range of lighting and viewing conditions and empirically determined the oscillation parameters that are dominant in raindrops . using these parameters , we have rendered thousands of rain streaks to create a database that captures the variations in streak appearance with respect to lighting and viewing directions . we have developed an efficient image based rendering algorithm that uses our streak database to add rain to a single image or a captured video with moving objects and sources . the rendering algorithm is very simple to use as it only requires a coarse depth map of the scene and the locations and properties of the light sources . we have rendered rain in a wide range of scenarios and the results show that our physically based rain streak model greatly enhances the visual realism of rendered rain .
distributed part differentiation in a smart surface . <eos> distributed differentiation of parts in a smart surface is considered . synchronous and asynchronous distributed discrete state acquisition algorithms are proposed their convergence is studied and implementation models are given . distributed part differentiation methods are proposed . a multithreaded java smart surface simulator ( sss ) which runs on multi core machines is presented . a series of computational results obtained with sss is given and analyzed . ( c ) <digit> elsevier ltd. all rights reserved .
cared cautious adaptive red gateways for tcp ip networks . <eos> random early detection ( red ) is a widely deployed active queue management algorithm that improves the overall performance of the network in terms of throughput and delay . the effectiveness of red algorithm , however , highly depends on appropriate setting of its parameters . moreover , the performance of red is quite sensitive to abrupt changes in the traffic load . in this paper , we propose a cautious adaptive random early detection ( cared ) algorithm that dynamically varies maximum drop probability based on the level of traffic load to improve the overall performance of the network . based on extensive simulations conducted using network simulator <digit> ( ns <digit> ) , we show that cared algorithm reduces the packet drop rate and achieves high throughput as compared to red , adaptive red and refined adaptive red . unlike other red based algorithms , cared algorithm does not introduce new parameters to achieve performance gain and hence can be deployed without any additional complexity .
a prolongation projection algorithm for computing the finite real variety of an ideal . <eos> we provide a real algebraic symbolic numeric algorithm for computing the real variety v r ( i ) of an ideal i subset of r x , assuming v r ( i ) is finite ( while v c ( i ) could be infinite ) . our approach uses sets of linear functionals on r x , vanishing on a given set of polynomials generating i and their prolongations up to a given degree , as well as on polynomials of the real radical ideal ( r ) root i obtained from the kernel of a suitably defined moment matrix assumed to be positive semidefinite and of maximum rank . we formulate a condition on the dimensions of projections of these sets of linear functionals , which serves as a stopping criterion for our algorithm this new criterion is satisfied earlier than the previously used stopping criterion based on a rank condition for moment matrices . this algorithm is based on standard numerical linear algebra routines and semidefinite optimization and combines techniques from previous work of the authors together with an existing algorithm for the complex variety . ( c ) <digit> elsevier b.v. all rights reserved .
on decomposition for incomplete data . <eos> in this paper we present a method of data decomposition to avoid the necessity of reasoning on data with missing attribute values . this method can be applied to any algorithm of classifier induction . the original incomplete data is decomposed into data subsets without missing values . next , methods for classifier induction are applied to these sets . finally , a conflict resolving method is used to obtain final classification from partial classifiers . we provide an empirical evaluation of the decomposition method accuracy and model size with use of various decomposition criteria on data with natural missing values . we present also experiments on data with synthetic missing values to examine the properties of proposed method with variable ratio of incompleteness .
nonuniform noise propagation by using the ramp filter in fan beam computed tomography . <eos> it is observed that when the homogeneity property of the ramp filter is used to derive a filtered backprojection algorithm in fan beam tomography , the reconstructed images have nonstationary frequency components and nonstationary noise . when a short focal length is used , higher frequency components are amplified more at the edge of the image than at the center of the image , resulting in higher noise at the edge of the image .
performance prediction of throughput centric pipelined global interconnects with voltage scaling . <eos> due to the ever increasing demand for computing capacity , throughput centric design for on chip global interconnects has played an important role in the emerging parallel computing architectures . in this paper , we explore the performance of flip flop based pipelined global interconnects with more design freedoms under the voltage and technology scaling for different applications . based on the derived accurate voltage scaled models of pipelined interconnects , we propose a general evaluation flow using numerical experiments to study the impact of pipelining depth , voltage scaling , and different processes on the performance of pipelined interconnects under four different design objectives . our experimental results show that , with the dedicated throughput centric optimization , at <digit> nm node , up to 25x overall throughput per energy area ( tpea ) improvement can be obtained with only 4x increase on the interconnect latency compared with the conventional minimum latency design , making this new design methodology more promising in the future nodes .
impact of meta analytic decisions on the conclusions drawn on the business value of information technology . <eos> meta analysis is a quantitative methodology that allows for summarizing the results of primary research studies in a field to provide new insights in terms of the phenomenon observed or the outcomes reported . this paper attempts to answer the fundamental question , do methodological decisions in a meta analytic study affect the conclusions drawn from that study specifically , this paper examines the effects of meta analytic decisions when applied to the business value of information technology ( bvit ) research stream . a closer examination of the variation in the methodological decisions informs us that , with each different decision alternative , we are examining slightly different phenomenon . the findings reveal that study outcomes do change , depending on the meta analytic decisions that are tested . in other words , methodological decisions matter . based on the data from <digit> primary studies and <digit> effect sizes , we tested seven hypotheses , in the bvit research stream , using a comprehensive set of different methodological conditions . we find support for two findings that are consistent across all the different conditions . first , investing in information technology ( it ) is positively associated with the firm 's performance . we also find that large firms get more benefits from it than small firms . these and the overall findings suggest that researchers should be cognizant of their methodological decisions , as they may be observing the phenomenon under different boundary conditions with different methodologies .
prediction of gastric ulcers based on the change in electrical resistance of acupuncture points using fuzzy logic decision making . <eos> many theories of reflexology use ancient concepts which do not coincide with the modern medical terminology of anatomy , physiology and biophysics . this substantially reduces the trust of physicians in reflexology methods . during this research , several mathematical models for the interaction of the internal and biological active points of meridian structures have been proposed . the analysis of these models allows the specification of a list of gastric diseases for which reflex diagnostics and reflex therapy methods are most effective and also allows increasing the effectiveness of these procedures . it is shown that good results for the prediction and early diagnosis of diseases from the reaction energy of biologically active points ( acupuncture points ) are obtained using fuzzy logic decision making .
sspx a program to compute strain from displacement velocity data . <eos> sspx is a macintosh , cocoa universal application to compute strain from displacement velocity data in two and three dimensions . sspx solves small and large deformation problems , in either the undeformed ( lagrangian ) or deformed ( eulerian ) configuration . the program offers several options to compute strain best fit for all or selected data , strain at a point , strain at stations , delaunay , grid nearest neighbor , and grid distance weighted . except for the strain at stations option in 3d , the program computes the strain on a surface that is either flat ( slice ) or that follows the topography of the data . in the case of a slice and in 3d , the location and orientation of the slice can be varied to visualize the strain along horizontal or vertical eastwest , northsouth sections . sspx plots strain in a 2d view ( 3d strain is displayed as stereonets ) . we show the application of the program to gps data , analogue and dem mechanical simulations .
optimising area under the roc curve using gradient descent . <eos> this paper introduces rankopt , a linear binary classifier which optimises the area under the roc curve ( the auc ) . unlike standard binary classifiers , rankopt adopts the auc statistic as its objective function , and optimises it directly using gradient descent . the problems with using the auc statistic as an objective function are that it is non differentiable , and of complexity o ( n <digit> ) in the number of data observations . rankopt uses a differentiable approximation to the auc which is accurate , and computationally efficient , being of complexity o ( n. ) this enables the gradient descent to be performed in reasonable time . the performance of rankopt is compared with a number of other linear binary classifiers , over a number of different classification problems . in almost all cases it is found that the performance of rankopt is significantly better than the other classifiers tested .
internet based decision support for evidence based medicine . <eos> the protocol assistant is a knowledge based system , developed by the department of artificial intelligence and aiai at the university of edinburgh , which advises on the treatment of parotid tumours . it has been developed to support both adherence to a clinical protocol based on the latest evidence and the use of clinical judgment where the evidence is weak or inconsistent . it was developed using a knowledge modelling technique named proforma , which is specifically designed for representing best practice guidelines the proforma models were used as the basis for a user interface , which was implemented in html . a set of rules were developed in jess ( the java expert system shell ) which were capable of running the protocol a simple method of reasoning with certainties , based on the goodness of each relevant item of published evidence , was used to recommend which path to follow at choice points . however , the user is also supplied with access to the abstracts of all relevant published articles , using the hypertext facilities of html . the protocol assistant can thus be used either as a wizard which guides users through the decision making process , or as a hypertext manual which leads them to the information relevant to the decision they are making . this dual role capability is crucial for the acceptance of kbs in the real world .
hierarchical concept formation in associative memory composed of neuro window elements . <eos> in the present letter , the authors propose a model of hierarchical concept formation in auto associative memory composed of neuro window elements . when hierarchically correlated patterns are memorized , each center of the correlated memory patterns can be defined as a concept pattern which represents those similar patterns . it is shown that the concept patterns are formed under a hebbian learning rule in the neural network where all the neuro window elements are just mutually connected , and that the memory patterns and the concept patterns in each level of hierarchy can be retrieved selectively by adjusting the parameters of the neuro window elements . copyright <digit> elsevier science ltd
a priori and a posteriori analysis for a locking free low order quadrilateral hybrid finite element for reissnermindlin plates . <eos> this paper proposes a quadrilateral finite element method of the lowest order for reissnermindlin ( rm ) plates on the basis of hellingerreissner variational principle , which includes variables of displacements , shear stresses and bending moments . this method uses continuous piecewise isoparametric bilinear interpolation for the approximation of transverse displacement and rotation . the piecewise independent shear stress bending moment approximation is constructed by following a self equilibrium criterion and a shear stress enhanced condition . a priori and reliable a posteriori error estimates are derived and shown to be uniform with respect to the plate thickness t. numerical experiments confirm the theoretical results .
an electronic environment and contact direction sensitive scoring function for predicting affinities of proteinligand complexes in contour . <eos> contour grows drug like molecules in protein binding site . atomic model comprises hybridization and polarization state . score captures electrostatic interactions and solvation . contour predicts binding energies with r2 0.57 .
processor embedded distributed smart disks for i o intensive workloads architectures , performance models and evaluation . <eos> processor embedded disks , or smart disks , with their network interface controller , can in effect be viewed as processing elements with on disk memory and secondary storage . the data sizes and access patterns of today 's large i o intensive workloads require architectures whose processing power scales with increased storage capacity . to address this concern , we propose and evaluate disk based distributed smart storage architectures . based on analytically derived performance models , our evaluation with representative workloads show that offloading processing and performing point to point data communication improve performance over centralized architectures . our results also demonstrate that distributed smart disk systems exhibit desirable scalability and can efficiently handle i o intensive workloads , such as commercial decision support database ( tpc h ) queries , association rules mining , data clustering , and two dimensional fast fourier transform , among others .
soft semirings . <eos> molodtsov introduced the concept of soft sets , which can be seen as a new mathematical tool for dealing with uncertainty . in this paper , we initiate the study of soft semirings by using the soft set theory . the notions of soft semirings , soft subsemirings , soft ideals , idealistic soft semirings and soft semiring homomorphisms are introduced , and several related properties are investigated .
a lower bound on the greedy weights of product codes . <eos> a greedy <digit> subcode is a one dimensional subcode of minimum ( support ) weight . a greedy r subcode is an r dimensional subcode with minimum support weight under the constraint that it contain a greedy ( r <digit> ) subcode . the r th greedy weight e ( r ) is the support weight of a greedy r subcode . the greedy weights are related to the weight hierarchy . we use recent results on the weight hierarchy of product codes to develop a lower bound on the greedy weights of product codes .
fuzzy modeling of high dimensional systems complexity reduction and interpretability improvement . <eos> fuzzy modeling of high dimensional systems is a challenging topic . this paper proposes an effective approach to data based fuzzy modeling of high dimensional systems . an initial fuzzy rule system is generated based on the conclusion that optimal fuzzy rules cover extrema <digit> , redundant rules are removed based on a fuzzy similarity measure , then , the structure and parameters of the fuzzy system are optimized using a genetic algorithm and the gradient method , during optimization , rules that have a very low firing strength are deleted . finally , interpretability of the fuzzy system is improved by fine training the fuzzy rules with regularization . the resulting fuzzy system generated by this method has the following distinct features <digit> ) the fuzzy system is quite simplified <digit> ) the fuzzy system is interpretable and <digit> ) the dependencies between the inputs and the output are clearly shown . this method has successfully been applied to a system that has <digit> inputs and one output with <digit> <digit> training data and <digit> <digit> test data .
the analysis of spatial association on a regular lattice by join count statistics without the assumption of first order homogeneity . <eos> one of the widely used classical pieces of spatial statistics for the assessment of spatial association of nominal data , such as colors on a map , is the join count statistic ( jcs ) . its application assumes first order ' homogeneity , that is , the probability of colors is assumed to be uniform across the map . with recent developments in spatial analysis , particularly in remote sensing and landscape ecology , jcs and related measures are frequently applied in cases when this assumption is violated and can produce misleading conclusions . we present a new method with formulas and algorithms implemented in s plus for handling first order heterogeneity on a regular lattice . based on the probability distribution of colors at each location ( cell or pixel ) , we compute the expected value and variance of same color neighbors . using a stochastic simulation experiment we also confirm that the asymptotic gaussian approximation holds . environmental assessment and mapping application examples illustrate the impact of spatially heterogeneous probabilities of nominal variables on significance testing of their spatial association . ( c ) <digit> elsevier science ltd. all rights reserved .
a general approach for minimizing the maximum interference of a wireless ad hoc network in plane . <eos> the interference reduction is one of the most important problems in the field of wireless sensor networks . wireless sensor network elements are small mobile receiver and transmitters . the energy of processor and other components of each device is supplied by a small battery with restricted energy . one of the meanings that play an important role in energy consumption is the interference of signals . the interference of messages through a wireless network , results in message failing and transmitter should resend its message , thus the interference directly affect on the energy consumption of transmitter . this paper presents an algorithm which suggests the best subgraph for the input distribution of the nodes in the plane how the maximum interference of the proposed graph has the minimum value . the input of the application is the complete network graph , which means we know the cost of each link in the network graph . without any lose of generality the euclidean distance could be used as the weight of each link . the links are arranged and ranked according to their weights , in an iterative process the link which imposition minimum increase on the network interference with some extra conditions which is proposed in future sections , is added to resulting topology and is eliminated from list until all nodes are connected together . experimental results show the efficiency of proposed algorithm not only for one dimensional known distribution like exponential node chain , but also for two dimensional distributions like two exponential node chains and alpha spiral node chains .
new conditions for global exponential stability of continuous time neural networks with delays . <eos> in this paper , we investigate the global exponential stability of delayed neural network systems . for this purpose , the activation functions are assumed to be globally lipschitz continuous . the properties of norms and the relationship of homeomorphism are adjusted to ensure the existence as well as the uniqueness of the equilibrium point . then by employing suitable lyapunov functional , some delay independent sufficient conditions are derived for exponential convergence toward global equilibrium state associated with different input sources . the obtained results are shown to be more general and less restrictive than the previous results derived in the literature . lastly , a number of examples are provided to demonstrate the validity of the results proposed .
modelling collaboration processes through design patterns . <eos> enterprise 2.0 has been introduced in the sme ( small medium enterprise ) modifying common organizational and operative practices . this brings the ' knowledge workers ' to change their working practices through the use of web 2.0 communication tools . unfortunately , these tools do not allow intercepting and tracing the exchanged data , which can produce a loss of information . this is an important problem in an enterprise context because knowledge of the exchanged information can increase the efficiency and competitiveness of the company . in this article we demonstrate that it is possible to extract this knowledge by an abstraction process of the new operative practices , named collaboration processes , thanks to the use of design patterns . therefore , we propose design patterns for the collaboration processes useful for modelling typical enterprise 2.0 activities , having the goal of making more flexible and traceable the use of emerging operative practices .
off line and on line guaranteed start up delay for media on demand with stream merging . <eos> we address the problem of designing efficient solutions for media on demand in systems that use stream merging . in a stream merging system , the receiving bandwidth of clients is larger than the playback bandwidth and clients can buffer parts of the transmission to be played back later . intelligent use of these resources allows bandwidth usage to be reduced exponentially over traditional unicast delivery of popular media . we design an off line algorithm that , in o ( n ) time , computes an optimal off line stream merging solution for the case when the time horizon n is known ahead of time . in addition , we describe an on line delay guaranteed solution that operates without knowledge of the time horizon size , and show that it performs asymptotically close to the optimal off line algorithm . the on line algorithm is simpler to implement than previously proposed on line stream merging algorithms , and empirically performs well when the intensity of client arrivals is high .
balancing efficiency and interpretability in an interactive statistical assistant . <eos> making an interface more efficient , in a task analysis sense , can make it more difficult for an automated reasoning system to infer user goals , by eliminating some user actions , by presenting information without requiring overt user selection , and so forth . we call the extent to which a system can make such inferences interpretability . in this paper we describe the tradeoff between interpretability and efficiency . we give some general heuristics for improving interpretability in a system and explain how they apply in an implemented system , an assistant for exploratory statistical analysis . increased interpretability in the system is provided by navigation techniques for data exploration and a data mountain for organizing results a formative evaluation illustrates some of the potential benefits of applying interpretability heuristics to an intelligent user interface
adaptive group scheduling mechanism using mobile agents in peer to peer grid computing environment . <eos> peer to peer grid computing is an attractive computing paradigm for high throughput applications . however , both volatility due to the autonomy of volunteers ( i.e. , resource providers ) and the heterogeneous properties of volunteers are challenging problems in the scheduling procedure . therefore , it is necessary to develop a scheduling mechanism that adapts to a dynamic peer to peer grid computing environment . in this paper , we propose a mobile agent based adaptive group scheduling mechanism ( maagsm ) . the maagsm classifies and constructs volunteer groups to perform a scheduling mechanism according to the properties of volunteers such as volunteer autonomy failures , volunteer availability , and volunteering service time . in addition , the maagsm exploits a mobile agent technology to adaptively conduct various scheduling , fault tolerance , and replication algorithms suitable for each volunteer group . furthermore , we demonstrate that the maagsm improves performance by evaluating the scheduling mechanism in korea home .
differential evolution training algorithm for feed forward neural networks . <eos> an evolutionary optimization method over continuous search spaces , differential evolution , has recently been successfully applied to real world and artificial optimization problems and proposed also for neural network training . however , differential evolution has not been comprehensively studied in the context of training neural network weights , i.e. , how useful is differential evolution in finding the global optimum for expense of convergence speed . in this study , differential evolution has been analyzed as a candidate global optimization method for feed forward neural networks . in comparison to gradient based methods , differential evolution seems not to provide any distinct advantage in terms of learning rate or solution quality . differential evolution can rather be used in validation of reached optima and in the development of regularization terms and non conventional transfer functions that do not necessarily provide gradient information .
geometric validation of a ground based mobile laser scanning system . <eos> this paper outlines a study , carried out on behalf of a national mapping agency , to validate laser scanned point cloud data collected by a ground based mobile mapping system . as the need for detailed three dimensional data about our environment continues to grow , ground based mobile systems are likely to find an increasingly important niche in national mapping agency applications . for example , such systems potentially provide the most efficient data capture for numerical modelling and or visualisation in support of decision making , filling a void between static terrestrial and mobile airborne laser scanning . this study sought to assess the precision and accuracy of data collected using the streetmapper system across two test sites a peri urban residential housing estate with low density housing and wide streets , and a former industrial area consisting of narrow streets and tall warehouses . an estimate of system precision in both test sites was made using repeated data collection passes , indicating a measurement precision ( <digit> % ) of between 0.029 m and 0.031 m had been achieved in elevation . elevation measurement accuracy was assessed against check points collected using conventional surveying techniques at the same time as the laser scanning survey , finding rms errors in elevation in the order of 0.03 m. planimetric accuracy was also assessed , with results indicating an accuracy of approximately 0.10 m , although difficulties in reliably assessing planimetric accuracy were encountered . the results of this validation were compared against a theoretical error pre analysis which was also used to show the relative components of error within the system . finally , recommendations for future validation methodologies are outlined and possible applications of the system are briefly discussed .
asymptotic preserving particle in cell method for the vlasov poisson system near quasineutrality . <eos> this paper deals with the numerical resolution of the vlasov poisson system in a nearly quasineutral regime by particle in cell ( pic ) methods . in this regime , classical pic methods are subject to stability constraints on the time and space steps related to the small debye length and large plasma frequency . here , we propose an asymptotic preserving pic scheme which is not subjected to these limitations . additionally , when the plasma period and debye length are small compared to the time and space steps , this method provides a consistent pic discretization of the quasineutral vlasov equation . we perform several one dimensional numerical experiments which provide a solid validation of the method and its underlying concepts , and compare the method with classical pic and direct implicit methods . ( c ) <digit> elsevier inc. all rights reserved .
the structure factor of dense two dimensional polymer solutions . <eos> according to the generalized porod law the intramolecular structure factor f ( q ) f ( q ) of compact objects with surface dimension ds d s scales as f ( q ) n <digit> ( r ( n ) q ) 2d ds f ( q ) n <digit> ( r ( n ) q ) <digit> d d s in the intermediate range of the wave vector q with d being the dimension of the embedding space , n the mass of the objects and r ( n ) n1 d r ( n ) n <digit> d their typical size . by means of molecular dynamics simulations of a bead spring model with chain lengths up to n <digit> n <digit> it is shown that dense self avoiding polymers in strictly two dimensions ( d <digit> d <digit> ) adopt compact configurations of surface dimension ds <digit> <digit> d s <digit> <digit> . in agreement with the generalized porod law the kratky representation of f ( q ) f ( q ) thus reveals a nonmonotonous behavior with q2f ( q ) <digit> ( n1 2q ) <digit> <digit> q <digit> f ( q ) <digit> ( n <digit> <digit> q ) <digit> <digit> . using a similar data analysis we argue briefly that melts of non concatenated rings in three dimensions become marginally compact with ds d <digit> d s d <digit> , i.e. q2f ( q ) n0 q q <digit> f ( q ) n <digit> q , for asymptotically long chains .
an analytic approach to fuzzy robot control synthesis . <eos> the main advantage of a fuzzy control system is the fact that no mathematical model of the controlled plant is required . instead of that model , it is necessery to construct a fuzzy rule base for each particular application case . a vexing problem in fuzzy control , however , is the exponential growth in rules as the number of variables increases . this problem is avoided here by the introduction of a new , nonconventional analytic method for synthesising the fuzzy control . for this purpose a new analytic function is defined that determines the positions of the centres of the output fuzzy sets , instead of the definition of a fuzzy rule base . this function can be adapted to each concrete application case by changing the free fuzzy set parameters . the proposed analytic approach to the synthesis of fuzzy control , has been tested by a numerical simulation of an analytic fuzzy control system for a robot with four degrees of freedom .
reduction of stimulation coherent artifacts in electrically evoked auditory brainstem responses . <eos> <digit> channel auditory brainstem responses from bilateral cochlear implant subjects . a customized eeg cap is presented with holes at the cochlear implant coil locations . facial nerve stimulation artifacts contaminate the wave ev in some ci subjects . principle component analysis was applied to remove the facial nerve artifacts . electric pulse artifact reduction strategies applied with bilateral stimulation .
a bayes comparison of weibull extension and modified weibull models for data showing bathtub hazard rate . <eos> a number of models have been proposed in the literature to model data reflecting bathtub shaped hazard rate functions . mixture distributions provide the obvious choice for modelling such data sets but these contain too many parameters and hamper the accuracy of the inferential procedures particularly when the data are meagre . recently , a few distributions have been proposed which are simply generalizations of the two parameter weibull model and are capable of producing bathtub behaviour of the hazard rate function . the weibull extension and the modified weibull models are two such families . this study focuses on comparing these two distributions for data sets exhibiting bathtub shape of the hazard rate . bayesian tools are preferred due to their wide range of applicability in various nested and non nested model comparison problems . real data illustrations are provided so that a particular model can be recommended based on various tools of model comparison discussed in the paper .
noisy but non malicious user detection in social recommender systems . <eos> social recommender systems largely rely on user contributed data to infer users ' preference . while this feature has enabled many interesting applications in social networking services , it also introduces unreliability to recommenders as users are allowed to insert data freely . although detecting malicious attacks from social spammers has been studied for years , little work was done for detecting noisy but non malicious users ( nnmus ) , which refers to those genuine users who may provide some untruthful data due to their imperfect behaviors . unlike colluded malicious attacks that can be detected by finding similarly behaved user profiles , nnmus are more difficult to identify since their profiles are neither similar nor correlated from one another . in this article , we study how to detect nnmus in social recommender systems . based on the assumption that the ratings provided by a same user on closely correlated items should have similar scores , we propose an effective method for nnmu detection by capturing and accumulating user 's self contradictions , i.e. , the cases that a user provides very different rating scores on closely correlated items . we show that self contradiction capturing can be formulated as a constrained quadratic optimization problem w.r.t. a set of slack variables , which can be further used to quantify the underlying noise in each test user profile . we adopt three real world data sets to empirically test the proposed method . the experimental results show that our method ( i ) is effective in real world nnmu detection scenarios , ( ii ) can significantly outperform other noisy user detection methods , and ( iii ) can improve recommendation performance for other users after removing detected nnmus from the recommender system .
performance evaluation for a beacon enabled ieee 802.15.4 scheme with heterogeneous unsaturated conditions . <eos> the successful release of the ieee 802.15.4 standard offers a great convenience to applications of low power and low rate wireless sensor networks ( wsns ) which almost touch upon all aspects of our life . analyses of the ieee 802.15.4 carrier sense multiple access with collision avoidance ( csma ca ) scheme have received considerable attention on saturated or homogeneous traffic recently . more realistic stochastic analysis approach to evaluate the performance of csma ca scheme with heterogeneous unsaturated traffic is proposed in our applications . we adopt two modified semi markov chains and one macro markov chain to characterize such an asymmetric system , in which traffic arrivals and packets accessing the channel are bestowed with non preemptive priority over each other instead of prioritization , and the behaviors of heterogeneous nodes interact with each other rather than simple independent behavior superposition . throughput , packet delay and energy consumption of unsaturated , unacknowledged ieee 802.15.4 beacon enabled networks are predicted based on these models . comprehensive simulations demonstrate that the analysis results of these simplified models match well with the simulation results , and not undermine the accuracy at the same time .
fostering multimedia learning of science exploring the role of an animated agent 's image . <eos> research suggests that students learn better when studying a picture coupled with narration rather than on screen text in a computer based multimedia learning environment . moreover , combining narration with the visual presence of an animated pedagogical agent may also encourage students to process information deeper than narration or on screen text alone . the current study was designed to evaluate three effects among students learning about the human cardiovascular system the modality effect ( narration vs. on screen text ) , the embodied agent effect ( narration agent vs. on screen text ) , and the image effect ( narration agent vs. narration ) . the results of this study document large and significant embodied agent and image effects on the posttest ( particularly retention items ) but surprisingly no modality effect was found . overall , the results suggest that incorporating an animated pedagogical agent programmed to coordinate narration with gaze and pointing into a science focused multimedia learning environment can foster learning . ( c ) <digit> elsevier ltd. all rights reserved .
using uml class diagrams for a comparative analysis of relational , object oriented , and object relational database mappings . <eos> this paper illustrates the manner in which uml can be used to study mappings to different types of database systems . after introducing uml through a comparison to the eer model , uml diagrams are used to teach different approaches for mapping conceptual designs to the relational model . as we cover object oriented and object relational database systems , different features of uml are used over the same enterprise example to help students understand mapping alternatives for each model . students are required to compare and contrast the mappings in each model as part of the learning process . for object oriented and object relational database systems , we address mappings to the odmg and sql99 standards in addition to specific commercial implementations .
a conwip model for fms control . <eos> production inventory control is one of the most important aspects of a flexible manufacturing system ( fms ) design . constant work in process ( conwip ) , which is a hybrid of push and pull type systems , offers an alternative to effective utilization of the expensive fms equipment while still meeting customer requirements . in the selection of an fms control method , material handling often becomes one of the capacity constraints which forms the basis of various research interests . in this paper , a structure based model for a conwip controlled fms is proposed , and within it , the node type characteristics concept is used to describe the constraints in fms . furthermore , simulation is used to determine the card number based on the structure based model . the simulation results demonstrate that the model is suitable for the design and operation of fms . the model can be used as a manufacturing execution system of enterprise resources planning . an architecture for this integrated design based on internet intranet systems is also proposed .
system level modeling and synthesis of flow based microfluidic biochips . <eos> microfluidic biochips are replacing the conventional biochemical analyzers and are able to integrate the necessary functions for biochemical analysis on chip . there are several types of microfluidic biochips , each having its advantages and limitations . in this paper we are interested in flow based biochips , in which the flow of liquid is manipulated using integrated microvalves . by combining several microvalves , more complex units , such as micropumps , switches , mixers , and multiplexers , can be built . although researchers have proposed significant work on the system level synthesis of droplet based biochips , which manipulate droplets on a two dimensional array of electrodes , no research on system level synthesis of flow based bioch ips has been reported so far . the focus has been on application modeling and component level simulation . therefore , for the first time to our knowledge , we propose a system level modeling and synthesis approach for flow based biochips . we have developed a topology graph based model of the biochip architecture , and we have used a sequencing graph to model the biochemical applications . we consider that the architecture of the biochip is given , and we are interested to synthesize an implementation , consisting of the binding of operations in the application to the functional units of the architecture , the scheduling of operations and the routing and scheduling of the fluid flows , such that the application completion time is minimized . we propose a list scheduling based heuristic for solving this problem . the proposed heuristic has been evaluated using two real life case studies and a set of four synthetic benchmarks .
hybrid multiobjective genetic algorithm with a new adaptive local search process . <eos> this paper is concerned with a specific brand of evolutionary algorithms memetic algorithms . a new local search technique with an adaptive neighborhood setting process is introduced and assessed against a set of test functions presenting different challenges . two performance criteria were assessed the convergence of the achieved results towards the true pareto fronts and their distribution .
control of teleoperators with joint flexibility , uncertain parameters and time delays . <eos> the problem of controlling a rigid bilateral teleoperator has been the subject of study since the late 1980s and several control approaches have been reported to deal with time delays , position tracking and transparency . however , the general flexible case is still an open problem . the present paper reports an adaptive and damping injection controller and a proportional plus damping injection ( p d ) ( p d ) controller which are capable of globally stabilizing a nonlinear bilateral teleoperator with joint flexibility and time delays . more precisely , the adaptive scheme is able to cope with uncertainty in the parameters and constant time delays , while the p d p d scheme is shown to treat variable time delays . in both cases , the teleoperator is composed of a rigid local manipulator and a flexible joint remote manipulator . the extension to the case where the local and remote manipulators exhibit joint flexibility is also reported using the p d p d scheme . under the common assumption that the human operator and the environment are passive it is proven , for the p d p d schemes , that the joint and actuator velocities as well as the local and remote position errors are bounded . moreover , if the human operator and remote environment forces are zero then , for both controllers , position tracking is established and local and remote velocities asymptotically converge to zero . simulations and experiments are presented to depict the performance of the proposed schemes .
a machining process planning activity model for systems integration . <eos> a key issue of integrating process planning systems with design systems and production planning systems is how to overcome barriers in data exchange and sharing amongst software systems . a machining process planning activity model was developed to address some of the barriers . this model represents functional components and data requirements in process planning systems . the purpose of the model is to create the context in which data requirements and data flow for numerically controlled machining process planning are defined . furthermore , the model was developed as a unification of many previously developed process planning activity models .
very large electronic structure calculations using an out of core filter diagonalization method . <eos> we present an out of core filter diagonalization method which can be used to solve very large electronic structure problems with in the framework of the one electron pseudopotential based methods . the approach is based on the following three steps . first , nonorthogonal states in a desired energy range are generated using the filter diagonalization method . next , these states are orthogonalized using the householder qr orthogonalization method . finally , the hamiltonian is diagonalized within the subspace spanned by the orthogonal states generated in the second step . the main limitin step in the calculation is the orthogonalization step , which requires a huge main memory for large systems . to overcome this limitation we have developed and implemented an out of core orthogonal ization method which allows us to store the states on disks without significantly slowing the computation . we apply the out of core filter diagonalization method to solve the electronic structure of a quantum dot within the framework of the semiempirical pseudopotential method and show that problems which require tens of gigabytes to represents the electronic states and electronic density can be solved on a personal computer . ( c ) <digit> elsevier science ( usa ) .
issues in knowledge access , retrieval and sharing case studies in a caribbean health sector . <eos> in knowledge sharing forums different actors access each others knowledge to assist in their own decision making process . however , in absence of a formal knowledge management system this knowledge may not be available for use reuse . in this research , we explore different scenarios in the healthcare sector to identify the factors which either facilitate or impede the knowledge flows while sharing knowledge . furthermore , based on the analysis of the scenarios we theorize about use and reuse of knowledge items and the issues in accessing and retrieving them in knowledge sharing events . the barriers and enablers in different case studies are identified which can be applicable to any knowledge sharing event where its actors are working towards a common objective . we examine the patterns of accessibility of a knowledge item in knowledge sharing events and its effect on perceived usability , and perceived usefulness and relevance of a knowledge item .
sparse kernel logistic regression based on l <digit> <digit> regularization . <eos> the sparsity driven classification technologies have attracted much attention in recent years , due to their capability of providing more compressive representations and clear interpretation . two most popular classification approaches are support vector machines ( svms ) and kernel logistic regression ( klr ) , each having its own advantages . the sparsification of svm has been well studied , and many sparse versions of <digit> norm svm , such as <digit> norm svm ( <digit> svm ) , have been developed . but , the sparsification of klr has been less studied . the existing sparsification of klr is mainly based on l <digit> norm and l <digit> norm penalties , which leads to the sparse versions that yield solutions not so sparse as it should be . a very recent study on l <digit> <digit> regularization theory in compressive sensing shows that l <digit> <digit> sparse modeling can yield solutions more sparse than those of <digit> norm and <digit> norm , and , furthermore , the model can be efficiently solved by a simple iterative thresholding procedure . the objective function dealt with in l <digit> <digit> regularization theory is , however , of square form , the gradient of which is linear in its variables ( such an objective function is the so called linear gradient function ) . in this paper , through extending the linear gradient function of l <digit> <digit> regularization framework to the logistic function , we propose a novel sparse version of klr , the <digit> <digit> quasi norm kernel logistic regression ( <digit> <digit> klr ) . the version integrates advantages of klr and l <digit> <digit> regularization , and defines an efficient implementation scheme of sparse klr . we suggest a fast iterative thresholding algorithm for <digit> <digit> klr and prove its convergence . we provide a series of simulations to demonstrate that <digit> <digit> klr can often obtain more sparse solutions than the existing sparsity driven versions of klr , at the same or better accuracy level . the conclusion is also true even in comparison with sparse svms ( <digit> svm and <digit> svm ) . we show an exclusive advantage of <digit> <digit> klr that the regularization parameter in the algorithm can be adaptively set whenever the sparsity ( correspondingly , . the number of support vectors ) is given , which suggests a methodology of comparing sparsity promotion capability of different sparsity driven classifiers . as an illustration of beneffis of <digit> <digit> klr , we give two applications of <digit> <digit> klr in semi supervised learning , showing that <digit> <digit> klr can be successfully applied to the classification tasks in which only a few data are labeled .
will negative experiences impact future it outsourcing . <eos> though information systems ( is ) outsourcing is growing at a rapid pace , there are several risk factors and potential negative outcomes . the objectives of this study are i ) to conceptualize the relationships among the is outsourcing risk factors , the negative outcomes of these risk factors , and their impact on future reoutsourcing decisions , and ii ) to test these relationships empirically using survey data and structural equation modeling . based on data from <digit> firms , we have found that future re outsourcing decisions are strongly influenced by the negative outcomes of loss of control and the degradation of is services . while all the risk factors influence future re outsourcing decisions , either directly or indirectly , vendor competence problems ( related to infrastructure ) and vendor coordination problems have a direct effect on re outsourcing decisions . such variables as vendor attitude problems , vendor competence problems ( related to staff quality ) , and in house competence problems have an indirect effect on re outsourcing decisions .
an erp system performance assessment model development based on the balanced scorecard approach . <eos> previously completed research has not been significant when regarding the aspect of deriving a model for measuring the performance of an enterprise resource planning ( erp ) system . therefore , this research attempts to present an objective and quantitative assessment model based on the balance scorecard approach for the purpose of appraising the performance of the erp system . the methodology used in this research involves the grounded theory , expert questionnaire , the analytic hierarchy process , and the fuzzy theory to filter out and develop the kpis for the erp system performance assessment model . it is expected that such a model may be used by enterprises to assess the efficiency of the erp system during the various stages of management and support within the system . finally , this assessment model is verified in a case company through the examination of its unbiased and quantifiable assessment approach . this result allows us to further understand authentic efficiency , and explore if enterprises have fulfilled their proposed objectives after the introduction of the erp system .
kalman filter based error concealment for video transmission . <eos> a novel error concealment method using a kalman filter is presented ill this paper , in order to successfully utilize the kalman filter , its state transition and observation models that are suitable for the video error concealment are newly defined as follows . the state transition model represents the video decoding process by a notion compensated prediction . furthermore , the new observation model that represents all image blurring process is defined . and calculation of the kalman gain becomes possible . the problem of the traditional methods is solved by using the kalman filter in the proposed method , and accurate reconstruction of corrupted video frames , is achieved . consequently . an effective error concealment method using the kalman filter is realized . experimental results showed that the proposed method has better performance than that of traditional methods .
a quantitative approach for assessing the priorities of software quality requirements . <eos> there are two challenges in software quality requirements specification and analysis ( a ) software requirements are usually imprecise ( b ) software quality requirements often conflict with each other . software quality requirements are often prioritized to resolve their conflicts . however , the priorities of quality requirements are very difficult to assess and determine subjectively . in this paper , a quantitative approach is developed to assess the relative priority of requirements based on the trade off analysis between conflicting requirements . one technique is based on the trade off analysis between the satisfaction degrees of requirements and another is based on the marginal rate of substitution in decision science , which specifies the maximal amount of a decision attribute the customer is willing to sacrifice for a unit increase in another decision attribute . a technique is then introduced to transform relative priorities into numeric priorities of requirements so that they can be used in aggregation of requirements . the analytic approach developed in this paper enables an objective assessment of priorities of the requirements , which provides a solid formal foundation for resolving conflicts by prioritizing conflicting requirements .
scaling into ambient intelligence . <eos> envision the situation that high quality information and entertainment is easily accessible to anyone , anywhere , at any time , and on any device . how realistic is this vision and what does it require from the underlying technology ambient intelligence ( ami ) integrates concepts ranging from ubiquitous computing to autonomous and intelligent systems . an ami environment will be highly dynamic in many aspects . underlying technology must be very flexible to cope with this dynamism . scalability of technology is only one crucial aspect . this paper explores scalability from the processing , the communication , and the software perspectives .
identifying the medical practice after total hip arthroplasty using an integrated hybrid approach . <eos> a critical option of total hip arthroplasty ( tha ) is considered only when tried more conservative treatments but continued to have pain , stiffness , or problems with the function of ones hip . tha plays one of major concerns under the waves of the rapid growth of aging populations and the constrained health care resources in taiwan . moreover , prior studies indicated that imbalanced class distribution problems do exist in the constructed classification model and cause seriously negative effects on model performances in the health care industry . therefore , this study proposes an integrated hybrid approach to provide an alternate method for classifying the quality ( e.g. , the staying length in hospital ) of medical practice with an imbalanced class problem after performing a tha procedure for hip replacement patients and their doctors in the health care industry . the proposed approach is constituted by seven components expert knowledge , global discretization , imbalanced bootstrap technique , reduct and core methods , rough sets , rule induction , and rule filter . the proposed approach is illustrated in practice by examining an experimental dataset from the national health insurance research database ( nhird ) in taiwan . the experimental results reveal that the proposed approach has better performance than the listed methods under evaluation criteria . the output created by the rough set lem2 algorithm is a comprehensible decision rule set that can be applied in knowledge based health care services as desired . the analytical results provide useful tha information for both academics and practitioners and these results could be applicable to other diseases or to other countries with similar social and cultural practices .
ordered coloring based resource binding for datapaths with improved skew adjustability . <eos> this paper proposes a novel high level synthesis for post silicon skew adjustable datapaths . our objective in high level synthesis is to maximize the skew adjustability , i.e. the probability of the success of skew adjustment under delay variations . skew adjustability is first shown to be reduced to the probability for a skew constraint graph ( a weighted directed graph ) to have no positive cycle . since the computation of the skew adjustability is intractable , the original problem is transformed into selective ordered coloring problem , which tries to minimize hazardous cycles instead of an exact skew adjustability . an ilp approach of the selective ordered coloring approach is then proposed . experimental results show not only the effectiveness of our approach , but also how much improvement in the skew adjustability is achieved by equipping one or two extra registers to a datapath circuit .
discontinuous galerkin methods for the stokes equations using divergence free approximations . <eos> a discontinuous galerkin ( dg ) method with solenoidal approximation for file simulation of incompressible flow is proposed . it is applied to the solution of the stokes equations . the interior penalty method is employed to construct the dg weak form . for every element , the approximation space for the velocity field is decomposed as the direct sum of a solenoidal space and an irrotational space . this allows to split the dg weak form into two uncoupled problems the first one solves for the velocity and the hybrid pressure ( pressure along the mesh edges ) and the second one allows the computation of the pressure in the element interior . furthermore , the introduction of an extra penalty term leads to an alternative dg formulation for the computation of solenoidal velocities with no presence of pressure terms . pressure can then be computed as a post process of the velocity solution . numerical examples demonstrate the applicability of the proposed methodologies . copyright ( c ) <digit> john wiley sons , ltd .
a hybrid multiobjective evolutionary algorithm for multiobjective optimization problems . <eos> recently , the hybridization between evolutionary algorithms and other metaheuristics has shown very good performances in many kinds of multiobjective optimization problems ( mops ) , and thus has attracted considerable attentions from both academic and industrial communities . in this paper , we propose a novel hybrid multiobjective evolutionary algorithm ( hmoea ) for real valued mops by incorporating the concepts of personal best and global best in particle swarm optimization and multiple crossover operators to update the population . one major feature of the hmoea is that each solution in the population maintains a nondominated archive of personal best and the update of each solution is in fact the exploration of the region between a selected personal best and a selected global best from the external archive . before the exploration , a selfadaptive selection mechanism is developed to determine an appropriate crossover operator from several candidates so as to improve the robustness of the hmoea for different instances of mops . besides the selection of global best from the external archive , the quality of the external archive is also considered in the hmoea through a propagating mechanism . computational study on the biobjective and three objective benchmark problems shows that the hmoea is competitive or superior to previous multiobjective algorithms in the literature .
a simple java package for gui like interactivity . <eos> this paper discusses the motivation for a simple package designed to incorporate user interactivity into a first course in computer science . the package enables novice programmers to build programs with gui like interactivity while maintaining good design principles . an advantage of this package is that it is easy to implement using the swing class . therefore , it can be used as a case study to illustrate java features .
evolving patches for software repair . <eos> defects are a major concern in software systems . unsurprisingly , there are many tools and techniques to facilitate the removal of defects through their detection and localisation . however , there are few tools that attempt to repair defects . to date , evolutionary tools for software repair have evolved changes directly in the program code being repaired . in this work we describe an implementation pyedb , that encodes changes as a series of code modifications or patches . these modifications are evolved as individuals . we show pyedb to be effective in repairing some small errors , including variable naming errors in python programs . we also demonstrate that evolving patches rather than whole programs simplifies the removal of spurious errors .
implementing and evaluating nested parallel transactions in software transactional memory . <eos> transactional memory ( tm ) is a promising technique that simplifies parallel programming for shared memory applications . to date , most tm systems have been designed to efficiently support single level parallelism . to achieve widespread use and maximize performance gains , tm must support nested parallelism available in many applications and supported by several programming models . we present nestm , a software tm ( stm ) system that supports closed nested parallel transactions . nestm is based on a high performance , blocking stm that uses eager version management and word granularity conflict detection . its algorithm targets the state and runtime overheads of nested parallel transactions . we also describe several subtle correctness issues in supporting nested parallel transactions in nestm and discuss their performance impact . through our evaluation , we quantitatively analyze the performance of nestm using stamp applications and microbenchmarks based on concurrent data structures . first , we show that the performance overhead of nestm is reasonable when single level parallelism is used . second , we quantify the incremental overhead of nestm when the parallelism is exploited in deeper nesting levels and draw conclusions that can be useful in designing a nesting aware tm runtime environment . finally , we demonstrate a use case where nested parallelism improves the performance of a transactional microbenchmark .
temperature characteristics and analysis of monolithic microwave cmos distributed oscillators with g ( m ) varied gain cells and folded coplanar interconnects . <eos> the performance of a novel monolithic microwave cmos distributed oscillator is reported over a temperature range of <digit> degrees c to <digit> degrees c for the first time , along with an analysis of its design characteristics and its temperature stability . the oscillator is stable over the entire temperature range of <digit> degrees c. the monolithic distributed oscillator ( do ) is designed and fabricated in an industry standard 0.18 mu m cmos process , using an n fet based traveling wave amplifier ( twa ) , coplanar waveguides ( cpw ) , and a new coplanar interconnect structure called a ' folded cpw ' . the measured loss of the folded cpw is 1.259 db at <digit> ghz . the distributed oscillator uses a novel architecture of g ( m ) varied gain cells and operates at a bias of 1.8 v. the measured oscillation frequency is 11.7 ghz with 6.1 dbm output power and the measured phase noise is 116.02 dbc hz at <digit> mhz offset , which represent the best reported power and one of the best phase noise results for silicon dos with temperature stability .
emotions in text dimensional and categorical models . <eos> text often expresses the writer 's emotional state or evokes emotions in the reader . the nature of emotional phenomena like reading and writing can be interpreted in different ways and represented with different computational models . affective computing ( ac ) researchers often use a categorical model in which text data are associated with emotional labels . we introduce a new way of using normative databases as a way of processing text with a dimensional model and compare it with different categorical approaches . the approach is evaluated using four data sets of texts reflecting different emotional phenomena . an emotional thesaurus and a bag of words model are used to generate vectors for each pseudo document , then for the categorical models three dimensionality reduction techniques are evaluated latent semantic analysis ( lsa ) , probabilistic latent semantic analysis ( plsa ) , and non negative matrix factorization ( nmf ) . for the dimensional model a normative database is used to produce three dimensional vectors ( valence , arousal , dominance ) for each pseudo document . this three dimensional model can be used to generate psychologically driven visualizations . both models can be used for affect detection based on distances amongst categories and pseudo documents . experiments show that the categorical model using nmf and the dimensional model tend to perform best .
e service quality competition through personalization under consumer privacy concerns . <eos> one important factor that determines the quality of web based customer service is the ability of a firm 's website to provide individual caring and attention . in this sense , online vendors try to offer varieties of web based personalization . however , many previous studies show that there is an obvious trade off between personalization and customer privacy . the main objective of this research is , therefore , to verify the impact of consumers ' information privacy concerns on firms ' collection and use of consumer information for web based personalization where firms compete with different levels of ability in consumer information utilization for personalization . our result shows that a firm of inferior ability in customer information utilization is more affected by privacy concerns than a firm of superior ability in choosing to collect and use consumer information for personalization . however , this does not mean that a firm of inferior ability , which chooses not to provide personalization due to privacy concerns , is worse off than a firm of superior ability , which chooses to provide personalization , in generating profits . on the contrary , a firm of superior ability can become worse off than a firm with inferior ability . ( c ) <digit> elsevier b.v. all rights reserved .
leveraging both quantitative and qualitative data sources to improve it help desk support services . <eos> information technology organizations in higher education look to customer satisfaction surveys or help desk software tracking data such as number of tickets resolved to measure effectiveness of help desk operations . while that is an excellent starting point to discuss effectiveness is it enough . a specific case study will be presented and discussed in this paper . winona state university in <digit> launched its laptop program ( now called the e warrior digital life and learning program ) , providing every student with a laptop computer to enhance his her studies . after <digit> years , winona state is still gaining experience with its digital life and learning program . the program continues to evolve as more faculty and students familiarize themselves with the changing needs of technology for students . winona state will continue to learn more about the role of our technology program 's impact on its students ' educational opportunities for years to come . using data collected from the e warrior assessment plan information technology user services team improved existing student and faculty annual surveys that result in more specific , valid , and reliable indicators of self reported academic benefits . stratified random sampling replaced the current convenience sampling method , improving the ability to generalize . in addition , information will be presented on how the help desk satisfaction survey was used a starting point for follow up faculty and student focus groups to provide more context to issues of it student support services . finally all of these changes fed a broader plan of assessment to evaluate program success .
an eoq model for deteriorating items with price and stock dependent selling rates under inflation and time value of money . <eos> this study applies the discounted cash flow ( dcf ) approach for the analysis of ' a replenishment problem over a finite planning horizon . thus . a deterministic economic order quantity ( eoq ) inventory model taking into account iflation and time value of money is developed for deteriorating items with price and stock dependent selling rates . an efficient solution procedure is presented to determine the optimal number of replenishment , the cycle time and selling price . then the optimal order quantity and the total present value of ' profits are obtained . numerical examples are presented to illustrate the proposed model and particular cases of the model are also discussed .
die bedeutung kurzfristiger und langfristiger speichertechnologien in der energiewende . <eos> die energiewende fhrt in europa zum raschen ausbau der erneuerbaren energien ( ee ) , insbesondere von windenergie und photovoltaik . wegen der kurzen volllaststunden sind hohe leistungen zu installieren , die aber die netzspitzenlast und die vorhandenen speicherpotenziale berschreiten knnen und damit nicht adquat genutzt werden knnen . der ausbau der zentralen und dezentralen speicherkapazitten ist eine notwendigkeit auf dem weg zur berwiegend regenerativen energieversorgung .
online robust optimization framework for qos guarantees in distributed soft real time systems . <eos> in distributed soft real time systems , maximizing the aggregate quality of service ( qos ) is a typical system wide goal , and addressing the problem through distributed optimization is challenging . subtasks are subject to unpredictable failures in many practical environments , and this makes the problem much harder . in this paper , we present a robust optimization framework for maximizing the aggregate qos in the presence of random failures . we introduce the notion of k failure to bound the effect of random failures on schedulability . using this notion we define the concept of k robustness that quantifies the degree of robustness on qos guarantee in a probabilistic sense . the parameter k helps to tradeoff achievable qos versus robustness . the proposed robust framework produces optimal solutions through distributed computations on the basis of lagrangian duality , and we present some implementation techniques . our simulation results show that the proposed framework can probabilistically guarantee sub optimal qos which remains feasible even in the presence of random failures .
a generalized <digit> d hilbert scan using look up tables . <eos> the hilbert curve is a one to one mapping between multidimensional space and one dimensional ( <digit> d ) space . due to the advantage of preserving high correlation of multidimensional points , it receives much attention in many areas . especially in image processing , hilbert curve is studied actively as a scan technique ( hilbert scan ) . currently there have been several hilbert scan algorithms , but they usually have strict implementation conditions . for example , they use recursive functions to generate scans , which makes the algorithms complex and difficult to implement in real time systems . moreover the length of each side in a scanned region should be same and equal to the power of two , which limits the application of hilbert scan greatly . in this paper , to remove the constraints and improve the hilbert scan for a general application , an effective generalized three dimensional ( <digit> d ) hilbert scan algorithm is proposed . the proposed algorithm uses two simple look up tables instead of recursive functions to generate a scan , which greatly reduces the computational complexity and saves storage memory . furthermore , the experimental results show that the proposed generalized hilbert scan can also take advantage of the high correlation between neighboring lattice points in an arbitrarily sized cuboid region , and give competitive performance in comparison with some common scan techniques . ( c ) <digit> elsevier inc. all rights reserved .
video coding using fast geometry adaptive partitioning and an elastic motion model . <eos> effective motion compensated prediction is the key to high performance video coding . to ensure continuous improvement of video coders , emerging motion compensation technologies will need to be successfully integrated into future standards . higher order elastic motion models and geometry adaptive block partitioning are such advanced techniques that are good candidates for integration into future generations of video coders . however , it is vital that these techniques are additive in performance , non interfering and maintain justifiable complexity . in this paper , we propose an efficient block partitioning scheme that incorporates both geometry adaptive partitioning and an elastic motion model as extensions to the standard motion estimation procedure . our experiments suggest that geometric partitioning in combination with the use of an elastic motion model can provide enhanced performance , although the increased complexity is of some concern for real time applications .
development of a reduced human user input task allocation method for multiple robots . <eos> task allocation mechanisms are employed by multi robot systems to efficiently distribute tasks between different robots . currently , many task allocation methods rely on detailed expert knowledge to coordinate robots . however , it may not be feasible to dedicate an expert human user to a multi robot system . hence , a non expert user may have to specify tasks to a team of robots in some situations . this paper presents a novel reduced human user input multi robot task allocation technique that utilises fuzzy inference systems ( fiss ) . a two stage primary and secondary task allocation process is employed to select a team of robots comprising manager and worker robots . a multi robot mapping and exploration task is utilised as a model task to evaluate the task allocation process . experiments show that primary task allocation is able to successfully identify and select manager robots . similarly , secondary task allocation successfully identifies and selects worker robots . both task allocation processes are also robust to parameter variation permitting intuitive selection of parameter values .
clowns to the left of me , jokers to the right ( pearl ) dissecting data structures . <eos> this paper introduces a small but useful generalisation to the ' derivative ' operation on datatypes underlying huet 's notion of ' zipper ' ( huet <digit> mcbride <digit> abbott et al. 2005b ) , giving a concrete representation to one hole contexts in data which is undergoing transformation . this operator , ' dissection ' , turns a container like functor into a bifunctor representing a one hole context in which elements to the left of the hole are distinguished in type from elements to its right . i present dissection here as a generic program , albeit for polynomial functors only . the notion is certainly applicable more widely , but here i prefer to concentrate on its diverse applications . for a start , map like operations over the functor and fold like operations over the recursive data structure it induces can be expressed by tail recursion alone . further , the derivative is readily recovered from the dissection . indeed , it is the dissection structure which delivers huet 's operations for navigating zippers . the original motivation for dissection was to define ' division ' , capturing the notion of leftmost hole , canonically distinguishing values with no elements from those with at least one . division gives rise to an isomorphism corresponding to the remainder theorem in algebra . by way of a larger example , division and dissection are exploited to give a relatively efficient generic algorithm for abstracting all occurrences of one term from another in a first order syntax . the source code for the paper is available online ( <digit> ) and compiles with recent extensions to the glasgow haskell compiler .
dendroid a text mining approach to analyzing and classifying code structures in android malware families . <eos> the rapid proliferation of smartphones over the last few years has come hand in hand with and impressive growth in the number and sophistication of malicious apps targetting smartphone users . the availability of reuse oriented development methodologies and automated malware production tools makes exceedingly easy to produce new specimens . as a result , market operators and malware analysts are increasingly overwhelmed by the amount of newly discovered samples that must be analyzed . this situation has stimulated research in intelligent instruments to automate parts of the malware analysis process . in this paper , we introduce dendroid , a system based on text mining and information retrieval techniques for this task . our approach is motivated by a statistical analysis of the code structures found in a dataset of android os malware families , which reveals some parallelisms with classical problems in those domains . we then adapt the standard vector space model and reformulate the modelling process followed in text mining applications . this enables us to measure similarity between malware samples , which is then used to automatically classify them into families . we also investigate the application of hierarchical clustering over the feature vectors obtained for each malware family . the resulting dendo grams resemble the so called phylogenetic trees for biological species , allowing us to conjecture about evolutionary relationships among families . our experimental results suggest that the approach is remarkably accurate and deals efficiently with large databases of malware instances . ( c ) <digit> elsevier ltd. all rights reserved .
a novel method using adaptive hidden semi markov model for multi sensor monitoring equipment health prognosis . <eos> multi sensor monitoring equipment health prognosis is analyzed . adaptive hidden semi markov model is proposed for health prognosis . the proposed model and hazard rate equations are used to predict rul . the performance of the proposed methods by one case study is analyzed . the proposed methods have better performance than other methods .
a hitchcock assisted video edited night at the opera . <eos> hitchcock is a semi automatic video editing system . this video shows users collaboratively authoring a home video .
frequency domain analysis of model order reduction techniques . <eos> in this study , some popular model order reduction and superelement techniques are studied in frequency and time domains . frequency domain identification ( fdi ) methods are also applied to semidiscrete finite element equations to obtain reduced order models for structural systems . an fdi algorithm that determines the coefficients of the reduced order model by using nonlinear least squares ( nls ) method and subspace based identification ( sbi ) method are applied to a sample problem and the results are compared with component mode synthesis ( cms ) and quasi static mode synthesis ( qsm ) methods . in literature , phase errors of model order reduction techniques have not been studied however , it is shown in this paper that phase errors are also important in evaluating the performance of model order reduction techniques . in general , the nls and sbi methods have better performance than the cms and qsm methods however , as the size of problems increases , the nls method may have convergence problems and the sbi method may yield estimated models having large orders .
estimation of mean based on modified robust extreme ranked set sampling . <eos> in this paper , double robust extreme ranked set sampling ( drerss ) and its properties for estimating the population mean are considered . it turns out that , when the underlying distribution is symmetric , drerss gives unbiased estimators of the population mean . also , it is found that drerss is more efficient than the simple random sampling ( srs ) , ranked set sampling ( rss ) , and extreme ranked set sampling ( erss ) methods . for asymmetric distributions considered in this study , the drerss has a small bias and it is more efficient than srs , rss , and erss . a real data set is used to illustrate the drerss method .
half price architecture . <eos> current generation microprocessors are designed to process instructions with one and two source operands at equal cost . handling two source operands requires multiple ports for each instruction in structures such as the register file and wakeup logic which are often in the processor 's critical timing paths . we argue that these structures are overdesigned since only a small fraction of instructions require two source operands to be processed simultaneously . in this paper , we propose the half price architecture that judiciously removes this overdesign by restricting the processor 's capability to handle two source operands in certain timing critical cases . two techniques are proposed and evaluated one for the wakeup logic is sequential wakeup , which decouples half of the tag matching logic from the wakeup bus to reduce the load capacitance of the bus . the other technique for the register file is sequential register access , which halves the register read ports by sequentially accessing two values using a single port when needed . we show that a pipeline that optimizes scheduling and register access for a single operand achieves nearly the same performance as an ideal base machine that fully handles two operands , with 2.2 % ( worst case 4.8 % ) ipc degradation .
an integer programming approach for frequent itemset hiding . <eos> the rapid growth of transactional data brought , soon enough , into attention the need of its further exploitation . in this paper , we investigate the problem of securing sensitive knowledge from being exposed in patterns extracted during association rule mining . instead of hiding the produced rules directly , we decide to hide the sensitive frequent itemsets that may lead to the production of these rules . as a first step , we introduce the notion of distance between two databases and a measure for quantifying it . by trying to minimize the distance between the original database and its sanitized version ( that can safely be released ) , we propose a novel , exact algorithm for association rule hiding and evaluate it on real world datasets demonstrating its effectiveness towards solving the problem .
an efficient and flexible web services based multidisciplinary design optimisation framework for complex engineering systems . <eos> multidisciplinary design optimisation ( mdo ) involves multiple disciplines , multiple coupled relationships and multiple processes , which is implemented by different specialists dispersed geographically on heterogeneous platforms with different analysis and optimisation tools . the product design data integration and data sharing among the participants hampers the development and applications of mdo in enterprises seriously . therefore , a multi hierarchical integrated product design data model ( mh ipdm ) supporting the mdo in the web environment and a web services based multidisciplinary design optimisation ( web mdo ) framework are proposed in this article . based on the enabling technologies including web services , ontology , workflow , agent , xml and evidence theory , the proposed framework enables the designers geographically dispersed to work collaboratively in the mdo environment . the ontology based workflow enables the logical reasoning of mdo to be processed dynamically . the evidence theory based uncertainty reasoning and analysis supports the quantification , aggregation and analysis of the conflicting epistemic uncertainty from multiple sources , which improves the quality of product . finally , a proof of concept prototype system is developed using j2ee and an example of supersonic business jet is demonstrated to verify the autonomous execution of mdo strategies and the effectiveness of the proposed approach .
flexible and extensible preference evaluation in database systems . <eos> personalized database systems give users answers tailored to their personal preferences . while numerous preference evaluation methods for databases have been proposed ( e.g. , skyline , top k , k dominance , k frequency ) , the implementation of these methods at the core of a database system is a double edged sword . core implementation provides efficient query processing for arbitrary database queries , however , this approach is not practical since each existing ( and future ) preference method requires implementation within the database engine . to solve this problem , this article introduces flexpref , a framework for extensible preference evaluation in database systems . flexpref , implemented in the query processor , aims to support a wide array of preference evaluation methods in a single extensible code base . integration with flexpref is simple , involving the registration of only three functions that capture the essence of the preference method . once integrated , the preference method lives at the core of the database , enabling the efficient execution of preference queries involving common database operations . this article also provides a query optimization framework for flexpref , as well as a theoretical framework that defines the properties a preference method must exhibit to be implemented in flexpref . to demonstrate the extensibility of flexpref , this article also provides case studies detailing the implementation of seven state of the art preference evaluation methods within flexpref . we also experimentally study the strengths and weaknesses of an implementation of flexpref in postgresql over a range of single table and multitable preference queries .
irregular community discovery for cloud service improvement . <eos> utility services provided by cloud computing rely on virtual customer communities forming spontaneously and evolving continuously . clarifying the explicit boundaries of these communities is thus essential to the quality of utility services in cloud computing . communities with overlapping features or prominent peripheral vertexes are usually typical irregular communities . traditional community identification algorithms are limited in discovering irregular topological structures from cr networks , whereas these irregular shapes typically play an important role in finding prominent customers which are ignored in social crm otherwise . we present a novel method of discovering irregular communities . it firstly finds and merges primitive maximal cliques and the irregular features of overlapping and prominent sparse vertices are further considered . an empirical case and a methodology comparison confirm the feasibility and efficiency of our approach .
communication aware process and thread mapping using online communication detection . <eos> we perform online detection of inter process and inter thread communication . detected communication pattern is used to migrate processes and threads . operating system based mechanism , no changes to applications or runtime libraries . we reduce execution time and energy consumption . evaluation on shared memory machines and a cluster show substantial improvements .
a reversible data hiding scheme based on the sudoku technique . <eos> sudoku technique is used for data embedding . a reversible data hiding scheme that is based on the sudoku technique and can achieve the higher embedding capacity . the reference matrix is built and applied to obtain better embedding capacity . the experimental results showed that the proposed scheme obtained higher embedding capacity than some other previous schemes . the proposed scheme also achieved more consistent results for the different test images .
a variant of boneh franklin ibe with a tight reduction in the random oracle model . <eos> the first practical identity based encryption ( ibe ) scheme was published by boneh and franklin at crypto <digit> , based on the elliptic curve pairing . since that time , many other ibe schemes have been published . in this paper , we describe a variant of boneh franklin with a tight reduction in the random oracle model . our new scheme is quite efficient compared to existing schemes moreover , upgrading from boneh franklin to our new scheme is straightforward .
deployment experience with differentiated services . <eos> while ubiquitous qos mechanisms are not yet deployed widely across the public internet , the differentiated services ( diffserv ) architecture has in fact proven itself to be a good match for the technical needs of many service providers . in this paper we consider the state of deployment of qos mechanisms in large service provider ip networks ( many of which happen to be offering vpn or voip services rather than public internet service . ) we discuss the factors that have helped and hindered the deployment of qos mechanisms in general and diffserv in particular . we conclude that many if not most of the barriers to qos deployment are business issues rather than technical shortcomings of the existing qos architectures .
reale a reconnection based arbitrary lagrangian eulerian method . <eos> we present a new reconnection based arbitrary lagrangian eulerian ( ale ) method . the main elements in a standard ale simulation are an explicit lagrangian phase in which the solution and grid are updated , a rezoning phase in which a new grid is defined , and a remapping phase in which the lagrangian solution is transferred ( conservatively interpolated ) onto the new grid . in standard ale methods the new mesh from the rezone phase is obtained by moving grid nodes without changing connectivity of the mesh . such rezone strategy has its limitation due to the fixed topology of the mesh . in our new method we allow connectivity of the mesh to change in rezone phase , which leads to general polygonal mesh and allows to follow lagrangian features of the mesh much better than for standard ale methods . rezone strategy with reconnection is based on using voronoi tessellation . we demonstrate performance of our new method on series of numerical examples and show it superiority in comparison with standard ale methods without reconnection . ( c ) <digit> elsevier inc. all rights reserved .
compressing repeated content within large scale remote sensing images . <eos> large scale remote sensing images , including both satellite and aerial photographs , are widely used to render terrain scenes in real time geographic visualization systems . such systems often require large memories in order to store fine terrain details and fast network speeds to transfer image data , if they are built as web applications . in this paper , we propose a progressive texture compression framework to reduce the memory and bandwidth cost by compressing repeated content within and among large scale remote sensing images . different from existing image factorization methods , our algorithm incrementally find similar regions in new images so that large scale images can be more efficiently compressed over time . we further propose a descriptor , the gray split rotate ( gsr ) descriptor , to accelerate the similarity search . the reconstruction quality is finally improved by compressing residual error maps using customized s3tc like compression . our experiment shows that even with the error maps , our system still has higher compression rate and higher compression quality than using s3tc alone , which is a typical compression solution in most existing visualization systems .
evolutionary cepstral coefficients . <eos> evolutionary algorithms provide flexibility and robustness required to find satisfactory solutions in complex search spaces . this is why they are successfully applied for solving real engineering problems . in this work we propose an algorithm to evolve a robust speech representation , using a dynamic data selection method for reducing the computational cost of the fitness computation while improving the generalisation capabilities . the most commonly used speech representation are the mel frequency cepstral coefficients , which incorporate biologically inspired characteristics into artificial recognizers . recent advances have been made with the introduction of alternatives to the classic mel scaled filterbank , improving the phoneme recognition performance in adverse conditions . in order to find an optimal filterbank , filter parameters such as the central and side frequencies are optimised . a hidden markov model is used as the classifier for the evaluation of the fitness for each individual . experiments were conducted using real and synthetic phoneme databases , considering different additive noise levels . classification results show that the method accomplishes the task of finding an optimised filterbank for phoneme recognition , which provides robustness in adverse conditions .
neural network constitutive model for rate dependent materials . <eos> neural network ( nn ) constitutive model adjusts itself to describe given stress and strain relationship . it is capable of capturing complex material behavior , using stress and strain sets from experiments . this paper presents a rate dependent nn constitutive model formulation and its implementation in finite element analysis . the proposed nn model is verified for a standard solid viscoelasticity model . the model is then applied to analysis of time dependent behavior of concrete . the proposed model has potential of capturing any rate dependent material models , provided enough data sets are given . the issue of what constitutes a sufficient data set to train a neural network constitutive model must be addressed in future research .
re ranking algorithm using post retrieval clustering for content based image retrieval . <eos> in this paper , we propose a re ranking algorithm using post retrieval clustering for content based image retrieval ( cbir ) . in conventional cbir systems , it is often observed that images visually dissimilar to a query image are ranked high in retrieval results . to remedy this problem , we utilize the similarity relationship of the retrieved results via post retrieval clustering . in the first step of our method , images are retrieved using visual features such as color histogram . next , the retrieved images are analyzed using hierarchical agglomerative clustering methods ( hacm ) and the rank of the results is adjusted according to the distance of a cluster from a query . in addition , we analyze the effects of clustering methods , query cluster similarity functions , and weighting factors in the proposed method . we conducted a number of experiments using several clustering methods and cluster parameters . experimental results show that the proposed method achieves an improvement of retrieval effectiveness of over <digit> % on average in the average normalized modified retrieval rank ( anmrr ) measure .
do nonnative listeners benefit as much as native listeners from spatial cues that release speech from masking . <eos> since most everyday communication takes place in less than optimal acoustic settings , it is important to understand how such environments affect nonnative listeners . in this study we compare the speech reception abilities of native and nonnative english speakers when they are asked to repeat semantically anomalous sentences masked by steady state noise or two other talkers in two conditions when the target and masker appear to be colocated and when the target and masker appear to emanate from different loci . we found that the later the age of language acquisition , the higher the threshold for speech reception under all conditions , suggesting that the ability to extract speech information from masking sounds in complex acoustic situations depends on language competency . interestingly , however , native and nonnative listeners benefited equally from perceived spatial separation ( an acoustic cue that releases speech from masking ) independent of whether the speech target was masked by speech or noise , suggesting that the acoustic factors that release speech from masking are not affected by linguistic competence . in addition speech reception thresholds were correlated with vocabulary scores in all individuals , both native and nonnative . the implications of these findings for nonnative listeners in acoustically complex environments are discussed .
fe doped sno2 obtained by mechanical alloying . <eos> in this work , iron doped sno2 powders were prepared by two methods mechanical alloying and mechanochemical alloying with successive thermal treatment . the influence of different milling conditions such as ball to powder weight ratio , milling time , rotation velocity of supporting disc and the type of iron starting reactive and their fe concentration on the structural and magnetic properties of the products were investigated . a greater incorporation of fe in the sno2 structure was observed when the samples were prepared by using mechanochemical alloying and successive thermal treatment .
noncoding rnas persistent viral agents as modular tools for cellular needs . <eos> it appears that all the detailed steps of evolution stored in dna that are read , transcribed , and translated in every developmental and growth process of each individual cell depend on rna mediated processes , in most cases interconnected with other rnas and their associated protein complexes and functions in a strict hierarchy of temporal and spatial steps . life could not function without the key agents of dna replication , namely mrna , trna , and rrna . not only rrna , but also trna and the processing of the primary transcript into the pre mrna and the mature mrna are clearly descended from retro elements with obvious retroviral ancestry . they seem to be remnants of viral infection events that did not kill their host but transferred phenotypic competences to their host and changed both the genetic identity of the host organism and the identity of the former infectious viral swarms . in this respect , noncoding rnas may represent a great variety of modular tools for cellular needs that are derived from persistent nonlytic viral settlers .
fast synthesis of cross coupled resonator filters using hybrid optimization algorithm . <eos> this article describes a hybrid optimization algorithm for synthesizing prototype filters with arbitrary topology . two cost functions are constructed from the eigenvalues of the coupling matrix and its principal submatrices or at specially selected frequencies involving the values s11 and s21 . the values of non zero elements of the coupling matrix are found using solvopt and fmincon to minimize the cost functions . with the initial coupling matrix synthesized in a tridiagonal form by solving the jacobi inverse eigenvalue problem , the proposed method provides fast convergence and good accuracy to find the final solution . in addition , as the hybrid method may be run from random initial variables whose values lie within specified limits , it has the potential to be used for series of coupling matrix synthesis problems . for demonstrating the proposed hybrid optimization algorithm , some extraordinary prototype topologies have been synthesized to validate the proposed synthesis procedure . <digit> wiley periodicals , inc. int j rf and microwave cae 25 445452 , <digit> .
vaudeville a high performance , voice activated teleconferencing application . <eos> we present vaudeville , a voice activated , hands free , atm based video conferencing application . this system is scalable although video bandwidth is normally a limiting factor in the number of conferences participants , the bandwidth attributed to the video is not a function of conference size . this is achieved through an automatic , distributed floor control mechanism that gives the appearance of an open floor . audio and video are encoded in hardware using a platform independent , atm hardware multimedia interface . vaudeville features digitally transmitted ntsc video , voice activated audio transmission , audio bridging of two audio streams , and voice activated video switching . multiple simultaneous multiparty conferences are supported . users can move freely among conferences without knowledge of the underlying network structure . we describe how vaudeville was built using a component based distributed programming environment . we also describe the algorithms used to control the audio and video of the application .
printed arabic character recognition using hmm . <eos> the arabic language has a very rich vocabulary . more than <digit> million people speak this language as their native speaking , and over <digit> billion people use it in several religion related activities . in this paper a new technique is presented for recognizing printed arabic characters . after a word is segmented , each character word is entirely transformed into a feature vector . the features of printed arabic characters include strokes and bays in various directions , endpoints , intersection points , loops , dots and zigzags . the word skeleton is decomposed into a number of links in orthographic order , and then it is transferred into a sequence of symbols using vector quantization . single hidden markov model has been used for recognizing the printed arabic characters . experimental results show that the high recognition rate depends on the number of states in each sample .
measuring user rated language quality development and validation of the user interface language quality survey ( lqs ) . <eos> development of a survey to measure user rated user interface language quality ( lqs ) . the survey shows good psychometric properties . the lqs correlates moderately to usability metrics . a case study is provided to show how the lqs can be used to improve language quality . the lqs has been translated into <digit> languages and applied to several products .
managing the complexity in first year programming . <eos> in this paper , we describe a method for managing the complexity in cs2 .
survivable millimeter wave mesh networks . <eos> millimeter wave mesh networks have the potential to provide cost effective high bandwidth solutions to many current bandwidth constrained networks including cellular backhaul . however , the availability of such networks is severely limited due to their susceptibility to weather , such as precipitation and humidity . in this paper , we present a rigorous approach to survivable millimeter wave mesh networks based on experimentation , modeling , and simulation . individual link performance is characterised using frame error rate measurements from millimeter wave transmissions on test links over a period of one year . a geometric model based on radar reflectivity data is used to characterise rain storms and determine their impact on spatially correlated links of a mesh network . to mitigate the impact of link impairments on network services , we present two cross layered routing protocols to route around the failures p warp ( predictive weather assisted routing protocol ) and xl ospf ( cross layered open shortest path first ) . we conduct a performance analysis of the proposed mesh network under the presence of actual weather events as recorded by the us national weather service . results show that the proposed approach yields the highest dependability when compared against existing routing methods . ( c ) <digit> published by elsevier b.v.
loops prevention in multi nested mobile networks nemo . <eos> the election of the best point of connection toward the network of infrastructure plays a fundamental role during the process of configuration of a nested mobile network . thus , in this work the associated problems to the loops generation are revised during the process of connection , and a mechanism is described that permits to solve them for arrogance of routers with multiple egress interfaces , by means of a novel algorithm of loops control and the analysis of the messages of the protocol .
enumeration of inequivalent irreducible goppa codes . <eos> we consider irreducible goppa codes over fq f q of length qn q n defined by polynomials of degree r , where q is a prime power and n , r n , r are arbitrary positive integers . we obtain an upper bound on the number of such codes .
robust estimation and wavelet thresholding in partially linear models . <eos> this paper is concerned with a semiparametric partially linear regression model with unknown regression coefficients , an unknown nonparametric function for the non linear component , and unobservable gaussian distributed random errors . we present a wavelet thresholding based estimation procedure to estimate the components of the partial linear model by establishing a connection between an l ( <digit> ) penalty based wavelet estimator of the nonparametric component and huber 's m estimation of a standard linear model with outliers . some general results on the large sample properties of the estimates of both the parametric and the nonparametric part of the model are established . simulations are used to illustrate the general results and to compare the proposed methodology with other methods available in the recent literature .
existence of solutions for third order boundary value problems on a time scale . <eos> let t be a closed subset of r with inf t infinity and sup t infinity . for the third order nonlinear equation , u ( delta3 ) f ( t , u , u ( delta ) , u ( deltadelta ) ) , t is an element of t , where f t x r <digit> > r is continuous , we assume solutions of initial value problems are unique and extend to t. we consider questions of the uniqueness of solutions implying the existence of solutions for conjugate boundary problems on t. ( c ) <digit> elsevier science ltd. all rights reserved .
the painful face pain expression recognition using active appearance models . <eos> pain is typically assessed by patient self report . self reported pain , however , is difficult to interpret and may be impaired or in some circumstances ( i.e. , young children and the severely ill ) not even possible . to circumvent these problems behavioral scientists have identified reliable and valid facial indicators of pain . hitherto , these methods have required manual measurement by highly skilled human observers . in this paper we explore an approach for automatically recognizing acute pain without the need for human observers . specifically , our study was restricted to automatically detecting pain in adult patients with rotator cuff injuries . the system employed video input of the patients as they moved their affected and unaffected shoulder . two types of ground truth were considered . sequence level ground truth consisted of likert type ratings by skilled observers . frame level ground truth was calculated from presence absence and intensity of facial actions previously associated with pain . active appearance models ( aam ) were used to decouple shape and appearance in the digitized face images . support vector machines ( svm ) were compared for several representations from the aam and of ground truth of varying granularity . we explored two questions pertinent to the construction , design and development of automatic pain detection systems . first , at what level ( i.e. , sequence or frame level ) should datasets be labeled in order to obtain satisfactory automatic pain detection performance second , how important is it , at both levels of labeling , that we non rigidly register the face
model based tests for simplification of lattice processes . <eos> separable processes represent a convenient class of models for data collected on a regular rectangular lattice . three model based tests , for testing separability and testing axial symmetry and separability together , are presented . these are shown to be much more powerful than existing model free tests using the sample periodogram , provided the model assumptions hold . a simulation study also suggests that these tests are not very sensitive to small departures from the assumed process .
structured streams a new transport abstraction . <eos> internet applications currently have a choice between stream and datagram transport abstractions . datagrams efficiently support small transactions and streams are suited for long running conversations , but neither abstraction adequately supports applications like http that exhibit a mixture of transaction sizes , or applications like ftp and sip that use multiple transport instances . structured stream transport ( sst ) enhances the traditional stream abstraction with a hierarchical hereditary structure , allowing applications to create lightweight child streams from any existing stream . unlike tcp streams , these lightweight streams incur neither <digit> way handshaking delays on startup nor time wait periods on close . each stream offers independent data transfer and flow control , allowing different transactions to proceed in parallel without head of line blocking , but all streams share one congestion control context . sst supports both reliable and best effort delivery in a way that semantically unifies datagrams with streams and solves the classic large datagram problem , where a datagram 's loss probability increases exponentially with fragment count . finally , an application can prioritize its streams relative to each other and adjust priorities dynamically through out of band signaling . a user space prototype shows that sst is tcp friendly to within <digit> % , and performs comparably to a user space tcp and to within <digit> % of kernel tcp on a wifi network .
novelty detection in a changing environment a negative selection approach . <eos> in the recent past , there have been a number of engineering studies motivated by analogies with the human immune system . the immune system has provided a rich source of inspiration for pattern recognition , machine learning and data mining analyses . one of the properties of the immune system which proves particularly useful for novelty detection is that of self non self discrimination and this forms the basis of the negative selection algorithm which has previously been applied by other researchers to the problem of time series novelty detection . the object of the current paper is to apply the negative selection algorithm to more general feature sets and also to consider the case of novelty detection where the normal condition set is significantly non gaussian or varies with operational or environmental conditions .
identifying and locating dominating codes np completeness results for directed graphs . <eos> let g ( v , a ) be a directed , asymmetric graph and c a subset of vertices , and let b r ( ) ( v ) denote the set of all vertices x such that there exists a directed path from x to v with at most r arcs . if the sets b r ( ) ( v ) boolean and c , v is an element of v ( respectively , v is an element of v c ) , are all nonempty and different , we call c an r identifying code ( respectively , an r locating dominating code ) of g. in other words , if c is an r identifying code , then one can uniquely identify a vertex v is an element of v only by knowing which codewords belong to b r ( ) ( v ) , and if c is r locating dominating , the same is true for the vertices v in v c. we prove that , given a directed , asymmetric graph g and an integer k , the decision problem of the existence of an r identifying code , or of an r locating dominating code , of size at most k in g , is np complete for any r greater than or equal to <digit> and remains so even when restricted to strongly connected , directed , asymmetric , bipartite graphs or to directed , asymmetric , bipartite graphs without directed cycles .
improving change descriptions with change contexts . <eos> software archives are one of the best sources available to researchers for understanding the software development process . however , much detective work is still necessary in order to unravel the software development story . during this process , researchers must isolate changes and follow their trails over time . in support of this analysis , several research tools have provided different representations for connecting the many changes extracted from software archives . most of these tools are based on textual analysis of source code and use line based differencing between software versions . this approach limits the ability to process changes structurally resulting in less concise and comparable items . adoption of structure based approaches have been hampered by complex implementations and overly verbose change descriptions . we present a technique for expressing changes that is fine grained but preserves some structural aspects . the structural information itself may not have changed , but instead provides a context for interpreting the change . this in turn , enables more relevant and concise descriptions in terms of software types and programming activities . we apply our technique to common challenges that researchers face , and then we discuss and compare our results with other techniques .
minimum energy disjoint path routing in wireless ad hoc networks . <eos> we develop algorithms for finding minimum energy disjoint paths in an all wireless network , for both the node and link disjoint cases . our major results include a novel polynomial time algorithm that optimally solves the minimum energy <digit> link disjoint paths problem , as well as a polynomial time algorithm for the minimum energy k node disjoint paths problem . in addition , we present efficient heuristic algorithms for both problems . our results show that link disjoint paths consume substantially less energy than node disjoint paths . we also found that the incremental energy of additional link disjoint paths is decreasing . this finding is somewhat surprising due to the fact that in general networks additional paths are typically longer than the shortest path . however , in a wireless network , additional paths can be obtained at lower energy due to the broadcast nature of the wireless medium . finally , we discuss issues regarding distributed implementation and present distributed versions of the optimal centralized algorithms presented in the paper .
the volumetric barrier for convex quadratic constraints . <eos> let q ( x s ( i ) ( x ) greater than or equal to <digit> , i <digit> , ... , k where s ( i ) ( x ) a ( i ) ( t ) x x ( t ) q ( i ) x c ( i ) , and q ( i ) is an n x n positive semidefinite matrix . we prove that the volumetric and combined volumetric logarithmic barriers for q are o ( rootkn ) and o ( rootkn ) self concordant , respectively . our analysis uses the semidefinite programming ( sdp ) representation for the convex quadratic constraints defining q , and our earlier results on the volumetric barrier for sdp . the self concordance results actually hold for a class of sdp problems more general than those corresponding to the sdp representation of q.
comb snp calling and mapping analysis for color and nucleotide space platforms . <eos> the determination of single nucleotide polymorphisms ( snps ) has become faster and more cost effective since the advent of short read data from next generation sequencing platforms such as roche 's <digit> sequencer , illumina 's solexa platform , and applied biosystems solid sequencer . the solid sequencing platform , which is capable of producing more than 6gb of sequence data in a single run , uses a unique encoding scheme where color reads represent transitions between adjacent nucleotides . the determination of snps from color reads usually involves the translation of color alignments to likely nucleotide strings to facilitate the use of tools designed for nucleotide reads . this technique results in the loss of significant information in the color read , producing many incorrect snp calls , especially if regions exist with dense or adjacent polymorphism . additionally , color reads align ambiguously and incorrectly more often than nucleotide reads making integrated snp calling a difficult challenge . we have developed comb , a snp calling tool which operates directly in color space , using a bayesian model to incorporate unique and ambiguous reads to iteratively determine snp identity . comb is capable of accurately calling short consecutive nucleotide polymorphisms and densely clustered snps both of which other snp calling tools fail to identify . comb , which is capable of using billions of short reads to accurately and efficiently perform whole human genome snp calling in parallel , is also capable of using sequence data or even integrating sequence and color space data sets . we use real and simulated data to demonstrate that comb 's iterative strategy and recalibration of quality scores allow it to discover more true snps while calling fewer false positives than tools which use only color alignments as well as tools which translate color reads to nucleotide strings .
comparative study of gigahertz cmos lc quadrature voltage controlled oscillators with relevance to phase noise . <eos> this review paper presents a comparative study of published integrated submicron cmos quadrature voltage controlled oscillator designs , based on lc resonator tanks operating at gigahertz frequencies . although special reference to phase noise reduction is made , the comparison also concerns issues such as power consumption , tuning range and the phase accuracy of the quadrature signals . the effect of supply voltage reduction on the choice of the oscillator topology is also included in the discussion .
saba a security aware and budget aware workflow scheduling strategy in clouds . <eos> in this paper we address the workflow scheduling in clouds . we consider security and cost considerations . we propose a security aware and budget aware algorithm . we present rigorous theoretical analysis and verify on six different real datasets . we demonstrate a trade off relationship between make span and the monetary cost .
low cost implementation of single frequency estimation scheme using auto correlation function . <eos> this letter proposes a low complexity scheme for estimating the frequency of a complex sinusoid in flat fading channels . the proposed estimator yields an estimation performance that is comparable to the existing autocorrelation based frequency estimator , while retaining the same frequency range . its implementation complexity is much lower than the conventional scheme , thus this allows for fast estimation in real time .
optimizing large join queries using a graph based approach . <eos> although many query tree optimization strategies have been proposed in the literature , there still is a lack of a formal and complete representation of all possible permutations of query operations ( i.e. , execution plans ) in a uniform manner . a graph theoretic approach presented in this paper provides a sound mathematical basis for representing a query and searching for an execution plan . in this graph model , a node represents an operation and a directed edge between two nodes indicates the order of executing these two operations in an execution plan . each node is associated with a weight and so is an edge . the weight is an expression containing optimization required parameters , such as relation size , tuple size , join selectivity factors . all possible execution plans are representable in this graph and each spanning tree of the graph becomes an execution plan , it is a general model which can be used in the optimizer of a dbms for interal query representation . on the basis of this model , we devise an algorithm that finds a near optimal execution plan using only polynomial time . the algorithm is compared with a few other popular optimization methods . experiments show that the proposed algorithm is superior to the others under most circumstances .
temporal uncertainty time warp an implementation based on java and actorfoundry . <eos> this article describes temporal uncertainty time warp ( tutw ) , a distributed control engine designed for exploitation of temporal uncertainty ( tu ) in general optimistic simulations . novel in tutw is an event model in which time intervals are attached to events instead of classical punctual or precise time stamps . all of this complies with system specifications where the occurrence time of events can not be known with complete certainty . tutw is able to take advantage of tu by resolving events in such a way as to reduce the number of rollbacks . the simulator performance can thus be improved without necessarily compromising the accuracy of the results . an agent based implementation of tutw was achieved that enables simulations to be carried out over the internet . the implementation depends on java and the actorfoundry middleware . the implementation is totally portable and simplifies configuration and control of a distributed simulation virtual computer that includes heterogeneous computing platforms . the article reports performance and accuracy results of tutw applied to a large qnet simulation model .
on the number of spanning trees of k n ( m ) g graphs . <eos> the k n complement of a graph g , denoted by k n g , is defined as the graph obtained from the complete graph kn by removing a set of edges that span g if g has n vertices , then k n g coincides with the complement g of the graph g. in this paper we extend the previous notion and derive determinant based formulas for the number of spanning trees of graphs of the form k n ( m ) g , where k n ( m ) is the complete multigraph on n vertices with exactly m edges joining every pair of vertices and g is a multigraph spanned by a set of edges of k m n the graph k n ( m ) g ( resp . k n ( m ) g ) is obtained from k n ( m ) by adding ( resp . removing ) the edges of g. moreover , we derive determinant based formulas for graphs that result from k m n by adding and removing edges of multigraphs spanned by sets of edges of the graph k n ( m ) . we also prove closed formulas for the number of spanning tree of graphs of the form k n ( m ) g , where g is ( i ) a complete multipartite graph , and ( ii ) a multi star graph . our results generalize previous results and extend the family of graphs admitting formulas for the number of their spanning trees .
a definition of partial derivative of random functions and its application to rbfnn sensitivity analysis . <eos> considering the inputs of a feed forward neural network as random variables , this paper proposes a definition of partial derivative of a function with respect to a random variable in the probability measure space . the mathematical expectation of the mean square or absolute value of the partial derivative is regarded as a type of measure of the network 's sensitivity , which extends zurada 's sensitivity definition of networks in zurada et al , perturbation method for deleting redundant inputs of perceptron networks , neurocomputing <digit> ( <digit> ) <digit> from the certain environment to the stochastic environment . furthermore , for the purpose of network 's redundant feature deletion or feature selection , the new sensitivity measure is applied to the sensitivity analysis of radial basis function neural networks ( rbfnns ) . the feasibility and the effectiveness of the sensitivity approach to redundant feature deletion are illustrated .
investigation of nialn as gate material for submicron cmos technology . <eos> nickelaluminiumnitride ( nialn ) is investigated as gate material for submicron cmos technology for the first time . the nialn films have been reactively sputtered from a ni0 .5 al0 .5 target in a mixture of argon and nitrogen gas . the influence of the reactive gas content and process temperatures on the work function is presented . electrical properties are extracted from high and low frequency capacitancevoltage measurements ( qscv , hfcv ) . resistivity measurements are shown for various process conditions . interface properties are observed by transmission electron microscopy . primarily results show nialns suitability for use as gate material in a cmos replacement gate technology . fabrication of n type metal oxide semiconductor field effect transistors with a nialn gates activated at <digit> c is demonstrated .
characterizing the performance penalties induced by irregular code using pointer structures and indirection arrays on the intel core <digit> architecture . <eos> irregularity is one of the fundamental causes for performance degradation in applications . both hardware and software have a hard time coping with irregular memory access patterns and irregularity in flow control . on the hardware side , execution is optimized for regular data accesses and irregular memory access streams can not be predicted . on the software side , compilers are are not able to reason about memory locations and loop bounds . this prevents many optimizations to be applied . in this paper , we measure and characterize the impact of various facets of irregularity using spark00 , a set of benchmarks that explicitly targets the measurement of the impact of irregularity , on one of themost commonly used architectures today , the intel core <digit> . the benchmarks consist of kernels that are based on pointers , a notorious cause of irregularity , kernels that use indirection arrays , and kernels that implement regular counterparts of some of the irregular kernels . by employing different data sets and different memory layouts these benchmarks are used to characterize architectural features .
stability analysis of numerical methods for systems of functional differential and functional equations . <eos> this paper is concerned with the numerical solution of functional differential and functional equations which include functional differential equations of neutral type as special cases . the adaptation of linear multistep methods , one leg methods , and runge kutta methods is considered . the emphasis is on the linear stability of numerical methods . it is proved that a stable methods can inherit the asymptotic stability of underlying linear systems . some general results of stability on explicit and implicit methods are also given . ( c ) <digit> elsevier science ltd. all rights reserved .
complexity classification of some edge modification problems . <eos> in an edge modification problem one has to change the edge set of a given graph as little as possible so as to satisfy a certain property . we prove the np hardness of a variety of edge modification problems with respect to some well studied classes of graphs . these include perfect , chordal , chain , comparability , split and asteroidal triple free . we show that some of these problems become polynomial when the input graph has bounded degree . we also give a general constant factor approximation algorithm for deletion and editing problems on bounded degree graphs with respect to properties that can be characterized by a finite set of forbidden induced subgraphs .
characterization of logic circuit techniques for high leakage cmos technologies . <eos> channel subthreshold and gate leakage currents are predicted by many to become much more significant in advanced cmos technologies and are expected to have a substantial impact on logic circuit design strategies . to reduce static power , techniques such as the use of monotonic logic and management of various evaluation and idle modes within logic stages may become important options in circuit optimization . in this paper , we present a general , multilevel model for logic blocks consisting of logic gates that include a wide range of options for static power reduction , in both the domains of topology and timing . existing circuit techniques are classified within this framework and experiments are presented showing how aspects of performance might vary across this range in a hypothetical technology . the framework also allows exploration of optimal mixing of techniques .
evaluating existing security and privacy requirements for legal compliance . <eos> governments enact laws and regulations to safeguard the security and privacy of their citizens . in response , requirements engineers must specify compliant system requirements to satisfy applicable legal security and privacy obligations . specifying legally compliant requirements is challenging because legal texts are complex and ambiguous by nature . in this paper , we discuss our evaluation of the requirements for itrust , an open source electronic health records system , for compliance with legal requirements governing security and privacy in the healthcare domain . we begin with an overview of the method we developed , using existing requirements engineering techniques , and then summarize our experiences in applying our method to the itrust system . we illustrate some of the challenges that practitioners face when specifying requirements for a system that must comply with law and close with a discussion of needed future research focusing on security and privacy requirements .
warehouse contextual factors affecting the impact of rfid . <eos> purpose empowered by the possibility to automatically identify unique instances , radio frequency identification ( rfid ) is expected to revolutionize warehouse processes . however , every warehouse differs from each other in several ways . given such dimensionality , a credible assessment of the true value of rfid requires that the contextual factors that differentiate one warehouse from another are taken into account . the same rfid implementation may generate high productivity in one warehouse but not in another , because the former warehouse may have characteristics that may influence the impact of rfid . as a result , the purpose of this paper is to provide a framework for identifying key contextual factors that appear to be contingent on the link between rfid and warehouse performance . design methodology approach the framework derived from a two phase research design . the first phase incorporated two case studies . this was an exploratory study and , therefore , there was a great deal of iteration between the cases studies and the literature . the objective was to identify important contextual factors that may moderate the impact of rfid . the second phase incorporated simulation modelling . this was a confirmatory study . the objective was to develop two simulation models of the cases from the previous phase , and as a result , verify the effects of particular contextual factors on process performance . findings as an outcome of this research , an initial subset of warehouse contextual factors is developed that may moderate the impact of rfid on warehouse performance . the framework is not an evaluating technique , but is a useful starting point for examining the value of rfid in the warehouse context . research limitations implications further work is required to support the significance of the moderating effects of the proposed contextual factors . practical implications for practicing managers the paper directs attention to key warehouse contextual factors that appear to be contingent on the link between rfid and warehouse performance . it also confirms that the achievement of rfid value is attainable only in combination with the redesign of business processes . originality value the paper integrates both theoretical and practical considerations regarding formalization of the contextual factors that may moderate the impact of rfid on warehouse performance . therefore , it represents an initial step in building theory to develop guidelines for understanding the variance in the performance between different rfid enabled warehouse settings .
cop a new corner detector . <eos> most conventional derivative based corner detectors have shortcomings such as missing junctions , poor localization , sensitivity to noise and high computational cost . this paper presents a new , simple and effective low level processing method to detect corners . in this corner detection algorithm , two oriented cross operators called crosses as oriented pair ( cop ) are used , which provide useful information to extract low level features due to its characteristics , preference for edge with different direction and simple direction determination . fast , accurate and noise robust corner detection is accomplished with the cop .
precoloring extension involving pairs of vertices of small distance . <eos> in this paper , we consider coloring of graphs under the assumption that some vertices are already colored . let g g be an r r colorable graph and let p v ( g ) p v ( g ) . albertson ( <digit> ) has proved that if every pair of vertices in p p has distance at least four , then every ( r <digit> ) ( r <digit> ) coloring of g p g p can be extended to an ( r <digit> ) ( r <digit> ) coloring of g g , where g p g p is the subgraph of g g induced by p p . in this paper , we allow p p to have pairs of vertices of distance at most three , and investigate how the number of such pairs affects the number of colors we need to extend the coloring of g p g p . we also study the effect of pairs of vertices of distance at most two , and extend the result by albertson and moore ( <digit> ) .
digital representation of park use and visual analysis of visitor activities . <eos> urban public parks can serve an important function by contributing to urban citizens quality of life . at the same time , they can be the location of processes of displacement and exclusion . despite this ambiguous role , little is known about actual park use patterns . to learn more about park use in three parks in zurich , switzerland , extensive data on visitor activities was collected using a new method based on direct recording via a portable gis solution . then , the data was analyzed using qualitative and quantitative methods . this paper examines whether geographic visualization of these data can help domain experts like landscape designers and park managers to assess park use . to maximize accessibility , the visualizations are made available through a web interface of a common , off the shelf gis . the technical limitations imposed by this choice are critically assessed , before the available visualization techniques are evaluated in respect to the needs and tasks of practitioners with limited knowledge on spatial analysis and gis . key criteria are each techniques level of abstraction and graphical complexity . the utility and suitability of the visualization techniques is characterized for the distinct phases of exploration , analysis and synthesis . the findings suggest that for a target user group of practitioners , a combination of dot maps showing the raw data and surface maps showing derived density values for several attributes serves the purpose of knowledge generation best .
minimum phi divergence estimation in misspecified multinomial models . <eos> the consequences of model misspecification for multinomial data when using minimum phi divergence or minimum disparity estimators to estimate the model parameters are considered . these estimators are shown to converge to a well defined limit . two applications of the results obtained are considered . first , it is proved that the bootstrap consistently estimates the null distribution of certain class of test statistics for model misspecification detection . second , an application to the model selection test problem is studied . both applications are illustrated with numerical examples . ( c ) <digit> elsevier b.v. all rights reserved .
combining quality of services path first routing and admission control to support voip traffic . <eos> existing interactive communication applications , like voip , are designed to manage losses and delay in real ip networks . we present a novel call admission control for voip based on blocking percentage calculation ( bpc ) . blocking rates were measured carrying voip for every router across the sourcedestination path . ingress router selects the optimal path for the new request by calculating blocking rates across all sourcedestination paths . the proposed scheme dynamically adjusts the admission threshold to guarantee the voice quality of each flow . a simple algorithm design and evaluation results show that the proposed scheme is effective to achieve high utilization of network resources , while satisfying end to end targets in terms of blocking rate , delay , packet loss rate and fairness .
dynamic connection establishment and network re optimization in flexible optical networks . <eos> we consider the problem of dynamic connection establishment and spectrum defragmentation in flexible optical networks . when the spectrum is fragmented , blocking a connection establishment , the algorithm reactively re optimizes the network by shifting ( pushing ) in the spectrum domain and or rerouting existing connections . we start by presenting an algorithm based on integer linear programming formulation that searches among all combinations of shiftings and reroutings and selects the one that minimizes the changes in existing connections . we also present a heuristic algorithm that recursively shifts reroutes connections around a void . the solution space of the heuristic can also be very large , so we use a threshold on the recursion depth to reduce the complexity and also provide a trade off between performance and running time . our simulation results show that the blocking probability can be substantially reduced using the proposed techniques as opposed to a network that does not reactively defragments the spectrum . the proposed heuristic achieves near optimal performance , for cases that we were able to find optimal solutions , while the selection of the recursion threshold was shown to provide a good trade off of performance for running time .
improving performance of simple cores by exploiting loop level parallelism through value prediction and reconfiguration . <eos> there is a growing trend towards designing simpler cpu cores that have considerable area , complexity , and power advantages . these cores are then leveraged in large scale multicore processors or in socs for hand held devices . the most significant limitation of such simple cpu cores is their lower performance . in this paper , we propose a technique to improve the performance of simple cores with minimal increase in complexity and area . in particular , we integrate a reconfigurable hardware unit ( rhu ) that exploits loop level parallelism to increase the core 's overall performance . the rhu is reconfigured to execute instructions with highly predictable operand values from the future iterations of loops . our experiments show that the proposed architecture improves the performance by an average of about <digit> % across a wide range of applications , while incurring a area overhead of only about 5.6 % .
shape effects on the activity of synthetic major groove binding ligands . <eos> in this work we present the results of a molecular simulation study of two different tetracationic bis iron ( ii ) supramolecular cylinders interacting with dna . one cylinder has been shown to bind in the major groove of dna and to induce dramatic coiling of the dna the second is a derivative of the first , with additional methyl groups attached so as to give a larger cylinder radius . the simulations show that both cylinders bind strongly to the major groove of the dna , and induce complex structural changes in at rich regions . whereas the parent cylinder tends to bind along the major groove , the derivatised cylinder tends to twist so that only one end remains within the major groove . both gc rich and at rich binding sites for the derivatised cylinder are discussed .
quantitative differences in intervertebral discmatrix composition with age related degeneration . <eos> this study was carried out to determine the effect of age on the intervertebral disc , using a rabbit model . anulus fibrosus and nucleus pulposus tissue from new zealand white rabbits aged <digit> years old ( old rabbits ) and <digit> months old ( young rabbits ) were used . the water content , the proteoglycan , the dna content , and the mrna levels of aggrecan , type i collagen , and type ii collagen were all measured for each sample . water , proteoglycan , dna , and the mrna levels of aggrecan and type ii collagen were all greater in the nucleus pulposus of the young rabbits as compared to the old . for the anulus fibrosus , the difference between young and old is less marked with only proteoglycan and dna being greater in the young disc as compared to the old . clearly , according to our results , it is the nucleus pulposus that suffers the brunt of the changes with age .
generalized variational principles for micromorphic magnetoelectroelastodynamics . <eos> a family of generalized variational principles is established for the initialboundary value problem of micromorphic magnetoelectroelastodynamics by hes semi inverse method . this paper aims at providing a more complete theoretical basis for the finite element applications .
domain independent feature extraction for multi classification using multi objective genetic programming . <eos> we propose three model free feature extraction approaches for solving the multiple class classification problem we use multi objective genetic programming ( mogp ) to derive ( near ) optimal feature extraction stages as a precursor to classification with a simple and fast to train classifier . statistically founded comparisons are made between our three proposed approaches and seven conventional classifiers over seven datasets from the uci machine learning database . we also make comparisons with other reported evolutionary computation techniques . on almost all the benchmark datasets , the mogp approaches give better or identical performance to the best of the conventional methods . of our proposed mogp based algorithms , we conclude that hierarchical feature extraction performs best on multi classification problems .
an image reconstruction algorithm based on the semiparametric model for electrical capacitance tomography . <eos> electrical capacitance tomography ( ect ) is considered as a promising tomography technology , and exactly reconstructing the original objects is highly desirable in real applications . in this paper , a generalized image reconstruction model that simultaneously considers the inaccurate property in the measured capacitance data and the linearization approximation error is presented . a generalized objective function , which has been developed using a combinational m estimation and an extended stabilizing item , is proposed . the objective function unifies six estimation methods into a concise formula , where different estimation methods can be easily obtained by selecting different parameters . the homotopy method that integrates the beneficial advantages of the alternant iteration scheme is employed to solve the proposed objective function . numerical simulations are implemented to evaluate the numerical performances and effectiveness of the proposed algorithm , and the numerical results reveal that the proposed algorithm is efficient and overcomes the numerical instability in the process of ect image reconstruction . for the reconstructed objects in this paper , a dramatic improvement in accuracy and spatial resolution can be achieved , which indicates that the proposed algorithm is a promising candidate for solving ect inverse problems .
reliability studies of mocvd tisin and encore ta ( n ) ta . <eos> passivated single damascene copper sio2 damascene lines were evaluated in combination with tisin and ta ( n ) ta diffusion barriers . leakage current , breakdown and time dependent dielectric breakdown properties were investigated on a wafer level basis for temperatures ranging between room temperature and 150c . it is found that the leakage performance of the wafers with a tisin barrier is better at room temperature , but at 150c the performance levels out with ta ( n ) ta . time dependent dielectric breakdown measurements at 150c show that the lifetime of the interconnect is higher with the selected ta ( n ) ta barrier than for tisin .
load balanced agent activation for value added network services . <eos> in relation to its growth in size and user population , the internet faces new challenges that have triggered the proposals of value added network services , e.g. , ip multicast , ip traceback , diffserv , intserv , etc. in addition , recent advances in processor and hardware techniques have enabled the production of high speed and powerful routers . therefore , it is not unreasonable to expect the internet to provide a variety of value added network services other than packet forwarding in the near future . depending on their purposes , value added services may improve the scalability and efficiency of end user applications or may enhance the reliability and security of the network infrastructure . on the other hand , they may incur non trivial overhead on the routers providing these services . it is a thorny problem to reach a balance between the performance of value added services and the incurred overhead . in this paper , we study this problem in the context of both reliable multicast and distributed denial of service ( ddos ) defense . in either scenario , a software agent is activated at some routers in a tree topology to provide the required functionality . we formulate the problem as load balanced agent activation problem ( lbaa p ) . our goal is to develop a mechanism to activate value added service agents in the network for the purpose of reaching a balance between the performance and overhead . we develop a polynomial time algorithm to solve the lbaap problem in single tree case , and propose a heuristic for the lbaap problem in the case where multiple trees exist in the network , a problem we conjecture is np hard . finally we evaluate the performances of various approaches for activating value added service agents through simulation . published by elsevier b.v.
a family of pass oriented attribute grammar evaluators . <eos> a family of pass oriented attribute grammar evaluators is presented and some of its members explored . this family includes some evaluators that have appeared in the literature and allows the simplification of one of these . two of the new members presented are of particular interest . the first incorporates explicit ordering of attributes belonging to the same node of the parse tree and the second is well suited to coupling with a bottom up parser . the family provides a framework for description and comparison of its members and has interesting applications in compiler writing systems .
an enterprise ontology based approach to service specification . <eos> in recent years , the web service definition language ( wsdl ) and universal description discovery integration ( uddi ) standards arose as ad hoc standards for the definition of service interfaces and service registries . however , even together these standards do not provide enough basis for a service consumer to get a full understanding of the behavior of a service . in practice , this often leads to a serious mismatch between the provider 's intent and the consumer 's expectations concerning the functionality of the corresponding service . though additional standards have been proposed , a holistic view of what aspects of a service need to be specified is still lacking . this paper proposes a service definition , a service classification , and service specification framework , all based on a founded theory , the psi theory . the psi theory originates from the scientific fields of language philosophy and systemic ontology . according to this theory , the operation of organizations is all about communication between and production by social actors . the service specification framework can be applied both for specifying human services , i.e. , services executed by human beings , and it services ( i.e. , services executed by it systems ) .
crechaindo an iterative and interactive web information retrieval system based on lattices . <eos> this paper presents an iterative and interactive information retrieval ( ir ) system for web search using formal concept analysis ( fca ) . fca provides a natural way to organise objects according to their properties and it has been used in recent work to organise search engine results . the navigation over the lattice helps the user to explore a structured and synthetic result . such a lattice contains concepts that are relevant and others that are not relevant regarding a given ir task . in this way , lattices are introduced in an interactive and iterative system . the user expresses his negative or positive agreement with some concept of the lattice in respect of his objective of ir . these user choices are converted into operations over the lattice . the lattice is dynamically updated for a better fit to the request .
learning computer science concepts with scratch . <eos> scratch is a visual programming environment that is widely used by young people . we investigated if scratch can be used to teach concepts of computer science . we developed new learning materials for middle school students that were designed according to the constructionist philosophy of scratch and evaluated them in two schools . the classes were normal classes , not extracurricular activities whose participants are self selected . questionnaires and a test were constructed based upon a novel combination of the revised bloom taxonomy and the solo taxonomy . these quantitative instruments were augmented with a qualitative analysis of observations within the classes . the results showed that in general students could successfully learn important concepts of computer science , although there were some problems with initialization , variables and concurrency these problems can be overcome by modifications to the teaching process .
recognition of in hand manipulation using contact state transition for multifingered robot hand control . <eos> this paper proposes a method for recognizing in hand manipulation of the operator by observing a contact state transition between an object and the human hand . an instrumented object with a pressure distribution sensor and a position orientation sensor has been developed . by processing information from the sensors , the contact regions on the operators palm surface are detected . a contact state transition diagram is created by taking practical contact states into account . a recognition algorithm based on dynamic programming ( dp ) is proposed to recognize the type of in hand manipulation by comparing the similarity of the contact state transition between an input sequence and template manipulation primitives . the validity of the proposed method is confirmed by experiments .
on the link between inventory and responsiveness in multi product supply chains . <eos> manufacturing systems in many industries face the challenge of manufacturing products that are assembled from multi variant components . demand for these component variants is correlated , and often subjected to an overall capacity constraint ( e.g. fixed production volumes in the final assembly plant ) . therefore , the modelling of supply chain systems under multi variant product conditions is conceptually difficult as the demands for the individual variants are not independent . previous approaches to modelling supply chain systems have commonly dealt with this complication by either focusing on single product multi tier systems , or single tier systems with multiple products . in this article , we propose a modelling approach capable of studying the responsiveness of multi tier systems with correlated demands , which is used as the input for a simulation model . based on the simulation outcomes , generic relationships between product variety , responsiveness and inventory levels in a supply chain system are derived and a novel safety stock formula for such settings is developed .
on the suppression of variables in boolean equations . <eos> the resultant of suppression of variables from a boolean equation is a boolean equation , derived from the parent equation , whose solutions are exactly those of the parent equation that do not involve the suppressed variables . two examples in the literature are discussed , in which it is necessary to solve a boolean equation while excluding solutions involving certain variables . in such cases it would be advantageous to solve the resultant of suppression of those variables rather than solving the original equation and filtering the desired solutions from the results .
indus an object oriented language for ubiquitous computing . <eos> indus is a new object oriented programming language for ubiquitous computing . the indus programming language enables implementation of software agents that have the ability to cooperatively execute tasks by coordinating with other agents and composing components using connectors . the indus programming model tightly integrates a set of run time libraries that automates the management of distributed agents and components across a variety of platforms and networks . indus also differentiates itself from other general purpose , object oriented programming languages by being able to generate code on a variety of <digit> <digit> <digit> <digit> bit platforms , thus making it ideal as a language for implementing pervasive applications .
weighted triangular approximation of fuzzy numbers . <eos> in this paper , we use the weighted distance between fuzzy numbers to investigate the approximation of arbitrary fuzzy numbers by weighted distance . we then discuss properties of the approximation strategy including continuity , translation invariance , scale invariance and identity and give an application to the generation of fuzzy partitions . ( c ) <digit> elsevier inc. all rights reserved .
a <digit> m <digit> m <digit> w class ab cmos differential flipped voltage follower with output driving capability up to 100pf . <eos> a compact differential flipped voltage follower ( dfvf ) with low power consumption , capable to deliver currents several orders of magnitude larger than its quiescent current and with large capacitive loads is presented . in the proposed circuit , a current comparator activates an auxiliary transistor whenever is required to hand over additional current and reach class ab operation . furthermore , miller compensation is performed , by taking advantage of the large impedance node of the comparator it is possible to reduce forty times the compensation capacitor compared to other topologies under the same conditions . the proposed architecture is validated by post layout simulations using the parameters of an on semi , double poly , three metal layers , 0.5 m cmos technology and the pelgrom 's mismatch model . a winner takes all circuit , a median filter and a current conveyor are presented as examples of application of the proposed topology .
intelligent fingerprint quality analysis using online sequential extreme learning machine . <eos> because the quality of fingerprints can be degraded by diverse factors , recognizing the quality of fingerprints in advance can be beneficial for improving the performance of fingerprint authentication systems . this paper proposes an effective fingerprint quality analysis approach based on the online sequential extreme learning machine ( os elm ) . the proposed method is based not only on basic fingerprint properties , but also on the physical properties of the various sensors . instead of splitting a fingerprint image into traditional small blocks , direction based segmentation using the gabor filter is used . from the segmented image , a feature set which consists of four selected independent local or global features orientation certainty , local orientation quality , consistency , and ridge distance , is extracted . the selected feature set is robust against various factors responsible for quality degradation and can satisfy the requirements of different types of capture sensors . with the contribution of the os elm classifier , the extracted feature set is used to determine whether or not a fingerprint image should be accepted as an input to the recognition system . experimental results show that the proposed method performs better in terms of accuracy and time consumed than bpnn based and svm based methods . an obvious improvement to the fingerprint recognition system is achieved by adding a quality analysis system . other comparisons to traditional methods also show that the proposed method outperforms others .
cartoon rendering of smoke animations . <eos> we describe a technique for generating cartoon style animations of smoke . our method takes the output of a physically based simulator and uses it to drive particles that are rendered using a variant of the depth differences technique ( originally used for rendering trees ) . specific issues we address include the placement and evolution of primitives in the flow and the maintenance of temporal coherence . the results are visually simple , flicker free animations that convey the turbulent , dynamic nature of the gas with simple outlines .
evaluating the accuracy of java profilers . <eos> performance analysts profile their programs to find methods that are worth optimizing the hot methods . this paper shows that four commonly used java profilers ( xprof , hprof , jprofile , and yourkit ) often disagree on the identity of the hot methods . if two profilers disagree , at least one must be incorrect . thus , there is a good chance that a profiler will mislead a performance analyst into wasting time optimizing a cold method with little or no performance improvement . this paper uses causality analysis to evaluate profilers and to gain insight into the source of their incorrectness . it shows that these profilers all violate a fundamental requirement for sampling based profilers to be correct , a sampling based profiler must collect samples randomly . we show that a proof of concept profiler , which collects samples randomly , does not suffer from the above problems . specifically , we show , using a number of case studies , that our profiler correctly identifies methods that are important to optimize in some cases other profilers report that these methods are cold and thus not worth optimizing .
q2 q <digit> index quantitative and qualitative evaluation based on the number and impact of papers in the hirsch core . <eos> bibliometric studies at the micro level are increasingly requested by science managers and policy makers to support research decisions . different measures and indices have been developed at this level of analysis . one type of indices , such as the h index and g index , describe the most productive core of the output of a researcher and inform about the number of papers in the core . other indices , such as the a index and m index , depict the impact of the papers in the core . in this paper , we present a new index which relates two different dimensions in a researchers productive core a quantitative one ( number of papers ) and a qualitative one ( impact of papers ) . in such a way , we could obtain a more balanced and global view of the scientific production of researchers . this new index , called q2 q <digit> index , is based on the geometric mean of h index and the median number of citations received by papers in the h core , i.e. , the m index , which allows us to combine the advantages of both kind of indices .
an optimistic and conservative register assignment heuristic for chordal graphs . <eos> this paper presents a new register assignment heuristic for procedures in ssa form , whose interference graphs are chordal the heuristic is called optimistic chordal coloring ( occ ) . previous register assignment heuristics eliminate copy instructions via coalescing , in other words , merging nodes in the interference graph . node merging , however , can not preserve the chordal graph property , making it unappealing for ssa based register allocation . occ is based on graph coloring , but does not employ coalescing , and , consequently , preserves graph chordality , and does not increase its chromatic number in this sense , occ is conservative as well as optimistic . occ is observed to eliminate at least as many dynamically executed copy instructions as iterated register coalescing ( irc ) for a set of chordal interference graphs generated from several mediabench and mibench applications . in many cases , occ and irc were able to find optimal or near optimal solutions for these graphs . occ ran 1.89 x faster than irc , on average .
the birth of prolog . <eos> the programming language , prolog , was born of a project aimed not at producing a programming language but at processing natural languages in this case , french . the project gave rise to a preliminary version of prolog at the end of <digit> and a more definitive version at the end of <digit> . this article gives the history of this project and describes in detail the preliminary and then the final versions of prolog . the authors also felt it appropriate to describe the q systems since it was a language which played a prominent part in prolog 's genesis .
a resilient architecture for dht based distributed collaborative environments . <eos> distributed hash tables ( dhts ) provide a flexible and reliable infrastructure for data storage and retrieval in peer to peer communities . we propose to apply kademlia dht to organize data management and cooperation between users participating in different work groups . particularly , in this paper we propose a mechanism for increasing the resilience and the overall performance of a kademlia based distributed work sharing system , taking into account frequent joins and leaves of network nodes . to achieve this goal we propose a new flexible scheme for resource management that provides more resilience and fault tolerance than other mechanisms used by existent cooperative storage systems with a collaborative nature . in this work we try to extend and generalize our solution to fit several application contexts of collaborative computing , thereby addressing some common problems about resilience of existent distributed collaborative systems .
perceptual subspace speech enhancement using variance of the reconstruction error . <eos> in this paper , a new signal subspace based approach for enhancing a speech signal degraded by environmental noise is presented . the perceptual karhunenlove transform ( pklt ) method is improved here by including the variance of the reconstruction error ( vre ) criterion , in order to optimize the subspace decomposition model . the incorporation of the vre in the pklt ( namely the pklt vre hybrid method ) yields a good tradeoff between the noise reduction and the speech distortion thanks to the combination of a perceptual criterion and the optimal determination of the noisy subspace dimension . in adverse conditions , the experimental tests , using objective quality measures , show that the proposed method provides a higher noise reduction and a lower signal distortion than the existing speech enhancement techniques .
dynamics of entanglement in heisenberg chains with asymmetric dzyaloshinskii moriya interactions . <eos> dynamics of the nearest neighbor concurrence and the block block entanglement in closed heisenberg chains with dzyaloshinskii moriya ( dm ) interactions are simulated numerically . it is found that the nearest neighbor concurrence and the block block entanglement can be generated dynamically from an initial antiferromagnetic ( af ) state , and dm interactions will enhance the amplitude of oscillation of concurrence and that of block block entanglement . furthermore , dynamics of block block entanglement s l in the chain with even number spins is coincident with the dynamics of sn l for d <digit> . however , with the consideration of dm interactions , the evolution of s l will not be consistent with that of sn l for the chain with odd number spins . in a given time interval , the maximal value of the generated nearest neighbor concurrence c i , i <digit> ( max ) will first decrease with the increase of chain 's length , then oscillate with the increase of n. the maximal value and the minimal value of the oscillation becomes stable for a large n , and c i , i <digit> ( max ) of the chain with odd spins is larger than that of the chain with even spins for a large n. the influence of dm interaction on the value of c i , i <digit> ( max ) becomes unobvious with the increase of chain 's length .
knowledge discovery using genetic algorithm for maritime situational awareness . <eos> a novel leverage on multiple data sources for maritime security is demonstrated . a new knowledge discovery method based on genetic algorithm is proposed . a prototype maritime security system was tested and produced promising results . the proposed method can be applied to develop systems in other applications .
the use of animation to explain genetic algorithms . <eos> algorithm animation has been applied to a huge variety of algorithms , and has been found to be of enormous benefit in assisting with their comprehension . however , the radical nature of several classes of algorithm has caused them to be largely overlooked in this regard noticeable among these are genetic algorithms . in addressing this , we have used the xtango software system to develop a set of animation sequences designed to illustrate the behaviour of a genetic algorithm applied to a real world problem . the resulting package is general enough to be adapted easily to a range of problems .
gpu chariot a programming framework for stream applications running on multi gpu systems . <eos> this paper presents a stream programming framework , named gpu chariot , for accelerating stream applications running on graphics processing units ( gpus ) . the main contribution of our framework is that it realizes efficient software pipelines on multi gpu systems by enabling out of order execution of cpu functions , kernels , and data transfers . to achieve this out of order execution , we apply a runtime scheduler that not only maximizes the utilization of system resources but also encapsulates the number of gpus available in the system . in addition , we implement a load balancing capability to flow data efficiently through multiple gpus . furthermore , a callback interface enables overlapping execution of functions in third party libraries . by using kernels with different performance bottlenecks , we show that our out of order execution is up to <digit> % faster than in order execution . finally , we conduct several case studies on a <digit> gpu system and demonstrate the advantages of gpu chariot over a manually pipelined code . we conclude that gpu chariot can be useful when developing stream applications with software pipelines on multiple gpus and cpus .
more network conscious than ever challenges , strategies , and analytic labor of users in the facebook environment . <eos> as is widely observed , social network sites ( sns ) constitute a new environment of interaction where users encounter various challenges that they usually do not encounter in other environments . this study aims to provide an in depth understanding of how users deal with the challenges in this unique environment , paying particular attention to the ways in which they examine and reflect on their social ties and networks . on the basis of <digit> semistructured interviews with facebook users , the article presents the hypothesis that participants of sns develop a tendency to become highly observant and inquisitive about their networks and are frequently involved in an activity that the authors call analytic labor .
automatic extraction of paralinguistic information using prosodic features related to f0 , duration and voice quality . <eos> the use of acousticprosodic features related to f0 , duration and voice quality is proposed and evaluated for automatic extraction of paralinguistic information ( intentions , attitudes , and emotions ) in dialogue speech . perceptual experiments and acoustic analyses were conducted for monosyllabic interjections spoken in several speaking styles , conveying a variety of paralinguistic information . experimental results indicated that the classical prosodic features , i.e. , f0 and duration , were effective for discriminating groups of paralinguistic information expressing intentions , such as affirm , deny , filler , and ask for repetition , and accounted for <digit> % of the global detection rate , in a task of discriminating seven groups of paralinguistic information . on the other hand , voice quality features were effective for identifying part of the paralinguistic information expressing emotions or attitudes , such as surprised , disgusted and admired , leading to a <digit> % improvement in the global detection rate .
steady state analysis of timed event graphs with time window constraints . <eos> for discrete event systems , time window constraints between two events are often required to control their behavior . such time constrained discrete event systems are found in various industrial systems such as chemical vapor deposition processes for wafer fabrication , electroplating lines for printed circuit board fabrication , microcircuit design etc. a negative event graph ( neg ) which is an extension of the timed event graph ( teg ) has been proposed by lee and park ( <digit> ) to model and analyze discrete event systems with time window constraints . an neg can model time window constraints between any two transitions by introducing negative places and negative tokens . this study examines the steady state behavior of an neg that satisfies the time window constraints . we develop a recurrent equation for the feasible steady firing epochs based on the minimax algebra . in addition , we identify four classes of steady states that correspond to the earliest and latest feasible steady firing schedules for each of the minimum and maximum cycle times by extending the steady state results of a conventional teg based on the minimax algebra . besides , we characterize how the cycle times and the steady schedules are computed through the matrix algebra and the associated graph algorithms .
part <digit> brain science , information science and associative memory model . <eos> i review recent progress on the associative memory model , which is a kind of neural network model . first , i introduce this model and a mathematical theory called statistical neurodynamics describing its properties . next , i discuss an associative memory model with hierarchically correlated memory patterns . initially , in this model , the state approaches a mixed state that is a superposition of memory patterns . after that , it diverges from the mixed state , and finally converges to a memory pattern . i show that this retrieval dynamics can qualitatively replicate the temporal dynamics of face responsive neurons in the inferior temporal cortex , which is considered to be the final stage of visual perception in the brain . finally , i show an unexpected link between associative memory and mobile phones ( cdma ) . the mathematical structure of the cdma multi user detection problem resembles that of the associative memory model . it enables us to apply a theoretical framework of the associative memory model to cdma .
superior relation between cardiac parameters and blood pressure . <eos> purpose the purpose of this paper is to study the superior relationship among the parameters of age , casual blood pressure and <digit> hour ambulatory blood pressure with left atrial and ventricular structure and left ventricular filling function in systolic hypertensives ( sh ) . design methodology approach a total of <digit> male and seven female patients aged from <digit> <digit> ( averaged at <digit> <digit> ) years with stage i similar to ii systolic hypertension were studied . the criterion of the subjects was sbp > <digit> mmhg and dbp <digit> x ( dbp <digit> mmhg ) and dbp > <digit> mmhg . the parameters acted as independent variables were age , casual systolic and diastolic blood pressure ( csbp and cdbp ) and <digit> hour ambulatory systolic and diastolic blood pressure ( asbp24 and adbp24 ) . the parameters acted as dependent variables were ratio of the early rapid filling peak flow velocity and late atrial contractive filling peak flow velocity below the mitral valve on the pulsed wave spectral doppler echocardiography ( lad aod ) . the collected data were analyzed via the superior grey relational analyses . findings the results showed that the superior grey relational order in independent variables was asbp24 > adbp24 > csbp > cdbp > age and the superior grey relational order in dependent variables was lad aod > mve a > lvmi . originality value among the independent variables asbp24 is the most important factor of affecting the cardiac structure and function in sh . the first cardiac variation in sh is the left atrial structure affected by the independent variables . after that left ventricular diastolic dysfunction and the left ventricular hypertrophic changes take place in succession .
validation of the mr simulation approach for evaluating the effects of immersion on visual analysis of volume data . <eos> in our research agenda to study the effects of immersion ( level of fidelity ) on various tasks in virtual reality ( vr ) systems , we have found that the most generalizable findings come not from direct comparisons of different technologies , but from controlled simulations of those technologies . we call this the mixed reality ( mr ) simulation approach . however , the validity of mr simulation , especially when different simulator platforms are used , can be questioned . in this paper , we report the results of an experiment examining the effects of field of regard ( for ) and head tracking on the analysis of volume visualized micro ct datasets , and compare them with those from a previous study . the original study used a cave like display as the mr simulator platform , while the present study used a high end head mounted display ( hmd ) . out of the <digit> combinations of system characteristics and tasks tested on the two platforms , we found that the results produced by the two different mr simulators were similar in <digit> cases . however , only one of the significant effects found in the original experiment for quantitative tasks was reproduced in the present study . our observations provide evidence both for and against the validity of mr simulation , and give insight into the differences caused by different mr simulator platforms . the present experiment also examined new conditions not present in the original study , and produced new significant results , which confirm and extend previous existing knowledge on the effects of for and head tracking . we provide design guidelines for choosing display systems that can improve the effectiveness of volume visualization applications .
quality of service provision assessment in the healthcare information and telecommunications infrastructures . <eos> the continuous increase in the complexity and the heterogeneity of corporate and healthcare telecommunications infrastructures will require new assessment methods of quality of service ( qos ) provision that are capable of addressing all engineering and social issues with much faster speeds . speed and accessibility to any information at any time from anywhere will create global communications infrastructures with great performance bottlenecks that may put in danger human lives , power supplies , national economy and security . regardless of the technology supporting the information flows , the final verdict on the qos is made by the end user . the users perception of telecommunications network infrastructure qos provision is critical to the successful business management operation of any organization . as a result , it is essential to assess the qos provision in the light of user 's perception . this article presents a cost effective methodology to assess the user 's perception of quality of service provision utilizing the existing staffordshire university network ( sun ) by adding a component of measurement to the existing model presented by walker . this paper presents the real examples of cisco networking solutions for health care givers and offers a cost effective approach to assess the qos provision within the campus network , which could be easily adapted to any health care organization or campus network in the world .
empirical evaluation of consistency and accuracy of methods to detect differentially expressed genes based on microarray data . <eos> background in this study , we empirically evaluated the consistency and accuracy of five different methods to detect differentially expressed genes ( degs ) based on microarray data . methods five different methods were compared , including the t test , significance analysis of microarrays ( sam ) , the empirical bayes t test ( ebayes ) , t tests relative to a threshold ( treat ) , and assumption adequacy averaging ( aaa ) . the percentage of overlapping genes ( pog ) and the percentage of overlapping genes related ( pogr ) scores were used to rank the different methods on their ability to maintain a consistent list of degs both within the same data set and across two different data sets concerning the same disease . the power of each method was evaluated based on a simulation approach which mimics the multivariate distribution of the original microarray data . results for smaller sample sizes ( <digit> or less per group ) , moderated versions of the t test ( sam , ebayes , and treat ) were superior in terms of both power and consistency relative to the t test and aaa , with treat having the highest consistency in each scenario . differences in consistency were most pronounced for comparisons between two different data sets for the same disease . for larger sample sizes aaa had the highest power for detecting small effect sizes , while treat had the lowest . discussion for smaller sample sizes moderated versions of the t test can generally be recommended , while for larger sample sizes selection of a method to detect degs may involve a compromise between consistency and power .
a fast simulation framework for ieee 802.11 operated wireless lans . <eos> in this paper , we develop a fast simulation framework for ieee 802.11 operated wireless lans ( wlans ) , in which a large number of packets are abstracted as a single fluid chunk , and their behaviors are approximated with analytic fluid models and figured into the simulation . we first derive the analytical model that characterizes data transmission activities in ieee 802.11 operated wlans with without the rts cts mechanism . all the control overhead incurred in the physical and mac layers , as well as system parameters specified in ieee 802.11 <digit> are faithfully figured in . we validate the model with simulation in cases in which the network is and is not saturated . we then implement , with the use of the time stepping technique <digit> , the fast simulation framework for wlans in ns <digit> <digit> , and conduct a comprehensive simulation study to evaluate the framework in terms of speed up and errors incurred under a variety of network configurations.the simulation results indicate that the proposed framework is indeed effective in simulating ieee 802.11 operated wlans . it achieves as much as two orders of magnitude improvement in terms of execution time as compared to packet level simulation . the performance improvement is more pronounced when the number of wireless nodes , the number of applications running on each wireless node , or the number of wlans increases . the relative error , on the other hand , falls within <digit> % in all cases , as long as the value of the time step is appropriately determined .
a tool for enforcing system structure . <eos> reliability considerations had little influence in the design of the first generation of computers . notable exceptions being , on the hardware side , some circuitry incorporated into the univac i for checking purposes , and on the software side , the frequency counts employed by von neumann and goldstine for measurement purposes . the second generation of computers witnessed a wider interest in hardware reliability while essentially ignoring the software reliability problem . current computer systems ( e.g. , ibm <digit> ) incorporate elaborate hardware reliability considerations in their design while continuing to ignore reliable software design .
investigating suitability for server virtualization using business application benchmarks . <eos> server virtualization is now required for data center systems to reduce the number of servers . however , it is still unclear which business applications are suitable for virtualization . we present our evaluation results for four types of business application benchmarks on our virtualization system . the results show that the virtualization performance of a tpc h workload , which mainly executes referencing on a database , uses about <digit> % of the non virtualized performance , and that the virtualization performance of the tpc h workload is better than that of the other benchmark applications . the results of a new performance characteristic for virtualization indicated that application programs , which have performance bottlenecks in disk i os and low cpu utilizations in a non virtualized environment , are suitable for virtualization .
polynomial neural network for linear and non linear model selection in quantitative structure activity relationship studies on the internet . <eos> this article presents a self organising multilayered iterative algorithm that provides linear and non linear polynomial regression models thus allowing the user to control the number and the power of the terms in the models . the accuracy of the algorithm is compared to the partial least squares ( pls ) algorithm using fourteen data sets in quantitative structure activity relationship studies . the calculated data show that the proposed method is able to select simple models characterized by a high prediction ability and thus provides a considerable interest in quantitative structure activity relationship studies . the software is developed using client server protocol ( java and c languages ) and is available for world wide users on the web site of the authors .
large histological serial sections for computational tissue volume reconstruction . <eos> objectives . a proof of principle study was conducted for microscopic tissue volume reconstructions using a new image processing chain operating on alternately stained large histological serial sections . methods digital histological images were obtained from conventional brightfield transmitted light microscopy . a powerful nonparametric nonlinear optical flowbased registration approach was used . in order to apply a simple but computationally feasible sum of squared differences similarity measure even in case of differing histological stainings , a new consistent tissue segmentation procedure was placed upstream . results two reconstructions from uterine cervix carcinoma specimen were accomplished , one alternately stained with p16 ( ink4a ) , ( surrogate tumor marker ) and h e ( routine reference ) , and another with three different alternate stainings , h e , p16 ( inka ) , and cd3 ( a i lymphocyte marker ) . for both cases , due to our segmentation based reference free nonlinear registration procedure , resulting tissue reconstructions exhibit utmost smooth image to image transitions without impairing warpings . conclusions . our combination of modern nonparametric nonlinear registration and consistent tissue segmentation has turned out to provide a superior tissue reconstruction quality .
computation and data transfer co scheduling for interconnection bus minimization . <eos> high instruction level parallelism in dsp and media applications demands highly clustered architecture . it is challenge to design an efficient , flexible yet cost saving interconnection network to satisfy the rapid increasing inter cluster data transfer needs . this paper presents a computation and data transfer co scheduling technique to minimize the number of partially connected interconnection buses required for a given embedded application while minimizing its schedule length . previous researches in this area focused on scheduling computations to minimize the number of inter cluster data transfers . the proposed co scheduling technique in this paper not only schedules computations to reduce the number of inter cluster data transfers , but also schedules inter cluster data transfers to minimize the number of required partially connected buses for inter cluster connection network . experimental results indicate that 39.4 % fewer buses required compared to current best known technique while achieving the same schedule length minimization .
forest species recognition based on dynamic classifier selection and dissimilarity feature vector representation . <eos> multiple classifiers on the dissimilarity space are proposed to address the problem of forest species recognition from microscopic images . to that end , classical texture based features such as gabor filters , local binary patterns ( lbp ) and local phase quantization ( lpq ) , as well as two keypoint based features , the scale invariant feature transform ( sift ) and the speeded up robust features ( surf ) , are used to generate a pool of diverse classifiers on the dissimilarity space . a comprehensive set of experiments on a database composed of 2,240 microscopic images from <digit> different forest species was used to evaluate the performance of each individual classifier of the generated pool , the combination of all classifiers , and different dynamic selection of classifiers ( dsc ) methods . the best result ( 93.03 % ) was observed by incorporating probabilistic information in a dsc method based on multiple classifier behavior .
an opportunistic platform for android based mobile devices . <eos> this paper describes a novel android based opportunistic platform for mobile computing applications . it has the aim to incentive the growth of practical experiences that should give an answer to the following question can opportunistic networks actually compete with cellular networks to support urban wide mobile computing applications
designing an algorithm for evaluating decision making units based on neural weighted function . <eos> fuzzy systems have gained more and more attention from researchers and practitioners of various fields . in such systems , the output represented by a fuzzy set sometimes needs to be transformed into a scalar value , and this task is known as the defuzzification process . several analytic methods have been proposed for this problem , but in this paper , firstly the researchers introduce a novel parametric distance between fuzzy numbers and secondly suggest a new approach to the problem of defuzzification , using this distance . this defuzzification can be used as a crisp approximation with respect to fuzzy quantity . by considering this and with benchmark between fuzzy numbers , we can present a method for evaluating . the method can effectively evaluate various fuzzy numbers and their images and overcome the shortcomings of the previous techniques .
cross family comparative proteomic study and molecular phylogeny of map kinases in plants . <eos> mitogen activated protein kinases are serine threonine specific protein kinases and they are closely related to cyclin dependent kinases . they constitute functionally significant family of proteins that is involved in various cellular functions like response to mitogens , osmotic stress , heat shock and proinflammatory cytokines as well as known to play key role in proliferation , gene expression , differentiation , mitosis , cell survival , and apoptosis . map kinases are characteristically found in eukaryotes only , though they are fairly diverse and encountered in all animals , fungi and plants , and even in an array of unicellular eukaryotes . in this study <digit> map kinase sequences from various plant species were selected in order to compare their conserved regions , amino acid composition , evolutionary orders and other statistical parameters .
soundness and completeness warnings in esc java2 . <eos> usability is a key concern in the development of verification tools . in this paper , we present an usability extension for the verification tool esc java2 . this enhancement is not achieved through extensions to the underlying logic or calculi of esc java2 , but instead we focus on its human interface facets . user awareness of the soundness and completeness of the tool is vitally important in the verification process , and lack of information about such is one of the most requested features from esc java2 users , and a primary complaint from esc java2 critics . areas of unsoundness and incompleteness of esc java2 exist at three levels the level of the underlying logic the level of translation of program constructs into verification conditions and at the level of the theorem prover . the user must be made aware of these issues for each particular part of the source code analysed in order to have confidence in the verification process . our extension to esc java2 provides clear warnings to the user when unsound or incomplete reasoning may be taking place .
a robust objective function for topology optimization . <eos> purpose robust design is very important for manufacturers to ensure the quality of the finished product . therefore , a robustness measure is needed for the topological design of electromagnetic problems which may be sensitive to parameter variations . the purpose of this paper is to propose a robust objective function for topological design problems . design methodology approach in this paper , a robust objective function for topology optimization is defined on an uncertainty set using the worst case analysis . the robustness of a topological design is defined as the worst response due to the variations of the location of the topology change . the approach is based on the definition of a topological gradient . findings the robust topology optimization ( rto ) was applied to eddy current crack reconstruction problems . the numerical applications showed that this method can provide more reliable results for the reconstruction in the presence of significant noise in the measured signal . research limitations implications the rto may be applied to some more complicated design problems however large computational costs may result . originality value this paper has defined a robustness metric for topology design and a robust design model is proposed for topology optimization problems .
bernstein bezoutian matrices and curve implicitization . <eos> a new application of bemstein bezoutian matrices , a type of resultant matrices constructed when the polynomials are given in the bernstein basis , is presented . in particular , the approach to curve implicitization through sylvester and bezout resultant matrices and bivariate interpolation in the usual power basis is extended to the case in which the polynomials appearing in the rational parametric equations of the curve are expressed in the bernstein basis , avoiding the basis conversion from the bemstein to the power basis . the coefficients of the implicit equation are computed in the bivariate tensor product bernstein basis , and their computation involves the bidiagonal factorization of the inverses of certain totally positive matrices . ( c ) <digit> elsevier b.v. all rights reserved .
noc based fpga acceleration for monte carlo simulations with applications to spect imaging . <eos> as the number of transistors that are integrated onto a silicon die continues to increase , the compute power is becoming a commodity . this has enabled a whole host of new applications that rely on high throughput computations . recently , the need for faster and cost effective applications in form factor constrained environments has driven an interest in on chip acceleration of algorithms based on monte carlo simulations . though field programmable gate arrays ( fpgas ) , with hundreds of on chip arithmetic units , show significant promise for accelerating these embarrassingly parallel simulations , a challenge exists in sharing access to simulation data among many concurrent experiments . this paper presents a compute architecture for accelerating monte carlo simulations based on the network on chip ( noc ) paradigm for on chip communication . we demonstrate through the complete implementation of a monte carlo based image reconstruction algorithm for single photon emission computed tomography ( spect ) imaging that this complex problem can be accelerated by two orders of magnitude on even a modestly sized fpga over a <digit> ghz intel core <digit> duo processor . the architecture and the methodology that we present in this paper is modular and hence it is scalable to problem instances of different sizes , with application to other domains that rely on monte carlo simulations .
statins for the treatment of antiphospholipid syndrome . <eos> fluvastatin has been shown to revert proinflammatory prothrombotic effects of antiphospholipid antibodies ( apl ) in vitro and in mice . here , we examined whether fluvastatin affects the levels of proinflammatory prothrombotic markers in antiphospholipid syndrome ( aps ) patients . vascular endothelial growth factor ( vegf ) , soluble tissue factor ( stf ) , tumor necrosis factor ( tnf ) , soluble intercellular adhesion molecule <digit> ( sicam <digit> ) , se selectin ( e sel ) , c reactive protein ( crp ) , and soluble vascular cell adhesion molecule ( svcam <digit> ) , were measured in the sera of <digit> aps patients and <digit> controls and in the sera of nine patients with aps before and after <digit> days of treatment with fluvastatin . elevated levels of vegf , stf , and tnf were found in aps patients . fluvastatin significantly reduced those markers in the majority of treated subjects . the data from this study show that statins may be beneficial in apl positive patients and warrant larger clinical trials to confirm the efficacy of the drug for the treatment of aps clinical manifestations .
towards the principled design of software engineering diagrams . <eos> diagrammatic specification , modelling and programming languages are increasingly prevalent in software engineering and , it is often claimed , provide natural representations which permit of intuitive reasoning . a desirable goal of software engineering is the rigorous justification of such reasoning , yet many formal accounts of diagrammatic languages confuse or destroy any natural reading of the diagrams . hence they can not be said to be intuitive . the answer , we feel , is to examine seriously the meaning and accuracy of the terms natural and intuitive in this context . this paper highlights , and illustrates by means of examples taken from industrial practice , an ongoing research theme of the authors . we take a deeper and more cognitively informed consideration of diagrams which leads us to a more natural formal underpinning that permits ( i ) the formal justification of informal intuitive arguments , without placing the onus of formality upon the engineer constructing the argument and ( ii ) a principled approach to the identification of intuitive ( and counter intuitive ) features of diagrammatic languages .
an efficient design pattern algorithm for the environmental and hydrologic hydraulic ubiquitous model developments . <eos> in this paper , we propose an efficient design pattern algorithm for the environmental and hydrologic hydraulic ubiquitous model developments which specifies pattern names for retrieving , exploring the adapted patterns on the stage of design without pattern language that is redundant abstraction . by applying composite design pattern to the design of the recursive river and basin interface for total maximum daily load , we significantly improve the performance of polymorphism and especially of reusability than conventional basin models . thus , this study can contribute on the reducing iterations and repetitions of up dating and committing after the spatial data changes that are frequently occurred in the process of the environmental gis model developments .
development an interactive vr training for cnc machining . <eos> the article introduces the basic features of a vr training system for computer numerical control ( cnc ) . the system is designed and implemented based on the world toolkit ( wtk ) software to support the interactive training for workpiece machining .
a new true rms to dc converter using up down translinear loop in cmos technology . <eos> in this paper a new true current mode rms to dc converter circuit based on a square root domain squarer divider and simplified current mode low pass filter is presented . the circuit is designed by employing up down translinear loop and using of mosfet transistors that operate in strong inversion saturation region . the converter offer advantages of two quadrant input current , low circuit complexity , large dynamic range , low supply voltage ( 1.2 v ) and immunity from the body effect . moreover , the power consumption of the circuit for the maximum accepted input current is less than <digit> mu w and does not need extra biasing to inject current into transistors . the circuit has been simulated by hspice . the simulation results with 0.18 mu m cmos technology are seen to conform to the theoretical analysis and shows benefits of the proposed circuit . simulation results show high performance of the proposed circuit .
tf df indexing for mocap data segments in measuring relevance based on textual search queries . <eos> many techniques have been proposed to address the problem of mocap data retrieval by using a short motion as input , and they are commonly categorized as content based retrieval . however , it is difficult for users who do not have equipments to create mocap data samples to take advantage of them . on the contrary , simple retrieval methods which only require text as input can be used by everyone . nevertheless , not only that it is not clear how to measure mocap data relevance in regard to textual search queries , but the search results will also be limited to the mocap data samples , the annotations of which contain the words in the search query . in this paper , the authors propose a novel method that builds on the tf ( term frequency ) and idf ( inverse document frequency ) weights , commonly used in text document retrieval , to measure mocap data relevance in regard to textual search queries . we extract segments from mocap data samples and regard these segments as words in text documents . however , instead of using idf which prioritizes infrequent segments , we opt to use df ( document frequency ) to prioritize frequent segments . since motions are not required as input , everybody will be able to take advantage of our approach , and we believe that our work also opens up possibilities for applying developed text retrieval methods in mocap data retrieval .
simulating robot collective behavior using starlogo . <eos> robot simulation is a very important tool to the development of novel real world techniques for cooperation of teams of robots . one major difficulty when trying to introduce students to robotics is that the teaching of major abstractions used to coordinate group robot behavior is not easily visualized it is not always true that one has enough robots available to be used in real demonstrations . in this paper , we attempt to improve the situation above by implementing a robot simulator for <digit> ( five ) of the major abstractions used in robotics . this simulator concentrates on group coordination in a scenario where robots are required to find their way out of a room or maze . this paper describes our initial version of this simulator , as well as our future plans for the simulator , both in usage and in enhancement of the feature set .
broadband beamforming using laguerre filters . <eos> spacetime processing is a well substantiated method for designing broadband beamformers . in the conventional frost spacetime beamformer , tapped delay line ( tdl ) filters are used in each branch of the array to create a wideband response for interference suppression . in this article a new spacetime beamforming method is introduced in which laguerre filters replace the traditional tdl filters in the frost beamformer . the laguerre filters are fundamentally iir filters but with only one pole in their structure . unlike other iir based spacetime beamforming methods , the proposed method does not need an adaptive procedure for the pole adjustment and is inherently stable . simulation results show superior performance of the proposed method compared to the frost beamformer and comparable results against other iir based beamformers with much less computational complexity and guaranteed stability .
modeling and visualization of classification based control schemes for upper limb prostheses . <eos> during the development of control schemes for upper limb prostheses , the selection of a classification method is the decisive factor on predicting the correct hand movements . this contribution brings forward an approach to validate and visualize the output of a chosen classifier by simulative means . using features extracted from a collection of recorded myoelectric signals ( mes ) , a training set for different classes of hand movements is produced and validated with additional mes recordings . using the output of the classifier , the behavior of an actual prosthesis is simulated by controlling the 3d model of a prosthetic hand . for systematic comparison of feature sets and classification methods , a toolbox for matlab ( tm ) has been developed . our classification results show , that existing classification schemes based on emg data can be improved significantly by adding nir sensor data . employing only two combined emg nir sensors , five motion classes comprising full movements , including pronation and supination , can be distinguished with <digit> % accuracy .
mdsm microarray database schema matching using the hungarian method . <eos> current microarray databases use different terminologies and structures and thereby limit the sharing of data and collating of results between laboratories . consequently , an effective integrated microarray data model is required . one important process to develop such an integrated database is schema matching . in this paper , we propose an effective schema matching approach called mdsm , to syntactically and semantically map attributes of different microarray schemas . the contribution from this work will be used later to create microarray global schemas . since microarray data is complex , we use microarray ontology to improve the measuring accuracy of the similarity between attributes . the similarity relations can be represented as weighted bipartite graphs . we determine the best schema matching by computing the optimal matching in a bipartite graph using the hungarian optimisation method . experimental results show that our schema matching approach is effective and flexible to use in different kinds of database models such as database schema , xml schema , and web site map . finally , a case study on an existing public microarray schema is carried out using the proposed method .
hemivariational inequality approach to evolutionary constrained problems on star shaped sets . <eos> in this paper , we consider a nonconvex evolutionary constrained problem for a star shaped set . the problem is a generalization of the classical evolution variational inequality of parabolic type . we provide an existence result the proof is based on the hemivariational inequality approach , a surjectivity theorem for multivalued pseudomonotone operators in reflexive banach spaces , and a penalization method . the admissible set of constraints is closed and star shaped with respect to a certain ball this allows one to use a discontinuity property of the generalized clarke subdifferential of the distance function . an application of our results to a heat conduction problem with nonconvex constraints is provided .
tensorial basis spline collocation method for poisson 's equation . <eos> this paper aims to describe the tensorial basis spline collocation method applied to poisson 's equation . in the case of a localized 3d charge distribution in vacuum , this direct method based on a tensorial decomposition of the differential operator is shown to be competitive with both iterative bscm and fft based methods . we emphasize the o ( h ( <digit> ) ) and o ( h ( <digit> ) ) convergence of tbscm for cubic and quintic splines , respectively . we describe the implementation of this method on a distributed memory parallel machine . performance measurements on a gray t3e are reported , our code exhibits high performance and good scalability as an example , a <digit> gaops performance is obtained when solving poisson 's equation on a <digit> ( <digit> ) non uniform 3d cartesian mesh by using <digit> t3e <digit> processors . this represents <digit> mflops per processors . ( c ) <digit> academic press .
text classification for assisting moderators in online health communities . <eos> we propose a method for improving the quality of online health communities . we present a solution to reduce the workload of the online community moderators . we explore low cost text classification methods to a new social media domain . we uncover social , ethical , and legal issues in creating a gold standard . we discuss using precision versus recall in supporting the community moderators .
dex digital evidence provenance supporting reproducibility and comparison . <eos> the current standard and open formats for forensic data describe whole disk and memory image properties , but do not describe the products of detailed investigations . we propose a simple canonical description of digital evidence provenance that explicitly states the set of tools and transformations that led from acquired raw data to the resulting product . our format , called digital evidence exchange ( dex ) is independent of the forensic tool that discovered the evidence , which has a number of advantages . using a dex description and the raw image file , evidence can be reproduced by other tools with the same functionality . additionally , dex descriptions can identify differences between two separate investigations of the same raw evidence . finally , as a standard product of tools , dex can allow quick fabrication of tool chains either as best of breed amalgams or for tool testing . we have implemented dex as an open source library .
scheme for quantum cloning of an unknown n particle entangled state with assistance . <eos> in this paper , we present a scheme which can realize quantum cloning of an unknown n particle entangled state and its orthogonal complement state with assistance offered by a state preparer . the first stage of the scheme requires usual teleportation via n non maximally entangled particle pairs as quantum channel . in the second stage of the scheme , with the assistance ( through a n particle projective measurement ) of the preparer , the perfect copies and complement copies of an unknown n particle entangled state can be obtained .
fast multipole method for time domain peec analysis . <eos> high speed electronic circuits are becoming more and more important in modern communication systems , thus leading to an increasing interest in printed circuit boards , interconnect , and packaging . nowadays , full wave numerical methods are widely used in order to investigate both signal integrity and electromagnetic compatibility issues arising in pcbs design . when broadband information is desired and transient effects dominate , it is more efficient using time domain numerical techniques , which may scale better than corresponding frequency domain methods . this paper presents the derivation of the time domain partial element equivalent circuit ( peec ) method enhanced by the three dimensional ( 3d ) fast multipole method ( fmm ) . it is shown that combining the full wave time domain peec method with the fmm allows performing the analysis of electrically large electronic systems , which reduces both memory and cpu time requirements . several examples are presented confirming the capability of the proposed approach to provide a significant reduction of the computational complexity associated with the transient analysis of large systems .
precomputing avatar behavior from human motion data . <eos> creating controllable , responsive avatars is an important problem in computer games and virtual environments . recently , large collections of motion capture data have been exploited for increased realism in avatar animation and control . large motion sets have the advantage of accommodating a broad variety of natural human motion . however , when a motion set is large , the time required to identify an appropriate sequence of motions is the bottleneck for achieving interactive avatar control . in this paper , we present a novel method of precomputing avatar behavior from unlabeled motion data in order to animate and control avatars at minimal runtime cost . based on dynamic programming , our method finds a control policy that indicates how the avatar should act in any given situation . we demonstrate the effectiveness of our approach through examples that include avatars interacting with each other and with the user .
customer driven collaborative product assembler for internet based commerce . <eos> in this study , a real time collaborative 3d assembling system ( co assembler ) has been described that allows casual customers to interactively and virtually configure the 3d products over the internet . it provides a framework for a web based 3d assembly system that uses a customizable product model to significantly simplify the configuration and the assembly process , and a customer driven recommendation helper to assist the customers make their design decisions . two assembly specific data formats assembly ml and parametric product ml are developed for archival and data transfer . the system aims to capture the customers ' needs and convert them into technical specifications , indirectly helping to promote e commerce for manufacturing enterprises .
looking into the seeds of time discovering temporal patterns in large transaction sets . <eos> this paper studies the problem of mining frequent itemsets along with their temporal patterns from large transaction sets . a model is proposed in which users define a large set of temporal patterns that are interesting or meaningful to them . a temporal pattern defines the set of time points where the user expects a discovered itemset to be frequent . the model is general in that ( i ) no constraints are placed on the interesting patterns given by the users , and ( ii ) two measuresinclusiveness and exclusivenessare used to capture how well the temporal patterns match the time points given by the discovered itemsets . intuitively , these measures indicate to what extent a discovered itemset is frequent at time points included in a temporal pattern p , but not at time points not in p. using these two measures , one is able to model many temporal data mining problems appeared in the literature , as well as those that have not been studied . by exploiting the relationship within and between itemset space and pattern space simultaneously , a series of pruning techniques are developed to speed up the mining process . experiments show that these pruning techniques allow one to obtain performance benefits up to <digit> times over a direct extension of non temporal data mining algorithms .
code efficiency evaluation for embedded processors . <eos> this paper evaluates the code efficiency of the arm , java , and x86 instruction sets by compiling the spec cpu95 cpu2000 jvm98 and caffeinemark benchmarks , from the aspects of code sizes , basic block sizes . instruction distributions , and average instruction lengths as a result . mainly because ( i ) the java architecture is a stack machine , ( ii ) there are only , four local variables which can be accessed by a <digit> byte instruction . and ( iii ) additional instructions are provided for the network security , the code efficiency of java turns out to be inferior to that of arm thumb . moreover , through this efficiency analysis it should be stressed that there exists the high potential of constructing a more efficient code architecture by taking minute account of the customization of an instruction set as well as the number of registers .
an overset grid method for large eddy simulation of turbomachinery stages . <eos> a coupling method based on the overset grid approach has been successfully developed to couple multi copies of a massively parallel unstructured compressible les solver avbp for turbomachinery applications . as proper les predictions require minimizing artificial dissipation as well as dispersion of turbulent structures , the numerical treatment of the moving interface between stationary and rotating components has been thoroughly tested on cases involving acoustical wave propagation , vortex propagation through a translating interface and a cylinder wake through a rotating interface . convergence and stability of the coupled schemes show that a minimum number of overlapping points are required for a given scheme . the current accuracy limitation is locally given by the interpolation scheme at the interface , but with a limited and localized error . for rotorstator type applications , the moving interface only introduces a spurious weak tone at the rotational frequency provided the latter is correctly sampled . the approach has then been applied to the qinetiq mt1 high pressure transonic experimental turbine to illustrate the potential of rotor stator les in complex , high reynolds number industrial turbomachinery configurations . both wave propagation and generation are considered . mean les statistics agree well with experimental data and bring improvement over previous rans or urans results .
design and performance evaluation of amplifier modules in stackable roadms for low cost cwdm access networks . <eos> coarse wavelength division multiplexing ( cwdm ) network has proven to be promising lower cost network architecture for a significant cost advantage over dense wavelength division multiplexing due to the lower cost of lasers and the filters used in cwdm modules . a compatible amplifier module having bidirectional amplification capability was deployed for introducing inside stackable reconfigurable optical add drop multiplexers in realizing large scale cwdm networks . the amplifier module for use in the bidirectional ip transmission confirmed that the insertion losses of the nodes and the losses of the fibers connecting the nodes can be compensated effectively , allowing the network administrator to increase the number of nodes and fiber length of the network . however , the noise generated from the amplification due to amplified spontaneous emission must be considered in network design issues . in this paper , optical power penalties due to the bidirectional amplification were estimated by conducting experimentation on minimum detectable power of optical transceivers . after analyzing the power penalty issue , an ip over cwdm ring network was implemented and the performance of network was evaluated by monitoring the power and packet transmissions before and after the amplifier module was turned on .
interference aware topology control in wireless mesh network . <eos> in wireless mesh networks , each node should help to relay packets of neighboring nodes toward gateway using multi hop routing mechanism . wireless mesh networks usually intensively deploy mesh nodes to cope with the problem of dead spot communication . however , the higher density of nodes deployed the higher radio interference occurred . and , this causes significant degradation of system performance . in this paper , we first transform network problem into geometry problem in graph . we define line intersection in a graph to reflect radio interference problem . we use plan sweep algorithm to find intersection lines , if any , employ voronoi diagram algorithm to delimit the regions among nodes , use delaunay triangulation algorithm to reconstruct the graph in order to minimize the interference among nodes . finally , we use standard deviation to prune off those longer links ( higher interference links ) to have a further refinement . this hybrid solution is able to significantly reduce interference in o ( n log n ) time complexity .
example based automatic music driven conventional dance motion synthesis . <eos> we introduce a novel method for synthesizing dance motions that follow the emotions and contents of a piece of music . our method employs a learning based approach to model the music to motion mapping relationship embodied in example dance motions along with those motions ' accompanying background music . a key step in our method is to train a music to motion matching quality rating function through learning the music to motion mapping relationship exhibited in synchronized music and dance motion data , which were captured from professional human dance performance . to generate an optimal sequence of dance motion segments to match with a piece of music , we introduce a constraint based dynamic programming procedure . this procedure considers both music to motion matching quality and visual smoothness of a resultant dance motion sequence . we also introduce a two way evaluation strategy , coupled with a gpu based implementation , through which we can execute the dynamic programming process in parallel , resulting in significant speedup . to evaluate the effectiveness of our method , we quantitatively compare the dance motions synthesized by our method with motion synthesis results by several peer methods using the motions captured from professional human dancers ' performance as the gold standard . we also conducted several medium scale user studies to explore how perceptually our dance motion synthesis method can outperform existing methods in synthesizing dance motions to match with a piece of music . these user studies produced very positive results on our music driven dance motion synthesis experiments for several asian dance genres , confirming the advantages of our method .
designing a hierarchical fuzzy logic controller using the differential evolution approach . <eos> in conventional fuzzy logic controllers , the computational complexity increases with the dimensions of the system variables the number of rules increases exponentially as the number of system variables increases . hierarchical fuzzy logic controllers ( hflc ) have been introduced to reduce the number of rules to a linear function of system variables . however , the use of hierarchical fuzzy logic controllers raises new issues in the automatic design of controllers , namely the coordination of outputs of sub controllers at lower levels of the hierarchy . in this paper , a method is described for the automatic design of an hflc using an evolutionary algorithm called differential evolution ( de ) . the aim in this paper is to develop a sufficiently versatile method that can be applied to the design of any hflc architecture . the feasibility of the method is demonstrated by developing a two stage hflc for controlling a cartpole with four state variables . the merits of the method are automatic generation of the hflc and simplicity as the number of parameters used for encoding the problem are greatly reduced as compared to conventional methods .
biform game based cognitive radio scheme for smart grid communications . <eos> smart grid is widely considered to be a next generation power grid , which will be integrated with information feedback communications . however , smart grid communication technologies are subject to inefficient spectrum allocation problems . cognitive radio networks can solve the problem of spectrum scarcity by opening the under utilized licensed bands to secondary users . in this paper , adaptive cognitive radio spectrum sensing and sharing algorithms are developed for smart grid environments . simulation results are presented to demonstrate the effectiveness of the proposed scheme in comparison with other existing schemes .
stability analysis and optimal control of a hand foot mouth disease ( hfmd ) model . <eos> in this paper , we propose a system of ordinary differential equations to model the hand foot mouth disease ( hfmd ) . we derive the expression of the basic reproduction number . when , the system only has the disease free equilibrium , which is globally asymptotically stable otherwise , the system is persistent . by sensitivity analysis , we identify the control parameters . then we formulate an optimal control problem to find the optimal control strategy . these results are applied to the spread of hfmd in mainland china . the basic reproduction number tells us that it is outbreak in china .
web site quality in the uk airline industry a longitudinal examination . <eos> web site quality is now considered a critical factor to attract customers ' attention and build loyalty . based on a review of the literature , this study focuses on five dimensions of web quality usability , web site design , service quality , information quality , and enjoyment . the aim of this research is to identify the development of web site quality in four uk airlines based oil the research dimensions during the period front <digit> to <digit> . for this purpose , the survey utilised the web archive ( www.archive , org ) for retrospective analysis ( n <digit> ) . according to the findings , the four airlines have improved their web site quality on the four dimensions in quite different ways during the period from <digit> to <digit> . one of the main conclusions of this study is that the airlines have specific advantages from their particular mix of web site attributes . the paper rounds off with conclusions and implications for research and practice .
a cross layer optimization based integrated routing and grooming algorithm for green multi granularity transport networks . <eos> a novel cross layer optimization model is developed for power efficient mtn . a green integrated routing and grooming algorithm is developed based on bbo . both the power consumption and the multi user qos satisfaction degree are optimized .
an order n complexity meshless algorithm for transport type pdes , based on local hermitian interpolation . <eos> this work describes a new numerical method utilising radial basis function interpolants . based on local hermitian interpolation of function values and boundary operators , and using an explicit time advancement formulation , the method is of order n complexity . computational cost to advance the solution in time is minimal , and is largely dependent on local system support size . the explicit time advancement formulation allows a novel solution technique for many nonlinear partial differential equations . the performance of the method is examined for a variety of linear convectiondiffusionreaction problems , featuring both steady and unsteady solutions . the method is also demonstrated with a nonlinear richards equation model , solving an unsaturated flow in porous media problem . the technique is named the local hermitian interpolation ( lhi ) method .
synergies between operations research and data mining the emerging use of multi objective approaches . <eos> operations research and data mining already have a long established common history . indeed , with the growing size of databases and the amount of data available , data mining has become crucial in modern science and industry . data mining problems raise interesting challenges for several research domains , and in particular for operations research , as very large search spaces of solutions need to be explored . hence , many operations research methods have been proposed to deal with such challenging problems . but the relationships between these two domains are not limited to these natural applications of operations research approaches . the counterpart is also important to consider , since data mining approaches have also been applied to improve operations research techniques . the aim of this article is to highlight the interplay between these two research disciplines . a particular emphasis will be placed on the emerging theme of applying multi objective approaches in this context .
the dual eigenvalue problems for the sturmliouville system . <eos> in this paper , we find the minimizer of the eigenvalue gap for the schrdinger equation and vibrating string equation . in the first part , we show the first two neumann eigenvalue gap of the schrdinger equation with single well potentials is not less than <digit> and the equality holds if and only if the potential is constant . in the second part , since the first neumann eigenvalue of the vibrating string equation is <digit> , we turn to show that the minimizing density function of the second neumann eigenvalue is of the form h ( a , a ) h <digit> , ( a , a ) h ( a , a ) h <digit> , ( a , a ) for some a a .
two efficient algorithms for determining intersection points between simple polygons . <eos> the paper describes two new algorithms for calculating the intersection points between two simple ( non self intersected ) polygons . both algorithms use a sweep line approach and differ only in how they store the so called sweep line status . the first algorithm ( set based intersection algorithm ) needs two one way dynamic lists of currently pierced line segments , whereas the second algorithm ( a binary search tree based intersection algorithm ) uses a binary search tree for this purpose . in the situation of the first algorithm , the theoretical time complexity is still o ( k2 ) ( k n m , where n and m are the number of the input polygons ' vertices ) whereas the second algorithm gives o ( ( k i ) log2 ( k i ) ) , where i is the number of intersection points between the polygon edges . however , as shown in the paper , the expected time complexity of the first algorithm is closer to the second algorithm than to the brute force implementation . the importance of the presented algorithms in gis applications is considered . boolean operations , polygon interference determination , splitting a polygon by a polyline , merging the set of polygons and constructing the geometric buffers are briefly described .
statistical decision tree based fault classification scheme for protection of power transmission lines . <eos> this paper presents a statistical algorithm for classification of faults on power transmission lines . the proposed algorithm is based upon the wavelet transform of three phase currents measured at the sending end of a line and the classification and regression tree ( cart ) method , a commonly available statistical method . wavelet transform of current signal provides hidden information of a fault situation as an input to cart algorithm , which is used to classify different types of faults . the proposed technique is simulated using matlab simulink software and it is tested upon the data created with the fault analysis of the 400kv sample transmission line considering wide variations in the operating conditions . the classification results are also compared with the results obtained using back propagation neural network .
performance analysis tool for reinforced concrete members . <eos> a computer program was developed to analyze the non linear , cyclic flexural performance of reinforced concrete structural members under various types of loading paths including non sequential variations in axial load . this performance is significantly affected by the loading history . different monotonic material models as well as hysteresis rules for confined and unconfined concrete and steel , some developed and calibrated against test results on material samples , were implemented in a fiber based moment curvature and in turn force deflection analysis . one of the assumptions on curvature distribution along the member was based on a method developed to address the variation of the plastic hinge length as a result of loading pattern . functionality of the program was verified by reproduction of analytical results obtained by others for several cases , and accuracy of the analytical process and the implemented models were evaluated against the experimental results from large scale reinforced concrete columns tested under the analyzed loading cases . while the program can be used to predict the response of a member under a certain loading pattern , it can also be used to examine various analytical models and methods or refine a custom material model against test data .
fault based test suite prioritization for specification based testing . <eos> context existing test suite prioritization techniques usually rely on code coverage information or historical execution data that serve as indicators for estimating the fault detecting ability of test cases . such indicators are primarily empirical in nature and not theoretically driven hence , they do not necessarily provide sound estimates . also , these techniques are not applicable when the source code is not available or when the software is tested for the first time . objective we propose and develop the novel notion of fault based prioritization of test cases which directly utilizes the theoretical knowledge of their fault detecting ability and the relationships among the test cases and the faults in the prescribed fault model , based on which the test cases are generated . method we demonstrate our approach of fault based prioritization by applying it to the testing of the implementation of logical expressions against their specifications . we then validate our proposal by an empirical study that evaluates the effectiveness of prioritization techniques using two different metrics . results a theoretically guided fault based prioritization technique generally outperforms other techniques under study , as assessed by two different metrics . our empirical results also show that the technique helps to reveal all target faults by executing only about <digit> % of the prioritized test suite , thereby reducing the effort required in testing . conclusions the fault based prioritization approach is not only applicable to the instance empirically validated in this paper , but should also be adaptable to other fault based testing strategies . we also envisage new research directions to be opened up by our work . ( c ) <digit> elsevier b.v. all rights reserved .
orthogonal rotations in latent semantic analysis an empirical study . <eos> orthogonal rotations influence factor interpretation in latent semantic analysis . varimax and equamax produce factors with similar interpretation . quartimax tends to summarize the content of text corpora in a single extracted factor .
one dimensional simulation of clay drying . <eos> drying of clay is simulated by a one dimensional model . the background of the work is to form a better basis for investigation of the drying process in production of clay based building materials . a model of one dimensional heat and mass transfer in porous material is used and modified to simulate drying of clay particles . the convective terms are discretized by first order upwinding , and the diffusive terms are discretized by central differencing . dassl was used to solve the set of algebraic and differential equations . the different simulations show the effect of permeability , initial moisture content and different boundary conditions . both drying of a flat plate and a spherical particle are modelled .
similarity based image organization and browsing using multi resolution self organizing map . <eos> an algorithm is presented in this paper to facilitate the exploration of large image collections based on visual similarities . starting with an unordered and unannotated set of images , the algorithm first extracts the salient details into feature vectors using both color and gradient information . the feature vectors are then used to train a self organizing map which maps high dimensional feature vectors onto a 2d canvas so that images with similar feature vectors are grouped together . when users browse the image collection , an image collage is generated that selects and displays the most pertinent set of images based on which portion of the 2d canvas is currently in view . flowing from an overview to details is a seamless operation controlled simply by pan and zoom , with representative images selected in a consistent and predictable way . to make organizing larger image collections practical in interactive time , the organization algorithm is designed to run in parallel on graphics processing units . overall this paper presents an end to end solution that facilitates the surfing of image collections in a fresh way . ( c ) <digit> elsevier b.v. all rights reserved .
invariant functionals on completely distributive lattices . <eos> in this paper we are interested in functionals defined on completely distributive lattices and which are invariant under mappings preserving arbitrary joins and meets . we prove that the class of nondecreasing invariant functionals coincides with the class of sugeno integrals associated with <digit> , <digit> valued capacities , the so called term functionals , thus extending previous results both to the infinitary case as well as to the realm of completely distributive lattices . furthermore , we show that , in the case of functionals over complete chains , the nondecreasing condition is redundant . characterizations of the class of sugeno integrals , as well as its superclass comprising all polynomial functionals , are provided by showing that the axiomatizations ( given in terms of homogeneity ) of their restriction to unitary functionals still hold over completely distributive lattices . we also present canonical normal form representations of polynomial functionals on completely distributive lattices , which appear as the natural extensions to their finitary counterparts , and as a by product we obtain an axiomatization of complete distributivity in the case of bounded lattices . ( c ) <digit> elsevier b.v. all rights reserved .
design tools for digital microfluidic biochips toward functional diversification and more than moore . <eos> microfluidics based biochips enable the precise control of nanoliter volumes of biochemical samples and reagents . they combine electronics with biology , and they integrate various bioassay operations , such as sample preparation , analysis , separation , and detection . compared to conventional laboratory procedures , which are cumbersome and expensive , miniaturized biochips offer the advantages of higher sensitivity , lower cost due to smaller sample and reagent volumes , system integration , and less likelihood of human error . this paper first describes the droplet based digital microfluidic technology platform and emerging applications . the physical principles underlying droplet actuation are next described . finally , the paper presents computer aided design tools for simulation , synthesis and chip optimization . these tools target modeling and simulation , scheduling , module placement , droplet routing , pin constrained chip design , and testing .
do men and women differ in privacy gendered privacy and ( in ) equality in the internet . <eos> we examined how personal privacy behavior and confidence differ by gender . gender had no direct effect on data release . however , there was a gender difference on confidence in privacy protection . significant interactions between gender and age were found .
bispectral analysis of traffic in high speed networks . <eos> in this paper , we report results regarding bispectral analysis of the long range dependent atm wan traffic . six different data sets were measured on <digit> mbps links of the sunet by a custom built tracing tool capable of recording over eight million consecutive cell arrivals . the complex fractal behavior of the atm traffic claims utilization of the higher order spectra analysis . for each of the analyzed data sets , it was found that the gamma distribution fits very well . the bispectrum was studied for extracting some additional information with respect to the long memory parameter . the nonlinearity of the time series was also tested with the help of the bispectrum . ( c ) <digit> elsevier science ltd. all rights reserved .
an empirical study of machine learning techniques for affect recognition in humanrobot interaction . <eos> given the importance of implicit communication in human interactions , it would be valuable to have this capability in robotic systems wherein a robot can detect the motivations and emotions of the person it is working with . recognizing affective states from physiological cues is an effective way of implementing implicit humanrobot interaction . several machine learning techniques have been successfully employed in affect recognition to predict the affective state of an individual given a set of physiological features . however , a systematic comparison of the strengths and weaknesses of these methods has not yet been done . in this paper , we present a comparative study of four machine learning methodsk nearest neighbor , regression tree ( rt ) , bayesian network and support vector machine ( svm ) as applied to the domain of affect recognition using physiological signals . the results showed that svm gave the best classification accuracy even though all the methods performed competitively . rt gave the next best classification accuracy and was the most space and time efficient .
a pilot finite element study of an osteoporotic l1 vertebra compared to one with normal t score . <eos> in this paper , two patient specific finite element ( fe ) models of both an l1 vertebra with a normal t score and a mildly wedging , osteoporotic one were created and analysed under usual action . utilising commercial software packages for image processing and fe analysis ( fea ) along with in house computer codes for a posteriori assignment of material properties , in vivo high resolution spiral computed tomography of the entire vertebrae and feawere combined . using the vertebra with a normal t score as baseline it was found that the maximum value of the von mises stress in the osteoporotic vertebra was <digit> % higher but still far below bone strength , while the maximum value of von mises strain in the same vertebra was <digit> % higher than that of the vertebra with normal t score . in the vertebra with normal t score , <digit> % of its volume exhibited values of von mises strain higher than the threshold of <digit> mu strains , referenced by homminga et al. as a threshold of fracture risk , while in the osteoporotic one this percentage was raised up to <digit> % . the results suggested that the osteoporotic vertebra is susceptible to fracture due to raised strains and not stresses .
robust feature selection for microarray data based on multicriterion fusion . <eos> feature selection often aims to select a compact feature subset to build a pattern classifier with reduced complexity , so as to achieve improved classification performance . from the perspective of pattern analysis , producing stable or robust solution is also a desired property of a feature selection algorithm . however , the issue of robustness is often overlooked in feature selection . in this study , we analyze the robustness issue existing in feature selection for high dimensional and small sized gene expression data , and propose to improve robustness of feature selection algorithm by using multiple feature selection evaluation criteria . based on this idea , a multicriterion fusion based recursive feature elimination ( mcf rfe ) algorithm is developed with the goal of improving both classification performance and stability of feature selection results . experimental studies on five gene expression data sets show that the mcf rfe algorithm outperforms the commonly used benchmark feature selection algorithm svm rfe .
a second order virtual node method for elliptic problems with interfaces and irregular domains . <eos> we present a second order accurate , geometrically flexible and easy to implement method for solving the variable coefficient poisson equation with interfacial discontinuities or on irregular domains , handling both cases with the same approach . we discretize the equations using an embedded approach on a uniform cartesian grid employing virtual nodes at interfaces and boundaries . a variational method is used to define numerical stencils near these special virtual nodes and a lagrange multiplier approach is used to enforce jump conditions and dirichlet boundary conditions . our combination of these two aspects yields a symmetric positive definite discretization . in the general case , we obtain the standard <digit> point stencil away from the interface . for the specific case of interface problems with continuous coefficients , we present a discontinuity removal technique that admits use of the standard <digit> point finite difference stencil everywhere in the domain . numerical experiments indicate second order accuracy in l .
extended cooperation in clinical studies through exchange of cdisc metadata between different study software solutions . <eos> objectives our objectives were to analyze the possibility of an exchange of an entire clinical study between two different and independent study software solutions . the question addressed was whether a software independent transfer of study metadata can be performed without programming efforts and with software routinely used for clinical research . methods study metadata was transferred with odm standard ( cdisc ) . study software systems employed were macro ( infermed ) and xtrial ( xclinical ) . for the proof of concept , a test study was created with macro and exported as odm . for modification and validation of the odm export file xml spy ( altova ) and odm checker ( xml4pharma ) were used . results through exchange of a complete clinical study between two different of concept of the technical feasibility of a system independent metadata exchange was conducted successfully . the interchange of study metadata between two different systems at different centers was performed with minimal expenditure . a small number of mistakes had to be corrected in order to generate a syntactically correct odm file and a vendor extension had to be inserted . after these modifications , xtrial exhibited the study , including all data fields , correctly . however , the optical appearance of both crfs ( case report forms ) was different . conclusions odm can be used as an exchange format for clinical studies between different study software . thus , new forms of cooperation through exchange of metadata seem possible , for example the joint creation of electronic study protocols or crfs at different research centers . although the odm standard represents a clinical study completely , it contains no information about the representation of data fields in crfs .
construction of 4d high definition cortical surface atlases of infants methods and applications . <eos> we propose a method for construction of consistent and unbiased 4d surface atlases . we construct the first 4d infant cortical surface atlases with seven time points . we study correlation patterns of early cortex development between different regions .
colour surface flow visualisation of interfering slender bodies at mach <digit> . <eos> a study to understand the flow physics produced by two slender bodies in close proximity in high speed airflow was undertaken . the interference flow field generated by the bodies is dominated by shock and expansion waves , and of particular significance is the complex interaction of the bow shock wave emanating from the disturbance generator , striking the surface of the disturbance receiver . to gain insight into the shock wave body interaction , the traditional surface oil flow visualisation technique was extended to include colour , which assists the eye in tracking the streak lines back to their separation and reattachment regions . in addition , the fine particle sizes of the colour pigment produced crisper and more definitive separation lines over the body , in comparison to the traditional monochrome pigments , such as lamp black or titanium dioxide . subsequently , the dried surface pattern was lifted off the body using matte acetate tape , digitised and then straightened using datum markings along the sting . this allowed the shock impingement location and shock diffraction path over the body to be established quantitatively , which was used to validate numerical simulations . the experimental and computational data showed good agreement for all configurations considered , providing complementary information about the disturbance induced effects generated in the interference flow field , and provided detailed insight into the near surface flow topology produced by the shock wave slender body interaction .
asymptotic mse distortion of mismatched uniform scalar quantization . <eos> asymptotic formulas are derived for the mean squared error ( mse ) distortion of n level uniform scalar quantizers designed to be mse optimal for one density function , but applied to another , as n > infinity . these formulas , which are based on the euler maclaurin formula , are then applied with generalized gamma , bucklew gallagher , and hui neuhoff density functions as the designed for and applied to densities . it is found that the mismatch between the designed for and applied to densities can disturb the delicate balance between granular and overload distortions in optimal quantization , with the result that , generally speaking , the granular or overload distortion dominates , respectively , depending on whether the applied to density function has a lighter or heavier tail than the designed for density . specifically , in the case of generalized gamma densities , a variance mismatch makes overload distortion dominate for an applied to source with a slightly larger variance , whereas a shape mismatch can tolerate a wider variance difference while retaining the dominance of the granular distortion . in addition , for the studied density functions , the euler maclaurin approach is used to derive asymptotic formulas for the optimal quantizer step size in a simpler , more direct , way than previous approaches .
allowing agents to be imprecise a proposal using multiple linguistic terms . <eos> in this paper we propose a decision making procedure where the agents judge the alternatives through linguistic terms such as very good , good , acceptable , etc. if the agents are not confident about their opinions , they can use a linguistic expression formed by several consecutive linguistic terms . to obtain a ranking on the set of alternatives , the method consists of three different stages . the first stage looks for the alternatives in which the overall opinion is closer to the ideal assessment . the overall opinion is developed by a distance based process among the individual assessments . the next two stages form a tie breaking process . firstly by using a dispersion index based on the gini coefficient , and secondly by taking into account the number of best assessments . the main characteristics of the proposed decision making procedure are analyzed .
gain based evaluation measure for ranked web results . <eos> internet is the era connecting millions of people online . such web makes a person even to think beyond his imagination . due to such phenomenal changes in life style especially after <digit> 's , research on web has got some importance . web mining poses a number of challenges involving different approaches like text mining , link mining , content mining or context mining . it also makes us to think of multi lingual mining , which leaves a bi challenge for research community . this paper focuses in depth on automated evaluation procedure of the mined web contents . we have made some effort to optimize the results given by a search engine through link mining and content mining . having obtained such mined and optimized data , we propose an automated evaluation metric to measure the quality of the retrieved content . the results seem to be promising which leads to ideas that can be enhanced through some automated agents .
a performance comparison of md5 authenticated routing traffic with eigrp , ripv2 , and ospf . <eos> routing is the process of forwarding data across an inter network from a designated source to a final destination along the way from source to destination at least one intermediate node is considered due to the major role that routing protocols play in computer network infrastructures special cares have been given to routing protocols with built in security constraints in this paper we conduct performance evaluation comparisons on message digest <digit> authenticated routing traffic with respect to eigrp ripv2 and ospf protocols a network model of four cisco routers has been employed with an on off traffic model used to describe text files transmissions over the network eventually analysis tool has been developed and used to measure the average delay time and average jitter the collected results show that the average delay time and jitter in the secured message digest <digit> case can become significantly larger when compared to the non secured case even in steady state conditions among all the secured ospf protocol shows the highest performance even when the system is extremely overloaded
maximum empirical likelihood estimation of continuous time models with conditional characteristic functions . <eos> for some popular financial continuous time models , tractable expressions of likelihood functions are unknown . for that reason , the maximum likelihood estimation method is infeasible . fortunately , closed functional forms of conditional characteristic functions of some of these models are known . we construct an empirical likelihood estimation method using tractable conditional characteristic functions to estimate such a model . this method resolves the problem of covariance matrix singularity in the standard generalized method of moments and fully utilizes information in conditional moment restrictions . it is applicable to many popular financial models such as some diffusion models , jump diffusion models , and stochastic volatility models . using a monte carlo comparison , we show that this method provides superior performance compared to other methods in some situations . ( c ) <digit> imacs . published by elsevier b.v. all rights reserved .
wavelet analysis of surface morphologies of magnetron sputtered al cu thin films . <eos> al cu thin films were deposited by dc magnetron sputtering . the films are characterized by atomic force microscopy and its surface morphologies are analyzed by wavelet technique . multiresolution signal decomposition wavelet technique was employed to extract the surface roughness from the afm images of al cu thin films . it is observed that the al cu thin films exhibit higher surface roughness value with increasing deposition time . the calculated surface roughness of the thin films , using wavelet technique , is comparable with that of its experimental values .
how effective is uml modeling . <eos> modeling has become a common practice in modern software engineering . since the mid 1990s the unified modeling language ( uml ) has become the de facto standard for modeling software systems . the uml is used in all phases of software development ranging from the requirement phase to the maintenance phase . however , empirical evidence regarding the effectiveness of modeling in software development is few and far apart . this paper aims to synthesize empirical evidence regarding the effectiveness of modeling using uml in software development , with a special focus on the cost and benefits .
identification of mouse mslp2 gene from est databases by repeated searching , comparison , and assembling . <eos> the nphs2 gene is expressed in podocytes and encodes the integral membrane protein called podocin , which is believed to play an important role in the renal function of glomerular filtration . mutations in this gene can cause serious renal function disorders . in this study , we used data mining techniques and bioinformatic tools to search for the mouse ortholog of the nphs2 related gene . it might be valuable for future studies of renal diseases . we employed repeated cycles of searching , comparison , and assembling to extend the assembled est sequences . the discovered gene sequence mslp2 , an ortholog of the human slp2 gene , was found to have a total length of 1253bp with the amino acid coding region located in 321093nt . it was further verified using the rt pcr and race techniques to ensure its biological accuracy and then registered with the genbank . when clustalw was used for comparing the mslp2 and human slp2 genes for similarities , the similarities were as high as <digit> % for nucleotide and <digit> % for amino acid sequences . in conclusion , we propose a method for rapid identification of the mouse ortholog gene from the human genome .
qsar studies of the pyrethroid insecticides part <digit> . a putative pharmacophore derived using methodology based on molecular dynamics and hierarchical cluster analysis . <eos> previous studies of the conformational behaviour of a group of synthetic pyrethroid insecticides have been extended to a more structurally diverse set . this includes compounds with different backbones and differing stereochemistry , with both types i and ii biological activity . these compounds also encompass a large range of biological activities . a parameterisation of the charmm force field for these compounds has been performed and the extra parameters are reported . conformational sampling , using molecular dynamics ( md ) , has been performed for each of the <digit> active structures . the accessible conformations of each have been characterised by the values of the common torsion angles using hierarchichal cluster analysis ( hca ) . a further ca , based on the centroids derived from the conformational sampling , identified a conformation common to at least <digit> of the <digit> structures . the critical torsion angles of this conformation lie at the centre of the molecule about the ester linkage and are defining an extended conformation , which differs from the minimum energy conformation of deltamethrin used previously . this may represent a putative pharmacophore for kill . the methods used here improve significantly on those used previously . the charmm force field was parameterised for the compounds and an improved method of conformational sampling , based on centroid clustering , has also been used .
cooperation as a service in vanets . <eos> vehicular networks , including vehicular adhoc networks ( vanets ) and vehicular sensor networks ( vsns ) , stimulate a brand new variety of services , ranging from driver safety services , traffic information and warnings regarding traffic jams and accidents , to providing weather or road condition , parking availability , and advertisement . 3g networks and sophisticated intelligent transportation systems ( its ) , including deploying costly roadside base stations , can indeed be used to offer such services , but these come with a cost , both at network and hardware levels . in this paper we introduce cooperation as a service ( caas ) a novel architecture that will allow providing a set of services for free and without any additional infrastructure , by taking advantage of vehicle to vehicle communications . caas uses a hybrid publish subscribe mechanism where the driver ( or subscriber ) expresses his interests regarding a service ( or a set of services ) and where cars having subscribed to the same service will cooperate to provide the subscriber with the necessary information regarding the service he subscribed to , by publishing this information in the network . caas structures the network into clusters , and uses content based routing ( cbr ) for intra cluster communications and geographic routing for inter cluster communications .
consensus formation in weighted scale free networks . <eos> we develop a consensus model for studying opinion dynamics in weighed networks and investigate the effects of both the distribution of weights and the correlations between weight and degree on dynamical behaviors of opinion formation . our results suggest that a global consensus in the weighted networks to reach is more difficult than that in unweighted network , and strongly depends on the heterogeneity of connection strengths . in addition , in the weighted network with very large heterogeneity of connection strengths , only single macroscopic opinion cluster can be formed , which differs from the behavior in the unweighted network where the next largest macroscopic opinion cluster may exist .
acyclic colourings of graphs with bounded degree . <eos> a k colouring of a graph g is called acyclic if for every two distinct colours i and j , the subgraph induced in g by all the edges linking a vertex coloured with i and a vertex coloured with j is acyclic . in other words , there are no bichromatic alternating cycles . in <digit> boiron et al. conjectured that a graph g with maximum degree at most <digit> has an acyclic <digit> colouring such that the set of vertices in each colour induces a subgraph with maximum degree at most <digit> . in this paper we prove this conjecture and show that such a colouring of a cubic graph can be determined in polynomial time . we also prove that it is an np complete problem to decide if a graph with maximum degree <digit> has the above mentioned colouring .
a rank based goodness of fit approach to testing for non additivity in the two way layout with no replications . <eos> one of the major problems for which satisfactory small sample rank based test procedures are only recently being developed is that of evaluating the possible presence of interaction in the standard two way layout . this is especially true when there is only one observation on each treatment block combination . in this paper we take a goodness of fit approach to this problem using the idea of grouping similar block rank permutations to obtain a workable number of collapsed categories . this grouping is based on the concepts of decomposable and indecomposable classes of rank vectors considered by comtet ( c.r. acad . sci . paris <digit> ( <digit> ) <digit> in advanced combinatorics the art of finite and infinite expansions , d. reidel , boston , <digit> , p. <digit> ) . alignment within treatment levels is utilized to adjust for the possible presence of treatment effects prior to the application of the goodness of fit procedure to test for interaction . we present the results of a substantial monte carlo simulation power study of the proposed procedure .
component based technique for determining the effects of acupuncture for fighting migraine using spect images . <eos> in this work , spect brain images are analyzed automatically in order to determine the effects of acupuncture applied for fighting migraine . for this purpose , two different groups of patients are randomly collected and received verum and sham acupuncture , respectively . changes in the brain perfusion patterns can be measured quantitatively by dealing with the images in a classification context . a classification scheme consisting of a component based feature extraction technique in combination with support vector machines allows us to accurately determine the regions of interest ( rois ) where acupuncture produced more intense effects , and whether these effects are correlated with a decrease or an increase of the brain activity . effects produced by verum and sham acupuncture are studied , and the best method for intensity normalization is discussed . the result is a complete , objective system which can be used for general purposes in the visual assessment of perfusion images .
volumic patient specific reconstruction of muscular system based on a reduced dataset of medical images . <eos> three dimensional mechanical modelling of muscles is essential for various biomechanical applications and clinical evaluation , but it requires a tedious manual processing of numerous images . a muscle reconstruction method is presented based on a reduced set of images to generate an approximate parametric object from basic dimensions of muscle contours . a regular volumic mesh is constructed based on this parametric object . the approximate object and the corresponding mesh are deformed to fit the exact muscles contours yielding patient specific geometry . evaluation was performed by comparison of geometry to that obtained by contouring all computed tomography ( ct ) slices , and by quantification of the mesh quality criteria . muscle fatty infiltration was estimated using a threshold between fat and muscle . volumic fat index ( vfi ) of a muscle was computed using first all the complete ct scan slices containing the muscle ( vf ( ref ) ) and a second time only the slices used for reconstruction ( vfi ( recons ) ) . mean volume error estimation was 2.6 % and hexahedron meshes fulfilled quality criteria . vfi ( recons ) respect the individual variation of fat content .
eigenvoice modelling for cross likelihood ratio based speaker clustering a bayesian approach . <eos> this paper proposes the use of bayesian approaches with the cross likelihood ratio ( clr ) as a criterion for speaker clustering within a speaker diarization system , using eigenvoice modelling techniques . the clr has previously been shown to be an effective decision criterion for speaker clustering using gaussian mixture models . recently , eigenvoice modelling has become an increasingly popular technique , due to its ability to adequately represent a speaker based on sparse training data , as well as to provide an improved capture of differences in speaker characteristics . the integration of eigenvoice modelling into the clr framework to capitalize on the advantage of both techniques has also been shown to be beneficial for the speaker clustering task . building on that success , this paper proposes the use of bayesian methods to compute the conditional probabilities in computing the clr , thus effectively combining the eigenvoice clr framework with the advantages of a bayesian approach to the diarization problem . results obtained on the <digit> rich transcription ( rt <digit> ) evaluation dataset show an improved clustering performance , resulting in a 33.5 % relative improvement in the overall diarization error rate ( der ) compared to the baseline system .
bounding surface model for soil resistance to cyclic lateral pile displacements with arbitrary direction . <eos> the development of a two surface elasticplastic bounding surface py model for cyclic lateral pile motions is described . the kinematic hardening model is applicable to the analysis of pile foundations subjected to loading with arbitrary azimuths relative to the pile axis . the model realistically captures the hysteretic energy damping associated with dynamic loading of subsea foundations through physically correct plastic mechanisms and provides results consistent with those observed in physical tests including cyclic loading . its performance is demonstrated in element states of stress and in pile foundation analyses . the development based on the incremental theory of plasticity results in more robust solutions than may be obtained using alternative elastic , variable moduli and deformation plasticity formulations .
cost and precision tradeoffs of dynamic data slicing algorithms . <eos> dynamic slicing algorithms are used to narrow the attention of the user or an algorithm to a relevant subset of executed program statements . although dynamic slicing was first introduced to aid in user level debugging , increasingly applications aimed at improving software quality , reliability , security , and performance are finding opportunities to make automated use of dynamic slicing . in this paper we present the design and evaluation of three precise dynamic data slicing algorithms called the full preprocessing ( fp ) , no preprocessing ( np ) and limited preprocessing ( lp ) algorithms . the algorithms differ in the relative timing of constructing the dynamic data dependence graph and its traversal for computing requested dynamic data slices . our experiments show that the lp algorithm is a fast and practical precise data slicing algorithm . in fact we show that while precise data slices can be orders of magnitude smaller than imprecise dynamic data slices , for small number of data slicing requests , the lp algorithm is faster than an imprecise dynamic data slicing algorithm proposed by agrawal and horgan .
an enhanced probe based deadlock resolution scheme in distributed database systems . <eos> we suggest a new probe message structure and an efficient probe based deadlock detection and recovery algorithm that can be used in distributed database systems . we determine the characteristics of the probe messages and suggest an algorithm that can reduce the communication cost required for deadlock detection and recovery .
development of high performance operational strategies for polymerization reactor . <eos> this work presents the modeling and control of the temperature of a cstr polymerization reactor connected to a semi flooded horizontal condenser . the objectives of this work were to evaluate the cooling potential and the development of different algorithms to control the reactor temperature where bulk polymerization reactions take place via styrene free radicals . the control algorithms studied were classic pi , long range predictive gpc and predictive adaptive stgpc ( gpc connected to the identification algorithm ' rls ' ) . the results obtained here show that the system provides quick , efficient and highly competitive refrigeration compared with other systems when this system is associated with a control algorithm which adapts perfectly to its dynamics characteristics . the new approach presented in this work , which associates the system ' cstr semi flodded horizontal condenser ' to the control algorithm ( gpc and stgpc ) , makes the process highly competitive regarding refrigeration and performance . ( c ) <digit> elsevier science ltd. all rights reserved .
interfacial stability between zirconium oxide thin films and silicon . <eos> we studied the interfacial properties of zro2 thin films deposited by ultra high vacuum electron beam evaporation ( uhv ebe ) . some samples were annealed in o2 ambient by rapid thermal annealing ( rta ) at different temperatures ranging from <digit> to 700c . x ray photoelectron spectroscopy ( xps ) of all films , whether annealed or not , revealed that the binding energies of zr3d5 <digit> and zr3d3 <digit> are 183.5 and 185.7 ev , respectively , which are the typical peak values of zr4 . x ray diffraction ( xrd ) results showed that the as deposited film was amorphous , and it remained stable up to the annealing temperature of 600c . but when the temperature increased further attaining 700c , it began to crystallize . all the surfaces of the thin films were smooth and uniform . the typical rms roughness ranged from 0.546 to 0.666 nm across an area of <digit> m. steep and clear interfaces between zirconium oxide thin film and si substrate were obtained both by spreading resistance profile ( srp ) and cross sectional transmission electron microscopy ( xtem ) . high quality of the interface without interfacial oxide was achieved when the annealing temperatures were kept under 600c , but when the temperature was raised to 700c , <digit> nm thick oxide product was detected by xtem . the component of the oxide product is not exactly known yet , but may be siox or zrsixoy .
weaknesses and improvements of yang chang hwang 's password authentication scheme . <eos> in <digit> , tseng , jan , and chien proposed an improved version of peyravian zunic 's password authentication scheme based on the diffie hellman scheme . later , yang , chang , and hwang demonstrated that tseng jan chien 's scheme is vulnerable to a modification attack , and then described an improved scheme . in this paper , we show that yang chang hwang 's scheme is still vulnerable to a denial of service attack and a stolen verifier attack . in addition , we also propose an improved scheme with better security .
scalable hardware trojan diagnosis . <eos> hardware trojans ( hts ) pose a significant threat to the modern and pending integrated circuit ( ic ) . due to the diversity of hts and intrinsic process variation ( pv ) in ic design , detecting and locating hts is challenging . several approaches have been proposed to address the problem , but they are either incapable of detecting various types of hts or unable to handle very large circuits . we have developed a scalable ht detection and diagnosis approach that uses segmentation and gate level characterization ( glc ) . we ensure the detection of arbitrary malicious circuitry by measuring the overall leakage current for a set of different input vectors . in order to address the scalability issue , we employ a segmentation method that divides the large circuit into small sub circuits using input vector selection . we develop a segment selection model in terms of properties of segments and their effects on glc accuracy . the model parameters are calibrated by sampled data from the glc process . based on the selected segments we are able to detect and diagnose hts by tracing gate level leakage power . we evaluate our approach on several iscas85 iscas89 itc99 benchmarks . the simulation results show that our approach is capable of detecting and diagnosing hts accurately on large circuits .
linear combinations of radioactive decay models for generational garbage collection . <eos> a program 's distribution of object lifetimes is one of the factors that determines whether and how much it will benefit from generational garbage collection , and from what kind of generational collector . linear combinations of radioactive decay models appear adequate for modelling object lifetimes in many programs , especially when the goal is to analyze the relative or theoretical performance of simple generational collectors . the boundary between models that favor younger first generational collectors and models that favor older first generational collectors is mathematically complex , even for highly idealized collectors . for linear combinations of radioactive decay models , non generational collection is rarely competitive with idealized generational collection , even at that boundary . ( c ) <digit> elsevier b.v. all rights reserved .
mathltwa multiple lapse time window analysis using wolfram mathematica <digit> . <eos> the mathcad <digit> professional code to perform the multiple lapse time analysis ( mltwa ) has been revised and rewritten in mathematica <digit> . the new code contains two new procedures to find the minimum of the misfit function between observation and model and a new example of application to real data from chamoli earthquake aftershock sequence .
a vlsi array processing oriented fast fourier transform algorithm and hardware implementation . <eos> many parallel fast fourier transform ( ffi ) algorithms adopt multiple stages architecture to increase performance . however , data permutation between stages consumes volume memory and processing time . one fft array processing mapping algorithm is proposed in this paper to overcome this demerit . in this algorithm , arbitrary <digit> ( k ) butterfly units ( bus ) could be scheduled to work in parallel on n <digit> ( s ) data ( k <digit> , <digit> _ . . , s <digit> ) . because no inter stage data transfer is required , memory consumption and system latency are both greatly reduced . moreover , with the increasing of bus , not only does throughput increase linearly , system latency also decreases linearly . this array processing orientated architecture provides flexible tradeoff between hardware cost and system performance . in theory , the system latency is ( s x <digit> ( s k ) ) x t ( clk ) . and the throughput is n ( s x 2s k x t ( clk ) ) , where t ( clk ) is the system clock period . based on this mapping algorithm , several <digit> bit word length <digit> point fft processors implemented with tsmc0 .18 mu m cmos technology are given to demonstrate its scalability and high performance . the core area of <digit> bu design is 2.991 x 1.121 mm ( <digit> ) and clock frequency is <digit> mhz in typical condition ( 1.8 v , <digit> degrees c ) . this processor completes <digit> fft calculation in 7.839 ps .
masking based domain extenders for uowhfs bounds and constructions . <eos> we study the class of masking based domain extenders for universal one way hash functions ( uowhfs ) . our first contribution is to show that any correct masking based domain extender for uowhf which invokes the compression uowhf s times must use at least log ( <digit> ) s masks . as a consequence , we obtain the key expansion optimality of several known algorithms among the class of all masking based domain extending algorithms . our second contribution is to present a new parallel domain extender for uowhf . the new algorithm achieves asymptotically optimal speedup over the sequential algorithm and the key expansion is almost everywhere optimal , i.e. , it is optimal for almost all possible number of invocations of the compression uowhf .
robust stability criteria for interval cohengrossberg neural networks with time varying delay . <eos> robust exponential stability for interval cohengrossberg neural networks with time varying delay has received increasing interest in recent years . in this paper , some new criteria are derived using linear matrix inequality , matrix norm and halanay inequality techniques . compared with the existing results , these new criteria are not conservative and are convenient to check . three numerical examples are used to show the effectiveness of the obtained results .
dynamic resource allocation for cdma tdd indoor wireless systems . <eos> future wireless communication systems are expected to provide a broad range of multimedia services that have a significant traffic asymmetry between uplink and downlink . the code division multiple access time division duplex ( cdma tdd ) system is a promising solution to cope with the problem of traffic asymmetry . however , the tdd system is subject to inter cell interference compared to frequency division duplex ( fdd ) system . since both uplink and downlink share the same frequency in tdd , uplink and downlink may interfere each other especially when neighboring cells require different rates of traffic load . thus , the resource allocation among cells is an important issue in tdd . in this paper , the resource allocation in the cdma tdd is formulated as a mixed integer programming ( mip ) problem . a dynamic resource allocation algorithm ( draa ) is provided that effectively solves the traffic asymmetry problem . the mip problem is also solved by a well known branch and bound procedure . both the crossed slot and non crossed slot allocation are examined and compared to the draa . computational result shows that proposed draa gives a good performance as the traffic asymmetry increases between the uplink and downlink . copyright ( c ) <digit> john wiley sons , ltd .
the role of infections in the immunopathogensis of systemic sclerosisevidence from serological studies . <eos> infections are believed to often play a role in the immunopathogenesis of autoimmune disorders such is the case in systemic sclerosis ( ssc ) . in order to evaluate the potential role infections may have on the pathogenesis of ssc , we assessed serological reactivity against various infectious agents in patients with ssc and compared them with healthy controls . serological samples obtained from <digit> patients with ssc were compared with <digit> compatible healthy controls . both groups were of european origin . all samples were tested for the presence of antibodies directed against hepatitis b virus , hepatitis c virus , toxoplasmosis , rubella , cmv , ebv , and treponema pallidum . we applied bio rad commercial and experimental kits to assess most antigens and elisa assays to complete the panel . patients with ssc had elevated igm and igg against toxoplasma gondii and against cmv . higher titers were also detected against the hepatitis b virus core protein ( recombinant hbc antigen ) using monolisa anti hbc plus commercial kit ( bio rad ) . a significantly higher rate of igm antibodies against the capsid antigen of the ebv was detected in ssc patients compared with healthy controls , as well . these data demonstrate that antibodies against cmv , hbv , and toxoplasmosis were detected more often in patients with ssc . this association implies that infectious agents may have a role in disease pathogenesis and expression .
numerical study of the separation characteristics in a cyclone of different inlet particle concentrations . <eos> numerical studies were conducted for different inlet particle concentrations of a cyclone . the simplified eulerian model algebraic slip mixture model ( asmm ) was used for different inlet particle concentrations . with the increase of the inlet particle concentration , particle particle interaction becomes important . asmm can take into account this effect in this condition . the present work considered the particle of different size as different phase , and obtained a collision coefficient by experiment when considering the interaction between particles . the advantage of the asmm is that this model can save a lot of calculating time comparing with the full eulerian model . however , higher inlet particle concentration will result in the agglomeration between particles deduced by some mechanisms . in point of this fact , the calculated results based on asmm have some differences comparing with the practical ones . therefore , this model still has some discrepancies which plead for further improvement . in spite of this fact , this model can be used to analyze the effect of the inlet particle concentration on the separation characteristics of the cyclone qualitatively in practical applications . ( c ) <digit> elsevier ltd. all rights reserved .
the cost of redundancy in distributed real time systems in steady state . <eos> the paper presents considerations that refer to introducing the term cost of redundancy , and indicates general assumptions that allow defining this term and its quantitative determination . however , the cost of redundancy does not apply in this case to financial expenses but only time loses ( temporal cost ) associated with the use of redundancy in industrial real time distributed systems . several computer systems architectures are analyzed from the point of view of that temporal redundancy cost .
turbo equalization techniques toward robust pdm <digit> qam optical fiber transmission . <eos> in this paper , we show numerically and experimentally that turbo equalization ( te ) is an efficient technique to mitigate performance degradations stemming from optical fiber propagation effects in both optical fiber dispersion managed and unmanaged coherent detection links . the effectiveness of the proposed solution can be appreciated in both linear and nonlinear regimes for either scenario . we report on a system employing a polarization division multiplexing ( pdm ) <digit> quadrature amplitude modulation ( qam ) format for which we accomplish an increment in tolerance to link input power of up to <digit> db that represents a substantial improvement margin . the best bit error rate ( ber ) performances will therefore be guaranteed in a larger window , <digit> db , of link input power thanks to the implemented te scheme . moreover , our proposed approach is also proven to effectively mitigate interchannel impairments from surrounding amplitude shift keying interfering channels in a dispersion managed link achieving also in this case an increment in power tolerance of <digit> db . furthermore , in terms of ber performances , our proposed te approach guarantees a gain of about a half order of magnitude at the best operational point . as te can be included in the current coherent detection transceiver technologies and complement other equalization techniques , it has prospects for application in next generation high capacity and long reach optical transmission links .
keep your data safe and available while roaming . <eos> the possibility of accessing and or receiving local or remote data anywhere and at anytime constitutes an important advantage in many business environments . however , when working with mobile devices , users face many problems , such as ( <digit> ) devise exposure problems mobile devices are more vulnerable and fragile than stationary devices , because they can be easily stolen , lost or damaged , ( <digit> ) media problems wireless communications are often unstable , asymmetric and expensive , and ( <digit> ) availability problems mobile devices stay disconnected for long periods of time . to alleviate these problems , we present a service , data lockers , which offers to its users first , the possibility of keeping their data in a secure and safe space in a proxy , thus alleviating the device exposure problem . next , data stored in a data locker are available even when the mobile device is disconnected , thus providing a solution to the availability problem . finally , specific tasks are carried out at the fixed network on behalf of the mobile user , in this way relieving the media problem . the architecture of the locker rental service is based on mobile agents . these agents , and the locker , stay always close to the location of the user , traveling to meet the user wherever the user moves , therefore , allowing users to use anywhere anytime , ubiquitous persistent storage space located at the fixed network .
an efficient algorithm for gossiping in the multicasting communication environment . <eos> we present an algorithm for the gossiping problem defined over an n processor communication network , n , where message multicasting is allowed . the algorithm generates a communication schedule with a total communication time at most n r , where r is the radius of the network . our algorithm begins by constructing a spanning tree ( or tree network t ) with the least possible radius . then , all the communications are carried out in the tree network as follows each processor waits its turn to transmit almost consecutively to its parent and children all the messages in its subtree . during other times , each processor transmits to its children all the messages emanating elsewhere in the network .
iterative gradient based complex divider fpga core with dynamic configurability of accuracy and throughput . <eos> a field programmable gate array ( fpga ) implementation of a highly configurable complex divider is presented , based on an iterative gradient algorithm . the proposed architecture allows to configure both the accuracy and the throughput of the division operation , which makes it suitable for diverse applications with different requirements . results show how various throughputs can be achieved under different maximum error and iteration limit configurations . besides , the resource occupation is considerably small , compared with previous solutions .
molecular dynamics simulations studies and free energy analysis on inhibitors of mdm2p53 interaction . <eos> the md simulations and mm gbsa analyze the differences of three potent inhibitors binding to mdm2 . the residue located at <digit> is also important in inhibitormdm2 binding except f19 , w23 and l26 . conformation variability of residue y100 influences the binding conformation of inhibitormdm2 complex .
energy and throughput aware fuzzy logic based reconfiguration for mpsocs . <eos> multicore architectures offer an amount of parallelism that is often underutilized , as a result these underutilized resources become a liability instead of advantage . inefficient resource sharing on the chip can have a negative impact on the performance of an application and may result in greater energy consumption . a large body of research now focuses on reconfigurable multicore architectures in order to support algorithms to find optimal solutions for improved energy and throughput balance . an ideal system would be able to optimize such reconfigurable systems to a level that optimum resources are allocated to a particular workload and all the other underutilized resources remain inactive for greater energy savings . this paper presents a fuzzy logic based reconfiguration engine targeted to optimize a multicore architecture according to the workload requirements for optimum balance between power and performance of the system . the proposed fuzzy logic reconfiguration engine is designed around a <digit> core scmp architecture comprising of reconfigurable cache memories , power gated cores and adaptive on chip network routers for minimizing leakage energy effects for inactive components . a coarse grained architecture was selected for being able to reconfigure faster , thus making it feasible to be used for runtime adaptation schemes . the presented architecture is analyzed over a set of openmp based parallel benchmarks and results show significant energy savings in all cases .
resource brokering using a multi site resource allocation strategy for computational grids . <eos> grid computing employs heterogeneous resources which may be installed on different platforms , hardware software , computer architectures , and perhaps using different computer languages to solve large scale computational problems . as many more grids are being developed worldwide , the number of multi institutional collaborations is growing rapidly . however , to realize grid computing 's full potential , it is expected that grid participants must be able to share one another 's resources . this paper presents a resource broker that employs the multi site resource allocation ( msra ) strategy and the dynamic domain based network information model that we propose to allocate grid resources to submitted jobs , where the grid resources may be dispersed at different sites , and owned and governed by different organizations or institutes . the jobs and resources may also belong to different clusters sites . resource statuses collected by the ganglia , and network bandwidths gathered by the network weather service , are both considered in the proposed scheduling approach . a dynamic domain based model for network information measurement is also proposed to choose the most appropriate resources that meet the jobs ' execution requirements . experimental results show that msra outperformed the other tested strategies . copyright ( c ) <digit> john wiley sons , ltd .
approximation properties of the modification of kantorovich type q szasz operators . <eos> in this paper , we introduce a generalization of modification of kantorovich type q szasz operators if k n , k q based on the concept of q integral , these operators are different from those proposed by mahmudov and gupta mahmudov , n. , gupta , v. on certain q analogue of szasz kantorovich operators , j. appl . math . comput , 10.1007 s12190 <digit> <digit> <digit> ( <digit> ) . we investigate weighted statistical approximation properties and establish a local approximation theorem , we also give a convergence theorem for the lipschitz continuous functions . furthermore , we give the relationship between the derivative of q szasz mirakjan operators and the operators k n , k q.
the exact number of squares in fibonacci words . <eos> all our words ( sequences ) are binary . a square is a subword of the form uu ( concatenation ) . two squares are distinct if they are of different shape , not just translates of each other . otherwise they are repeated fibonacci words are defined by f ( <digit> ) <digit> , f ( <digit> ) <digit> , f ( m ) f ( m <digit> ) f ( m <digit> ) for m greater than or equal to <digit> . let f m f ( m ) . then f <digit> <digit> , f <digit> <digit> , f m fm <digit> fm <digit> ( m greater than or equal to <digit> ) are the fibonacci numbers . let d ( n ) and r ( n ) be the exact number of distinct and repeated squares , respectively , in f ( n ) . we prove d ( n ) <digit> ( fn <digit> <digit> ) ( n greater than or equal to <digit> ) , which implies , asymptotically , d ( n ) <digit> ( <digit> phi ) f n o ( <digit> ) ( <digit> ( <digit> phi ) approximate to 0.7639 ) , where phi is the golden section . we also prove r ( n ) <digit> 5nf ( n ) <digit> <digit> ( n <digit> ) fn <digit> fn <digit> n <digit> ( n greater than or equal to <digit> ) . this yields r ( n ) <digit> <digit> ( <digit> phi ) nf ( n ) o ( f n ) ( <digit> ( <digit> phi ) ( <digit> log ( <digit> ) phi ) ) f n log ( <digit> ) f n o ( f n ) ( <digit> ( <digit> phi ) ( <digit> log ( <digit> ) phi ) approximate to 0.7962 ) . ( c ) <digit> elsevier science b.v. all rights reserved .
non linear temporal scaling of surgical processes . <eos> simultaneous and global analysis of sets of surgeries . global non linear realignment of sets of sequences . comparative analysis of surgeons expertise on sets of neurosurgeries .
design of compact pentagonal slot antenna with bandwidth enhancement for multiband wireless applications . <eos> the compact design of a microstrip feed pentagonal shaped slot antenna with electromagnetically coupled pentagon parasitic patch is proposed in this paper for bandwidth enhancement . an aligned self similarity pentagonal shaped structures , for slot and parasitic patch , at its vertex are used to broaden the bandwidth . the proposed antenna gives a wide bandwidth of 4.17 ghz ( 3.2817.45 ghz ) , which corresponds to frb 77.72 % . the rotational behaviour of slot antenna illustrates good impedance matching over the wide bandwidth . over the entire operating bandwidth , the antenna gain remains constant about 4.24 dbi and group delay is less than 0.5 ns . the simulated and measured results show its use in wireless applications .
improved hash functions for cancelable fingerprint encryption schemes . <eos> in order to solve the prealignment problem , this paper constructs new fingerprint features which will not change along with the rotation of fingerprint image . and then the sha1 algorithm is improved in four aspects the modification of original register values , the modification of additive constants , the modification of logic functions , the modification of compression functions , then this paper improves the sha2 algorithm in two aspects the modification of compression functions and the modification of message word . based on that , two cancelable and irreversible fingerprint encryption schemes are proposed in this paper . the efficiency analysis and security authentication show that these schemes have enhanced the security without increasing the complexity .
financial time series forecasting using independent component analysis and support vector regression . <eos> as financial time series are inherently noisy and non stationary , it is regarded as one of the most challenging applications of time series forecasting . due to the advantages of generalization capability in obtaining a unique solution , support vector regression ( svr ) has also been successfully applied in financial time series forecasting . in the modeling of financial time series using svr , one of the key problems is the inherent high noise . thus , detecting and removing the noise are important but difficult tasks when building an svr forecasting model . to alleviate the influence of noise , a two stage modeling approach using independent component analysis ( ica ) and support vector regression is proposed in financial time series forecasting . ica is a novel statistical signal processing technique that was originally proposed to find the latent source signals from observed mixture signals without having any prior knowledge of the mixing mechanism . the proposed approach first uses ica to the forecasting variables for generating the independent components ( ics ) . after identifying and removing the ics containing the noise , the rest of the ics are then used to reconstruct the forecasting variables which contain less noise and served as the input variables of the svr forecasting model . in order to evaluate the performance of the proposed approach , the nikkei <digit> opening index and taiex closing index are used as illustrative examples . experimental results show that the proposed model outperforms the svr model with non filtered forecasting variables and a random walk model .
a structural and functional model for human bone sialoprotein . <eos> human bone sialoprotein ( bsp ) is an essential component of the extracellular matrix of bone . it is thought to be the primary nucleator of hydroxyapatite crystallization , and is known to bind to hydroxyapatite , collagen , and cells . mature bsp shows extensive post translational modifications , including attachment of glycans , sulfation , and phosphorylation , and is highly flexible with no specific 2d or 3d structure in solution or the solid state . these features have severely limited the experimental characterization of the structure of this protein . we have therefore developed a 3d structural model for bsp , based on the available literature data , using molecular modelling techniques . the complete model consists of <digit> amino acids , including six phosphorylated serines and two sulfated tyrosines , plus <digit> n and o linked glycan residues . a notable feature of the model is a large acidic patch that provides a surface for binding ca2 ions . density functional theory quantum calculations with an implicit solvent model indicate that ca2 ions are bound most strongly by the phosphorylated serines within bsp , along with reasonably strong binding to asp and glu , but weak binding to his and sulfated tyrosine . the process of early hydroxyapatite nucleation has been studied by molecular dynamics on an acidic surface loop of the protein the results suggest that the cationic nature of the loop promotes nucleation by attracting ca2 ions , while its flexibility allows for their rapid self assembly with po43 ions , rather than providing a regular template for crystallization . the binding of a hydroxyapatite crystal at the protein 's acidic patch has also been modelled . the relationships between hydroxyapatite , collagen and bsp are discussed .
combinatorial batch codes . <eos> in this paper , we study batch codes , which were introduced by ishai , kushilevitz , ostrovsky and sahai in <digit> . a batch code specifies a method to distribute a database of n items among m devices ( servers ) in such a way that any k items can be retrieved by reading at most t items from each of the servers . it is of interest to devise batch codes that minimize the total storage , denoted by n , over all m servers . we restrict out attention to batch codes in which every server stores a subset of the items . this is purely a combinatorial problem , so we call this kind of batch code a combinatorial batch code . we only study the special case t <digit> , where , for various parameter situations , we are able to present batch codes that are optimal with respect to the storage requirement , n. we also study uniform codes , where every item is stored in precisely c of the m servers ( such a code is said to have rate <digit> c ) . interesting new results are presented in the cases c <digit> , k <digit> and k <digit> . in addition , we obtain improved existence results for arbitrary fixed c using the probabilistic method .
a fast algorithm for the path <digit> packing problem . <eos> let g be an undirected graph and t t <digit> , ... , t k be a collection of disjoint subsets of nodes . nodes in t <digit> boolean or ... boolean or t k are called terminals , other nodes are called inner . by a t path we mean a path p such that p connects terminals from distinct sets in t and all internal nodes of p are inner . we study the problem of finding a maximum cardinality collection p of t paths such that at most two paths in p pass through any node . our algorithm is purely combinatorial and has the time complexity o ( mn ( <digit> ) ) , where n and m denote the numbers of nodes and edges in g , respectively .
higher level feature combination via multiple kernel learning for image classification . <eos> feature combination is an effective way for image classification . most of the work in this line mainly considers feature combination based on different low level image descriptors , while ignoring the complementary property of different higher level image features derived from the same type of low level descriptor . in this paper , we explore the complementary property of different image features generated from one single type of low level descriptor for image classification . specifically , we propose a soft salient coding ( ssac ) method , which overcomes the information suppression problem in the original salient coding ( sac ) method . we analyse the physical meaning of the ssac feature and the other two types of image features in the framework of spatial pyramid matching ( spm ) , and propose using multiple kernel learning ( mkl ) to combine these features for classification tasks . experiments on three image databases ( caltech <digit> , uiuc <digit> sports and <digit> scenes ) not only verify the effectiveness of the proposed mkl combination method , but also reveal that collaboration is more important than selection for classification when limited types of image features are employed .
evaluation of an architecture for intelligent query and exploration of time oriented clinical data . <eos> evaluate knave ii , a knowledge based framework for visualization , interpretation , and exploration of longitudinal clinical data , clinical concepts and patterns . knave ii mediates queries to a distributed temporal abstraction architecture ( idan ) , which uses a knowledge based problem solving method specializing in on the fly computation of clinical queries . a two phase , balanced cross over study to compare efficiency and satisfaction of a group of clinicians when answering queries of variable complexity about time oriented clinical data , typical for oncology protocols , using knave ii , versus standard methods both paper charts and a popular electronic spreadsheet ( ess ) in phase i an ess in phase ii . the measurements included the time required to answer and the correctness of answer for each query and each complexity category , and for all queries , assessed versus a predetermined gold standard set by a domain expert . user satisfaction was assessed by the standard usability score ( sus ) tool specific questionnaire and by a usability of tool comparison comparative questionnaire developed for this study . in both evaluations , subjects answered higher complexity queries significantly faster using knave ii than when using paper charts or an ess up to a mean of 255s difference per query versus the ess for hard queries ( p 0.0003 ) in the second evaluation . average correctness scores when using knave ii versus paper charts , in the first phase , and the ess , in the second phase , were significantly higher over all queries . in the second evaluation , 91.6 % ( <digit> <digit> ) of all of the questions asked within queries of all levels produced correct answers using knave ii , opposed to only 57.5 % ( <digit> <digit> ) using the ess ( p < 0.0001 ) . user satisfaction with knave ii was significantly superior compared to using either a paper chart or the ess ( p 0.006 ) . clinicians ranked knave ii superior to both paper and the ess . an evaluation of the functionality and usability of knave ii and its supporting knowledge based temporal mediation architecture has produced highly encouraging results regarding saving of physician time , enhancement of accuracy of clinical assessment , and user satisfaction .
parallel computation for spherical harmonic synthesis and analysis . <eos> the objectives of this research were to develop computational techniques for parallel computers capable of performing computations of spherical harmonic analysis and synthesis for ultra high degree and order . the fft method was used as a basis for the numerical scheme . the numerical stability of computing associated legendre functions and their integrals with both double and quadruple precision was investigated . the domain decomposition and collective communication strategy was used to achieve parallelism and was implemented by message passing . the experimental results showed that the parallel computations significantly shortened computation time and obtained a good implementation with high parallel efficiency .
a comparison of three total variation based texture extraction models . <eos> this paper qualitatively compares three recently proposed models for signal image texture extraction based on total variation minimization the meyer <digit> , veseosher ( vo ) <digit> , and tv l1 <digit> , <digit> , <digit> , <digit> , <digit> , <digit> , <digit> and <digit> models . we formulate discrete versions of these models as second order cone programs ( socps ) which can be solved efficiently by interior point methods . our experiments with these models on 1d oscillating signals and 2d images reveal their differences the meyer model tends to extract oscillation patterns in the input , the tv l1 model performs a strict multiscale decomposition , and the veseosher model has properties falling in between the other two models .
do n't be greedy when calculating hypervolume contributions . <eos> most hypervolume indicator based optimization algorithms like sibea zitzler et al. <digit> , sms emoa beume et al. <digit> , or mo cma es igel et al. <digit> remove the solution with the smallest loss with respect to the dominated hypervolume from the population . this is usually iterated times until the size of the population no longer exceeds a fixed size . we show that there are populations such that the contributing hypervolume of the solutions chosen by this greedy selection scheme can be much smaller than the contributing hypervolume of an optimal set of solutions . selecting the optimal set implies calculating ( over ) conventional hypervolume contributions , which is considered computationally too expensive . we present the first hypervolume algorithm which calculates directly the contribution of every set of solutions . this gives an additive term of ( over ) in the runtime of the calculation instead of a multiplicative factor of binomial ( over ) . given a population of size n , our algorithm can calculate a set of solutions with minimal d dimensional hypervolume contribution in time o ( n d <digit> log n ) for d > <digit> . this improves all previously published algorithms by a factor of order n min ( , , d <digit> for d > <digit> . therefore even if we remove the solutions one by one greedily as usual , we gain a speedup factor of n for all d > <digit> .
communication latency tolerant parallel algorithm for particle swarm optimization . <eos> particle swarm optimization ( pso ) algorithm is a population based algorithm for finding the optimal solution . because of its simplicity in implementation and fewer adjustable parameters compared to the other global optimization algorithms , pso is gaining attention in solving complex and large scale problems . however , pso often requires long execution time to solve those problems . this paper proposes a parallel pso algorithm , called delayed exchange parallelization , which improves performance of pso on distributed environment by hiding communication latency efficiently . by overlapping communication with computation , the proposed algorithm extracts parallelism inherent in pso . the performance of our proposed parallel pso algorithm was evaluated using several applications . the results of evaluation showed that the proposed parallel algorithm drastically improved the performance of pso , especially in high latency network environment .
design of two channel quadrature mirror filter bank using particle swarm optimization . <eos> in this paper , a particle swarm optimization ( pso ) is used to design the quadrature mirror filter ( qmf ) banks with linear phase in frequency domain . a unique pso is developed to optimize filter bank coefficients to match the ideal system response . the three main attributes used in assessing the performance of filter are reconstruction error , mean square error in pass band and mean square error in stop band . the proposed pso based method produces almost the same result as that of the existing methods . as compared to the existing methods this method is very simple to implement for the qmf bank optimization . to implement the proposed pso algorithm , a matlab program is developed and three examples have been presented to illustrate the performance of the proposed method .
globfit consistently fitting primitives by discovering global relations . <eos> given a noisy and incomplete point set , we introduce a method that simultaneously recovers a set of locally fitted primitives along with their global mutual relations . we operate under the assumption that the data corresponds to a man made engineering object consisting of basic primitives , possibly repeated and globally aligned under common relations . we introduce an algorithm to directly couple the local and global aspects of the problem . the local fit of the model is determined by how well the inferred model agrees to the observed data , while the global relations are iteratively learned and enforced through a constrained optimization . starting with a set of initial ransac based locally fitted primitives , relations across the primitives such as orientation , placement , and equality are progressively learned and conformed to . in each stage , a set of feasible relations are extracted among the candidate relations , and then aligned to , while best fitting to the input data . the global coupling corrects the primitives obtained in the local ransac stage , and brings them to precise global alignment . we test the robustness of our algorithm on a range of synthesized and scanned data , with varying amounts of noise , outliers , and non uniform sampling , and validate the results against ground truth , where available .
an efficient decomposition and dual stage multi objective optimization method for water distribution systems with multiple supply sources . <eos> incorporate graph decomposition for multi objective optimization of water networks . develop a two stage algorithm for multi objective optimization of water networks . use real world networks up to <digit> decision variables to verify the proposed method . proposed method shows great efficiency in terms of finding parent fronts . provide an efficient decision making tool for the water network optimization .
enhancing decision combination of face and fingerprint by exploitation of individual classifier space an approach to multi modal biometry . <eos> this paper presents a new approach to combine decisions from face and fingerprint classifiers for multi modal biometry by exploiting the individual classifier space on the basis of availability of class specific information present in the classifier space . we exploit the prior knowledge by training the face classifier using response vectors on a validation set , enhancing class separability ( using parametric and nonparametric linear discriminant analysis ) in the classifier output space and thereby improving the performance of the face classifier . fingerprint classifier often does not provide this information due to high sensitivity of available minutiae points , producing partial matches across subjects . the enhanced face and fingerprint classifiers are combined using a sum rule . we also propose a generalized algorithm for multiple classifier combination ( mcc ) based on our approach . experimental results show superiority of the proposed method over other existing fusion techniques , such as sum , product , max , min rules , decision template and dempstershafer theory .
piggybacking related domain names to improve dns performance . <eos> in this paper , we present a novel approach to exploit the relationships among domain names to improve the cache hit rate for a local dns server . using these relationships , an authoritative dns server ( adns ) can piggyback resolutions for future queries as part of the response message for an initial query . the approach improves the cache hit rate as well as reducing the total queries and responses . the approach is particularly attractive because it can be implemented with no changes to the existing dns protocol . trace based simulations show more than <digit> % of cache misses can be reduced in the best case while straightforward policies , using frequency and relevancy data for an adns , reduce cache misses by <digit> % and dns traffic by <digit> % . these percentages improve if we focus the policies on resource records with smaller authoritative ttls . we also show improved performance for hybrid approaches that combine the approach with renewal based approaches . in conjunction with this work we also did a study on current dns performance for <digit> locations in the united states . the outcome of this study is that the current average dns latency is generally in the range of 200300ms , but range from 500ms to multiple seconds if we look at the <digit> % response time . approaches , such as what we propose , that reduce the amount of dns traffic will improve the overall response time for applications .
getting away with murder how does the bcl <digit> family of proteins kill with immunity . <eos> the adult human body produces approximately one million white blood cells every second . however , only a small fraction of the cells will survive because the majority is eliminated through a genetically controlled form of cell death known as apoptosis . this review places into perspective recent studies pertaining to the bcl <digit> family of proteins as critical regulators of the development and function of the immune system , with particular attention on b cell and t cell biology . here we discuss how elegant murine model systems have revealed the major contributions of the bcl <digit> family in establishing an effective immune system . moreover , we highlight some key regulatory pathways that influence the expression , function , and stability of individual bcl <digit> family members , and discuss their role in immunity . from lethal mechanisms to more gentle ones , the final portion of the review discusses the nonapoptotic functions of the bcl <digit> family and how they pertain to the control of immunity .
co <digit> plex vertex partitions . <eos> this paper studies co k plex vertex partitions and more specifically co <digit> plex vertex partitions . co ( k ) plexes and ( k ) plexes were first introduced in <digit> in the context of social network analysis . however , the study of co k plex vertex partitions or decomposing a graphs into degree bounded subgraphs can be at least dated back to the work of lovasz ( studia sci math hung <digit> <digit> , <digit> ) . in this paper , we derive analogues for well known results on the chromatic number , and present two algorithms for constructing co <digit> plex vertex partitions . the first algorithm minimizes the number of partition classes while the second algorithm minimizes a weighted sum of the partition classes , where the weight of a partition class depends on the level of adjacency among its vertices .
a reliable and efficient broadcasting scheme for mobile ad hoc networks . <eos> this letter proposes a busy tone based scheme for reliable and efficient broadcasting in mobile ad hoc networks . control packets such as rts , cts and ack are ignored in the broadcast scheme , and two busy tones are used , one for channel reservation and the other for negative acknowledgement . unlike traditional schemes for reliable broadcasting . the proposed scheme is highly efficient as it achieves both collision avoidance and fast packet loss recovery . simulation results . are presented which show the effectiveness of the proposed scheme .
frequent items in streaming data an experimental evaluation of the state of the art . <eos> the problem of detecting frequent items in streaming data is relevant to many different applications across many domains . several algorithms , diverse in nature , have been proposed in the literature for the solution of the above problem . in this paper , we review these algorithms , and we present the results of the first extensive comparative experimental study of the most prominent algorithms in the literature . the algorithms were comprehensively tested using a common test framework on several real and synthetic datasets . their performance with respect to the different parameters ( i.e. , parameters intrinsic to the algorithms , and data related parameters ) was studied . we report the results , and insights gained through these experiments .
a video human computer interface to record paintings in progress . <eos> this communication describes a system to capture paintings in progress . an adapted algorithm based on statistical foreground substraction is used to mask the artist hand and his tools in image sequences . this allows a better understanding of the artist gesture and his mind .
performance optimized floor planning by graph planarization . <eos> a new procedure for vlsi floor planning that minimizes routing parasitics is presented . the procedure , based on rectangular dualization , maximizes adjacency of modules that are heavily connected or connected by critical nets . wiring macros are introduced to provide routing area for those modules that can not be located adjacent to one another these macros are located by planarizing the system interconnectivity graph using an edge crossing strategy that minimizes the cost of intersection . the rectangular dual is compacted using heuristics to approximate a quadratic area constraint by one or more linear constraints , thereby reducing the complexity of compaction from that of quadratic programming to linear programming .
the effectiveness of computerized drug lab alerts a systematic review and meta analysis . <eos> there is no evidence that computerized drug lab alerts improve clinically important outcomes . process outcomes improved but without evidence of clinical benefit . no studies examined cost effectiveness . stronger evidence is needed prior to widespread implementation in electronic medical records .
cell based approaches for cardiac repair . <eos> many forms of cardiovascular disease are associated with cardiomyocyte loss via necrosis and or apoptosis . the cumulative loss of contractile cells ultimately results in diminished cardiac function . numerous approaches have been employed to reduce the rate of cardiomyocyte loss , or alternatively , to repopulate the heart with new cardiomyocytes . strategies aimed at repopulating the heart include cardiomyocyte cell therapy , myogenic stem cell therapy , and cell cycle activation therapy . all three approaches are based on the assumption that the de novo cardiomyocytes will participate in a functional syncytium with the surviving myocardium . this review will discuss the current status of interventions aimed at repopulating the heart with functional cardiomyocytes
crossings embedding personal professional knowledge in a complex online community environment . <eos> purpose this paper aims to investigate how online communities of practice facilitate the embedding of personal professional knowledge in a complex online environment . design methodology approach this research consisted of exploratory , interpretivist case research , using qualitative methods . forty one individuals from five online communities in a national professional development programme were interviewed . additional data were drawn from diverse online records . data were coded via text analysis . a wiki was used for participant feedback . findings embedding of new knowledge was facilitated by individuals ' crossings between different engagement spaces communication and sense making contexts . community members repeatedly crossed between online and offline , visible and invisible , formal and informal , and reflective and active engagement spaces as they sought to meet diverse needs . as they did this , they had to continually recontextualise knowledge , adapting , varying and personalising it to fit the function , genre and conventions of each engagement space . this promoted the embedding of professional knowledge . the complex online environment in which they operated can be seen as providing a situation of enhanced polycontextuality , within which multiple boundary crossings facilitated strong personalisation . at the community level , knowledge convergence was fostered by the recurrence of dominant , powerful mnemonic themes . research limitations implications an opportunity exists to investigate the applicability of these findings in other online professional contexts . originality value the paper extends the concept of boundary crossing to crossings in a polycontextual online environment . it updates literature on communities of practice by outlining the dynamics of a complex online community system . it provides an explanation for how personal knowledge evolves to fit emerging trends and considers how information systems can support deep knowledge transfer .
power optimization of wireless media systems with space time block codes . <eos> we present analytical and numerical solutions to the problem of power control in wireless media systems with multiple antennas . we formulate a set of optimization problems aimed at minimizing total power consumption of wireless media systems subject to a given level of qos and an available bit rate . our formulation takes in to consideration the power consumption related to source coding , channel coding , and transmission of multiple transmit antennas . in our study , we consider gauss markov and video source models , rayleigh fading channels along with the bernoulli gilbert elliott loss models , and space time block codes .
speculative supplier identification for reducing power of interconnects in snoopy cache coherence protocols . <eos> in this work we reduce interconnect power dissipation in symmetric multiprocessors or smps . we revisit snoopy cache coherence protocols and reduce unnecessary interconnect activity by speculating nodes expected to provide a missing data . conventional snoopy cache coherence protocols broadcast requests to all nodes , reducing the latency of cache to cache transfer misses at the expense of increasing interconnect power . we show that it is possible to reduce the associated power dissipation if such requests are broadcasted selectively and only to nodes more likely to provide the missing data . we reduce power as we limit access only to the interconnect components between the requester and the supplier node . we evaluate our technique using shared memory applications and show that it is possible to reduce interconnect power by <digit> % in a <digit> way multiprocessor without compromising performance . this comes with negligible hardware overhead .
array pattern optimization using electromagnetism like algorithm . <eos> this paper proposes a design method of array pattern optimization for sidelobe level ( sll ) reduction . the element positions and their weights are determined by electromagnetism like ( em ) like algorithm . simulation results show that the em like algorithm is efficient in sll reduction for beamwidth restriction . synthesis of array pattern using em like algorithm is very straightforward and efficient . it does not require any gradient operation and is very easy in both formulation and programming . therefore , the em like algorithm can be applied to many other applications in array signal processing .
representation theory of goguen categories . <eos> goguen categories constitute a suitable algebraic formalisation for l fuzzy relations . it is well known that an y fuzzy relation may be represented by the set of all its a cuts . the aim of this paper is to show a similar result for goguen categories . furthermore , given an algebraic structure of relations , a dedekind category r , and a complete brouwerian lattice l , the idea above allows us to define a goguen category g such that the underlying structures are r and l. using our pseudo representation theorem we show that the representation theory of goguen categories is equivalent to the representation theory of simple dedekind categories . this result allows us to transfer known representation results for dedekind categories to the theory of goguen categories . ( c ) <digit> elsevier b.v. all rights reserved .
chemistry beyond positivism . <eos> chemistry is often thought to be quite factual , and therefore might be considered close to the positivist ideal of a value free science . a closer look , however , reveals that the field is coupled to the invisible realm of values , meanings , and purpose in various ways , and chemists interact with that realm loosely and unevenly . tacit knowledge is one important locus of such interactions . we are concerned in this essay with two questions . what is the nature of the knowledge when we are in the early stages of discovery and in what ways does the hidden reality we are seeking affect our search for an understanding of it the first question is partly answered by polanyi 's theory of tacit knowledge , while the second one leads us to realize the limitations of our language when discussing realityor certain chemical experimental results . a strictly positivist approach is of little use , but so is the opposite , the complete disregard of facts . the contrast between positivism and non formulable aspects of scientific reasoning amounts to a paradox that needs to be analyzed and can lead to a connected chemistry . this in turn resembles networks described by schweber and is more concerned than the chemistry as it is with aspects such as the image of chemistry , the challenges chemists face as citizens , and chemistry in liberal education .
on bent functions with some symmetric properties . <eos> this paper discusses a kind of bent functions that have some symmetric properties about some variables . section <digit> mainly discusses the bent functions symmetric about some two variables and gives the necessary and sufficient condition for these functions . section <digit> gives algebraic expressions of some bent functions .
solvability of infinite systems of singular integral equations in frchet space of continuous functions . <eos> the aim of this paper is to show how some measures of noncompactness in the frchet space of continuous functions defined on an unbounded interval can be applied to an infinite system of singular integral equations . the results obtained generalize and improve several ones .
detection of arbitrary planar shapes with 3d pose . <eos> in this work , two new methods to detect objects under perspective and scaled orthographic projection are shown . they also calculate the parameters of the transformations the object has undergone . the methods are based on the use of the generalized hough transform ( ght ) that compares a template with a projected image . the computational requirements of the algorithms are reduced by restricting the transformation to the image edge points and using invariant information during the comparison process . moreover , a multipass design of the algorithms speeds up the parameter calculations .
speechbot an experimental speech based search engine for multimedia content on the web . <eos> as the web transforms from a text only medium into a more multimedia rich medium , the need arises to perform searches based on the multimedia content . in this paper , we present an audio and video search engine to tackle this problem . the engine uses speech recognition technology to index spoken audio and video files from the world wide web ( www ) when no transcriptions are available . if transcriptions ( even imperfect ones ) are available , we can also take advantage of them to improve the indexing process . our engine indexes several thousand talk and news radio shows covering a wide range of topics and speaking styles from a selection of public web sites with multimedia archives . our web site is similar in spirit to normal web search sites it contains an index , not the actual multimedia content . the audio from these shows suffers in acoustic quality due to bandwidth limitations , coding , compression , and poor acoustic conditions . our word error rate ( wer ) results using appropriately trained acoustic models show remarkable resilience to the high compression , although many factors combine to increase the average wers over standard broadcast news benchmarks . we show that , even if the transcription is inaccurate , we can still achieve good retrieval performance for typical user queries ( 77.5 % ) .
topological sensitivity derivative and finite topology modifications application to optimization of plates in bending . <eos> the concept of topological sensitivity derivative is introduced and applied to study the problem of optimal design of structures . it is assumed , that virtual topology variation is described by topological parameters . the topological derivative provides the gradients of objective functional and constraints with respect to these parameters . this derivative enables formulation of the conditions of topology transformation . in this paper formulas for the topological sensitivity derivative for bending plates are derived . next , the topological derivative is used in the optimization process in order to formulate conditions of finite topology modifications and in order to localize positions of the modifications . in the case of plates they are related to introduction of holes and introduction of stiffeners . the theoretical considerations are illustrated by some numerical examples .
effects of pituitary adenylate cyclase activating polypeptide , vasoactive intestinal polypeptide , and somatostatin on the release of thyrotropin from the bullfrog pituitary . <eos> the recent development of a specific radioimmunoassay for amphibian ( bullfrog , rana catesbeiana ) thyrotropin ( tsh ) has made it possible to study the effects of various neuropeptides on the release of tsh from the pituitary in vitro . up to now , corticotropin releasing factor of bullfrog origin has been shown to have a potent tsh releasing activity , whereas gonadotropin releasing hormone and tsh releasing hormone exhibit a moderate tsh releasing effect on the adult , but not larval , pituitary . in the present study , the effects of pituitary adenylate cyclase activating polypeptide ( pacap ) , vasoactive intestinal polypeptide ( vip ) , and somatostatin ( ss ) on the in vitro release of tsh from the bullfrog pituitary were investigated . both frog ( r. ridibunda ) pacap <digit> and pacap <digit> caused a concentration dependent stimulation of the release of tsh from dispersed pituitary cells during a <digit> h culture . the pacap <digit> and pacap <digit> induced tsh release was suppressed by a simultaneous application of pacap638 . application of high concentrations of pacap638 alone caused a slight but significant stimulatory effect on the release of tsh . frog vip also stimulated tsh release from pituitary cells concentration dependently . frog ss1 ( homologous to mammalian somatostatin <digit> ) and ss2 ( homologous to mammalian cortistatin ) did not affect the basal release of tsh but caused a concentration dependent suppression of the pacap <digit> induced release of tsh . these results suggest the involvement of multiple neuropeptides in the regulation of the release of tsh from the amphibian pituitary
energy efficient design of portable wireless systems . <eos> portable wireless systems require long battery lifetime while still delivering high performance . the major contribution of this work is combining new it power management ( pm ) and it power control ( pc ) algorithms to trade off performance for power consumption at the system level in portable devices . first we present the formulation for the solution of the pm policy optimization based on renewaltheory . next we present the formulation for power control ( pc ) of the wireless link that enables us to obtain further energy savings when thesystem is active . finally , we discuss the measurements obtained for a set of pm and pc algorithms implemented for the wlan card on a laptop . the pm policy we developed based on our renewal model consumes three times less power as compared to the default pm policy for the wlan card with still high performance . power control saves additional <digit> % in energy at same bit error rate . with both power control and power management algorithms in place , we observe on average a factor of six in power savings .
mining cultural differences from a large number of geotagged photos . <eos> we propose a novel method to detect cultural differences over the world automatically by using a large amount of geotagged images on the photo sharingweb sites such as flickr . we employ the state of the art object recognition technique developed in the research community of computer vision to mine representative photos of the given concept for representative local regions from a large scale unorganized collection of consumer generated geotagged photos . the results help us understand how objects , scenes or events corresponding to the same given concept are visually different depending on local regions over the world .
robust adaptive photon tracing using photon path visibility . <eos> we present a new adaptive photon tracing algorithm which can handle illumination settings that are considered difficult for photon tracing approaches such as outdoor scenes , close ups of a small part of an illuminated region , and illumination coming through a small gap . the key contribution in our algorithm is the use of visibility of photon path as the importance function which ensures that our sampling algorithm focuses on paths that are visible from the given viewpoint . our sampling algorithm builds on two recent developments in markov chain monte carlo methods adaptive markov chain sampling and replica exchange . using these techniques , each photon path is adaptively mutated and it explores the sampling space efficiently without being stuck at a local peak of the importance function . we have implemented this sampling approach in the progressive photon mapping algorithm which provides visibility information in a natural way when a photon path contributes to a measurement point . we demonstrate that the final algorithm is strikingly simple , yet effective at sampling photons under lighting conditions that would be difficult for existing monte carlo ray tracing based methods .
representing older people towards meaningful images of the user in design scenarios . <eos> designing for older people requires the consideration of a range of design problems , which may be related to difficult and sometimes highly personal matters . issues such as fear , loneliness , dependency , and physical decline may be hard to observe or discuss in interviews . pastiche scenarios and pastiche personae are techniques that employ characters to create a space for the discussion of new technological developments and user experience . this paper argues that the use of fictional characters can help to overcome restrictive notions of older people by disrupting designers prior assumptions . in this paper , we reflect on our experiences using pastiche techniques in two separate technology design projects that sought to address the needs of older people . in the first pastiche scenarios were developed by the designers of the system and used as discussion documents with users . in the second pastiche personae were used by groups of users themselves to generate scenarios which were scribed for later use by the design team . we explore how the use of fictional characters and settings can generate new ideas and undercut the potential in scenarios , for weak characterisation of the user to permit scenario writers to fit characters to technology rather than vice versa . to assist in future development of pastiche techniques in designing for older people , we provide an array of fictional older characters drawn from literary and popular culture .
supportive decision making at the point of care refinement of a case based reasoning application for use in nursing practice . <eos> variations in nursing care have been observed , affecting patient outcomes and quality of care . case based reasoners that benchmark for patient indicators can reduce variation through decision support . this study evaluated and validated a case based reasoning application to establish benchmarks for nursing sensitive patient outcomes of pain , fatigue , and toilet use , using patient characteristic variables for generating similar cases . three graduate nursing students participated . each ranked <digit> patient cases using demographics of age , sex , diagnosis , and comorbidities against <digit> patients from a database . participant judgments of case similarity were compared with the case based reasoning system . feature weights for each indicator were adjusted to make the case based reasoning system 's similarity ranking correspond more closely to participant judgment . small differences were noted between initial weights and weights generated from participants . for example , initial weight for comorbidities was 0.35 , whereas weights generated by participants for pain , fatigue , and toilet use were 0.49 , 0.42 , and 0.48 , respectively . for the same outcomes , the initial weight for sex was 0.15 , but weights generated by the participants were 0.025 , 0.002 , and 0.000 , respectively . refinement of the case based reasoning tool established valid benchmarks for patient outcomes in relation to participants and assisted in point of care decision making .
a tight semidefinite relaxation of the max cut problem . <eos> we obtain a tight semidefinite relaxation of the max cut problem which improves several previous sdp relaxation in the literature . not only is it a strict improvement over the sdp relaxation obtained by adding all the triangle inequalities to the well known sdp relaxation , but also it satisfy slater constraint qualification ( strict feasibility ) .
universally manipulable body models dual quaternion representations in layered and dynamic mmcs . <eos> surprisingly complex tasks can be solved using a behaviour based , reactive control system , i.e. , a system that operates without an explicit internal representation of the environment and the own body . nevertheless , application of internal representations has gained interest in recent years because such internal representations can be used to solve problems of perception and motor control ( sensor fusion , inverse modeling ) and may in addition be applied to higher cognitive functions as are the ability to plan ahead . to endow such a system with the ability to find new behavioural solutions to a given problem in a broad range of possibilities , the internal representation must be universally manipulable , i.e. the model should be able to simulate all movements that are physically possible for the body given . using recurrent neural networks , models showing this faculty have been proposed being based on the principle of mean of multiple computation ( mmc ) . the extension of this approach to three dimensions requires the introduction of a joint angle representation which allows for computation of mean values . here we use dual quaternions that are singularity free and unambiguous which allow for shortest path interpolation . in addition , it has been shown that dual quaternions are the most efficient and most compact form for representing rigid transformations . the model can easily be adapted to bodies of arbitrary geometries . the extended mmc net introduced in this article represents a holistic system that can following the principle of pattern completion likewise be used as an inverse model , a forward model , for sensor fusion or other , related capabilities .
improvement of the computer methods for grounding analysis in layered soils by using high efficient convergence acceleration techniques . <eos> in the last years the authors have developed a numerical formulation based on the boundary element method for the analysis of grounding systems embedded in uniform soils . this approach has been implemented in a cad system that currently allows to analyze real grounding grids in real time in personal computers . the extension of this approach for the grounding analysis in layered soils is straightforward by application of the method of images . however in some practical cases the resulting series have a poor rate of convergence consequently , the analysis of real earthing grids in multilayer soils requires an out of range computational cost . in this paper we present a cad system based on this bem numerical formulation for grounding analysis in multilayer soils that include an efficient technique based on the aitken acceleration in order to improve the rate of convergence of the involved series expansions . finally , we show some examples by using the geometry of real grounding systems .
modeling exhaust gas pollution abatement part i single hydrocarbon propylene . <eos> the one dimensional models for catalytic converters are used to account for the reduction of pollutants like hydrocarbons ( hc ) , carbon monoxide ( co ) and nitric oxide ( no ) . the proposed model considers both gaseous as well as solid phase reactions of only one gas propylene that could possibly be occurring in the converter channels . metal substrates were considered , as the better heat conducting solid material . the rungekutta method and backward implicit schemes were employed to solve the coupled ordinary and partial differential equations .
the viuva negra crawler an experience report . <eos> this paper documents hazardous situations on the web that crawlers must address . this knowledge was accumulated while developing and operating the viuva negra ( vn ) crawler to feed a search engine and a web archive for the portuguese web for four years . the design , implementation and evaluation of the vn crawler are also presented as a case study of a web crawler design . the case study tested provides crawling techniques that may be useful for the further development of crawlers . copyright ( c ) <digit> john wiley sons , ltd .
predictive statistical models for user modeling . <eos> the limitations of traditional knowledge representation methods for modeling complex human behaviour led to the investigation of statistical models . predictive statistical models enable the anticipation of certain aspects of human behaviour , such as goals , actions and preferences . in this paper , we motivate the development of these models in the context of the user modeling enterprise . we then review the two main approaches to predictive statistical modeling , content based and collaborative , and discuss the main techniques used to develop predictive statistical models . we also consider the evaluation requirements of these models in the user modeling context , and propose topics for future research .
image retrieval system using r tree self organizing map . <eos> most of the retrieval systems concentrate much on low level features such as color , texture , shape , position , etc. the present system is mainly developed based on the visual descriptors of the image such as color , texture and shape descriptors . we have used unsupervised kohonens self organizing maps ( som ) technique to train the images and our own indexing scheme with reference system based on r tree som . we proposed an approach , fuzzy color histogram , for color retrieval and lie descriptors for the retrieval of shapes . we have performed experiments and tested the proposed approach with our own image database constructed from corel photo gallery .
on a versatile stochastic growth model . <eos> growth phenomena are ubiquitous and pervasive not only in biology and the medical sciences , but also in economics , marketing and the computer and social sciences . we introduce a three parameter version of the classic pure birth process growth model when suitably instantiated , can be used to model growth phenomena in many seemingly unrelated application domains . we point out that the model is computationally attractive since it admits of conceptually simple , closed form solutions for the time dependent probabilities .
performance analysis of the aodv ad hoc routing protocol in a dual radio network . <eos> existing ad hoc routing protocols supports multiple radio interfaces , but they are designed for radio interfaces with similar properties . in contrast , in a typical usage scenario the interfaces will have different properties . there will be a need for longer range resulting in a lower bandwidth , while some applications will have higher capacity requirements but for nodes in close proximity . a solution may be to use one interface with low bandwidth and long range for example in the uhf band , whilst the other interface has a higher bandwidth and shorter range in the vhf or shf band . such a solution will not be energy efficient , so it will be feasible only for vehicle mounted radios like in tactical military or emergency service networks . with the shortest path routing , the long range interfaces tend to be selected . the traffic is therefore shifted towards the links with the lowest capacity . firstly , we analyze and illustrate the problem for the typical reactive routing protocol , aodv . secondly , we propose two techniques that take the underlying radio capacity into account when a new route is established . compared to original aodv , the proposed algorithms provide higher network throughput , less routing overhead , less end to end delay and still provides connectivity . the applicability to this approach is one of practical interest to scenarios where nodes are typically unevenly distributed a network where some nodes only are reachable over long range radios , while others are also reachable over short range high capacity radios .
routing aware placement algorithm and efficient free space management for reconfigurable systems . <eos> in partially reconfigurable devices like fpga , logic resources and communication channels can be reconfigured without affecting other section of the device . this allows parallel execution of multiple tasks on a fpga . due to limited resources on a fpga , an effective management for efficient execution of tasks is required . we present a new approach for management of fpga logic resources and also communication channels in an online task placement . the approach creates communication channels between the tasks and also tasks with i o elements without requiring extra computation overhead . we present a fast algorithm for searching mers for management of fpga space . then , we present a exact routing algorithm to find distance and create exact path between two set of tasks . a new fitting strategy based on the rate of communication between the tasks is also presented . the results indicate the proposed strategy and also its combination with some other known strategies can improve the quality of placement .
java takes flight time portable real time programming with exotasks . <eos> existing programming methodologies for real time systems suffer from a low level of abstraction and non determinism in both the timing and the functional domains . as a result , real time systems are difficult to test and must be re certified every time changes are made to either the software or hardware environment . exotasks are a novel java programming construct that achieve deterministic timing , even in the presence of other java threads , and across changes of hardware and software platform . they are deterministic functional data flow tasks written in java , combined with an orthogonal scheduling policy based on the logical execution time ( let ) model . we have built a quad rotor model helicopter , the javiator , which we use as a testbed for this work . we evaluate our implementation of exotasks in ibm 's j9 real time virtual machine using actual flights of the helicopter . our experiments show that we are able to maintain deterministic behavior in the face of variations in both software load and hardware platform .
graphical representation of proteins as four color maps and their numerical characterization . <eos> we put forward a novel compact <digit> d graphical representation of proteins based on the concept of virtual genetic code and a four color map . the novel graphical representation uniquely represents proteins and allows one to easily and quickly visually observe and inspect similarity dissimilarity between them . it also leads to a novel protein descriptor , a <digit> dimensional vector derived from a novel structure matrix s associated with the map . the introduced numerical characterization of proteins is not only useful for their comparative study , but also for cataloguing information on a single protein . the approach is illustrated with the a chain of human insulin and the a chain of human insulin analogue glargine .
image reconstruction incorporated with the skull inhomogeneity for electrical impedance tomography . <eos> the structural similarity of the head model affects the accuracy of forward solution to electrical impedance tomography ( eit ) . generally , the four concentric circle model ( fccm ) is used as the head model , which ignores the inhomogeneous distribution of the conductivity of real skull . in order to decrease the errors caused by using fccm , a more accurate head model named inhomogeneous skull model ( ism ) has been proposed and a reconstruction algorithm incorporated with ism has been developed for brain eit . simulation results have shown improvement in image quality and localization accuracy when using ism . it is also suggested that the reconstructed image could be more sensitive to the location of bony sutures than to the variation of skull thickness . in conclusion , incorporating skull inhomogeneity into image reconstruction is an effective way to improve image quality and localization accuracy for brain eit .
feature preserving filtering with l0 gradient minimization . <eos> we introduce an efficient approximation algorithm for l0 gradient minimization problem . we apply l0 gradient minimization in edge preserving image smoothing . we apply l0 gradient minimization in feature preserving surface smoothing . we compare our method with some existing feature preserving filtering methods .
low cost soccer video summaries based on visual rhythm . <eos> the visual rhythm is a spatio temporal sampled representation of video data providing compact information while preserving several types of video events.we exploit these properties in the present work to propose two new low level descriptors for the analysis of soccer videos computed directly from the visual rhythm.the descriptors are related to dominant color and camera motion estimation.the new descriptors are applied in different tasks aiming the analysis of soccer videos such as shot transition detection , shot classification and attack direction estimation.we also present a simple automated soccer summary application to illustrate the use of the new descriptors .
achieving high momentum in the evolution of wireless infrastructures the battle over the 1g solutions . <eos> in this paper we examine the design of the first wireless systems as infrastructure making . an infrastructure is fundamentally a relational concept . accordingly , we shall analyze how relationships between entrepreneurs , system builders and regulators were organized so that some early wireless systems achieved a high momentum . in high momentum , a technological system seems to drive toward a specific direction with a certain autonomy and this demands that technological , economic , political and social challenges be integrated effectively into the overall design . such an integration can only succeed through an effective mobilization and coordination of both systems builders and other stakeholders ( entrepreneurs , and regulators in particular ) concerns . we analyze five cases of building first generation wireless solutions and examine to what extent specific relationships between key actors influenced whether these systems did achieve a high momentum . our analysis shows that managing critical relationships with regulators , correct timing , and effective and continuous meshing of both entrepreneurs and technologists concerns were typical for systems that achieved high momentum . in all successful cases , the meshing and associated institutional response demanded the creation of an open standard . these include both amps tacs family of standards , and especially the nordic nmt standard .
parallel lattice boltzmann method with blocked partitioning . <eos> this paper presents and discusses a blocked parallel implementation of bi and three dimensional versions of the lattice boltzmann method . this method is used to represent and simulate fluid flows following a mesoscopic approach . most traditional parallel implementations use simple data distribution strategies to parallelize the operations on the regular fluid data set . however , it is well known that block partitioning is usually better . such a parallel implementation is discussed and its communication cost is established . fluid flows simulations crossing a cavity have also been used as a real world case study to evaluate our implementation . the presented results with our blocked implementation achieve a performance up to <digit> % better than non blocked versions , for some data distributions . thus , this work shows that blocked , parallel implementations can be efficiently used to reduce the parallel execution time of the method .
an investigation into the implementation of virtual reality technologies in support of conceptual design . <eos> virtual reality ( vr ) technologies provide novel and enhanced modes of human computer interaction that can be used to overcome communication drawbacks that prevail in conventional computer aided design ( cad ) systems in the support of conceptual design , principally in concept generation and evaluation . the work reported here focuses on how vr technologies can proffer more natural and intuitive interaction between the designer and the cad system thus enabling a rapid and more straightforward approach to concept design creation and evaluation . as the result of this research , an innovative conceptual design system called the lucid system has been developed . lucid provides designers with a new interaction paradigm that enables them to take fuller advantage of their visual , auditory and tactile sensorial channels in order to create , view , touch , manipulate and listen to cad digital models more effectively . the lucid system has shown particular human computer interface ( hci ) benefits to designers during conceptual design stages based on the outcome from a user evaluation test .
action recognition from video using feature covariance matrices . <eos> we propose a general framework for fast and accurate recognition of actions in video using empirical covariance matrices of features . a dense set of spatio temporal feature vectors are computed from video to provide a localized description of the action , and subsequently aggregated in an empirical covariance matrix to compactly represent the action . two supervised learning methods for action recognition are developed using feature covariance matrices . common to both methods is the transformation of the classification problem in the closed convex cone of covariance matrices into an equivalent problem in the vector space of symmetric matrices via the matrix logarithm . the first method applies nearest neighbor classification using a suitable riemannian metric for covariance matrices . the second method approximates the logarithm of a query covariance matrix by a sparse linear combination of the logarithms of training covariance matrices . the action label is then determined from the sparse coefficients . both methods achieve state of the art classification performance on several datasets , and are robust to action variability , viewpoint changes , and low object resolution . the proposed framework is conceptually simple and has low storage and computational requirements making it attractive for real time implementation .
automatic analysis of pointer aliasing for untyped programs . <eos> in this paper we describe an automatic analysis based on abstract interpretation that discovers potential sharing relationships among the data structures created by an imperative program . the analysis is able to distinguish between elements in inductively defined structures and does not require any explicit data type declaration by the programmer . in order to construct the abstract interpretation we introduce a new class of abstract domains the cofibered domains . ( c ) <digit> elsevier science b.v. all rights resented .
computational complexity of finding meaningful association rules . <eos> recent developments in computer technology allow us to analyze all the data in a huge database . data mining is to analyze all the data in such a database and to obtain useful information for database users . one of the well studied problems in data mining is the search for meaningful association rules in a market basket database which contains massive amounts of transactions . one way to find meaningful association rules is to find all the large itemsets first , and then to find meaningful association rules from the large itemsets . although a number of algorithms for computing all the large itemsets have been proposed , the computational complexity of them is scarcely disscussed . in this paper , we show that it is np complete to decide whether there exists a large itemset that has a given cardinality . also , we propose subclasses of databases in which ail the meaningful association rules can be computed in time polynomial of the size of a database .
being polite while fulfilling different discourse functions in online classroom discussions . <eos> using a discourse analytic qualitative approach , we investigated the naturally occurring discourse that arose as part of two kinds of regular course activities , synchronous and asynchronous computer mediated discussions . the messages contributed by members of a graduate course were analyzed for the kind of discourse functions and the kind of politeness strategies they displayed . results indicated that synchronous cmd afforded more information seeking , information providing , and social comments than asynchronous cmd . asynchronous discussions were slightly more likely to allow for such functions as discussion generating , experience sharing , idea explanation , and self evaluation functions than synchronous discussions . proportionately the two modes were similar in how politeness was expressed . finally , in relating politeness and function , we found more politeness indicators when students were posting messages with such functions as positive evaluation and group conversation management , functions that carried the potential for face threat , and the least politeness associated with messages serving the function of experience sharing . ( c ) <digit> published by elsevier ltd .
vehicle to vehicle connectivity on parallel roadways with large road separation . <eos> this paper studies vehicle multihop connectivity of vehicular ad hoc networks ( vanets ) on two parallel roads . it complements our earlier studies by considering a road separation distance larger than <digit> <digit> l and considering signal blockage between roads , where l represents the transmission range . assuming vehicles follow poisson processes , we specifically derive exact formulas for the expectation , variance , and probability distribution of the information propagation distance . we further develop a closed form approximation for the expected distance . the analytical results are verified and compared with those from other headway distributions of varying coefficient of variation through monte carlo simulation .
fast shape optimization of metalization patterns for power mosfet based driver . <eos> this paper addresses the problem of optimizing metalization patterns of back end connections for the power mosfet based driver since the back end connections tend to dominate the on resistance r ( on ) of the driver . we propose a heuristic algorithm to seek for better geometric shapes for the patterns targeting at minimizing r ( on ) and at balancing the current distribution . in order to speed up the analysis , the equivalent resistance network of the driver is modified by inserting ideal switches to avoid repeatedly inverting the admittance matrix . with the behavioral model of the ideal switch , we can significantly accelerate the optimization . simulation on three drivers from industrial teg data demonstrates that our algorithm can reduce r ( on ) effectively by shaping metals appropriately within a given routing area .
optimal pmu placement using iterated local search . <eos> an essential tool for power system monitoring is state estimation . using pmus can greatly improve the state estimation process . however , for state estimation , the pmus should be placed appropriately in the network . the problem of optimal pmu placement for full observability is analysed in this paper . the objective of the paper is to minimise the size of the pmu configuration while allowing full observability of the network . the method proposed initially suggests a pmu distribution which makes the network observable . the iterated local search ( ils ) metaheuristic is then used to minimise the size of the pmu configuration needed to observe the network . the algorithm is tested on ieee test networks with <digit> , <digit> and <digit> nodes and compared to the results obtained in previous publications .
incompressible flow computations over moving boundary using a novel upwind method . <eos> in this paper a novel method for simulating unsteady incompressible viscous flow over a moving boundary is described . the numerical model is based on a 2d navier stokes incompressible flow in artificial compressibility formulation with arbitrary lagrangian eulerian approach for moving grid and dual time stepping approach for time accurate discretization . a higher order unstructured finite volume scheme , based on a harten lax and van leer with contact ( hllc ) type riemann solver for convective fluxes , developed for steady incompressible flow in artificial compressibility formulation by mandal and lyer ( aiaa paper <digit> <digit> ) , is extended to solve unsteady flows over moving boundary . viscous fluxes are discretized in a central differencing manner based on coirier 's diamond path . an algorithm based on interpolation with radial basis functions is used for grid movements . the present numerical scheme is validated for an unsteady channel flow with a moving indentation . the present numerical results are found to agree well with experimental results reported in literature . ( c ) <digit> elsevier ltd. all rights reserved .
learning query ambiguity models by using search logs . <eos> dentifying ambiguous queries is crucial to research on personalized web search and search result diversity . intuitively , query logs contain valuable information on how many intentions users have when issuing a query . however , previous work showed user clicks alone are misleading in judging a query as being ambiguous or not . in this paper , we address the problem of learning a query ambiguity model by using search logs . first , we propose enriching a query by mining the documents clicked by users and the relevant follow up queries in a session . second , we use a text classifier to map the documents and the queries into predefined categories . third , we propose extracting features from the processed data . finally , we apply a state of the art algorithm , support vector machine ( svm ) , to learn a query ambiguity classifier . experimental results verify that the sole use of click based features or session based features perform worse than the previous work based on top retrieved documents . when we combine the two sets of features , our proposed approach achieves the best effectiveness , specifically <digit> % in terms of accuracy . it significantly improves the click based method by 5.6 % and the session based method by 4.6 % .
recent trends of drug abuse in japan . <eos> methamphetamine has been a continuous cause for concern for many years in japan . japan experienced the pandemics of methamphetamine abuse twice after world war ii and is now facing the third pandemic . new data indicate that the abuse of drugs such as mdma ( ecstasy ) and cannabis is rising at an alarming pace . an epidemiologic survey conducted by the national research institute reveals that drug abuse in japan is still very low compared to that in other countries and regions however , the current situation seems to be changing for the worse . two major factors accelerate access to illicit drugs among young people . one is the presence of illicit drug dealing groups , which indiscriminately offer drugs . the other is new technology such as the internet and mobile phones . strict law enforcement and intensive public awareness and prevention activities are indispensable in protecting young people from drug abuse .
genetic optimisation of radial eddy current couplings . <eos> purpose the purpose of this paper is to obtain a fully analytical model of an eddy current coupler and to use it in a multi objective optimisation algorithm . design methodology approach analytical expressions of device performances are adopted in the objective function and are obtained from a closed solution of the field problem . the optimisation has been carried out by considering both the torque and the momentum of inertia of the object . two different structures have been considered . findings a fully analytical expression of the torque has been obtained for two different geometrical configurations . the optimisation procedure has been used to compare these structures and it is possible to observe that the dspm performances are better than the sspm ones . research limitations implications to obtain a closed form of the torque function , the non linearities of the iron have been neglected . nevertheless , in the optimisation procedure has been limited the magnetic flux density in the iron core to a feasible value in the linear part of the ferromagnetic characteristic . the thermal effects have been neglected . pratical implications in the industry , eddy current couplers can be used as transmission , dampers and brakes . the use of objective functions ( ofs ) in a closed formulation allows to perform a light optimisation from the point of view of the time computation and to drastically increase the development efficiency . originality value in this paper , a model for computing the electromagnetic behaviour of eddy current couplers is presented . the optimisation of both the torque and the inertia momentum allows to obtain good static and dynamic performances .
persistence and periodic orbits for an sis model in a polluted environment . <eos> usually , man is infected with some kinds of epidemic disease since they live in a polluted environment <digit> <digit> , such as air pollution ( e.g. , pulmonary tuberculosis ) , or water pollution ( e.g. , snail fever ) . these kinds of toxicant are generated by polluted biological ( e.g. , degradation of forests , creutzfeldt jakob disease result from bovine spongiform encephalopathy ( bse ) ) , physical ( e.g. , nuclear radiation , syndrome of the gulf war ) , or chemical environment ( e.g. , petroleum leaking , dioxin event in belgium ) . as we know , the environmental pollution has been a very serious global problem , which may influence the spread of infectious diseases , and hence , has big effects on human health . in this paper , we study an sis ( susceptible infected susceptible ) epidemic model with toxicology , using the brouwer fixed point theorem we show the existence of periodic solution of such a system , we also prove the global attraction of this solution , and we obtain the threshold between extinction and weakly persistent for the infected class . ( c ) <digit> elsevier ltd. all rights reserved .
efficient techniques for document sanitization . <eos> sanitization of a document involves removing sensitive information from the document , so that it may be distributed to a broader audience . such sanitization is needed while declassifying documents involving sensitive or confidential information such as corporate emails , intelligence reports , medical records , etc. in this paper , we present the erase framework for performing document sanitization in an automated manner . erase can be used to sanitize a document dynamically , so that different users get different views of the same document based on what they are authorized to know . we formalize the problem and present algorithms used in erase for finding the appropriate terms to remove from the document . our preliminary experimental study demonstrates the efficiency and efficacy of the proposed algorithms .
predictive combinations of monitor alarms preceding in hospital code blue events . <eos> bedside monitors are ubiquitous in acute care units of modern healthcare enterprises . however , they have been criticized for generating an excessive number of false positive alarms causing alarm fatigue among care givers and potentially compromising patient safety . we hypothesize that combinations of regular monitor alarms denoted as superalarm set may be more indicative of ongoing patient deteriorations and hence predictive of in hospital code blue events . the present work develops and assesses an alarm mining approach based on finding frequent combinations of single alarms that are also specific to code blue events to compose a superalarm set . we use <digit> way analysis of variance ( anova ) to investigate the influence of four algorithm parameters on the performance of the data mining approach . the results are obtained from millions of monitor alarms from a cohort of <digit> adult code blue and <digit> control patients using a multiple <digit> fold cross validation experiment setup . using the optimal setting of parameters determined in the cross validation experiment , final superalarm sets are mined from the training data and used on an independent test data set to simulate running a superalarm set against live regular monitor alarms . the anova shows that the content of a superalarm set is influenced by a subset of key algorithm parameters . simulation of the extracted superalarm set shows that it can predict code blue events one hour ahead with sensitivity between 66.7 % and 90.9 % while producing false superalarms for control patients that account for between 2.2 % and 11.2 % of regular monitor alarms depending on user supplied acceptable false positive rate . we conclude that even though the present work is still preliminary due to the usage of a moderately sized database to test our hypothesis it represents an effort to develop algorithms to alleviate the alarm fatigue issue in a unique way .
hybrid demosaicking scheme for artifact suppression around edges . <eos> digital cameras use a single electronic sensor overlaid with a color filter array ( cfa ) to capture images for reducing the production cost . only one primary color is sampled in each pixel such that the missing color primaries must be reconstructed by interpolation . in this paper , a hybrid demosaicking scheme for cfa interpolation is proposed to increase the visual quality of the restored image by artifact suppression in the edge region . a preliminary interpolation is first implemented by bilinear interpolation . the color difference model is then used to update green channel . the red and blue channels are successively updated using the proposed hue model that can reduce the distortion around edges . finally , an iteratively adaptive algorithm using an edge sensing interpolation mechanism and a luminance color difference model is exploited to improve visual quality of the restored image by artifact suppression in the edge region . simulation results demonstrate that the performance of the proposed algorithm is better than that of the existing approaches in terms of peak signal to noise ratio ( psnr ) and visual quality .
infinite dimensional duality theory applied to investment strategies in environmental policy . <eos> in this paper , we develop an infinite dimensional lagrangian duality framework for modeling and analyzing the evolutionary pollution control problem . specifically , we examine the situation in which different countries aim at determining the optimal investment allocation in environmental projects and the tolerable pollutant emissions , so as to maximize their welfare . we state the equilibrium conditions underlying the model , and provide an equivalent formulation in terms of an evolutionary variational inequality . moreover , by means of infinite dimensional duality tools , we prove the existence of lagrange multipliers that play a fundamental role in order to describe countries decision making processes .
a novel distributed call admission control for wireless mobile multimedia networks . <eos> this paper introduces a novel distributed call admission control framework developed for cellular mobile networks . the main feature of the proposed framework is a more efficient support for mobile multimedia users having dynamic bandwidth requirements . this is achieved by reducing the call dropping probability while maintaining a high network resource utilization . a call admission control algorithm is proposed in this paper and involves not only the original cell ( handling the new admission request ) but also a cluster of neighboring cells . the neighboring cells provide significant information about their ability to support the new mobile user in the future . this distributed process allows the original cell to make a more clear sighted admission decision for the new user . simulations are provided to show the improvements obtained using our framework .
high speed low power small area accumulator designs for direct digital frequency synthesizers . <eos> this paper presents high speed low power small area accumulator designs to be used in ddfs systems . to reduce the numerically controlled oscillator ( nco ) design complexity and size , only the most significant bits of the accumulator drive the phase to amplitude mapping block . those bits need to be updated on every sampling clock , while the least significant bits of the accumulator are not visible to the rest of the ddfs design and can be updated less frequently , which motivated the development of new accumulator designs . without performance degradation , the proposed designs relieve constraints in implementation , and hence they can be employed for ghz range ddfs , reduce power consumption up to <digit> % compared to standard accumulator design , and minimize chip area . for further power reduction , the proposed designs place the phase modulation adder at the front of the accumulator .
some generalizations of ekeland 's variational principle with applications to fixed point theory . <eos> in this paper , some extensions of the ekeland variational principle in metric spaces are given for a generalized pseudodistance . as an application we obtain caristi 's fixed point theorem . then , by using this result , we establish some fixed point theorems for set valued contractive mappings . ( c ) <digit> elsevier ltd. all rights reserved .
greedy receivers in ieee 802.11 hotspots impacts and detection . <eos> as wireless hotspot business becomes a tremendous financial success , users of these networks have increasing motives to misbehave in order to obtain more bandwidth at the expense of other users . such misbehaviors threaten the performance and availability of hotspot networks and have recently attracted increasing research attention . however , the existing work so far focuses on sender side misbehavior . motivated by the observation that many hotspot users receive more traffic than they send , we study greedy receivers in this paper . we identify a range of greedy receiver misbehaviors , and quantify their damage using both simulation and testbed experiments . our results show that even though greedy receivers do not directly control data transmission , they can still result in very serious damage , including completely shutting off the competing traffic . to address the issues , we further develop techniques to detect and mitigate greedy receiver misbehavior , and demonstrate their effectiveness .
on edge disjoint spanning trees in hypercubes . <eos> in this note we give a construction for obtaining the maximum number of edge disjoint spanning trees in a hypercube . the result has applications to multicast communication in wormhole routed parallel computers . ( c ) <digit> published by elsevier science b.v. all rights reserved .
towards automatic parallelization of tree reductions in dynamic programming . <eos> tree contraction algorithms , whose idea was first proposed by miller and reif , are important parallel algorithms to implement efficient parallel programs manipulating trees . despite their efficiency , the tree contraction algorithms have not been widely used due to the difficulties in deriving the tree contracting operations . in particular , the derivation of the tree contracting operations is much difficult when multiple values are referred and updated in each step of the contractions . such computations often appear in dynamic programming problems on trees . in this paper , we propose an algebraic approach to deriving tree contraction programs from recursive tree programs , by focusing on the properties of commutative semirings . we formalize a new condition for implementing tree reductions with the tree contraction algorithms , and give a systematic derivation of the tree contracting operations . based on it , we implemented a code generator for tree reductions , which has an optimization mechanism that can remove unnecessary computations in the derived parallel programs . as far as we are aware , this is the first step towards an automatic parallelization system for the development of efficient tree programs .
elliptic curves with low embedding degree . <eos> miyaji , nakabayashi and takano have recently suggested a construction of the so called mnt elliptic curves with low embedding degree , which are also of importance for pairing based cryptography . we give some heuristic arguments which suggest that there are only about z ( <digit> <digit> o ( <digit> ) ) of mnt curves with complex multiplication discriminant up to z. we also show that there are very few finite fields over which elliptic curves with small embedding degree and small complex multiplication discriminant may exist ( regardless of the way they are constructed ) .
visual classifier training for text document retrieval . <eos> performing exhaustive searches over a large number of text documents can be tedious , since it is very hard to formulate search queries or define filter criteria that capture an analyst 's information need adequately . classification through machine learning has the potential to improve search and filter tasks encompassing either complex or very specific information needs , individually . unfortunately , analysts who are knowledgeable in their field are typically not machine learning specialists . most classification methods , however , require a certain expertise regarding their parametrization to achieve good results . supervised machine learning algorithms , in contrast , rely on labeled data , which can be provided by analysts . however , the effort for labeling can be very high , which shifts the problem from composing complex queries or defining accurate filters to another laborious task , in addition to the need for judging the trained classifier 's quality . we therefore compare three approaches for interactive classifier training in a user study . all of the approaches are potential candidates for the integration into a larger retrieval system . they incorporate active learning to various degrees in order to reduce the labeling effort as well as to increase effectiveness . two of them encompass interactive visualization for letting users explore the status of the classifier in context of the labeled documents , as well as for judging the quality of the classifier in iterative feedback loops . we see our work as a step towards introducing user controlled classification methods in addition to text search and filtering for increasing recall in analytics scenarios involving large corpora .
an empirical investigation of mobile government adoption in rural china a case study in zhejiang province . <eos> demographic features affect rural inhabitants ' adoption of mobile government . integrity and benevolence have influences on adoption intention . near term and long term usefulness have influences on adoption intention . image and social influence have influences on adoption intention . interdependences between service perception and demography features are found .
introducing interactive story creators to conversation modelling . <eos> this paper presents an approach for non programming writers in interactive storytelling to structure interactive conversations in the authoring process to achieve high potential variability during interactions . at the same time , the presentation describes material that can be used as introduction to the concepts of interactive dialogue writing and interactive story creation in general , filling a gap of information in interdisciplinary discourses . it has been applied and tested in a student project and in tutorials at conferences .
real time edge enhanced dynamic correlation and predictive open loop car following control for robust tracking . <eos> we present a robust framework for a real time visual tracking system , based on a bpnn controlled fast normalized correlation ( bcfnc ) algorithm and a predictive open loop car following control ( pol cfc ) strategy . the search for the target is carried out in a dynamically generated resizable search window . in order to achieve the robustness , we use some edge enhancement operations before the correlation operation , and introduce an adaptive template updating scheme . the proposed tracking algorithm is compared with various correlation based techniques and ( in some cases ) with the mean shift and the condensation trackers on real world scenarios . a significant improvement in efficiency and robustness is reported . the pol cfc algorithm approximates the current velocity of an open loop pan tilt unit , computes the predicted relative velocity of the object using kalman filter , and generates the precise control signals to move the camera accurately towards the maneuvering target regardless of its changing velocity . the proposed system works in real time at the speed of <digit> <digit> frames second depending on the template size , and it can persistently track a distant or near object even in the presence of object fading , low contrast imagery , noise , short lived background clutter , object scaling , changing object velocity , varying illumination , object maneuvering , multiple objects , obscuration , and sudden occlusion .
on feature extraction for noninvasive kernel estimation of left ventricular chamber function indices from echocardiographic images . <eos> color doppler m mode images are used to characterize left ventricular function . estimation of end systolic peak elastance and time constant of relaxation rate . comparison and interpretation of different linear estimators . conditions where non linear estimators outperform linear ones . high fidelity study on mini pigs with echocardiographic images and invasive measures .
web based learning and instruction support system for pneumatics . <eos> this research presents a web based learning and instructional system for pneumatics . the system includes course material , remote data acquisition modules , and a pneumatic laboratory set . the course material is in the html format accompanied with text , still and animated images , simulation programs , and computer aided design tools . the data acquisition modules of the system can be connected through the computer i o with actual devices to verify and implement simulation results to enhance the corresponding pneumatics learning . therefore , this web based learning and instruction support system can be used to assist the pneumatics instruction , computer aided sequential control design and pneumatic laboratory practice . the course material is organized by chapters and sections , leads users from pneumatic devices to pneumatic systems . programmable logic controller , pc based sequential control , internet remote monitoring , and web based sequential control are also covered . in this study , combinations of software devices and hardware system circuit were used to demonstrate the motion of controlled system . to assist instruction for a pneumatics course , computer aided pneumatic circuit design software and computer aided electric circuit design software were fully integrated . in combining with the remote data acquisition modules , the web based sequential controller can be used to conduct pneumatic experiment . to gain consistency , this instructional system is mounted on a previously developed instructional platform . under this java based platform , users can apply the communication tools of the platform to gain efficiency while in a non co located multiuser session . a database server is used to store circuit designs for all users . and , because the software has collaborative function , it allows all users to design on same circuits collaboratively . ( c ) <digit> elsevier ltd. all rights reserved .
a novel artificial intelligence method for weekly dietary menu planning . <eos> objectives menu planning is an important part of personalized lifestyle counseling . the paper describes the results of an automated menu generator ( menugene ) of the web based lifestyle counseling system cordelic that provides personalized advice to prevent cardiovascular diseases . methods the menu generator uses genetic algorithms to prepare weekly menus for web users . the objectives are derived from personal medical data collected via forms in cordelia , combined with general nutritional guidelines . the weekly menu is modeled as a multilevel structure . results . results show that the genetic algorithm based method succeeds in planning dietary menus that satisfy strict numerical constraints on every nutritional level ( meal , daily basis , weekly basis ) . the rule based assessment proved capable of manipulating the mean occurrence of the nutritional components thus providing a method for adjusting the variety and harmony of the menu plans . conclusions . by splitting the problem into well determined sub problems , weekly menu plans that satisfy nutritional constraints and have well assorted components can be generated with the some method that is for daily and meal pion generation .
robust filtering based thinning algorithm for pattern recognition . <eos> we present a novel thinning algorithm to automatically extract skeletons from images without artefacts . it is well known that the major problem of existing thinning algorithms is the generation of artefacts such as redundant branches and lines due to noise in images . in this approach , we propose to use oriented gaussian filters to determine principal directions and to classify ridges , valleys and edges . as oriented filters are low pass filters in the principal directions , they are robust to noise and insignificant extremities . the thinning process of the proposed algorithm is guided by principal directions , thus it can remove edge points and valley points without the interference from noise and insignificant extremities . as a result , extracted skeletons of elongated shapes are smooth and without redundant branches and lines . the thinning algorithm is applied to handwriting recognition , fingerprint recognition and 3d plant analysis , where two 2d side view images of cereal plants are available to convert 2d skeletons to 3d structures . experimental results show that the proposed approach is able to handle noise and insignificant extremities and to generate smooth skeletons of objects , and also is used to automatically extract 3d structures of cereal plants .
hidden protocols modifying our expectations in an evolving world . <eos> when agents know a protocol , this leads them to have expectations about future observations . agents can update their knowledge by matching their actual observations with the expected ones . they eliminate states where they do not match . in this paper , we study how agents perceive protocols that are not commonly known , and propose a semantics driven logical framework to reason about knowledge in such scenarios . in particular , we introduce the notion of epistemic expectation models and a propositional dynamic logic style epistemic logic for reasoning about knowledge via matching agents expectations to their observations . it is shown how epistemic expectation models can be obtained from epistemic protocols . furthermore , a characterization is presented of the effective equivalence of epistemic protocols . we introduce a new logic that incorporates updates of protocols and that can model reasoning about knowledge and observations . finally , the framework is extended to incorporate fact changing actions , and a worked out example is given .
performance analysis of an ingress switch in a jumpstart optical burst switching network . <eos> we consider an ingress optical burst switching ( obs ) node employing the jumpstart signaling protocol . the switch serves a number of users , each connected to the switch with a fiber link that supports multiple wavelengths . each wavelength is associated with a <digit> state markovian burst arrival process which permits short and long bursts to be modeled . we model the ingress switch as a closed multi class non product form queueing network , which we analyze approximately by decomposition . specifically , we develop new techniques to analyze the queueing network , first assuming a single class of customers , and subsequently multiple classes of customers . these analytical techniques have applications to general queueing networks beyond the one studied in this paper . we also develop computationally efficient approximate algorithms to analyze an ingress switch in the limiting case where the number of wavelengths is large . the algorithms have a good accuracy , and they provide insight into the effect of various system parameters on the performance of an ingress obs switch .
boundary models in dpd . <eos> we present a model for treating solid boundaries of a dpd fluid . the basic idea is to model the stick boundary conditions by assuming that a layer of dfd particles is stuck on the boundary . by taking a continuum limit of this layer effective dissipative and stochastic forces on the fluid dpd particles are obtained . the boundary model is tested by a simulation of planar couette flow which allows the performance of vicosimetric measurements . we analyze the conditions that ensure a proper stick boundary condition for an impenetrable wall , comparing with previous methods used .
a rational high order compact adi method for unsteady convectiondiffusion equations . <eos> based on a fourth order compact difference formula for the spatial discretization , which is currently proposed for the one dimensional ( 1d ) steady convectiondiffusion problem , and the cranknicolson scheme for the time discretization , a rational high order compact alternating direction implicit ( adi ) method is developed for solving two dimensional ( 2d ) unsteady convectiondiffusion problems . the method is unconditionally stable and second order accurate in time and fourth order accurate in space . the resulting scheme in each adi computation step corresponds to a tridiagonal matrix equation which can be solved by the application of the 1d tridiagonal thomas algorithm with a considerable saving in computing time . three examples supporting our theoretical analysis are numerically solved . the present method not only shows higher accuracy and better phase and amplitude error properties than the standard second order peacemanrachford adi method in peaceman and rachford ( <digit> ) <digit> , the fourth order adi method of karaa and zhang ( <digit> ) <digit> and the fourth order adi method of tian and ge ( <digit> ) <digit> , but also proves more effective than the fourth order pad adi method of you ( <digit> ) <digit> , in the aspect of computational cost . the method proposed for the diffusionconvection problems is easy to implement and can also be used to solve pure diffusion or pure convection problems .
visualizing changing requirements with self organizing maps . <eos> this paper explores categorizing and visualizing changing software system requirements through self organizing neural maps . we present a field study that explores resulting visualizations and traces requirements through three project phases . the visualization models were applied to an ongoing e commerce project where evolving business models and changing plans forced a requirements team to continually assess impacts of change . analysts labeled requirements within twenty seven input dimensions . self organizing maps ( som ) were then generated to cluster requirements and to present visual models of change . som enhanced organization and traceability of business issues , use case documents , and project concerns .
anomaly detection using temporal data mining in a smart home environment . <eos> objectives to many people , home is a sanctuary . with the maturing of smart home technologies , many people with cognitive and physical disabilities can lead independent lives in their own homes for extended periods of time . in this paper , we investigate the design of machine learning algorithms that support this goal . we hypothesize that machine learning algorithms can be designed to automatically learn models of resident behavior in a smart home , and that the results can be used to perform automated health monitoring and to detect anomalies . methods specifically , our algorithms draw upon the temporal nature of sensor data collected in a smart home to build a model of expected activities and to detect unexpected , and possibly health critical , events in the home . results we validate our algorithms using synthetic data and real activity data collected from volunteers in an automated smart environment . conclusions the results from our experiments support our hypothesis that a model can be learned from observed smart home data and used to report anomalies , as they occur , in a smart home .
cross language information retrieval experiments based on clef <digit> corpora . <eos> search engines play an essential role in the usability of internet based information systems and without them the web would be much less accessible , and at the very least would develop at a much slower rate . given that non english users now tend to make up the majority in this environment , our main objective is to analyze and evaluate the retrieval effectiveness of various indexing and search strategies based on test collections written in four different languages english , french , german , and italian . our second objective is to describe and evaluate various approaches that might be implemented in order to effectively access document collections written in another language . as a third objective , we will explore the underlying problems involved in searching document collections written in the four different languages , and we will suggest and evaluate different database merging strategies capable of providing the user with a single unique result list .
design of h control of neural networks with time varying delays . <eos> this paper deals with the h control problem of neural networks with time varying delays . the system under consideration is subject to time varying delays and various activation functions . based on constructing some suitable lyapunovkrasovskii functionals , we establish new sufficient conditions for h control for two cases of time varying delays ( <digit> ) the delays are differentiable and have an upper bound of the delay derivatives and ( <digit> ) the delays are bounded but not necessary to be differentiable . the derived conditions are formulated in terms of linear matrix inequalities , which allow simultaneous computation of two bounds that characterize the exponential stability rate of the solution . numerical examples are given to illustrate the effectiveness of our results .
atomic layer deposited tantalum silicate as a gate dielectric for iiiv mos devices . <eos> tasiox thin films with si ( ta si ) mole fractions between <digit> and 0.6 have been deposited using atomic layer deposition on si and ingaas at 250c . interface defects on ingaas were on the order of <digit> cm 2ev <digit> , which is comparable to state of the art al2o3 deposited by atomic layer deposition using al ( ch3 ) <digit> and h2o while the dielectric permittivity of tasiox is considerably higher .
a demonstration of position and orientation sensor for two dimensional communication networks . <eos> this demonstration presents a networking infrastructure for a wireless , battery less , and location aware ubiquitous environment . our group has developed two dimensional communication ( 2dc ) technology , which enables network nodes placed on a thin sheet to communicate with one another and to receive electricity wirelessly . we have also developed a function for positions and orientation detection of devices placed on the sheet as well as the data and power transmission .
finite mixture clustering of human tissues with different levels of igf <digit> splice variants mrna transcripts . <eos> this study addresses a recurrent biological problem , that is to define a formal clustering structure for a set of tissues on the basis of the relative abundance of multiple alternatively spliced isoforms mrnas generated by the same gene . to this aim , we have used a model based clustering approach , based on a finite mixture of multivariate gaussian densities . however , given we had more technical replicates from the same tissue for each quantitative measurement , we also employed a finite mixture of linear mixed models , with tissue specific random effects .
improving procedural modeling with semantics in digital architectural heritage . <eos> we first introduce three challenges in the procedural modeling of digital architectural heritages and then present a general framework , which integrates several machine intelligence and semantic techniques , e.g. , the ontology design approach , pattern mining , auto annotating and rule reduction , to improve the procedural methods in architectural modeling . several evaluations and experiments are also presented . the experimental results illustrate the improvements following our approach .
influence processes for information technology acceptance an elaboration likelihood model . <eos> this study examines how processes of external influence shape information technology acceptance among potential users , how such influence effects vary across a user population , and whether these effects are persistent over time . drawing on the elaboration likelihood model ( elm ) , we compared two alternative influence processes , the central and peripheral routes , in motivating it acceptance . these processes were respectively operationalized using the argument qualify and source credibility constructs , and linked to perceived usefulness and attitude , the core perceptual drivers of it acceptance . we further examined how these influence processes were moderated by users ' it expertise and perceived job relevance and the temporal stability of such influence effects . nine hypotheses thus developed were empirically validated using a field survey of document management system acceptance at an eastern european governmental agency . this study contributes to the it acceptance literature by introducing elm as a referent theory for acceptance research , by elaborating alternative modes of influence , and by specifying factors moderating their effects . for practitioners , this study introduces influence processes as policy tools that managers can employ to motivate it acceptance within their organizations , benchmarks alternative influence strategies , and demonstrates the need for customizing influence strategies to the specific needs of a user population .
equivalent orthotropic elastic moduli identification method for laminated electrical steel sheets . <eos> an orthotropic elastic moduli identification method is proposed . least squares fit of frequencies is utlized with a novel mode selection algorithm . the method identifies the elastic moduli of laminated electrical steel sheets .
np muscl unsupervised global prediction of interaction networks from multiple data sources . <eos> inference of gene interaction networks from expression data usually focuses on either supervised or unsupervised edge prediction from a single data source . however , in many real world applications , multiple data sources , such as microarray and ish ( in situ hybridization ) measurements of mrna abundances , are available to offer multiview information about the same set of genes . we propose ish to estimate a gene interaction network that is consistent with such multiple data sources , which are expected to reflect the same underlying relationships between the genes . np muscl casts the network estimation problem as estimating the structure of a sparse undirected graphical model . we use the semiparametric gaussian copula to model the distribution of the different data sources , with the different copulas sharing the same precision ( i.e. , inverse covariance ) matrix , and we present an efficient algorithm to estimate such a model in the high dimensional scenario . results are reported on synthetic data , where np muscl outperforms baseline algorithms significantly , even in the presence of noisy data sources . experiments are also run on two real world scenarios two yeast microarray datasets and three drosophila embryonic gene expression datasets , where np muscl predicts a higher number of known gene interactions than existing techniques .
improving the quality of data models empirical validation of a quality management framework . <eos> this paper describes the results of a <digit> year research programme into evaluating and improving the quality of data models . the theoretical base for this work was a data model quality management framework proposed by moody and shanks ( in p. loucopolous ( ed . ) , proceedings of the 13th international conference on the entity relationship approach , manchester , england , december <digit> , <digit> ) . a combination of field and laboratory research methods ( action research , laboratory experiments and systems development ) was used to empirically validate the framework . this paper describes how the framework was used to ( a ) quality assure a data model in a large application development project ( product quality ) ( b ) reengineer application development processes to build quality into the data analysis process ( process quality ) ( c ) investigate differences between data models produced by experts and novices ( d ) provide automated support for the evaluation process ( the data model quality advisor ) . the results of the research have been used to refine and extend the framework , to the point that it is now a stable and mature approach .
record statistics in a random composition . <eos> a composition a1a2am a <digit> a <digit> a m of n n is an ordered collection of positive integers whose sum is n n . an element ai a i in is a strong ( weak ) record if ai > aj a i > a j ( ai aj a i a j ) for all j 1,2 , , i <digit> j <digit> , <digit> , , i <digit> . furthermore , the position of this record is i i . we derive generating functions for the total number of strong ( weak ) records in all compositions of n n , as well as for the sum of the positions of the records in all compositions of n n , where the parts ai a i belong to a d 1,2 , , d a d <digit> , <digit> , , d or a n a n . in particular when a n a n , we find the asymptotic mean values for the number , and for the sum of positions of records in compositions of n n .
dynamically coupled fluid body interactions in vorticity based numerical simulations . <eos> a novel method is presented for robustly simulating coupled dynamics in fluid body interactions with vorticity based flow solvers . in this work , the fluid dynamics are simulated with a viscous vortex particle method . in the first substcp of each time increment , the fluid convective and diffusive processes are treated , while a predictor is used to independently advance the body configuration . an iterative corrector is then used to simultaneously remove the spurious slip via vorticity flux and compute the end of step body configuration . fluid inertial forces are isolated and combined with body inertial terms to ensure robust treatment of dynamics for bodies of arbitrary mass . the method is demonstrated for dynamics of articulated rigid bodies , including a falling cylinder , flow induced vibration of it circular cylinder and free swimming of a three link ' fish ' . the error and momentum conservation properties of the method are explored . in the case of the vibrating cylinder , comparison with previous work demonstrates good agreement . ( c ) <digit> elsevier inc. all rights reserved .
a semneural network approach for understanding determinants of interorganizational system standard adoption and performances . <eos> this research examines the adoption of an interorganizational system standard and its benefits by using rosettanet as a case study . a comprehensive research framework derived from institutional theory and a technologyorganizationenvironment model was developed for this research . data were collected from a sample of <digit> malaysian manufacturing firms . a multi state analytic approach was proposed whereby the research model was tested using structural equation modeling ( sem ) , and the results from sem were used as inputs for a neural network model for predicting rosettanet adoption . the results showed that factors related to the environment , the interorganizational relationship ( ior ) and an information sharing culture have a positive influence on the adoption of rosettanet . in terms of organizational factors , top management support was found to have a positive and significant relationship with rosettanet adoption . the results also showed that rosettanet adoption has a significant and positive relationship with organizational performance . the research findings can also assist managerial decision making for those organizations planning to adopt rosettanet . this research reduces the previous research gap by advancing understanding on the relationship of adoption factors and rosettanet adoption , and extends previous approaches on rosettanet adoption by investigating the relationships between rosettanet adoption and organizational performance . improved existing technology adoption methodology was achieved by integrating both sem and neural network for examining the adoptions of rosettanet .
an algebraic approach to physical layer network coding . <eos> the problem of designing physical layer network coding ( pnc ) schemes via nested lattices is considered . building on the compute and forward ( c f ) relaying strategy of nazer and gastpar , who demonstrated its asymptotic gain using information theoretic tools , an algebraic approach is taken to show its potential in practical , nonasymptotic , settings . a general framework is developed for studying nested lattice based pnc schemes called lattice network coding ( lnc ) schemes for short by making a direct connection between c f and module theory . in particular , a generic lnc scheme is presented that makes no assumptions on the underlying nested lattice code . c f is reinterpreted in this framework , and several generalized constructions of lnc schemes are given . the generic lnc scheme naturally leads to a linear network coding channel over modules , based on which noncoherent network coding can be achieved . next , performance complexity tradeoffs of lnc schemes are studied , with a particular focus on hypercube shaped lnc schemes . the error probability of this class of lnc schemes is largely determined by the minimum intercoset distances of the underlying nested lattice code . several illustrative hypercube shaped lnc schemes are designed based on constructions a and d , showing that nominal coding gains of <digit> to 7.5 db can be obtained with reasonable decoding complexity . finally , the possibility of decoding multiple linear combinations is considered and related to the shortest independent vectors problem . a notion of dominant solutions is developed together with a suitable lattice reduction based algorithm .
a semantic web application framework for health systems interoperability . <eos> relevant biomedical advances happen daily , and the medical profession relies on this evolution to deliver an improved patient care . in addition , the growing magnitude of data generated by biomedical software and hardware since the initial discovery of the human genome is remarkable in size and variety . hence , best of breed software solutions are at best a couple years behind clinical practice demands . in this paper we detail an innovative semantic web interoperability framework , which provides developers with a complete software stack for semantic application deployment . interoperability is the defining feature of this framework . on the one hand , new instances are able to integrate several types of distributed and heterogeneous data . on the other hand , collected data are made available through a public sparql endpoint .
a potential based generalized cylinder representation . <eos> generalized cylinder ( gc ) is a class of parametric shapes that is very flexible and capable of modeling many different types of real world objects , and have subsequently been the focus of considerable research in the vision community . most of the related works proposed previously have dealt with the recovery of 3d shape description of objects based on the gc representation from one or more 2d image data . different from the objective of the previous works , in this paper , we will propose a new approach to obtain a gc based shape description of 3d objects . the proposed approach of deriving the gc axis is a further extension of the potential based skeletonization approach presented in ( ieee trans . pattern anal . mach . intell . <digit> ( <digit> ) ( <digit> ) <digit> ) . simulation results demonstrate that the derived gc representation will yield better approximation of object shape than that based on simpler subclasses of gc since there is , in principle , no restriction on the topology of the gc axis and the shape of the cross sections . ( c ) <digit> elsevier ltd. all rights reserved .
a business intelligence approach using web search tools and online data reduction techniques to examine the value of product enabled services . <eos> we suggest an intuitive business intelligence approach using online textual data . we found a difference in service value articulation by canadian and eu r d firms . canadian firms focus on market share , service quality and customer satisfaction . eu firms focus on product modernization and optimization of customer time efforts . canadian firms focus on collaboration and co creation aims at product innovation .
resurrector a tunable object lifetime profiling technique for optimizing real world programs . <eos> modern object oriented applications commonly suffer from severe performance problems that need to be optimized away for increased efficiency and user satisfaction . many existing optimization techniques ( such as object pooling and pretenuring ) require precise identification of object lifetimes . however , it is particularly challenging to obtain object lifetimes both precisely and efficiently precise profiling techniques such as merlin introduce several hundred times slowdown even for small programs while efficient approximation techniques often sacrifice precision and produce less useful lifetime information . this paper presents a tunable profiling technique , called resurrector , that explores the middle ground between high precision and high efficiency to find the precision efficiency sweetspot for various liveness based optimization techniques . our evaluation shows that resurrector is both more precise and more efficient than the gc based approximation , and it is orders of magnitude faster than merlin . to demonstrate resurrector 's usefulness , we have developed client analyses to find allocation sites that create large data structures with disjoint lifetimes . by inspecting program source code and reusing data structures created from these allocation sites , we have achieved significant performance gains . we have also improved the precision of an existing optimization technique using the lifetime information collected by resurrector .
design of a dual polarized omnidirectional antenna for broadband applications . <eos> a broadband dual polarized omnidirectional antenna is presented . the proposed antenna consists of two parts , an asymmetric biconical antenna and a cylindrical multilayer polarizer . to have an almost perfect omnidirectional radiation pattern in the horizontal plane and the main radiating beam position at around , in the elevation plane , the asymmetric biconical antenna is used . moreover , to provide dual polarization performance over the <digit> ghz operational bandwidth , a multilayer polarizer is designed and optimized . numerous simulations via ansoft hfss and cst microwave studio cad tools have been made to optimize the radiation pattern , gain , polarization , and the reflection coefficient of the antenna . simulation results show that the radiation characteristics of the proposed antenna are extremely sensitive to the configuration and dimensional parameters of the multilayer polarizer . the designed antenna was fabricated with high mechanical accuracy and measured . satisfactory agreement of computer simulations and experimental results was obtained . the main feature that distinguishes this antenna from the previous designs is the ability to provide the omnidirectional radiation pattern with small ripples , dual polarizations performance , and the wide bandwidth simultaneously . based on these characteristics , the proposed antenna can be useful for broadband communication applications . <digit> wiley periodicals , inc. int j rf and microwave cae 25 591600 , <digit> .
architectural level risk analysis using uml . <eos> risk assessment is an essential part in managing software development . performing risk assessment , during the early development phases enhances resource allocation decisions . in order to improve the software development process and the quality of software products , we need to be able to build risk analysis models based on data that can be collected , early in the development process . these models will help identify the high risk components and connectors of the product architecture , so that remedial actions may be taken in order to control and optimize the development process and improve the quality of the product . in this paper , we present a risk assessment methodology which can be used in the early phases of the software life cycle . we use the unified modeling language ( uml ) and commercial modeling environment rational rose real time ( rosert ) to obtain uml model statistics . first , for each component and connector in software architecture , a dynamic heuristic risk factor is obtained and severity is assessed based on hazard analysis . then , a markov model is constructed to obtain scenarios risk factors . the risk factors of use cases and the overall system risk factor are estimated using the scenarios risk factors within our methodology , we also identify critical components and connectors that would require careful analysis , design , implementation and more testing effort . the risk assessment methodology is applied on a pacemaker case study .
automatic program analysis and evaluation . <eos> there is currently considerable interest in the computing community in the evaluation of computer programming . however , in order to objectively evaluate such concepts , it is necessary to undertake a thorough evaluation of the programming process itself . most previous studies of this type have analyzed , by hand usually , a few instances of programs . this has led to some general conjectures however , the amount of information that must be processed precludes any large scale analysis . in order to avoid this problem , an automatic data collection facility has been implemented as part of a pl <digit> compiler at the university of maryland . this system automatically collects information on each program that has been compiled at almost no additional cost to the user of the compiler . this paper will describe the system and will evaluate some of the characteristics of some of the 25,000 programs that have been run since july , <digit> .
non linear analysis of the low cycle fracture behaviour of isolated tee stub connections . <eos> the present work deals with the numerical analysis of the low cycle fracture behaviour of isolated tee stub connections with partial fillet welds . the overall objective of the research is to assess the seismic performance of bolted partial strength beam to column joints under seismic loading but first , the complexity of these joints is approached after understanding the behaviour of more simple geometries which govern the response . in fact , the study focuses on the low cycle fracture behaviour of tee stub connections that are elemental components of extended end plate connections with partial fillet welds . first , the general experimental program dealing with bolted end plate joints and component parts is presented . the program comprises sets of constant and variable displacement amplitude cyclic tests both on complete specimens and on components . test results have shown good performance of bolted extended end plate moment joints under cyclic loading as well as that of fillet welds that represent an economic solution for thin and moderate extended end plates . then , the connection material is characterized from a microstructural and fracture mechanics standpoint . successively , detailed three dimensional non linear finite element analyses are carried out in order to tune model material parameters whilst two dimensional inelastic analyses are performed in a monotonic and cyclic loading regime to validate connection finite element models . lastly , a parametric study is conducted in order to define details able to reduce loading induced toughness demands . analyses have inferred that fracture driving force demands , quantified in terms of crack tip opening displacements , are reduced by using fillet welds matching the end plate material , by limiting welding induced residual stresses and by increasing the yield to ultimate strength ratio compatibly with plastic analysis requirements .
trichotomy and dichotomy results on the complexity of reasoning with disjunctive logic programs . <eos> we present trichotomy results characterizing the complexity of reasoning with disjunctive logic programs . to this end , we introduce a certain definition schema for classes of programs based on a set of allowed arities of rules . we show that each such class of programs has a finite representation , and for each of the classes definable in the schema , we characterize the complexity of the existence of an answer set problem . next , we derive similar characterizations of the complexity of skeptical and credulous reasoning with disjunctive logic programs . such results are of potential interest . on the one hand , they reveal some reasons responsible for the hardness of computing answer sets . on the other hand , they identify classes of problem instances , for which the problem is easy ( in p ) or easier than in general ( in np ) . we obtain similar results for the complexity of reasoning with disjunctive programs under the supported model semantics .
series of fuzzy sets . <eos> series of fuzzy sets with weakly closed , weakly compactor compact a levels are considered . the basic space is banach space . the subject of this paper is investigation of infinite addition of fuzzy sets and condition when the sum of a levels is equal to the a level of the sum . the results in this paper complement and complete some previous results proved by the same authors in stojakovic and stojakovic addition and series of fuzzy sets , fuzzy sets and systems <digit> ( <digit> ) <digit> <digit> for the series with compact a levels . ( c ) <digit> elsevier b.v. all rights reserved .
investigation on quantitative relationship between chemical shift of carbon <digit> nuclear magnetic resonance spectra and molecular topological structure based on a novel atomic distance edge vector ( adev ) . <eos> a set of novel graph theoretical parameters , called the atomic distance edge vector ( adev ) , was developed in our laboratories . a regression equation linking the carbon <digit> chemical shift ( cs ) to an adev containing <digit> descriptor variables of various chemically non equivalent carbon atoms in a molecule was obtained using multiple linear regression ( mlr ) . the regression model was used to predict the carbon <digit> nuclear magnetic resonance ( c <digit> nmr ) spectra of unknown alkanes . it was found that the estimated cs values were in good agreement with the experimental cs values . copyright ( c ) <digit> john wiley sons , ltd .
automatic composition of happy melodies based on relations . <eos> to compose some happy melodies which have hierarchical structures , this paper proposes an automatic melody composition algorithm based on relations . first , various types of melody structure are formalized and saved into a database , so the melody structure form preferred by a user can be elected by human computer interaction . second , some sequences of trunk note and several algorithms of splitting note are constructed by means of the pitch interval features of happy melody , and the theme phrase of happy melody is generated by splitting some trunk notes of the trunk note sequence . third , several types of operators for developing the theme phrase , which include pitch offsetting , phrase inversing and repeating developing , are constructed using relationship methods . finally , under the guidance of the elected melody structure , some happy melodies of songs are produced automatically by the interreaction of the theme phrase and these operators . experimental results demonstrate that this algorithm can make the obtained melodies have musically meaningful structures , and it is not easy to distinguish these machine generated melodies from human generated melodies .
understanding the local level costs and benefits of erp through organizational information processing theory . <eos> using organizational information processing theory ( oipt ) , we suggest several factors that influence some of the enterprise resource planning ( erp ) costs and benefits that organizations are experiencing . though we do not attempt to address all important factors that contribute to an erps impact , we suggest two organizational characteristics that may have received insufficient attention in other erp literature interdependence and differentiation . high interdependence among organizational sub units , contributes to the positive erp related effects because of erps ability to coordinate activities and facilitate information flows . however , when differentiation among sub units is high , organizations may incur erp related compromise or design costs . we provide a case study that explores the viability of this framework . the case describes some local level impacts of erp and provides some evidence of the validity of the model . unexpected findings are also presented .
distributing proprietary geographic data on the world wide web ucla gis database and map server . <eos> the university of california , los angeles ( ucla ) gis database and map server project is a cooperative effort of gis interested parties on the ucla campus , to deliver centrally stored geographic data via the world wide web ( www ) . both geographic data and geo referenced remotely sensed image data are served from this site . the main objective of this project is to serve gigabytes of data using limited online disk space , used to store the most recently requested data units , and a large tape robot for the offline storage of all the data in the central database . arcview internet map server ( ims ) is employed as the interactive gis mapping server and visual interface for downloading the data from the central database . the server design , database design and web client interface were developed to incorporate a visual interface , to distribute proprietary geo data and to provide gis capabilities .
relaxation of operational amplifier parameters after pulsed electron beam irradiation . <eos> the relaxation of operational amplifier parameters ( offset voltage and differential gain ) with time after pulsed electron beam irradiation has been studied as a function of total dose and amplifier type . four types of operational amplifiers were studied viz. , general purpose bipolar input ( a <digit> ) , super beta transistor input ( lm <digit> ) , jfet input ( lf <digit> ) and mosfet input ( ca <digit> ) from different vendors . the experiments were carried out mainly using <digit> ns pulses from a linear accelerator . the study , the first of its kind , shows that while the electrical transient at the output of the operational amplifier recovers in a few milliseconds , relaxation of parameters can take several to several tens of seconds . this relaxation is attributed to the build up and or anneal of damage in the oxide or at the interface of the internal transistor structures . the change and relaxation of parameters depend on operational amplifier type and total dose , and can have significant effects in certain application domains as illustrated by the response of a thermocouple amplifier after pulsed irradiation .
modified projection method for strongly pseudomonotone variational inequalities . <eos> a modified projection method for strongly pseudomonotone variational inequalities is considered . strong convergence and error estimates for the sequences generated by this method are studied in two versions of the method the stepsizes are chosen arbitrarily from a given fixed closed interval and the stepsizes form a non summable decreasing sequence of positive real numbers . we also propose some interesting examples to analyze the obtained results .
practical approach to determine sample size for building logistic prediction models using high throughput data . <eos> new method to determine sample size for building logistic prediction model . we performed simulations to examine representative null distribution concept . two real data sets were examined to compare full permutation method . drastically improvement when compared to the cpu time required for full permutations .
tabu search based metaheuristic algorithm for software system reliability problems . <eos> this paper presents a new metaheuristic based algorithm for complex reliability problems . the algorithm effectively uses features of the tabu search paradigm , with special emphasis on the exploitation of memory based mechanisms . it balances intensification with diversification via the use of short term and long term memory . the algorithm has been thoroughly tested on benchmark problems from the literature as well as on a pool of random generated instances of very large scale software systems . the proposed algorithm proves to be robust with respect to its parameters and it is especially suited for very large scale instances of the reliability problem , when exact approaches are doomed to fail .
a practical model to predict the repeat purchasing pattern of consumers in the c2c e commerce . <eos> in the recent decade , the c2c e commerce has developed very fast and played a key role in the internet transaction . however , no work has been reported about predicting the consumers repeat purchasing pattern in the area so far . to fill the gap , this paper develops a novel predicting model according to some special characteristics of the c2c e commerce . this model only uses the information about frequencies and timings of transactions . thus , it is simple and convenient to be implemented . to verify the validity of the model , we test some samples from the real transaction data . the numerical results show that the new model outperforms the benchmark pareto nbd model . thus , it is indeed an easy but powerful tool for the vendors on the c2c platform to predict the repeat purchasing pattern of consumers in the c2c e commerce .
exploring conflicts in rule based sensor networks . <eos> this paper addresses rule conflicts within wireless sensor networks . the work is situated within psychiatric ambulatory assessment settings where patients are monitored in and around their homes . detecting behaviours within these settings favours sensor networks , while scalability and resource concerns favour processing data on smart nodes incorporating rule engines . such monitoring involves personalisation , thereby becoming important to program node rules on the fly . since rules may originate from distinct sources and change over time , methods are required to maintain rule consistency . drawing on lessons from feature interaction , the paper contributes novel approaches for detecting and resolving rule conflict across sensor networks .
adaptive aircraft flight control simulation based on an artificial immune system . <eos> over the past decade much progress has been made in the development of adaptive , model following flight control systems . these systems are being designed to account for the degradation and even the failure of the actuators used to implement the control laws within aircraft . typically , these adaptive , model following flight control systems require software components capable of ( <digit> ) monitoring system performance , ( <digit> ) quantifying changes occurring in the performance characteristics of actuators , and ( <digit> ) adapting control laws based on changes in actuator performance . interestingly enough , the challenges facing natural immune systems also require the successful completion of three similar tasks ( <digit> ) monitoring organism performance , ( <digit> ) identification of antigens , and ( <digit> ) distribution of targeted antibodies . thus , the characteristics inherent in natural immune systems have been captured and employed in computational systems called artificial immune systems ( aiss ) . this paper describes an adaptive , model following flight control system based on an artificial immune system . the effectiveness of the approach is demonstrated in a system designed to maintain cruise conditions in the simulation of a boeing <digit> aircraft in the presence of atmospheric turbulence and degradations in the performance characteristics of actuators used to manipulate various control surfaces .
comparative study of cardiovascular markers data by various techniques of multivariate analysis . <eos> indication of cardiovascular risk is based on laboratory analysis of cardiovascular markers in blood . assessment of diagnostic performance of traditionally used as well as new cardiovascular markers is the main goal of this paper . performance of studied laboratory tests was evaluated by using the area below the corresponding receiver operating characteristic ( roc ) curve . application of an objective procedure for the determination of critical values of laboratory tests was based on maximum of efficiency . it was demonstrated that the patient 's diagnosis can be verified or predicted not only by using the results of a chosen individual laboratory test but also by a multivariate statistical approach , which jointly utilizes all performed tests in the form of their optimal linear combination . an effective combination of original variables ( laboratory tests ) is provided either by principal component or discriminant function or logit in case of logistic regression . ( c ) <digit> elsevier b.v. all rights reserved .
an incremental development of the mondex system in event b . <eos> a development of the mondex system was undertaken using event b and its associated proof tools . an incremental approach was used whereby the refinement between the abstract specification of the system and its detailed design was verified through a series of refinements . the consequence of this incremental approach was that we achieved a very high degree of automatic proof . the essential features of our development are outlined . we also present some modelling and proof guidelines that we found helped us gain a deep understanding of the system and achieve the high degree of automatic proof .
localization of the mobile agent using indirect kalman filter in distributed sensor networks . <eos> this paper proposes a localization algorithm for mobile agents in sensor networks using the data fusion method based on indirect kalman filter . the fusion method of an inertial navigation sensor and sensor nodes data is used to acquire the correct position information . since indirect kalman filter is used for estimation of the mobile agent position , the position of the mobile agent can be estimated even when the sensor node can not work or data acquired from the network are not reliable . because measurement errors are used for estimation , its implementation is easy . finally , a sequence of simulations is carried out to confirm the performance of the proposed localization algorithm .
ssc a classifier combination method based on signal strength . <eos> we propose a new classifier combination method , the signal strength based combining ( ssc ) approach , to combine the outputs of multiple classifiers to support the decision making process in classification tasks . as ensemble learning methods have attracted growing attention from both academia and industry recently , it is critical to understand the fundamental issues of the combining rule . motivated by the signal strength concept , our proposed ssc algorithm can effectively integrate the individual vote from different classifiers in an ensemble learning system . comparative studies of our method with nine major existing combining rules , namely , geometric average rule , arithmetic average rule , median value rule , majority voting rule , borda count , max and min rule , weighted average , and weighted majority voting rules , is presented . furthermore , we also discuss the relationship of the proposed method with respect to margin based classifiers , including the boosting method ( adaboost.m1 and adaboost.m2 ) and support vector machines by margin analysis . detailed analyses of margin distribution graphs are presented to discuss the characteristics of the proposed method . simulation results for various real world datasets illustrate the effectiveness of the proposed method .
bifurcations of plane wave ( cw ) solutions in the complex cubic quintic ginzburg landau equation . <eos> singularity theory is used to comprehensively investigate the bifurcations of the steady states of the traveling wave odes of the cubic quintic ginzburg landau equation ( cgle ) . these correspond to plane waves of the pde . in addition to the most general situation , we also derive the degeneracy conditions on the eight coefficients of the cgle under which the equation for the steady states assumes each of the possible quartic ( the quartic fold and an unnamed form ) , cubic ( the pitchfork and the winged cusp ) , and quadratic ( four possible cases ) normal forms for singularities of codimension up to three . since the actual governing equations are employed , all results are globally valid , and not just of local applicability . in each case , the recognition problem for the unfolded singularity is treated . the transition varieties , i.e. , the hysteresis , isola , and double limit curves are presented for each normal form . for both the most general case , as well as for various combinations of coefficients relevant to the particular cases , the bifurcations curves are mapped out in the various regions of parameter space delimited by these varieties . the multiplicities and interactions of the plane wave solutions are then comprehensively deduced from the bifurcation plots in each regime , and include features such as regimes of hysteresis among co existing states , domains featuring more than one interval of hysteresis , and isola behavior featuring dynamics unrelated to the primary , solution branch in limited ranges of parameter space . ( c ) <digit> imacs . published by elsevier b.v. all rights reserved .
effect of sequences on the shape of protein energy landscapes . <eos> protein folding is a long standing problem in biology , whose mechanism is still not completely understood . funnel shape energy landscape has been proposed as a plausible folding mechanism in which proteins can fold through multiple possible pathways from unfolded states to their specific three dimensional structures . however , the factors that determine the funnel shape energy landscapes are largely unknown . in this study , we use the hydrophobic hydrophilic ( hp ) model , a simplified model for studying protein folding , to investigate the factors that affect the shapes of protein energy landscapes . we designed a clustering method based on graph theory to analyze the conformations sampled using a recently developed monte carlo method , fress . we found that the way protein motions are modeled ( the move sets ) has a significant effect on the shape of protein energy landscapes . we also found that sequences with around <digit> <digit> % hydrophobic residues tend to have a stable state represented by a single dominant cluster , consisting of a large number of similar conformations . the energy landscapes resemble a funnel , where there are many paths to minimum energy conformations in the dominant cluster from conformations of higher energies . we also found that sequences with hydrophobic residues above or below the optimal range of <digit> <digit> % do not have a single stable state . instead , there are numerous small clusters , representing multiple local energy minima . our finding is consistent with the compositions of hydrophobic and polar residues in globular proteins ( which fold to unique structures ) , where on average there are around <digit> % hydrophobic residues in globular proteins . our study suggests that hydrophobic interaction is likely a major force leading to the funnel shape energy landscape of proteins and the composition of hydrophobic and polar residues is an important sequence feature for the formation of funnel shape of protein energy landscapes .
modelling of glioblastoma growth by linking a molecular interaction network with an agent based model . <eos> in this work , a mathematical model of malignant brain tumour growth is presented . in particular , the growth of glioblastoma is investigated on the intracellular and intercellular scale . the go or grow principle of tumour cells states that tumour cells either migrate or proliferate . for glioblastoma , microrna <digit> has been shown to be an energy dependent key regulator of the lkb1 ( liver kinase b1 ) and ampk ( amp activated protein kinase ) pathway that influences the signalling for migration or cell division . we introduce a mathematical model that reproduces these biological processes . the intracellular molecular interaction network is represented by a system of nine ordinary differential equations . this is put into a multiscale context by applying an agent based approach each cell is equipped with this interaction network and additional rules to determine its new phenotype as either migrating , proliferating or quiescent . the evaluation of the proposed model by comparison of the results with in vitro experiments indicates its validity .
agents for energy efficiency in ubiquitous environments . <eos> in ubiquitous environments a vast amount of mobile human and software entities , each with limited resources and knowledge , needs to interact with each other to achieve common and or individual goals within a specific context . due to their autonomy , proactiveness , mobility , social capability , and the successful implementation of agent mediated applications and services over the web , different scenarios have been proposed in literature for the use of agents in ubiquitous environments for a wide range of applications such as user interfaces , mobile computing , information retrieval and filtering , smart messaging , telecommunication and m commerce . in this paper , we address the problem of energy efficiency in ubiquitous environments from customer satisfaction , customer awareness and energy savings points of view . we propose an agent based information system in which agents act on behalf of the users , are situated within the appliances , sensors and actuators , or represent different stakeholders in the energy market . different scenarios have been proposed as the basis for the proposed information system .
using formal models to objectively judge quality of multi threaded programs in empirical studies . <eos> empirical studies are important for understanding how well current design methods and notations support development of multi threaded programs . unfortunately , concurrency exacerbates an already difficult problem in drawing conclusions from such studies how to objectively measure the quality of candidate solutions produced by participants in the studies . this paper explores the use of formal modeling and analysis for this purpose . we describe initial findings of a small pilot study to determine if we can objectively differentiate sample candidate solutions with respect to their use of synchronization primitives . to do so , we faithfully model these candidate solutions and various synchronization related properties in the finite state processes ( fsp ) notation and use the labeled transition system analyzer ( ltsa ) to analyze the solution models against the properties .
improved algorithms for unsupervised discriminant projection . <eos> dimension reduction has been applied in various areas of pattern recognition and data mining . while a traditional dimension reduction method , principal component analysis ( pca ) finds projective directions to maximize the global scatter in data , locality preserving projection ( lpp ) pursues linear dimension reduction to minimize the local scatter . however , the discriminative power by either global or local scatter optimization is not guaranteed to be effective for classification . a recently proposed method , unsupervised discriminant projection ( udp ) aims to minimize the local scatter among near points and maximize the global scatter of distant points at the same time . although its performance has been proven to be comparable to other dimension reduction methods , pca preprocessing step due to the singularity of global and local scatter matrices may degrade the performance of udp . in this paper , we propose several algorithms to improve the performances of udp greatly . an improved algorithm for udp is presented which applies the generalized singular value decomposition ( gsvd ) to overcome singularities of scatter matrices in udp . two dimensional udp and nonlinear extension of udp are also proposed . extensive experimental results demonstrate superiority of the proposed algorithms .
scalable and noise tolerant web knowledge extraction for search task simplification . <eos> the simplification of key tasks of search engine users by directly returning structured knowledge according to their query intents has attracted much attention from both the industry and the academia . the challenge lies in automatically extracting structured knowledge from noisy and complex web scale websites . although various automatic wrapper induction algorithms have been proposed , ineffectiveness or inefficiency issues beset many of their web scale applications . in this paper , we propose an unsupervised automatic wrapper induction algorithm , named skes , to efficiently extract knowledge from semi structured websites . skes induces the wrapper in a divide and conquer mode dividing the general wrapper into sub wrappers that can independently learn from data , making it efficient and easy to implement in a parallel mode . moreover , by employing techniques such as tag path representation of web pages , skes can dramatically reduce the number of tags and naturally differentiate their roles . the proposed solution was applied and evaluated on a large number of real websites as well as compared with two existing methods that are most related to it . the proposed method is much more efficient than the existing methods , and provided high extraction accuracy . we have extracted 2.5 million entities and <digit> million data fields from over <digit> thousand high traffic websites , which demonstrates the applicability of this method . furthermore , based on the automatically extracted data , we built a prototype to serve structured knowledge that simplifies the key search tasks of end users . the feedback received for the prototype was highly positive . ( c ) <digit> elsevier b.v. all rights reserved .
a rough margin based one class support vector machine . <eos> we propose a rough margin based one class support vector machine ( rough one class svm ) by introducing the rough set theory into the one class svm , to deal with the over fitting problem . we first construct rough lower margin , rough upper margin , and rough boundary and then maximize the rough margin rather than the margin in the one class svm . thus , more points are adaptively considered in constructing the separating hyper plane than those used in the conventional one class svm . moreover , different points staying at the different positions are proposed to give different penalties . specifically , the samples staying at the lower margin are given the larger penalties than those in the boundary of the rough margin . therefore , the new classifier can avoid the over fitting problem to a certain extent and yields great generalization performance . experimental results on one artificial dataset and eight benchmark datasets demonstrate the feasibility and validity of our proposed algorithm .
a global optimization method for nonconvex separable programming problems . <eos> conventional methods of solving nonconvex separable programming ( nsp ) problems by mixed integer programming methods requires adding numerous <digit> variables . in this work , we present a new method of deriving the global optimum of a nsp program using less number of <digit> variables . a separable function is initially expressed by a piecewise linear function with summation of absolute terms . linearizing these absolute terms allows us to convert a nsp problem into a linearly mixed <digit> program solvable for reaching a solution which is extremely close to the global optimum .
implementation and evaluation of active storage in modern parallel file systems . <eos> active storage is a technology aimed at reducing the bandwidth requirements of current supercomputing systems , and leveraging the processing power of the storage nodes used by some modern file systems . to achieve both objectives , active storage moves certain processing tasks to the storage nodes , near the data they manage . our proposal for active storage has several key features user space implementation which facilitates the port to different file systems , analytical model to anticipate the performance of active storage with respect to a traditional system , support for striped files and complex format files such as netcdf , and scientific friendly programming and run time environment .
empirical evaluation of procedures to generate flexibility in engineering systems and improve lifecycle performance . <eos> the design of engineering systems like airports , communication infrastructures , and real estate projects today is growing in complexity . designers need to consider socio technical uncertainties , intricacies , and processes in the long term strategic deployment and operations of these systems . flexibility in engineering design provides ways to deal with this complexity . it enables engineering systems to change in the face of uncertainty to reduce impacts from downside scenarios ( e.g. , unfavorable market conditions ) while capitalizing on upside opportunities ( e.g. , new technology ) . many case studies have shown that flexibility can improve anticipated lifecycle performance ( e.g. , expected economic value ) compared to current design and evaluation approaches . it is a difficult process requiring guidance and must be done at an early conceptual stage . the literature offers little guidance on procedures helping designers do this systematically in a collaborative context . this study investigated the effects of two educational training procedures on flexibility ( current vs. explicit ) and two ideation procedures ( free undirected brainstorming vs. prompting ) to guide this process and improve anticipated lifecycle performance . controlled experiments were conducted with ninety participants working on a simplified engineering systems design problem . results suggest that a prompting mechanism for flexibility can help generate more flexible design concepts than free undirected brainstorming . these concepts can improve performance significantly ( by up to <digit> % ) compared to a benchmark designeven though users did not expect improved quality of results . explicit training on flexibility can improve user satisfaction with the process , results , and results quality in comparison with current engineering and design training on flexibility . these findings give insights into the crafting and application of simple , intuitive , and efficient procedures to improve lifecycle performance by means of flexibility and performance that may be left aside with existing design approaches . the experimental results are promising toward further evaluation in a real world setting .
mining scenario based specifications with value based invariants . <eos> there have been a number of studies on mining candidate specifications from execution traces . some extract specifications corresponding to value based invariants , while others work on inferring ordering constraints . in this work , we merge our previous work on mining scenario based specifications , extracting ordering constraints in the form of live sequence charts ( lsc ) , a visual specification language , with daikon , a tool for mining value based invariants . the resulting approach strengthens the expressive power of the mined scenarios by enriching them with scenario specific value based invariants . the concept is illustrated using a preliminary case study on a real application .
sip enabled optical burst switching architectures and protocols for application aware optical networks . <eos> this paper presents a novel application aware network architecture for emerging it services and future internet applications . it proposes and analyses network architectures that integrate session initiation protocol ( sip ) with optical burst switched ( obs ) network technology in a unified manner . it demonstrates that the proposed sip enabled obs network can be used to manage application sessions and provide network and it services according to the application requirements . furthermore , various sip over obs layering architectures , utilizing a number of end to end resource discovery protocols ( both for network and non network ( it ) resources ) are presented and analyzed . finally , this paper reports on the validation of functionalities , mechanisms and protocols for the proposed sip enabled obs network , from the node level to the network level , through a test bed demonstrator .
genetic programming for medical classification a program simplification approach . <eos> this paper describes a genetic programming ( gp ) approach to medical data classification problems . in this approach , the evolved genetic programs are simplified online during the evolutionary process using algebraic simplification rules , algebraic equivalence and prime techniques . the new simplification gp approach is examined and compared to the standard gp approach on two medical data classification problems . the results suggest that the new simplification gp approach can not only be more efficient with slightly better classification performance than the basic gp system on these problems , but also significantly reduce the sizes of evolved programs . comparison with other methods including decision trees , naive bayes , nearest neighbour , nearest centroid , and neural networks suggests that the new gp approach achieved superior results to almost all of these methods on these problems . the evolved genetic programs are also easier to interpret than the hidden patterns discovered by the other methods .
a comparison of reward contingent neuronal activity in monkey orbitofrontal cortex and ventral striatum . <eos> we have investigated how neuronal activity in the orbitofrontal ventral striatal circuit is related to reward directed behavior by comparing activity in these two regions during a visually guided reward schedule task . when a set of visual cues provides information about reward contingency , that is , about whether or not a trial will be rewarded , significant subpopulations of neurons in both orbitofrontal cortex and ventral striatum encode this information . orbitofrontal and ventral striatal neurons also differentiate between rewarding and non rewarding trial outcomes , whether or not those outcomes were predicted . the size of the neuronal subpopulation encoding reward contingency is twice as large in orbitofrontal cortex ( <digit> % of neurons ) as in ventral striatum ( <digit> % ) . reward contingency dependent activity also appears earlier during a trial in orbitofrontal cortex than in ventral striatum . the peak reward contingency representation in orbitofrontal cortex ( <digit> % of neurons ) , occurs during the wait period , a period of high anticipation prior to any action . the peak ventral striatal representation of reward contingency ( <digit> % ) occurs during the go period , a time of action . we speculate that signals from orbitofrontal cortex bias ventral striatal activity , and that a flow of reward contingency information from orbitofrontal cortex to ventral striatum serves to guide actions toward rewards
choosing among alternative pasts . <eos> the primary difficulty with testing concurrent programs is their non determinism , where two executions with the same input can yield different results due to a changed thread schedule ( also known as interleaving ) . this problem is aggravated by the fact that most thread schedulers are almost deterministic , and generate the same interleavings over and over for a given testing environment . the traditional approach to testing concurrent programs is to identify and examine the race conditions . a different solution involves noise making , which generates different interleavings at runtime , for example , using embedded sleep statements . this paper proposes a totally different technique for generating a rich set of interleavings . in this approach , operations on shared variables are tracked . every time a shared variable is read , the value to be read is selected from the set of values that were held by this variable during the program execution . the algorithm identifies those values that the variable could hold in some interleaving consistent with the past observed events . within this subset , the value choice can be random , biased random , based on coverage , etc. the problem of identifying read values that are consistent with the past observations is far from simple , since past decisions on value selection affect future ones . our solution is computationally intensive and , therefore , impractical as is . however , insights gained from this solution lead to new heuristics for noise making . copyright ( c ) <digit> john wiley sons , ltd .
an efficient class and object encoding . <eos> an object encoding translates a language with object primitives to one without . similarly , a class encoding translates classes into other primitives . both are important theoretically for comparing the expressive power of languages and for transferring results from traditional languages to those with objects and classes . both are also important foundations for the implementation of object oriented languages as compilers typically include a phase that performs these translations.this paper describes a language with a primitive notion of classes and objects and presents an encoding of this language into one with records and functions . the encoding uses two techniques often used in compilers for single inheritance class based object oriented languages the self application semantics and the method table technique . to type the output of the encoding , the encoding uses a new formulation of self quantifiers that is more powerful than previous approaches .
improved collaborative filtering algorithm via information transformation . <eos> in this paper , we propose a spreading activation approach for collaborative filtering ( sa cf ) . by using the opinion spreading process , the similarity between any users can be obtained . the algorithm has remarkably higher accuracy than the standard collaborative filtering using the pearson correlation . furthermore , we introduce a free parameter beta to regulate the contributions of objects to user user correlations . the numerical results indicate that decreasing the influence of popular objects can further improve the algorithmic accuracy and personality . we argue that a better algorithm should simultaneously require less computation and generate higher accuracy . accordingly , we further propose an algorithm involving only the top n similar neighbors for each target user , which has both less computational complexity higher algorithmic accuracy .
three dimensional computer simulation of liquid drop evaporation . <eos> we use molecular dynamics simulation to describe a method that can be used to model liquid drop evaporation . for application , the liquid is taken to be water using the properties of the liquid and a lennard jones potential , we derive dynamical equations , which are used to describe the gross dynamical behavior of the liquid vapor molecular system . the resulting dynamical equations are solved numerically by a time stepping , numerical method . the evaporation of the liquid to the vapor phase is described . ( c ) <digit> elsevier science ltd. all rights reserved .
energy efficient beaconless geographic routing in wireless sensor networks . <eos> geographic routing is an attractive localized routing scheme for wireless sensor networks ( wsns ) due to its desirable scalability and efficiency . maintaining neighborhood information for packet forwarding can achieve a high efficiency in geographic routing , but may not be appropriate for wsns in highly dynamic scenarios where network topology changes frequently due to nodes mobility and availability . we propose a novel online routing scheme , called energy efficient beaconless geographic routing ( ebgr ) , which can provide loop free , fully stateless , energy efficient sensor to sink routing at a low communication overhead without the help of prior neighborhood knowledge . in ebgr , each node first calculates its ideal next hop relay position on the straight line toward the sink based on the energy optimal forwarding distance , and each forwarder selects the neighbor closest to its ideal next hop relay position as the next hop relay using the request to send clear to send ( rts cts ) handshaking mechanism . we establish the lower and upper bounds on hop count and the upper bound on energy consumption under ebgr for sensor to sink routing , assuming no packet loss and no failures in greedy forwarding . moreover , we demonstrate that the expected total energy consumption along a route toward the sink under ebgr approaches to the lower bound with the increase of node deployment density . we also extend ebgr to lossy sensor networks to provide energy efficient routing in the presence of unreliable communication links . simulation results show that our scheme significantly outperforms existing protocols in wireless sensor networks with highly dynamic network topologies .
approximation systems for functions in topological and in metric spaces . <eos> a notable feature of the tte approach to computability is the representation of the argument values and the corresponding function values by means of infinitistic names . two ways to eliminate the using of such names in certain cases are indicated in the paper . the first one is intended for the case of topological spaces with selected indexed denumerable bases . suppose a partial function is given from one such space into another one whose selected base has a recursively enumerable index set , and suppose that the intersection of base open sets in the first space is computable in the sense of weihrauch grubba . then the ordinary tte computability of the function is characterized by the existence of an appropriate recursively enumerable relation between indices of base sets containing the argument value and indices of base sets containing the corresponding function value . this result can be regarded as an improvement of a result of korovina and kudinov . the second way is applicable to metric spaces with selected indexed denumerable dense subsets . if a partial function is given from one such space into another one , then , under a semi computability assumption concerning these spaces , the ordinary tte computability of the function is characterized by the existence of an appropriate recursively enumerable set of quadruples . any of them consists of an index of element from the selected dense subset in the first space , a natural number encoding a rational bound for the distance between this element and the argument value , an index of element from the selected dense subset in the second space and a natural number encoding a rational bound for the distance between this element and the function value . one of the examples in the paper indicates that the computability of real functions can be characterized in a simple way by using the first way of elimination of the infinitistic names .
comparative assessment of nonlinear metrics to quantify organization related events in surface electrocardiograms of atrial fibrillation . <eos> atrial fibrillation ( af ) is today the most common sustained arrhythmia , its treatment being not completely satisfactory . electrical activity organization analysis within the atria could play a key role in the improvement of current af therapies . the application of a nonlinear regularity index , such as sample entropy ( sampen ) , to the atrial activity ( aa ) fundamental waveform has proven to be a successful noninvasive af organization estimator . however , the use of alternative nonlinear metrics within this context is a pending issue . the present work analyzes the ability of several nonlinear indices to assess regularity of patterns and , thus , organization , in the aa signal and its fundamental waveform , defined as the main atrial wave ( maw ) . precisely , fuzzy entropy , spectral entropy , lempelziv complexity and hurst exponents were studied , achieving more robust and accurate af organization estimates than sampen . results also provided better af organization estimates from the maw than from the aa signal for all the tested nonlinear metrics , which agrees with previous works only focused on sampen . furthermore , some of these indices reported a discriminant ability close to <digit> % in the classification of af organization dependent events , thus outperforming the diagnostic accuracy of sampen and other widely used noninvasive estimators , such as the dominant atrial frequency ( daf ) . as a conclusion , these nonlinear metrics could be considered as promising estimators of noninvasive af organization and could be helpful in making appropriate decisions on the patients management .
near optimal solutions for two point loads between two supports . <eos> in a recent work , sok and rozvany have shown a family of benchmark solutions for the two load problem , probably the optimal one . taking into account that the difference in cost from previously known solutions was small , we have looked for a simpler albeit sub optimal family of solutions . as a result , we have obtained three sub optimal families , including one with a simple circular arc . the three families are compared with the benchmarks , and we have found that all three are of practical interest , because they increase the cost only by a small amount in spite of their simplicity .
authenticated encryption on fpgas from the static part to the reconfigurable part . <eos> recently , techniques have been invented to combine encryption and authentication into a single algorithm which is called authenticated encryption ( ae ) . combining these two security services in hardware produces smaller area compared to two separate algorithms . ae is implemented in the static part of the fpga ( fpga silicon ) in order to secure the reconfiguration process to ensure the confidentiality and integrity of the bitstream . also , it is used in the reconfigurable part of the fpga to support applications which need security requirements like virtual private networks ( vpns ) . this paper presents two different directions for implementing ae cores on fpgas . first , we present efficient asic implementations of ae algorithms , counter with cipher block chaining mode ( ccm ) and galois counter mode ( gcm ) , which are used in the static part of the fpga in order to secure the reconfiguration process . our focus on state of the art algorithms for efficient implementations leads to propose efficient compact architectures in order to be used for fpga bitstream security . presented asic architectures were evaluated by using <digit> and 130nm technologies . second , high throughput gcm architectures are implemented in the reconfigurable part of the fpga by taking the advantage of slow changing key environments like vpns and embedded memory protection . the proposed architectures were evaluated using virtex5 and virtex4 fpgas . it is shown that the performance of the presented work outperforms the previously reported ones .
modeling context aware and socially enriched mashups . <eos> mashup platforms and end user centric composition tools have become increasingly popular . most tools provide web interfaces and visual programming languages to create compositions . much of the previous work has not considered compositions comprising human provided services ( hps ) and software based services ( sbs ) . we introduce a novel hps aware service mashup model which we call socially oriented mashups ( som ) . the inclusion of hps in service mashups raises many challenges such as a qos model that must account for human aspects and the need for flexible execution of mashups . we propose human quality attributes , for example delegation , and a context model capturing various information including location and time . the qos and context model is used at design time and for runtime adaptation of mashups . in this paper , we show how to model context aware soms that include hps and sbs and demonstrate the first results of our working prototype .
a new generation of power lateral and vertical floating islands mos structures . <eos> in this paper , new vertical and lateral mos structures are proposed , in which p layers called floating islands are located in the drift region . these new structures , called flimos ( floating islands metaloxide semiconductor ) transistors , are based on the flidiode concept in which the voltage handling capability is obtained by many pn junctions in series instead of the conventional diode , where the breakdown voltage is supported by only one pn junction . in the medium voltage range ( 2001000v ) , the on state performance of vertical flimosfet is strongly improved when compared to the conventional mosfet . for instance , for a <digit> v vertical flimosfet , a reduction of the specific on resistance of about <digit> % relative to the conventional structure and of <digit> % relative to the silicon limit is observed . but for a <digit> v lateral flimosfet , a reduction of <digit> % relative to the conventional structure is observed , which is not a very significant improvement when compared to medium voltage vertical devices .
assessment of business value from services re use on soa based e business platform . <eos> the top drivers for soa adoption include the expectation of greater reuse in existing and newly built web services , business flexibility , simplification and speed of integration . so , in order to maximize the intended benefits of service oriented architecture ( soa ) implementation , organizations need to develop a well articulated soa quality strategy to promote trust and reuse . however , the shift to this strategy does not come easy and cheap . building an effective soa platform requires tight integration between new and existing product categories , and it may require large investments . for this reason , conventional investment valuation methods need to be combined with other modern techniques to reflect soa 's long term strategic investment nature , the inherent uncertainty and the managerial discretion . some emerging value based analytical methodologies have started to be developed extensively under the umbrella concept of business value analysis ( bva ) . thus , in this paper , we apply modified cvp bep method developed under the bva umbrella for the evaluation of soa projects , in particular for the services reuse related .
a real time schedule method for aircraft landing scheduling problem based on cellular automaton . <eos> the aircraft landing scheduling ( als ) problem is a typical hard multi constraint optimization problem . in real applications , it is not most important to find the best solution but to provide a feasible landing schedule in an acceptable time . we propose a novel approach which can effectively solve the als while satisfying the real time need . it consists of two steps ( i ) use ca to simulate the landing process in the terminal airspace and to find a considerably good landing sequence ( ii ) a simple genetic algorithm associated with a relaxation operator is used to obtain a better result based on the ca result . experiments have shown that our method is much faster and suitable for real time als problem compared with traditional optimization methods . for all the <digit> data sets , the proposed approach can find satisfactory solutions in less than <digit> seconds .
specification and verification of dynamic evolution of software architectures ( retracted article . see vol . <digit> , pg . <digit> , <digit> ) . <eos> with software systems being more and more open and complex , a major challenge for those systems is to evolve themselves gradually , especially during their runtime , where software architectures can provide a foundation for dynamic software evolution . in this paper , we propose a specifying and verifying method for dynamic evolution of software architectures using hypergraph grammars . we propose two general atomic evolution rules and three general composite evolution rules of software architectures based on hypergraphs , and specify dynamic evolution of software architectures according to those rules and a pre defined architecture style through a case study . at last we verify the liveness property of dynamic evolution of software architectures using model checking , and give out corresponding verification algorithms . our approach provides both a user friendly graphical representation and formal models based on grammars . ( c ) <digit> elsevier b.v. all rights reserved .


a metamodel and a devs implementation for component based hierarchical simulation modeling . <eos> component based hierarchical simulation is based on having pre built , validated simulation model components that can be coupled to form a composed model that represents a system . the component based approach promises reuse of interoperable components and rapid development . although component based simulation looks like a promising field and the theory originates more than <digit> years ago , many studies do not seem to deliver on the promises , and many simulation projects face problems when reusing existing components in practice . in order to provide reusability and interoperability , it is useful to have generic model definitions that are independent from the implementation details . this means that we need to pay attention to conceptual modeling . the conceptual model is a key concept that forms a bridge between the modelers and the developers , but unfortunately there is no commonly accepted conceptual modeling technique for hierarchical simulation . this paper introduces a metamodel for component based hierarchical simulation and initiates a study of defining a formal component based conceptual modeling technique to overcome the problems in hierarchical simulation . this paper also presents a prototype of a simulation model design environment with a java interpreter which transforms the visual simulation models into devs models .
abstract interpretation of programs as markov decision processes . <eos> we propose a formal language for the specification of trace properties of probabilistic , nondeterministic transition systems , encompassing the properties expressible in linear time logic . those formulas are in general undecidable on infinite deterministic transition systems and thus on infinite markov decision processes . this language has both a semantics in terms of sets of traces , as well as another semantics in terms of measurable functions we give and prove theorems linking the two semantics . we then apply abstract interpretation based techniques to give upper bounds on the worst case probability of the studied property . we propose an enhancement of this technique when the state space is partitioned for instance along the program points allowing the use of faster iteration methods . ( c ) <digit> elsevier b.v. all rights reserved .
multiple scale neural architecture for enhancing regions in the colour image segmentation process . <eos> a dynamic multi scale neural model for enhancing regions and extracting contours in the colour image segmentation process is proposed . this model combines colour and textural information to coherently enhance images through the operation of two main components
a deterministic algorithm for min max and max min linear fractional programming problems . <eos> in this paper , a deterministic global optimization algorithm is proposed for solving min max and max min linear fractional programming problem ( p ) which have broad applications in engineering , management science , nonlinear system , economics and so on . by utilizing equivalent problem ( q ) of the ( p ) and two phase linear relaxation technique , the relaxation linear programming ( rlp ) about the ( p ) is established . the proposed algorithm is convergent to the global minimum of ( p ) through the successive refinement of the feasible region and solutions of a series of rlp . and finally the numerical examples are given to illustrate the feasibilty of the presented algorithm .

2lev d2p4 a package of high performance preconditioners for scientific and engineering applications . <eos> we present a package of parallel preconditioners which implements one level and two level domain decomposition algorithms on the top of the psblas library for sparse matrix computations . the package , named 2levd2p4 ( two level domain decomposition parallel preconditioners package based on psblas ) , currently includes various versions of additive schwarz preconditioners that are combined with a coarse level correction to obtain two level preconditioners . a pure algebraic formulation of the preconditioners is considered . 2lev d2p4 has been written in fortran <digit> , exploiting features such as abstract data type creation , functional overloading and dynamic memory management , while providing a smooth path towards the integration in legacy application codes . the package , used with krylov solvers implemented in psblas , has been tested on large scale linear systems arising from model problems and real applications , showing its effectiveness .
free material optimization via mathematical programming . <eos> this paper deals with a central question of structural optimization which is formulated as the problem of finding the stiffest structure which can be made when both the distribution of material as well as the material itself can be freely varied . we consider a general multi load formulation and include the possibility of unilateral contact . the emphasis of the presentation is on numerical procedures for this type of problem , and we show that the problems after discretization can be rewritten as mathematical programming problems of special form . we propose iterative optimization algorithms based on penalty barrier methods and interior point methods and show a broad range of numerical examples that demonstrates the efficiency of our approach . ( c ) <digit> the mathematical programming society , inc. published by elsevier science b.v.
computers in the faculty of health science <digit> years on . <eos> in <digit> , the faculty of health science at central queensland university ( cqu ) committed itself to a major instructional development project for its nursing education program . partially supported by a national priority ( reserve ) fund ( nprf ) grant , the faculty undertook a multi year project to develop computer based learning materials . the success of that project was mixed . however , by <digit> staff and students within the faculty were using computers regularly , many of the staff were involved in developing computer based instructional materials and some staff were using available computer based tools to extend the scope of the standard health science materials developed under the project . as well , the faculty was committed to using faculty funds for technical support staff to maintain its computer infrastructure and to assist both staff and students in using that infrastructure . the authors review the original nprf project , then explore the faculty 's continued development and use of computer based learning materials through an examination of student and staff evaluations , personal recollections and subsequent projects . this paper demonstrates that the nprf project spawned a number of other computer based developments , including the offering of health informatics degree programs at the undergraduate and postgraduate levels , the development of the faculty 's current internet site and a cd rom based interactive multimedia project for diabetes education . it also demonstrates that moderate levels of funding can maintain a computer literate faculty which views computers as essential tools for learning . ( c ) <digit> elsevier science ireland ltd. all rights reserved .
characterization of the binding of angiotensin ii receptor blockers to human serum albumin using docking and molecular dynamics simulation . <eos> human serum albumin ( hsa ) , the most abundant protein found in blood plasma , transports many drugs and ligands in the circulatory system . the drug binding ability of hsa strongly influences free drug concentrations in plasma , and is directly related to the effectiveness of clinical therapy . in current work , binding of hsa to angiotensin ii receptor blockers ( arbs ) are investigated using docking and molecular dynamics ( md ) simulations . docking results demonstrate that the main hsa arb binding site is subdomain iiia of hsa . simulation results reveal clearly how hsa binds with valsartan and telmisartan . interestingly , electrostatic interactions appear to be more important than hydrophobic interactions in stabilizing binding of valsartan to hsa , and vice versa for hsa telmisartan . the molecular distance between hsa trp214 ( donor ) and the drug ( acceptor ) can be measured by fluorescence resonance energy transfer ( fret ) in experimental studies . the average distances between trp <digit> and arbs are estimated here based on our md simulations , which could be valuable to future fret studies . this work will be useful in the design of new arb drugs with desired hsa binding affinity .
abstract state machines capture parallel algorithms correction and extension . <eos> we consider parallel algorithms working in sequential global time , for example , circuits or parallel random access machines ( prams ) . parallel abstract state machines ( parallel asms ) are such parallel algorithms , and the parallel asm thesis asserts that every parallel algorithm is behaviorally equivalent to a parallel asm . in an earlier article , we axiomatized parallel algorithms , proved the asm thesis , and proved that every parallel asm satisfies the axioms . it turned out that we were too timid in formulating the axioms they did not allow a parallel algorithm to create components on the fly . this restriction did not hinder us from proving that the usual parallel models , like circuits or prams or even alternating turing machines , satisfy the postulates . but it resulted in an error in our attempt to prove that parallel asms always satisfy the postulates . to correct the error , we liberalize our axioms and allow on the fly creation of new parallel components . we believe that the improved axioms accurately express what parallel algorithms ought to be . we prove the parallel thesis for the new , corrected notion of parallel algorithms , and we check that parallel asms satisfy the new axioms .
non linear straindisplacement equations exactly representing large rigid body motions . part iii . analysis of tm shells with constraints . <eos> the precise representation of arbitrarily large rigid body motions in the displacement patterns of curved timoshenkomindlin type ( tm ) shell elements has been considered in part i of the present work . in part ii it has been developed an enhanced mixed finite element formulation that allows using load increments that are much larger than possible with existing geometrically exact displacement based shell element formulations . in this paper the developed formulation is employed to solve frictionless contact problems for tm shells undergoing finite deformations and interacting with rigid bodies . the contact conditions are incorporated into the assumed stressstrain tm shell formulation by applying a perturbed lagrangian procedure with the fundamental unknowns consisting of <digit> displacements and <digit> strains of the bottom and top surfaces of the shell , <digit> conjugate stress resultants and the lagrange multiplier , associated with a nodal contact force , through using the non conventional technique . the efficiency and accuracy of the proposed finite element formulation are demonstrated by means of several numerical examples .
a boundary method of trefftz type for pdes with scattered data . <eos> this paper presents the application of a trefftz type method for partial differential equations ( pdes ) of the elliptic type with inhomogeneous term given by a set of scattered data . the method of particular solutions is used . basis functions of a new type were introduced to approximate the scattered data . using these basis functions , we get the approximation in the form of series over some orthogonal system of eigenfunctions . the particular case of the trigonometric eigenfunctions is considered . the corresponding approximation of the inhomogeneous term allows to get a particular solution for pdes with constant coefficients or for the systems of such pdes easily . we test our basis functions on recovering well known franke 's and peaks functions given by scattered data . we also present results of solution helnholtz pde , pde with differential operator of 4th order and system of pdes arising in shell deflection problems . a comparison of the numerical solutions with analytic solutions is performed for all the problems .
an environment for distributed collaboration among humans and software agents . <eos> this paper describes an implemented software prototype for the distributed collaboration and interaction ( dci ) system , which helps humans to act as an integrated part of a multi agent system . human interaction with agents who act autonomously most of the time , such as a process control agent in a refinery , has received little attention compared to human interaction with agents who provide a direct service to humans , such as information retrieval . this paper describes how liaison agents within the dci system can support human interaction with other agents that are not , by design , human centric but must be supervised by , or coordinated with , humans . the dci system provides a step toward future seamless integration of humans and software agents into a cohesive multi agent system .
medial spheres for shape approximation . <eos> we study the problem of approximating a 3d solid with a union of overlapping spheres . in comparison with a state of the art approach , our method offers more than an order of magnitude speedup and achieves a tighter approximation in terms of volume difference with the original solid while using fewer spheres . the spheres generated by our method are internal and tangent to the solid 's boundary , which permits an exact error analysis , fast updates under local feature size preserving deformation , and conservative dilation . we show that our dilated spheres offer superior time and error performance in approximate separation distance tests than the state of the art method for sphere set approximation for the class of ( sigma , theta ) fat solids . we envision that our sphere based approximation will also prove useful for a range of other applications , including shape matching and shape segmentation .
an experimental comparison of parallel algorithms for hyperspectral analysis using heterogeneous and homogeneous networks of workstations . <eos> imaging spectroscopy , also known as hyperspectral imaging , is a new technique that has gained tremendous popularity in many research areas , including satellite imaging and aerial reconnaissance . in particular , nasa is continuously gathering high dimensional image data from the surface of the earth with hyperspectral sensors such as the jet propulsion laboratorys airborne visible infrared imaging spectrometer ( aviris ) or the hyperion hyperspectral imager aboard nasas earth observing <digit> ( eo <digit> ) spacecraft . despite the massive volume of scientific data commonly involved in hyperspectral imaging applications , very few parallel strategies for hyperspectral analysis are currently available , and most of them have been designed in the context of homogeneous computing platforms . however , heterogeneous networks of workstations represent a very promising cost effective solution that is expected to play a major role in the design of high performance computing platforms for many on going and planned remote sensing missions . our main goal in this paper is to understand parallel performance of hyperspectral imaging algorithms comprising the standard hyperspectral data processing chain ( which includes pre processing , selection of pure spectral components and linear spectral unmixing ) in the context of fully heterogeneous computing platforms . for that purpose , we develop an exhaustive quantitative and comparative analysis of several available and new parallel hyperspectral imaging algorithms by comparing their efficiency on both a fully heterogeneous network of workstations and a massively parallel homogeneous cluster at nasas goddard space flight center in maryland .

feature selection with partition differentiation entropy for large scale data sets . <eos> feature selection , especially for large data sets , is a challenging problem in areas such as pattern recognition , machine learning and data mining . with the development of data collection and storage technologies , the data has become bigger than ever , thus making it difficult for learning from large data sets with traditional methods . in this paper , we introduce the partition differentiation entropy from the viewpoint of partition in rough sets to measure the significance and uncertainty of attributes , and present a feature selection method for large scale data sets based on the information theoretical measurement of attribute significance . given a large scale decision information system , the proposed method first divides it into small sub information systems according to the decision classes . then by computing partition differentiation entropy in the sub systems , the partition differentiation entropy of the attribute subset in the original decision information system is obtained . accordingly , the important features are selected based on the value of partition differentiation entropy . the experimental results show that the idea of the proposed method is feasible and valid .
mesh refinement algorithms in an unstructured solver for multiphase flow simulation using discrete particles . <eos> this study developed spray adaptive mesh refinement algorithms with directional sensitivity in an unstructured solver to improve spray simulation for internal combustion engine application . inadequate spatial resolution is often found to cause inaccuracies in spray simulation using the lagrangianeulerian approach due to the over estimated diffusion and inappropriate liquidgas phase coupling . dynamic mesh refinement algorithms adaptive to fuel sprays and vapor gradients were developed in order to increase the grid resolution in the spray region to improve simulation accuracy . the local refinement introduced the coarse fine face interface that requires advanced numerical schemes for flux calculation and grid rezoning with moving boundaries . to resolve the issue in flux calculation , this work implemented the refinement coarsening algorithms into a collocated solver to avoid tedious interpolations in solving the momentum equations . a pressure correction method was applied to address unphysical pressure oscillations due to the collocation of pressure and velocity . an edge based algorithm was used to evaluate the edge centered quantities in order to account for the contributions from all the cells around an edge at the coarse fine interface . a quasi second order upwind scheme with strong monotonicity was also modified to accommodate the coarse fine interface for convective fluxes . to resolve the issue related to grid rezoning , rezoning was applied to the initial baseline mesh only and the new locations of the refined grids were obtained by interpolating the updated baseline mesh . the time step constraints were also re evaluated to account for the change resulting from mesh refinement . the present refinement algorithm was used in simulating fuel sprays in an engine combustion chamber . it was found that the present approach could produce the same level of results as those using the uniformly fine mesh with substantially reduced computer time . results also showed that this approach could alleviate the artifacts related to the lagrangian discrete modeling of spray drops due to insufficient spatial resolution .
children 's perception of computer programming as an aid to designing programming environments . <eos> primary school pupils of two different age groups were asked to draw pictures of people programming computers and then asked questions to reveal their understanding of computer programming . as was expected neither group showed great understanding of how computer programs are produced . programming was seen as the productions of visual and audio effects . the older children recognised that programming was something to do with controlling the computer . these understandings are used to produce a list of four recommendations for the design of programming environments for children .
content based filtering for efficient online materialized view maintenance . <eos> real time materialized view maintenance has become increasingly popular , especially in real time data warehousing and data streaming environments . upon updates to base relations , maintaining the corresponding materialized views can bring a heavy burden to the rdbms . a traditional method to mitigate this problem is to use the where clause condition in the materialized view definition to detect whether an update to a base relation is relevant and can affect the materialized view . however , this detection method does not consider the content in the base relations and hence misses a large number of filtering opportunities . in this paper , we propose a content based method for detecting irrelevant updates to base relations of a materialized view . at the cost of using more space , this method increases the probability of catching irrelevant updates by judiciously designing filtering relations to capture the content in the base relations . based on the content based method , a prototype real time data warehouse has been implemented on top of ibm 's system s using ibm db2 . using an analytical model and our prototype , we show that the content based method can catch most ( or all ) irrelevant updates to base relations that are missed by the traditional method . thus , when the fraction of irrelevant updates is non negligible , the load on the rdbms due to materialized view maintenance can be significantly reduced .
countering code injection attacks with instruction set randomization . <eos> we describe a new , general approach for safeguarding systems against any type of code injection attack . we apply kerckhoff 's principle , by creating process specific randomized instruction sets ( e.g. , machine instructions ) of the system executing potentially vulnerable software . an attacker who does not know the key to the randomization algorithm will inject code that is invalid for that randomized processor , causing a runtime exception . to determine the difficulty of integrating support for the proposed mechanism in the operating system , we modified the linux kernel , the gnu binutils tools , and the bochs x86 emulator . although the performance penalty is significant , our prototype demonstrates the feasibility of the approach , and should be directly usable on a suitable modified processor ( e.g. , the transmeta crusoe ) . our approach is equally applicable against code injecting attacks in scripting and interpreted languages , e.g. , web based sql injection . we demonstrate this by modifying the perl interpreter to permit randomized script execution . the performance penalty in this case is minimal . where our proposed approach is feasible ( i.e. , in an emulated environment , in the presence of programmable or specialized hardware , or in interpreted languages ) , it can serve as a low overhead protection mechanism , and can easily complement other mechanisms .
broad histogram method for continuous systems the xy model . <eos> we propose a way of implementing the broad histogram monte carlo method to systems with continuous degrees of freedom , and we apply these ideas to investigate the three dimensional xy model with periodic boundary conditions . we have found an excellent agreement between our method and traditional metropolis results for the energy , the magnetization , the specific heat and the magnetic susceptibility on a very large temperature range . for the calculation of these quantities in the temperature range 0.7 < t < 4.7 our method took less cpu time than the metropolis simulations for <digit> temperature points in that temperature range . furthermore , it calculates the whole temperature range 1.2 < t < 4.7 using only 2.2 times more computer effort than the histogram monte carlo method for the range 2.1 < t < 2.2 . our way of treatment is general it can also be applied to other systems with continuous degrees of freedom .
a concurrent dualband distributed impedance matching network . <eos> in a number of applications , it is necessary a network which could provide well defined impedance values at different frequencies , that is a multiband impedance matching network . in this paper , we present a novel approach for dual band impedance matching network design using multiresonant circuits . these multiresonant circuits provide short and open circuit conditions at different frequencies , thus enabling and disabling capacitors and inductors to form the desired impedance matching network . to illustrate the proposed method , experimental and simulation results of a dual band impedance matching network operating at frequencies 2.4 ghz and 5.2 ghz are also presented .
a fast algorithm for level set like active contours . <eos> this paper describes a fast algorithm for topology independent tracking of moving interfaces under curvature and velocity field dependent speed laws . this is usually done in the level set framework using the narrow band algorithm , which accurately solves the level set equation but is too slow to use in real time or near real time image segmentation applications . in this paper we introduce a fast algorithm for tracking moving interfaces in a level set like manner . the algorithm relies on two key components first , it tracks the interface by scheduling point wise propagation events using a heap sorted queue . second , the local geometric properties of the interface are defined so that they can be efficiently updated in an incremental manner and so that they do not require the presence of the signed distance function . finally examples are given that indicate that the algorithm is fast and accurate enough for near real time segmentation applications .
design and analysis of bilinear pairing based mutual authentication and key agreement protocol usable in multi server environment . <eos> with the increasing popularity and demand for various applications , the internet user accesses remote server by performing remote user authentication protocol using smart card over the insecure channel . in order to resist insider attack , most of the users remember a set of identity and password for accessing different application servers . therefore , remembering set of identity and password is an extra overhead to the user . to avoid the mentioned shortcoming , many remote user authentication and key agreement protocols for multi server architecture have been proposed in the literature . recently , hsiehleu proposed an improve protocol of liao et al. scheme and claimed that the improve protocol is applicable for practical implementation . however , through careful analysis , we found that hsiehleu scheme is still vulnerable to user anonymity , password guessing attack , server masquerading attack and the password change phase is inefficient . therefore , the main aim of this paper was to design a bilinear pairing based three factors remote user authentication scheme using smart card for providing security weaknesses free protocol . in order to validate security proof of the proposed protocol , this paper uses ban logic which ensures that the same protocol achieves mutual authentication and session key agreement property securely . furthermore , this paper also informally illustrates that the proposed protocol is well protected against all the relevant security attacks . the performance analysis and comparison with other schemes are also made , and it has been found that the proposed protocol achieves complete security requirements with comparatively lesser complexities .
a mechanical chest compressor closed loop controller with an effective trade off between blood flow improvement and ribs fracture reduction . <eos> chest compression ( cc ) is a significant emergency medical procedure for maintaining circulation during cardiac arrest . although cc produces the necessary blood flow for patients with heart arrest , improperly deep cc will contribute significantly to the risk of chest injury . in this paper , an optimal cc closed loop controller for a mechanical chest compressor ( occ mcc ) was developed to provide an effective trade off between the benefit of improved blood perfusion and the risk of ribs fracture . the trade off performance of the occ mcc during real automatic mechanical ccs was evaluated by comparing the occ mcc and the traditional mechanical cc method ( tmcm ) with a human circulation hardware model based on hardware simulations . a benefit factor ( bf ) , risk factor ( rf ) and benefit versus risk index ( bri ) were introduced in this paper for the comprehensive evaluation of risk and benefit . the occ mcc was developed using the labview control platform and the mechanical chest compressor ( mcc ) controller . pid control is also employed by mcc for effective compression depth regulation . in addition , the physiological parameters model for mcc was built based on a digital signal processor for hardware simulations . a comparison between the occ mcc and tmcm was then performed based on the simulation test platform which is composed of the mcc , labview control platform , physiological parameters model for mcc and the manikin . compared with the tmcm , the occ mcc obtained a better trade off and a higher bri in seven out of a total of nine cases . with a higher mean value of cardiac output ( 1.35 l min ) and partial pressure of end tidal co2 ( 15.7 mmhg ) , the occ mcc obtained a larger blood flow and higher bf than tmcm ( 5.19 vs. 3.41 ) in six out of a total of nine cases . although it is relatively difficult to maintain a stable cc depth when the chest is stiff , the occ mcc is still superior to the tmcm for performing safe and effective cc during cpr . the occ mcc is superior to the tmcm in performing safe and effective cc during cpr and can be incorporated into the current version of mechanical cc devices for high quality cpr , in both in hospital and out of hospital cpr settings .
combinatorial optimization solutions for the maximum quartet consistency problem . <eos> phylogenetic analysis is a widely used technique , for example in biology and biomedical sciences . the construction of phylogenies can be computationally hard . a commonly used solution for construction of phylogenies is to start from a set of biological species and relations among those species . this work addresses the case where the relations among species are specified as quartet topologies . moreover , the problem to be solved consists of computing a phylogeny that satisfies the maximum number of quartet topologies . this is referred to as the maximum quartet consistency ( mqc ) problem , and represents an np hard optimization problem . mqc has been solved both heuristically and exactly . exact solutions formqc include those based on constraint programming , answer set programming , pseudo booleanoptimization ( pbo ) , and satisfiability modulo theories ( smt ) . this paper provides a comprehensive overview of the use of pbo and smt for solving mqc , and builds on recent work in this area . moreover , the paper provides new insights on how to use smt for solving optimization problems , by focusing on the concrete case of mqc . the solutions based on pbo and smt were experimentally compared with other exact solutions . the results show that for instances with small percentage of quartet errors , the models based on smt can be competitive , whereas for instances with higher number of quartet errors the pbo models are more efficient .
an approach to simulation based parameter and structure optimization of matlab simulink models using evolutionary algorithms . <eos> in engineering , a broad range of environments exist for modeling and simulation with integrated parameter optimization . the established techniques only optimize model parameter values , the model structure is considered to be fixed . as system performance is optimized , one may have to redesign the model structure . the redesign is done manually by an analyst . the suboptimal combination of automatic parameter optimization and manual structural changes leads to an optimization task that is prone to error . this paper details an approach that provides optimization through automatic reconfiguration of both the model structure and model parameters . an optimization method that uses an evolutionary algorithm is supported by a model management method . this method is based on the system entity structure model base framework . the admissible model structures and their associated model parameter sets are specified using the system entity structure ontology . basic dynamic model components are organized in a model base . in addition to this , new algorithms are introduced . these map knowledge coded in the system entity structure to a set of numerical ( structure ) parameters , and also perform this mapping in reverse . in this manner a combined structure and parameter optimization problem is derived . since both methods evolutionary algorithm and model management work together concurrently , different system configurations can be evaluated automatically . the objective is to provide an optimal solution a model optimized for both parameter and structure .
a visual probe localization and calibration system for cost effective computer aided 3d ultrasound . <eos> the 3d ultrasound systems produce much better reproductions than 2d ultrasound , but their prohibitively high cost deprives many less affluent organization this benefit . this paper proposes using the conventional 2d ultrasound equipment readily available in most hospitals , along with a single conventional digital camera , to construct 3d ultrasound images . the proposed system applies computer vision to extract position information of the ultrasound probe while the scanning takes place . the probe , calibrated in order to calculate the offset of the ultrasound scan from the position of the marker attached to it , is used to scan a number of geometrical objects . using the proposed system , the 3d volumes of the objects were successfully reconstructed . the system was tested in clinical situations where human body parts were scanned . the results presented , and confirmed by medical staff , are very encouraging for cost effective implementation of computer aided 3d ultrasound using a simple setup with 2d ultrasound equipment and a conventional digital camera .
mixed methods research a new approach to evaluating the motivation and satisfaction of university students using advanced visual technologies . <eos> a mixed methods study evaluating the motivation and satisfaction of architecture degree students using interactive visualization methods is presented in this paper . new technology implementations in the teaching field have been largely extended to all types of levels and educational frameworks . however , these innovations require approval validation and evaluation by the final users , the students . in this paper , the advantages and disadvantages of applying mixed evaluation technology are discussed in a case study of the use of interactive and collaborative tools for the visualization of 3d architectonical models . the main objective was to evaluate architecture and building science students the motivation to use and satisfaction with this type of technology and to obtain adequate feedback that allows for the optimization of this type of experiment in future iterations .
euler rotations in plate tectonic reconstructions . <eos> euler rotations describe the motions of rigid plates on a sphere and can be used to perform paleogeographic reconstructions and to compare paleomagnetic data from different continents . the properties and mathematical equations for these rotations are summarized . subroutines and example programs in turbo pascal that perform euler rotations and calculate angular velocities and relative plate velocities are presented .
an agent based congestion control and notification scheme for tcp over abr . <eos> we overview in this paper the enhancement of tcp 's congestion control mechanisms using explicit congestion notification ( ecn ) over atm networks , congestion is indicated by not only packet losses as is currently the case but an agent implemented at the network 's edge as well . the agent bridges the gap between the atm layer and the tcp layer in the protocol stack at the receiver end and coordinates the congestion control algorithms of the tcp transport protocol and the atm cell oriented switching architecture . the novel idea uses abr rate based flow control to notify congestion and adjust the credit based window size of tcp . the effects of running tcp ecn over abr ( available bit rate ) are studied with the help of two simulation models ( lan and wan ) . the simulation results indicate that lans having a single switch and wans with multiple switches benefit most from tcp ecn . in almost all scenarios , tcp ecn achieved significantly lower cell loss , packet retransmissions , buffer utilisation and exhibited better throughput than tcp reno . ( c ) <digit> elsevier science b.v. all rights reserved .
opening the black box of system usage user adaptation to disruptive it . <eos> according to benbasat and barki ( <digit> ) , systems usage has remained a black box in spite of the fact that the construct lies at the heart of a host of studies in the field . we know very little about how exactly users cope with information technology ( it ) , especially disruptive it . to answer such questions , we grounded our current work in beaudry and pinsonneault 's coping model of user adaptation ( <digit> cmua ) , a model that explains user strategies appraising an it event . these strategies are a response to threats and opportunities embedded in the it event and are impacted by the level of control users have over the situation . in the current study , following cmua , we develop and test measures for a deeper understanding of systems usage and user adaptation to it through a <digit> x <digit> laboratory experiment . overall , we found strong support for the cmua model . european journal of information systems ( <digit> ) <digit> , <digit> <digit> . doi 10.1057 ejis .2010.23 published online <digit> april <digit>
product focused software process improvement concepts and experiences from industry . <eos> management problems in the development of software have been addressed over the last years by a strong focus on the improvement of the development processes . software process improvement ( spi ) activities are characterized by an internal focus on a software development department and its procedures . however , the quality of the product is hardly addressed in software process improvement programs . this paper presents the application of a model for product focused spi ( p spi ) and describes experiences with this model in practice . the main conclusions are that p spi puts products to be developed in a central position in improvement programs , results in fulfilling specific quality goals of a company and project , and industrial experiments show interesting benefits . applying the approach in industrial projects showed that p spi is relatively cheap and gives fast results and high benefits .
the walk through approach to authoring multimedia documents . <eos> this paper describes a novel approach to authoring multimedia documents based on the walk through paradigm . using this approach , multimedia authoring tasks can be performed in the context of the multimedia presentation under construction . it greatly simplifies the authoring process by hiding the use of composition constructs and eliminating the turn around time from the editing and testing parts of the development cycle . end user multimedia authoring and fast prototyping can therefore be realized .
an ie and ir approach to deal with geographic information scope in textual documents . <eos> we briefly present requirements and a methodology of semantic annotation for automatic indexing and geo referencing of text documents . the first evaluation results shows that combining a spatial approach with a classical ( statistical based ) ir one , improves in a significant way retrieval accuracy , namely in the case of realistic queries .
pairwise matching of 3d fragments using cluster trees . <eos> we propose a novel and efficient surface matching approach for reassembling broken solids as well as for matching assembly components using cluster trees of oriented points . the method rapidly scans through the space of all possible contact poses of the fragments to be ( re ) assembled using a tree search strategy , which neither relies on any surface features nor requires an initial solution . the new method first decomposes each point set into a binary tree structure using a hierarchical clustering algorithm . subsequently the fragments are matched pairwise by descending the cluster trees simultaneously in a depth first fashion . in contrast to the reassemblage of pottery and thin walled artifacts , this paper addresses the problem of matching broken 3d solids on the basis of their 2.5 d fracture surfaces , which are assumed to be reasonable large . our proposed contact area maximization is a powerful common basis for most surface matching tasks , which can be adapted to numerous special applications . the suggested approach is very robust and offers an outstanding efficiency .
adaptive autonomous positioning of a robot vision system application to quality control on production lines . <eos> an adaptive robot vision system detects obstacles along the camera line of sight . the robot vision system automatically repositions the camera to re establish a correct target imaging . the method employs an fft to determine obstacle rotation and to compute the new position of the robot . perspective distortions are corrected using the knowledge of 3d working space of the robot . the method is demonstrated for on line quality control along appliances production lines .
the optimal human ventral stream from estimates of the complexity of visual objects . <eos> the part of the primate visual cortex responsible for the recognition of objects is parcelled into about a dozen areas organized somewhat hierarchically ( the region is called the ventral stream ) . why are there approximately this many hierarchical levels here i put forth a generic information processing hierarchical model , and show how the total number of neurons required depends on the number of hierarchical levels and on the complexity of visual objects that must be recognized . because the recognition of written words appears to occur in a similar part of inferotemporal cortex as other visual objects , the complexity of written words may be similar to that of other visual objects for humans for this reason , i measure the complexity of written words , and use it as an approximate estimate of the complexity more generally of visual objects . i then show that the information processing hierarchy that accommodates visual objects of that complexity possesses the minimum number of neurons when the number of hierarchical levels is approximately <digit> .
wireless sensor network key management survey and taxonomy . <eos> wireless sensor networks ( wsn ) are mobile ad hoc networks in which sensors have limited resources and communication capabilities . secure communications in some wireless sensor networks are critical . key management is the fundamental security mechanism in wireless sensor network . many key management schemes have been developed in recent years . in this paper , we present wireless sensor network key management survey and taxonomy . we classify proposed wireless sensor network key management schemes into three categories based on the encryption key mechanism . we then divide each category into several subcategories based on key pre distribution and key establishment .
logismos layered optimal graph image segmentation of multiple objects and surfaces cartilage segmentation in the knee joint . <eos> a novel method for simultaneous segmentation of multiple interacting surfaces belonging to multiple interacting objects , called logismos ( layered optimal graph image segmentation of multiple objects and surfaces ) , is reported . the approach is based on the algorithmic incorporation of multiple spatial inter relationships in a single n dimensional graph , followed by graph optimization that yields a globally optimal solution . the logismos method 's utility and performance are demonstrated on a bone and cartilage segmentation task in the human knee joint . although trained on only a relatively small number of nine example images , this system achieved good performance . judged by dice similarity coefficients ( dsc ) using a leave one out test , dsc values of 0.84 0.04 , 0.80 0.04 and 0.80 0.04 were obtained for the femoral , tibial , and patellar cartilage regions , respectively . these are excellent dsc values , considering the narrow sheet character of the cartilage regions . similarly , low signed mean cartilage thickness errors were obtained when compared to a manually traced independent standard in <digit> randomly selected <digit> d mr image datasets from the osteoarthritis initiative database 0.11 0.24 , 0.05 0.23 , and 0.03 0.17 mm for the femoral , tibial , and patellar cartilage thickness , respectively . the average signed surface positioning errors for the six detected surfaces ranged from 0.04 0.12 mm to 0.16 0.22 mm . the reported logismos framework provides robust and accurate segmentation of the knee joint bone and cartilage surfaces of the femur , tibia , and patella . as a general segmentation tool , the developed framework can be applied to a broad range of multiobject multisurface segmentation problems .
semantic hierarchical abstraction of web site structures for web searchers . <eos> the hierarchical abstraction of a web site is useful in organising information and reducing the number of alternatives that must be considered at any one time when browsing the site . we present such an abstraction , which is computed from a user 's query and a web site . a web site is viewed as a directed graph with web nodes and web arcs , where the web nodes correspond to html files ( i.e. , web pages ) and the web arcs correspond to hyperlinks . the abstraction is based on computing semantic weights for the hyperlinks within the site . we have developed a pilot system , called anchor woman , for displaying a map representing the abstraction of a web site . this map helps a user to avoid the feeling of being lost in space and makes it easier for him her to browse the site effectively and efficiently .
multiple sequence alignment as a facility location problem . <eos> a connection is made between certain multiple sequence alignment problems and facility location problems , and the existence of a ptas ( polynomial time approximation scheme ) for these problems is shown . moreover , it is shown that multiple sequence alignment with sp score and fixed gap penalties is max snp hard .
line recognition algorithm for 3d polygonal model using a parallel computing platform . <eos> line recognition based rendering technique has been used effectively for shape transmission of 3d polygon model . line recognition is defined by multifarious forms and characteristics of lines , and has been a fundamental key point in expressing shape of 3d polygon model in non photorealistic rendering technique . line recognition , however , requires a long period of calculation time and thus , various methods have been studied to accelerate the speed of the operation . this paper presents a new method that will accelerate the overall operation compared to the standard cpu based method of extracting ink line . the new method will enhance the efficiency of the calculation speed by applying the parallel processing technique cuda ( compute unified device architecture ) to the complex processes that consume a lot of time such as implicit surface calculation and feature point extraction . the overall performance will be tested and verified through various types of experiments with 3d polygon model .
a platform to support decentralized and dynamically distributed p2p composite owl s service execution . <eos> in a large variety of applications , it is increasingly important to provide application functionality in a modular way by means of ( web ) services . at the same time , pre defined applications are no longer suitable to cope with the high functional dynamics that can be found in novel e business , e health , and e science applications . in contrast , dynamic application creation , i.e. , applications that are assembled ad hoc by service composition and usually instantiated very few times , are more and more becoming prevalent . form a systems point of view , large scale application environments like the internet create scalability requirements towards distributed execution of composite ( web ) services which go beyond the traditional non distributed approach to manage composite services . the contribution of this paper is threefold . first , we present a novel approach that combines those aspects by using different technologies in a distributed environment to dynamically distribute composite service execution in situations where it is beneficial or required . second , the approach considers semantic annotation of services to facilitate new possibilities for data and service co ordination . third , the approach also incorporates the interfaces needed to integrate service execution with semantic service composition planners to allow for dynamic forward failure recovery by contingency service re planning . these concepts are currently developed on the basis of the peer to peer platform osiris next which supports dynamically distributed and decentralized execution of composite semantic services that are described based on owl s.
modeling communication networks with hybrid systems . <eos> this paper introduces a general hybrid systems framework to model the how of traffic in communication networks . the proposed models use averaging to continuously approximate discrete variables such as congestion window and queue size . because averaging occurs over short time intervals , discrete events such as the occurrence of a drop and the consequent reaction by congestion control can still be captured . this modeling framework , thus , fills a gap between purely packet level and fluid based models , faithfully capturing the dynamics of transient phenomena and yet providing significant flexibility in modeling various congestion control mechanisms , different queueing policies , multicast transmission , etc. the modeling framework is validated by comparing simulations of the hybrid models against packet level simulations . it is shown that the probability density functions produced by the ns <digit> network simulator match closely those obtained with hybrid ' models . moreover , a complexity analysis supports the observation that in networks with large per how bandwidths , simulations using hybrid models require significantly less computational resources than ns <digit> simulations . tools developed to automate the generation and simulation of hybrid systems models are also presented . their use is showcased in a study , which simulates tcp flows with different roundtrip times over the abilene backbone .
learning effective color features for content based image retrieval in dermatology . <eos> we investigate the extraction of effective color features for a content based image retrieval ( cbir ) application in dermatology . effectiveness is measured by the rate of correct retrieval of images from four color classes of skin lesions . we employ and compare two different methods to learn favorable feature representations for this special application limited rank matrix learning vector quantization ( liram lvq ) and a large margin nearest neighbor ( lmnn ) approach . both methods use labeled training data and provide a discriminant linear transformation of the original features , potentially to a lower dimensional space . the extracted color features are used to retrieve images from a database by a k nearest neighbor search . we perform a comparison of retrieval rates achieved with extracted and original features for eight different standard color spaces . we achieved significant improvements in every examined color space . the increase of the mean correct retrieval rate lies between <digit> % and <digit> % in the range of k <digit> retrieved images , and the correct retrieval rate lies between <digit> % and <digit> % . we present explicit combinations of rgb and cie lab color features corresponding to healthy and lesion skin . liram lvq and the computationally more expensive lmnn give comparable results for large values of the method parameter of lmnn ( <digit> <digit> ) while liram lvq outperforms lmnn for smaller values of . we conclude that feature extraction by liram lvq leads to considerable improvement in color based retrieval of dermatologic images .
synergies of operations research and data mining . <eos> in this contribution we identify the synergies of operations research and data mining . synergies can be achieved by integration of optimization techniques into data mining and vice versa . in particular , we define three classes of synergies and illustrate each of them by examples . the classification is based on a generic description of aims , preconditions as well as process models of operations research and data mining . it serves as a framework for the assessment of approaches at the intersection of the two procedures .
a synthesis of canonical variate analysis , generalised canonical correlation and procrustes analysis . <eos> canonical variate analysis ( cva ) is concerned with the analysis of j classes of samples , all described by the same variables . generalised canonical correlation analysis ( gcca ) is concerned with the analysis of k sets of variables , all describing the same samples . a generalised procrustes analysis context is used for data partitioned into j classes of samples and k sets of variables to explore the links between gcca and cva . biplot methodology is used to exploit the visualisation properties of these techniques . this methodology is illustrated by an example of <digit> samples described by three sets of variables ( k <digit> ) , the initial analysis of which suggests a grouping of the samples into four classes ( j <digit> ) , followed by subsequent more detailed analyses .
component based development of dsp software for mobile communication terminals . <eos> dsp software development has been tied down by extreme computational requirements . furthermore , the dsp development tools available today are less advanced than in other embedded software design . this has lead to dsp software architectures that have not taken into account future expansion needs . therefore , dsp software architectures have been inherently closed . now , as system complexity increases , this design methodology becomes more of a burden , since it does not support component based dsp software development that requires open interfaces . in this paper , mobile communications dsp software architectures are studied as cases , and key areas for improvements towards more open dsp software development are identified . proposed solutions are judged against the limited resources of mobile communication terminals and the characteristics of communication dsps .
least squares spectral method for the solution of a fractional advection dispersion equation . <eos> fractional derivatives provide a general approach for modeling transport phenomena occurring in diverse fields . this article describes a least squares spectral method for solving advection dispersion equations using caputo or riemann liouville fractional derivatives . a gauss lobatto jacobi quadrature is implemented to approximate the singularities in the integrands arising from the fractional derivative definition . exponential convergence rate of the operator is verified when increasing the order of the approximation . solutions are calculated for fractional time and fractional space differential equations . comparisons with finite difference schemes are included . a significant reduction in storage space is achieved by lowering the resolution requirements in the time coordinate . ( c ) <digit> published by elsevier inc .
clustered publish subscribe in wireless actuator and sensor networks . <eos> in this paper , we present a new method to realize a selforganizing and self stabilizing publish subscribe middleware for wireless actuator and sensor networks . by using simple , yet powerful compositional rules , our approach forms hierarchical clusters of nodes and sets up many small publish subscribe networks , that interact with each other . thereby , we are able to integrate the necessary routing of messages between clusters into the publish subscribe substrate itself . this technique reveals several benefits , such as redundant routes and automatic route recovery , which show up as emergent behavior at no additional cost . the provided publish subscribe middleware allows an elegant design of eventbased applications for pervasive environments .
designing of a new monitoring t chart using repetitive sampling . <eos> a new t chart to monitor a process when the time between events follows the exponential distribution . the t chart having double control limits using repetitive sampling . we discussed the performance of the proposed chart .
structures and electron affinities of triatomic molecules consisting of al , p and x ( x b , al , ga c , si , ge n , p , as o , s and se ) . <eos> the structures and electronic properties of the triatomic molecules containing al , p , x atoms ( x b , al , ga c , si , ge n , p , as o , s and se ) and their anions are investigated at the b3lyp cc pvtz and the b3lyp aug cc pvtz levels . the results show that the most stable structures of the anions are alxp ( ) ( x b , c , n ) and palx ( ) ( x s , se ) , while for the neutral molecules , the most stable structures are pxal ( x c , n and o ) . the order of the vdes of the anions molecules and the aeas of the neutral species are c < n < o < si a parts per thousand ge < p a parts per thousand as < al ga < b < s a parts per thousand se and c < o < n < si a parts per thousand ge < p a parts per thousand as < b < al a parts per thousand ga < s a parts per thousand se , respectively .
teaching medical image analysis with the insight toolkit . <eos> we present several case studies which examine the role that the insight toolkit ( itk ) played in three medical image analysis courses and several conference tutorials . these courses represent the first use of itk in a teaching environment , and we believe that a discussion of the teaching approach in each case and the benefits and challenges of itk will be useful to future medical image analysis course development . itk was found to provide significant value in a classroom setting since it provides both working canned algorithms , including some recently developed methods that are unavailable elsewhere , as well as a framework for developing new techniques and applications . several areas of difficulty , particularly in regards to code complexity and advanced object oriented design techniques , have been identified which may make the learning curve of itk somewhat more complex than a language such as matlab .
using information in task models to support design of interactive safety critical applications . <eos> the use of models has entered into current practice when developing various types of software product . however , there is a lack of methods able to use the information contained in relevant models concerning human computer interaction for supporting the design and development of user interfaces . in this paper , we propose a method for using information contained in formally represented task models in order to support the design of interactive applications , with particular attention to those applications where both usability and safety are the main concern . examples taken from our experience in a case study from the domain of air traffic control are introduced and further discussed to explain how the method can be applied .
a case study of financial statements reporting system based on xbrl taxonomy in accordance with korean public institutions adoption of k ifrs . <eos> this study aims to propose a model to convert public institution financial reporting system ( pifrs ) into a xbrl based online in accordance with adopted korean international financial reporting standards ( k ifrs ) . the financial reporting systems before the adaptation of pifrs has found themselves with many input errors and time consumption in adding in the process of manual collection of excel based data and combining .
on the management of user obligations . <eos> this paper is part of a project investigating authorization systems that assign obligations to users . we are particularly interested in obligations that require authorization to be performed and that , when performed , may modify the authorization state . in this context , a user may incur an obligation she is unauthorized to perform . prior work has introduced a property of the authorization system state that ensures users will be authorized to fulfill their obligations . we call this property accountability because users that fail to perform authorized obligations are accountable for their non performance . while a reference monitor can mitigate violations of accountability , it can not prevent them entirely . this paper presents techniques to be used by obligation system managers to restore accountability . we introduce several notions of dependence among pending obligations that must be considered in this process . we also introduce a novel notion we call obligation pool slicing , owing to its similarity to program slicing . an obligation pool slice identifies a set of obligations that the administrator may need to consider when applying strategies proposed here for restoring accountability . the paper also presents the system architecture of an authorization system that incorporates obligations that can require and affect authorizations .
variable latency floating point multipliers for low power applications . <eos> this paper proposes a variable latency floating point multiplier architecture , which is compliant with ieee <digit> <digit> and suitable for low power applications . the architecture splits the significand multiplier into the upper and lower parts , and predicts the carry bit , sticky bit , and significand product from the upper part . in the case of correct prediction , the computation of lower part is disabled and the rounding operation is significantly simplified so that the floating point multiplication can consume less power , and be completed early while maintaining the correct ieee rounding and product . experimental results show that the proposed multiplier can save respectable power and energy when compared to the fast multiplier at the expense of slight area and acceptable delay overheads .
face recognition using independent component analysis and support vector machines . <eos> support vector machines ( svm ) and independent component analysis ( ica ) are two powerful and relatively recent techniques . svms are classifiers which have demonstrated high generalization capabilities in many different tasks , including the object recognition problem . ica is a feature extraction technique which can be considered a generalization of principal component analysis ( pca ) . ica has been mainly used on the problem of blind signal separation . in this paper we combine these two techniques for the face recognition problem . experiments were made on two different face databases , achieving very high recognition rates . as the results using the combination pca svm were not very far from those obtained with ica svm , our experiments suggest that svms are relatively insensitive to the representation space . thus as the training time for ica is much larger than that of pca , this result indicates that the best practical combination is pca with svm . ( c ) <digit> elsevier science b.v. all rights reserved .
from neuroelectrodynamics to thinking machines . <eos> natural systems can provide excellent solutions to build artificial intelligent systems . the brain represents the best model of computation that leads to general intelligent action . however , current mainstream models reflect a weak understanding of computations performed in the brain that is translated in a failure of building powerful thinking machines . specifically , temporal reductionist neural models elude the complexity of information processing since spike timing models reinforce the idea of neurons that compress temporal information and that computation can be reduced to a communication of information between neurons . the active brain dynamics and neuronal data analyses reveal multiple computational levels where information is intracellularly processed in neurons . new experimental findings and theoretical approach of neuroelectrodynamics challenge current models as they now stand and advocate for a change in paradigm for bio inspired computing machines .
satisfaction and continuous use intention of e learning service in brazilian public organizations . <eos> we examined two public organizations that offer e learning service for your employees . a structural model of satisfaction and continuous use intention was tested . performance and technology readiness high influence each other . performance exceeds expectancy and results in high satisfaction .
on advance reservation of heterogeneous network paths . <eos> the availability of information about properties and status of resources is essential for grid resource brokers . however , while abstractions of computing and storage resources already exist , the notion of grid network resource is far from being understood today . as a result , the integration of advanced network services is still difficult when a grid system spans large scale heterogeneous network infrastructures . in this paper , we propose a single definition of a grid network resource abstraction for multiple types of network connectivity . this abstraction was successfully implemented and tested in a network resource management prototype supporting a variety of network technologies .
should hand actions be observed when learning hand motor skills from instructional animations . <eos> do learners need to observe hands in learning from animated hand manipulation tasks animations with and without hands were much more effective than equivalent statics . learning from animations with hands was more efficient than without hands .
critical groups of graphs with reflective symmetry . <eos> the critical group of a graph is a finite abelian group whose order is the number of spanning forests of the graph . for a graph g with a certain reflective symmetry , we generalize a result of ciucuyanzhang factorizing the spanning tree number of g by interpreting this as a result about the critical group of g. our result takes the form of an exact sequence , and explicit connections to bicycle spaces are made .
managing prediction markets . <eos> prediction markets have recently emerged as a group decision making tool with a range of potential applications , including forecasting , estimation , and innovation management . this research aims to identify the factors that drive or inhibit prediction market adoption within organizations .
an interactive local flattening operator to support digital investigations on artwork surfaces . <eos> analyzing either high frequency shape detail or any other 2d fields ( scalar or vector ) embedded over a 3d geometry is a complex task , since detaching the detail from the overall shape can be tricky . an alternative approach is to move to the 2d space , resolving shape reasoning to easier image processing techniques . in this paper we propose a novel framework for the analysis of 2d information distributed over 3d geometry , based on a locally smooth parametrization technique that allows us to treat local 3d data in terms of image content . the proposed approach has been implemented as a sketch based system that allows to design with a few gestures a set of ( possibly overlapping ) parameterizations of rectangular portions of the surface . we demonstrate that , due to the locality of the parametrization , the distortion is under an acceptable threshold , while discontinuities can be avoided since the parametrized geometry is always homeomorphic to a disk . we show the effectiveness of the proposed technique to solve specific cultural heritage ( ch ) tasks the analysis of chisel marks over the surface of a unfinished sculpture and the local comparison of multiple photographs mapped over the surface of an artwork . for this very difficult task , we believe that our framework and the corresponding tool are the first steps toward a computer based shape reasoning system , able to support ch scholars with a medium they are more used to .
he 's homotopy perturbation method an effective tool for solving a nonlinear system of two dimensional volterra fredholm integral equations . <eos> in this paper , we conducted a comparative study between he 's homotopy perturbation method ( hpm ) and the adomian decomposition method ( adm ) for solving a nonlinear system of two dimensional volterra fredholm integral equations . he 's homotopy perturbation method is implemented in a straightforward manner . the proper implementation of he 's homotopy perturbation method can extremely minimize the size of computational work compared with the existing traditional techniques . the analysis is accompanied by examples that demonstrate the comparison , and shows the pertinent features of the homotopy perturbation technique . ( c ) <digit> elsevier ltd. all rights reserved .
avlr ebp a variable step size approach to speed up the convergence of error back propagation algorithm . <eos> a critical issue of neural network based large scale data mining algorithms is how to speed up their learning algorithm . this problem is particularly challenging for error back propagation ( ebp ) algorithm in multi layered perceptron ( mlp ) neural networks due to their significant applications in many scientific and engineering problems . in this paper , we propose an adaptive variable learning rate ebp algorithm to attack the challenging problem of reducing the convergence time in an ebp algorithm , aiming to have a highspeed convergence in comparison with standard ebp algorithm . the idea is inspired from adaptive filtering , which leaded us into two semi similar methods of calculating the learning rate . mathematical analysis of avlr ebp algorithm confirms its convergence property . the avlr ebp algorithm is utilized for data classification applications . simulation results on many well known data sets shall demonstrate that this algorithm reaches to a considerable reduction in convergence time in comparison to the standard ebp algorithm . the proposed algorithm , in classifying the iris , wine , breast cancer , semeion and spect heart datasets shows a reduction of the learning epochs relative to the standard ebp algorithm .
optimising multi product multi chance constraint inventory control system with stochastic period lengths and total discount under fuzzy purchasing price and holding costs . <eos> while the usual assumptions in multi periodic inventory control problems are that the orders are placed at the beginning of each period ( periodic review ) or depending on the inventory level they can happen at any time ( continuous review ) , in this article , we relax these assumptions and assume that the periods between two replenishments of the products are independent and identically distributed random variables . furthermore , assuming that the purchasing price are triangular fuzzy variables , the quantities of the orders are of integer type and that there are space and service level constraints , total discount are considered to purchase products and a combination of back order and lost sales are taken into account for the shortages . we show that the model of this problem is a fuzzy mixed integer nonlinear programming type and in order to solve it , a hybrid meta heuristic intelligent algorithm is proposed . at the end , a numerical example is given to demonstrate the applicability of the proposed methodology and to compare its performance with one of the existing algorithms in real world inventory control problems .
on the security and the efficiency of multi signature schemes based on a trapdoor one way permutation . <eos> up to present , proposed are many multi signature schemes in which signers use respective moduli in the signature generation process . the fdh based schemes are proposed by mitomi et al. and lysyanskaya et al. . the pss based schemes are proposed by kawauchi et al. and komano et al. . the fdh based schemes have the advantage that the signature size is independent of the number of the signers . however , since the signature generation algorithm is deterministic , it has a bad reduction rate as a defect . consequently , the signers must unfortunately use the keys large enough to keep the security . on the other hand , in the pss based schemes , good reduction rates can be obtained since the signature generation algorithms are probabilistic . however , the size of the random component shall overflow the security parameter , and thereby the signature size shall grow by the total size of the random components used the signers . that means , if the size of the random component is smaller , the growth of the signature size can be kept smaller . in this paper , we propose new probabilistic multi signature scheme , which can be proven secure despite that smaller random components are used . we compare the proposed scheme and two existing schemes . finally , we conclude that the proposed scheme is so called optimal due to <digit> .
the representer method for state and parameter estimation in single phase darcy flow . <eos> the objective of this paper is to study the representer method for the inverse problem of combined parameter and state estimation in single phase darcy flow in porous media . a variational formulation is posed for the generalized inverse problem in the sense of weighted least squares . an iterative representer based algorithm is derived to approximate the eulerlagrange equations . it is proved that the linearized problem can be solved exactly by an extension of the representer method which inherits the same well known properties of this technique . implementation of the numerical algorithm for synthetic numerical experiments shows improved estimates of the poorly known prior guess .
a new estimator of significance of correlation in time series data . <eos> many expression array experiments monitor gene activity as an organism goes through some biological process . it is desirable to find genes with similar expression patterns in the resulting time series data . we propose a new simulation approach that assesses the statistical significance of similarity scores between expression patterns . the simulation takes into account the dependence between columns of data .
understanding the economic potential of service oriented architecture . <eos> service oriented architecture ( soa ) is one of the most discussed topics in the information systems ( is ) discipline . while most computer scientists agree that the service oriented paradigm has clear benefits in terms of technical quality attributes , it has been difficult to justify soa economically . the few studies that have investigated the strategic and economic aspects of soa are mostly exploratory and lack a more comprehensive framework for understanding the sources of its economic potential . based on is and soa literature , our work goes further in suggesting the soa economic potential model , which describes the causal relationships between the soa 's style characteristics and value it can provide on the business side . using this model , we investigate <digit> soa cases published between <digit> and <digit> to explore the economic rationale for adopting soa . our findings suggest that soa 's business benefits are currently mainly driven by operational and information technology infrastructural improvements . however , enterprises also realize strategic benefits from soa for example , by electronically integrating with their business partners by means of soa . we use the results of our study to derive propositions and suggest a research model for future studies on soa 's economic potential .
an efficient expert system wpsc and implementation . <eos> in previous research , the author has proposed a prototype invention as to the stabilization system , water pressure stabilizer control system , and the patent no . is i301935 , in taiwan . there are some attractive features of that invention including ( a ) pump , ( b ) pressure sensors , ( c ) feedback controller , ( d ) micro chip proportional integral derivative ( mcpid ) controller and ( e ) frequency transformer . the proposed design with the ability is to substitute the traditional water tower and stabilize water pressure in a building . in this paper , the author makes more concrete modulation and makes some improvements as to the feedback controller in the previous invention and presents an expert system efficient water pressure stabilizing control ( ewsc ) system . first of all , an efficient feedback theorybased algorithm is imbedded in the micro chip processor to increase the efficiency of the proposed ewsc system . furthermore , we implement ewsc system in four examples , a <digit> floor building and a <digit> area factory , a two times more complicated system with respect to the previous and make numerous tests to demonstrate the stabilizing efficiency . test results show that the efficiency of the proposed ewsc system is more significant while the test system getting bigger and complicated as well as the stabilized water pressure increase .
matchmaking for online games and other latency sensitive p2p systems . <eos> the latency between machines on the internet can dramatically affect users ' experience for many distributed applications . particularly , in multiplayer online games , players seek to cluster themselves so that those in the same session have low latency to each other . a system that predicts latencies between machine pairs allows such matchmaking to consider many more machine pairs than can be probed in a scalable fashion while users are waiting . using a far reaching trace of latencies between players on over 3.5 million game consoles , we designed htrae , a latency prediction system for game matchmaking scenarios . one novel feature of htrae is its synthesis of geolocation with a network coordinate system . it uses geolocation to select reasonable initial network coordinates for new machines joining the system , allowing it to converge more quickly than standard network coordinate systems and produce substantially lower prediction error than state of the art latency prediction systems . for instance , it produces 90th percentile errors less than half those of iplane and pyxida . our design is general enough to make it a good fit for other latency sensitive peer to peer applications besides game matchmaking .
a <digit> ghz 1.5 v dual modulus prescaler in 0.18 mu m copper cmos technology . <eos> a dual modulus prescaler using true single phase clock ( tspc ) logic is implemented in a 0.18 mum copper cmos technology . with careful design and optimization the prescaler is able to operate at frequency up to 7.14 ghz at 1.5 v supply voltage . the high speed operation is attributed to the adoption of the tspc dynamic logic , and the all copper interconnect cmos process which has much less interconnect parasitics than conventional aluminum technology . the design facilitates the implementation of a fully integrated rf cmos phase locked loop for applications in the 5.8 ghz ism band such as wireless lan .
properties and cape from present uses to future challenges . <eos> the role of properties in the solution of computer aided process engineering ( cape ) problems is described in terms of current trend , future challenges and important issues . three distinct roles of properties in cafe have been identified a service role , a service plus advice role and a service , advice plus solve role . the cafe problems solved under each of these roles are described together with simple illustrative examples . finally , the paper describes how some of the future problems related to integration of synthesis , design and control might be dealt with efficiently and reliably through co operative cafe and properties methodologies . ( c ) <digit> elsevier science ltd. all rights reserved .
skin segmentation based on cellular learning automata . <eos> in this paper , we propose a novel algorithm that combines color and texture information of skin with cellular learning automata to segment skin like regions in color images . first , the presence of skin colors in an image is detected , using a committee structure , to make decision from several explicit boundary skin models . detected skin color regions are then fed to a color texture extractor that extracts the texture features of skin regions via their color statistical properties and maps them to a skin probability map . cellular learning automatons use this map to make decision on skin like regions . the proposed algorithm has demonstrated true positive rate of about 83.4 % and false positive rate of about 11.3 % on the compaq skin database . experimental results show the effectiveness of the proposed algorithm .
evaluating display fidelity and interaction fidelity in a virtual reality game . <eos> in recent years , consumers have witnessed a technological revolution that has delivered more realistic experiences in their own homes through high definition , stereoscopic televisions and natural , gesture based video game consoles . although these experiences are more realistic , offering higher levels of fidelity , it is not clear how the increased display and interaction aspects of fidelity impact the user experience . since immersive virtual reality ( vr ) allows us to achieve very high levels of fidelity , we designed and conducted a study that used a six sided cave to evaluate display fidelity and interaction fidelity independently , at extremely high and low levels , for a vr first person shooter ( fps ) game . our goal was to gain a better understanding of the effects of fidelity on the user in a complex , performance intensive context . the results of our study indicate that both display and interaction fidelity significantly affect strategy and performance , as well as subjective judgments of presence , engagement , and usability . in particular , performance results were strongly in favor of two conditions low display , low interaction fidelity ( representative of traditional fps games ) and high display , high interaction fidelity ( similar to the real world ) .
a method for fuzzy risk analysis based on the new similarity of trapezoidal fuzzy numbers . <eos> at present , some researchers provide a type of fuzzy risk analysis algorithms for dealing with fuzzy risk analysis problems , where the values of the evaluating items are represented by trapezoidal fuzzy numbers . in those algorithms , the main operations are two one is arithmetic operators of the trapezoidal fuzzy numbers the other is the similarity of the trapezoidal fuzzy numbers . however , the arithmetic operators of some algorithms do not satisfy some properties of the trapezoidal fuzzy numbers and the similarity of the trapezoidal fuzzy numbers does not coincide with the intuition of the human being . due to this situation , in this paper , we present an efficient approach for fuzzy risk analysis based on some new arithmetic operators of the trapezoidal fuzzy numbers and propose a new similarity of the trapezoidal fuzzy numbers to deal with fuzzy risk analysis problems . at the same time , we make an experiment to use <digit> sets of trapezoidal fuzzy numbers to compare the experimental results of our proposed approach with the existing similarity measures . at last , we use an example to illustrate the efficiency of the new approach .
stochastic performance analysis and capacity planning of publish subscribe systems . <eos> publish subscribe systems are used increasingly often as a communication mechanism in loosely coupled distributed applications . with their gradual adoption in mission critical areas , it is essential that systems are subjected to a rigorous performance analysis before they are put into production . however , existing approaches to performance modeling and analysis of publish subscribe systems suffer from many limitations that seriously constrain their practical applicability . in this paper , we present a set of generalized and comprehensive analytical models of publish subscribe systems employing different peer to peer and hierarchical routing schemes . the proposed analytical models address the major limitations underlying existing work in this area and are the first to consider all major performance relevant system metrics including the expected broker and link utilization , the expected notification delay , the expected time required for new subscriptions to become fully active , as well as the expected routing table sizes and message rates . to illustrate our approach and demonstrate its effectiveness and practicality , we present a case study showing how our models can be exploited for capacity planning and performance prediction in a realistic scenario .
hotspot entropy based data forwarding in opportunistic social networks . <eos> performance of data forwarding in opportunistic social networks benefits considerably if one can make use of human mobility in terms of social contexts . however , it is difficult and time consuming to calculate the centrality and similarity of nodes by using solutions of traditional social networks analysis , this is mainly because of the transient node contact and the intermittently connected link . in this paper , we are interested in the following question can we exploit some other stable social attributes to quantify the centrality and similarity of nodes aggregating gps traces of human walks from the real world , we find that there exist two types of phenomena . one is public hotspot , the other is personal hotspot . motivated by this observation , we propose hotent ( hotspot entropy ) , a novel data forwarding metric to improve the performance of opportunistic routing . first , we use the relative entropy between the public hotspots and the personal hotspots to compute node centrality . second , we utilize the inverse symmetrized entropy of the personal hotspots between two nodes to evaluate their similarity . third , we integrate the two social metrics by using the law of universal gravitation . besides , we use the entropy of personal hotspots of a node to characterize its personality . finally , we compare our routing strategy with the state of the art works through extensive trace driven simulations , the results show that hotent largely outperforms other solutions , especially in terms of packet delivery ratio and the average number of hops per message .
towards a taxonomy of computer architecture based on the machine data type view . <eos> existing taxonomies of computer architecture lack the descriptive tools to deal with the large variety of existing principles , features , and mechanisms of the existing spectrum of single processor , multi processor , and multi computer architectures . consequently , they lack the discriminating power to be able to taxonomize computer architecture . in the paper , a new approach toward a complete taxonomy is presented . the key to the taxonomy is to start with the dichotomy of ' operational principle ' and ' hardware structure ' as the foundation of a computer architecture and describe the constituents of the operational principle in terms of ' machine data types ' consisting of ' machine data objects ' , their representations , and the functions applicable on the objects . the resulting taxonomy provides a systematic approach to the design of innovative computer architectures .
an improved intelligent water drops algorithm for solving multi objective job shop scheduling . <eos> improved intelligent water drops algorithm . multi objective optimization . job shop scheduling .
some optimal codes related to graphs invariant under the alternating group a ( <digit> ) . <eos> the alternating group a ( <digit> ) , acts as a primitive rank <digit> group of degree <digit> on the set of lines of v <digit> ( <digit> ) with line stabilizer isomorphic to <digit> ( <digit> ) ( s <digit> x s <digit> ) and orbits of lengths <digit> , <digit> and <digit> respectively . this action defines the unique strongly regular ( <digit> , <digit> , <digit> , <digit> ) graph . the paper examines the binary ( resp . ternary ) codes spanned by the rows of this graph , and its complement . we establish some properties of the codes and use the geometry of the designs and graphs to give an account on the nature of some classes of codewords , in particular those of minimum weight . further , we show that the codes with parameters <digit> , <digit> , <digit> ( <digit> ) , <digit> , <digit> , <digit> ( <digit> ) , <digit> , <digit> , <digit> ( <digit> ) , <digit> , <digit> , <digit> ( <digit> ) , <digit> , <digit> , <digit> ( <digit> ) , <digit> , <digit> , <digit> ( <digit> ) , <digit> , <digit> , <digit> ( <digit> ) and <digit> , 56,4 ( <digit> ) are all optimal . in addition , we show that the codes with parameters <digit> , <digit> , <digit> ( <digit> ) , <digit> , <digit> , <digit> ( <digit> ) , <digit> , <digit> , <digit> ( <digit> ) , <digit> , <digit> , <digit> ( <digit> ) are all near optimal for the given length and dimension .
speed ups by changing the order in which sets are enumerated ( preliminary version ) . <eos> in a suitably general context , the following analogue of the blum speed up theorem is proven there are some infinite sets which are so difficult to enumerate that , given any order for enumerating the set , there is some other order , and some one method of enumerating the set in this second order which is much faster than any method of enumerating the set in the first ordering . it may be possible to interpret this result as a statement about the relative merits of hardware vs . programming speed ups . the proof itself is one of the first nontrivial applications of priority methods to questions of computational complexity . as such , it perhaps represents an advance in bringing the results and techniques of contemporary pure recursion theory to bear on questions of computational complexity . in this paper we shall prove , in a suitably general context , the following analogue of the blum speed up theorem , b1 there are some infinite sets which are so difficult to enumerate that , given any order for enumerating the set , there is some other order , and some one method of enumerating the set in this second order which is much faster than any method of enumerating the set in the first ordering .
semi analytical techniques for substrate characterization in the design of mixed signal ics . <eos> a number of methods are presented for highly efficient calculation of substrate current transport . a three dimensional green function based substrate representation , in combination with the use of the fast fourier transform , significantly speeds up the computation of sensitivities with respect to all parameters associated with a given architecture . substrate sensitivity analysis is used in a number of physical optimization tools , such as placement and trend analysis for the estimation of the impact of technology migration and or layout re design .
finding nonnormal bent functions . <eos> the question if there exist nonnormal bent functions was an open question for several years . a boolean function in n variables is called normal if there exists an affine subspace of dimension n <digit> n <digit> on which the function is constant . in this paper we give the first nonnormal bent function and even an example for a nonweakly normal bent function . these examples belong to a class of bent functions found in j.f. dillon , h. dobbertin , new cyclic difference sets with singer parameters , in finite fields and applications , to appear , namely the kasami functions . we furthermore give a construction which extends these examples to higher dimensions . additionally , we present a very efficient algorithm that was used to verify the nonnormality of these functions .
kinetin induced differentiation of normal human keratinocytes undergoing aging in vitro . <eos> abstract kinetin ( n6 furfuryladenine ) is a cytokinin growth factor having several anti aging effects reported for human cells and fruit flies . we have observed that short term culturing of human keratinocytes in the presence of <digit> to <digit> m kinetin results in a significant inhibition of cell growth . studies were undertaken to analyze the process of differentiation as a reason for growth inhibition . keratinocytes at different passage levels were treated with fetal calf serum ( fcs ) and calcium as differentiation inducing positive controls , with different concentrations of kinetin , and with a combination of kinetin and calcium . the induction and progression of differentiation was monitored by morphological observations and by using several differentiation markers , including keratins ( k10 and k14 ) , involucrin , epidermal transglutaminase , and some new keratinocyte specific antibodies isolated by the phage display method . in young keratinocytes , two days of calcium treatment reduced the k14 level by <digit> % , and increased the levels of k10 and involucrin by <digit> % and <digit> % , respectively . in comparison , <digit> m kinetin had no effect on the k14 level , but increased the k10 level by <digit> % and that of involucrin by four fold . the combination of calcium and <digit> m kinetin led to a decrease by <digit> % in the k14 level , to an increase in the level of k10 by <digit> % , and to a two fold rise in the involucrin level . these results suggest that the rate , extent , and quality of differentiation depend on the inducing agent , and that kinetin may be useful in promoting the differentiation of human keratinocytes , especially in the presence of calcium .
towards group transport by swarms of robots . <eos> we examine the ability of a swarm robotic system to transport cooperatively objects of different shapes and sizes . we simulate a group of autonomous mobile robots that can physically connect to each other and to the transported object . controllers artificial neural networks are synthesised by an evolutionary algorithm . they are trained to let the robots self assemble , that is , organise into collective physical structures and transport the object towards a target location . we quantify the performance and the behaviour of the group . we show that the group can cope fairly well with objects of different geometries as well as with sudden changes in the target location . moreover , we show that larger groups , which are made of up to <digit> robots , make possible the transport of heavier objects . finally , we discuss the limitations of the system in terms of task complexity , scalability and fault tolerance and point out potential directions for future research .
isochores merit the prefix ' iso ' . <eos> the isochore concept in the human genome sequence was challenged in an analysis by the international human genome sequencing consortium ( ihgsc ) . we argue here that a statement in the ihgsc 's analysis concerning the existence of isochores is misleading , because the homogeneity was not examined at a large enough length scale and consequently an inappropriate statistical test was applied . a test of the existence of isochores should be equivalent to a test of homogeneity or equality of windowed gc % . the statistical test applied in the ihgsc 's analysis , the binomial test , is a test of whether individual bases are independent and identically distributed ( iid ) . for testing the existence of isochores , or homogeneity in windowed gc % , we propose to use another statistical test the analysis of variance ( anova ) . it can be shown that dna sequences that are rejected by the binomial test may not be rejected by the anova test . ( c ) <digit> elsevier science ltd. all rights reserved .
computable separation in topology , from t <digit> to t <digit> . <eos> this article continues the study of computable elementary topology started in weihrauch and grubba <digit> . for computable topological spaces we introduce a number of computable versions of the topological separation axioms t <digit> , t <digit> and t <digit> . the axioms form an implication chain with many equivalences . by counterexamples we show that most of the remaining implications are proper . in particular , it turns out that computable t <digit> is equivalent to computable t <digit> and that for spaces without isolated points the hierarchy collapses , that is , the weakest computable t <digit> axiom wct0 is equivalent to the strongest computable t <digit> axiom sct2 . the sct2 spaces are closed under cartesian product , this is not true for most of the other classes of spaces . finally we show that the computable version of a basic axiom for an effective topology in intuitionistic topology is equivalent to sct2 .
comparing software rejuvenation policies under different dependability measures . <eos> software rejuvenation is a preventive and proactive solution that is particularly useful for counteracting the phenomenon of software aging . in this paper , we consider both the periodic and non periodic software rejuvenation policies under different dependability measures . as is well known , the steady state system availability is the probability that the software system is operating in the steady state and , at the same time , is often regarded as the mean up rate in the system operation period . we show that the mean up rate should be defined as the mean value of up rate , but not as the mean up time per mean operation time . we derive numerically the optimal software rejuvenation policies which maximize the steady state system availability and the mean up rate , respectively , for each periodic or non periodic model . numerical examples show that the real mean up rate is always smaller than the system availability in the steady state and that the availability overestimates the ratio of operative time of the software system .
identification of pointing difficulties of two individuals with parkinson 's disease via a sub movement analysis . <eos> we present a study of the sub movement characteristics of two individuals with parkinson 's disease completing pointing tasks . we describe the performance of the two individuals and we compare it with that of young children and older able body adults . the analysis suggests that we need new strategies that incorporate an individual assessment of difficulties , and provide personalized methods of assistance .
cognitive radio systems specific for imt systems operators view and perspectives . <eos> cognitive radio ( cr ) is still an emerging and disruptive communication technology which is expected to improve the overall efficiency of the spectrum use . it is envisaged that cognitive radio systems ( crs ) could impact many aspects of communications and in particular could facilitate accommodation of the increasing amount of services and applications in wireless networks . intensive research on cr aims at maximising the utilisation of the limited radio spectrum resource . there have been many advances in cr regarding the technology development aspects however supplementary research on regulation , policy and market structure reforms in relation with application specific deployment is still required before any cr based spectrum access could be implemented for specific broadband mobile applications . indeed , mobile community is still at an early stage of understanding and development of cr capabilities and it is premature to envisage wide deployment of crs without careful consideration of regulatory and business issues . therefore , this paper gives a classification of cr based network and application scenarios , and investigates the feasibility of them from a regulatory perspective at a global level ( itu r ) . main part of this paper presents the wireless network operators approach to crs specific for international mobile telecommunications ( imt ) systems and proposes the radio environment map ( rem ) concept as a cognitive tool that increases environmental awareness in wireless network operators networks . studies , which the authors performed internally and within the framework of a collaborative european project as well as within itu r yield the conclusion that , at shorter term , only intra operator based crs maximises the possibility for cr capabilities to be implemented .
parallel information retrieval with query expansion . <eos> an information retrieval ( ir ) system with query expansion on a low cost high performance pc cluster environment is implemented . we study how query performance is affected by query expansion and two declustering methods using two standard korean test collections . according to the experiments . the greedy method shows about <digit> % enhancement overall when compared with the lexical method .
endometriosis treatment strategies . <eos> endometriosis is often a perplexing medical condition for both the physician and the patient . accordingly , development of treatment strategies based on the needs of the individual patient is highly desirable . although endometriosis has been part of the clinical practice for almost a century , many questions remain relating to the relationship between endometriosis and infertility as well as endometriosis and pelvic pain . endometriosis is a disease of reproductive age women , and it is now well recognized that a genetic susceptibility appears probable . the prevalence in the general population has never been clearly established . factors to consider in management include the age and reproductive desires of the patient , the stage of the disease , and , most importantly , the symptoms . therapeutic options include no treatment , medical therapy , surgery , or combination therapy . oral contraceptives , androgenic agents , progestins , and gonadotropin releasing hormone ( gnrh ) analogs have all been used successfully , although at the present time , the latter preparations are the most popular medical therapy for endometriosis . leuprolide acetate , goserelin acetate , and nafarelin acetate are all effective agents . surgical therapy is appropriate , especially for advanced stages of the disease . laparoscopy is an effective surgical approach with the goal of excision of visible endometriosis in a hemostatic fashion . since endometriosis is a chronic condition , it is not uncommon for recurrences to occur . while endometriosis remains an enigmatic disease , the introduction of new pharmacologic agents , such as gnrh analogs and newer endoscopic methods of surgical treatment , have facilitated and improved the overall management of this disease .
a slam overview from a users perspective . <eos> this paper gives a brief overview on the simultaneous localization and mapping ( slam ) problem from the perspective of using slam for an application as opposed to the common view in slam research papers that focus on investigating slam itself .
a logical theory of concurrent objects . <eos> a new theory of concurrent objects is presented . the theory has the important advantage of being based directly on a logic called rewriting logic in which concurrent object oriented computation exactly corresponds to logical deduction . this deduction is performed by concurrent rewriting modulo structural axioms of associativity , commutativity and identity that capture abstractly the essential aspects of communication in a distributed object oriented configuration made up of concurrent objects and messages . thanks to this axiomatization , it becomes possible to study the behavior of concurrent objects by formal methods in a logic intrinsic to their computation . the relationship with actors and with other models of concurrent computation is also discussed . a direct fruit of this theory is a new language , called maude , to program concurrent object oriented modules in an entirely declarative way using rewriting logic modules written in this language are used to illustrate the main ideas with examples . maude contains obj3 as a functional sublanguage and provides a simple and semantically rigorous integration of functional programming and concurrent object oriented programming .
on the effects of adding objectives to plateau functions . <eos> in this paper , we examine how adding objectives to a given optimization problem affects the computational effort required to generate the set of pareto optimal solutions . experimental studies show that additional objectives may change the running time behavior of an algorithm drastically . often it is assumed that more objectives make a problem harder as the number of different tradeoffs may increase with the problem dimension . we show that additional objectives , however , may be both beneficial and obstructive depending on the chosen objective . our results are obtained by rigorous running time analyses that show the different effects of adding objectives to a well known plateau function . additional experiments show that the theoretically shown behavior can be observed for problems with more than one objective .
a consensus based method for tracking modelling background scenario and foreground appearance . <eos> modelling of the background ( uninteresting parts of the scene ) , and of the foreground , play important roles in the tasks of visual detection and tracking of objects . this paper presents an effective and adaptive background modelling method for detecting foreground objects in both static and dynamic scenes . the proposed method computes sample consensus ( sacon ) of the background samples and estimates a statistical model of the background , per pixel . sacon exploits both color and motion information to detect foreground objects . sacon can deal with complex background scenarios including nonstationary scenes ( such as moving trees , rain , and fountains ) , moved inserted background objects , slowly moving foreground objects , illumination changes etc. however , it is one thing to detect objects that are not likely to be part of the background it is another task to track those objects . sample consensus is again utilized to model the appearance of foreground objects to facilitate tracking . this appearance model is employed to segment and track people through occlusions . experimental results from several video sequences validate the effectiveness of the proposed method .
supersd an object based stochastic simulation program for modeling the locations of undiscovered petroleum accumulations . <eos> object based stochastic simulation is a widely applicable method that has been used to predict channel sand bodies in fluvial depositional systems . petroleum pools are spatial objects and the location and characteristics of the undiscovered pools can be predicted using similar techniques . an object based stochastic simulation program , simulating undiscovered petroleum resource spatial distribution ( supersd ) , is presented for the purpose of predicting the locations of undiscovered petroleum accumulations in a play . this program simultaneously considers all the necessary geological conditions for the formation of petroleum accumulations and the spatial correlation of these accumulations in the simulation . an independence chain of the hastings algorithm is used to generate an appropriate structure of pool combinations . the uncertainty associated with the data and the geological models for predicted locations is expressed as a relative probability map . the executable codes of the supersd program are available from the geological survey of canada .
health diagnostics using multi attribute classification fusion . <eos> this paper presents a classification fusion approach for health diagnostics that can leverage the strengths of multiple member classifiers to form a robust classification model . the developed approach consists of three primary steps ( i ) fusion formulation using a k fold cross validation model ( ii ) diagnostics with multiple multi attribute classifiers as member algorithms and ( iii ) classification fusion through a weighted majority voting with dominance approach . state of the art classification techniques from three broad categories ( i.e. , supervised learning , unsupervised learning , and statistical inference ) were employed as member algorithms . the diagnostics results from the fusion approach will be better than , or at least as good as , the best result provided by all individual member algorithms . the developed classification fusion approach is demonstrated with the <digit> phm challenge problem and rolling bearing health diagnostics problem . case study results indicated that , in both problems , the developed fusion diagnostics approach outperforms any stand alone member algorithm with better diagnostic accuracy and robustness .
combining declarative , procedural , and predictive knowledge to generate , execute , and optimize robot plans . <eos> one of the main challenges in motor control is expressing high level goals in terms of low level actions . to do so effectively , motor control systems must reason about actions at different levels of abstraction . grounding high level plans in low level actions is essential semantic knowledge for plan based control of real robots . we present a robot control system that uses declarative , procedural and predictive knowledge to generate , execute and optimize plans . declarative knowledge is represented in pddl , durative actions constitute procedural knowledge , and predictive knowledge is learned by observing action executions . we demonstrate how learned predictive knowledge enables robots to autonomously optimize plan execution with respect to execution duration and robustness in real time . the approach is evaluated in two different robotic domains .
analytical modeling of tcp dynamics in infrastructure based ieee 802.11 wlans . <eos> ieee 802.11 wireless local area network ( wlan ) has become the prevailing solution for wireless internet access while transport control protocol ( tcp ) is the dominant transport layer protocol in the internet . it is known that , in an infrastructure based wlan with multiple stations carrying long lived tcp flows , the number of tcp stations that are actively contending to access the wireless channel remains very small . hence , the aggregate tcp throughput is basically independent of the total number of tcp stations . this phenomenon is due to the closed loop nature of tcp flow control and the bottleneck downlink ( i.e. , access point to station ) transmissions in infrastructure based wlans . in this paper , we develop a comprehensive analytical model to study tcp dynamics in infrastructure based 802.11 wlans . we calculate the average number of active tcp stations and the aggregate tcp throughput using our model for given total number of tcp stations and the maximum tcp receive window size . we find out that the default minimum contention window sizes specified in the standards ( i.e. , <digit> and <digit> for 802.11 b and 802.11 a , respectively ) are not optimal in terms of tcp throughput maximization . via ns <digit> simulation , we verify the correctness of our analytical model and study the effects of some of the simplifying assumptions employed in the model . simulation results show that our model is reasonably accurate , particularly when the wireline delay is small and or the packet loss rate is low .
on graham 's bound for cyclic scheduling . <eos> this paper addresses the performance of list scheduling a cyclic set of n non preemptive dependent generic tasks on m identical processors . the reduced precedence graph is assumed to be strongly connected but the number of simultaneously active instances of a generic task is not restricted to be at most one . some properties on arbitrary schedules are first given . then , we restrict to regular schedules for which it is shown that the number of ready or active tasks at any instant is at least the minimum height h of a directed circuit of the reduced precedence graph . the average cycle time of any regular list schedule is then shown to be at most ( <digit> ( min h , m m ) ) times the absolute minimum average cycle time . this result , which is similar well known ( <digit> ( <digit> m ) ) graham 's bound applying for non cyclic scheduling , shows to what extent regular list schedules take the parallelism of the cyclic task system into account .
implementation techniques for main memory database systems . <eos> with the availability of very large , relatively inexpensive main memories , it is becoming possible keep large databases resident in main memory in this paper we consider the changes necessary to permit a relational database system to take advantage of large amounts of main memory we evaluate avl vs b tree access methods for main memory databases , hash based query processing strategies vs sort merge , and study recovery issues when most or all of the database fits in main memory as expected , b trees are the preferred storage mechanism unless more than <digit> <digit> % of the database fits in main memory a somewhat surprising result is that hash based query processing strategies are advantageous for large memory situations
evolving genetic networks for synthetic biology . <eos> the sibling disciplines , systems and synthetic biology , are engaged in unraveling the complexity of the biological networks . one is trying to understand the design principle of the existing networks while the other is trying to engineer artificial gene networks with predicted functions . the significant and important role that computational intelligence can play to steer the life engineering discipline towards its ultimate goal , has been acknowledged since its time of birth . however , as the field is facing many challenges in building complex modules systems from the simpler parts devices , whether from scratch or through redesign , the role of computational assistance becomes even more crucial . evolutionary computation , falling under the broader domain of artificial intelligence , is well acknowledged for its near optimal solution seeking capability for poorly known and partially understood problems . since the post genome period , these natural selection simulating algorithms are playing a noteworthy role in identifying , analyzing and optimizing different types of biological networks . this article calls attention to how evolutionary computation can help synthetic biologists in assembling larger network systems from the lego like parts .
direct coupled cavity filters design using a hybrid feedforward neural network finite elements procedure . <eos> in many filtering applications , direct coupled cavity filters are often used for their symmetry and handling easiness . in this paper , a method is described for the design of these filtering devices by using an artificial neural network ( ann ) . starting from design requirements , the procedure discussed herein directly determines the geometric dimensions of the filtering device with good accuracy and very short processing time . ( c ) <digit> john wiley sons , inc .
plugged in but not connected individuals views of and responses to online and in person ostracism . <eos> we conducted two studies to examine perceptions of , and reactions to , ostracism occurring either in person or online . in study <digit> , participants read a vignette describing either in person or online ostracism , then estimated their psychological and interpersonal responses as if they experienced such ostracism . participants anticipated experiencing distress , and this was consistent across ostracism method . ostracism method did predict negative affect ( na ) , with greater na increases anticipated for in person exclusion , compared to online . a significant interaction between gender and ostracism method predicted anticipated belonging . males anticipated higher belonging in the in person condition ( relative to online ) females anticipated more belonging in the online condition . in study <digit> , participants experienced in person or online ostracism during a brief interaction with study confederates . both conditions elicited similar reports of low inclusion , high exclusion , and significant decreases in positive and negative affect . ostracism method qualified self esteem ( se ) results chat room participants indicated an increase in se following ostracism , whereas in person participants reported a slight decrease . males and females were similarly affected by both conditions . these studies demonstrate that online experiences of ostracism may be as meaningful as those experienced in person . whether this finding generalizes to those with less technological familiarity should be examined further .
integration of modal and fuzzy methods of knowledge representation in artificial agents . <eos> to enable artificial systems to meaningfully use a semantic language of communication is one of the long term and key targets not only in the field of artificial cognitive agents , but also of ai research in general . given existing solutions for grounding of modal statements of a language of communication and an idea to model internal concepts of the agent as zadehian fuzzy linguistic concepts , this paper shows how to meaningfully combine the two within a single framework . an accomplished goal is a model for grounding of modal and non modal statements of a language of communication based on concepts modelled internally as fuzzy sets spanned over the domain of observation . this paper describes a way in which fuzzy linguistic concepts are activated by perceptual inputs and how an agents grounds respective non modal statements . further , an agent supposed to describe an unobserved part of the environment can use autoepistemic operators of possibility , belief , and knowledge to describe its cognitive attitude toward it . it is discussed how the modal extensions of statements with fuzzy linguistic concepts should be grounded in order to preserve the common sense . the resulting constraints put on the model of grounding are formally represented in a form of analytical restrictions put on the so called relation of epistemic satisfaction .
understanding file access mechanisms for embedded ubicomp collaboration interfaces . <eos> this paper explores the nature of interfaces to support people in accessing their files at tabletop displays embedded in the environment . to do this , we designed a study comparing people 's interaction with two very different classes of file system access interface focus , explicitly designed for tabletops , and the familiar hierarchical windows explorer . in our within subjects double crossover study , participants collaborated on <digit> planning tasks . based on video , logs , questionnaires and interviews , we conclude that both classes of interface have a place . notably , focus contributed to improved collaboration and more efficient use of the workspace than with explorer . our results inform a set of recommendations for future interfaces enabling this important class of interaction supporting access to files for collaboration at tabletop devices embedded in an ubicomp environment .
hierarchical evolution of linear regressors . <eos> we propose an algorithm for function approximation that evolves a set of hierarchical piece wise linear regressors . the algorithm , named hire lin , follows the iterative rule learning approach . a genetic algorithm is iteratively called to find a partition of the search space where a linear regressor can accurately fit the objective function . the resulting ruleset performs an approximation to the objective function formed by a hierarchy of locally trained linear regressors . the approach is evaluated in a set of objective functions and compared to other regression techniques .
quadratic cost flow and the conjugate gradient method . <eos> by introducing quadratic penalty terms , a convex non separable quadratic network program can be reduced to an unconstrained optimization problem whose objective function is a piecewise quadratic and continuously differentiable function . a conjugate gradient method is applied to the reduced problem and its convergence is proved . the computation exploits the special network data structures originated from the network simplex method . this algorithmic framework allows direct extension to multicommodity cost flows . some preliminary computational results are presented .
a fast direct solver for scattering from periodic structures with multiple material interfaces in two dimensions . <eos> we present a new integral equation method for the calculation of two dimensional scattering from periodic structures involving triple points ( multiple materials meeting at a single point ) . the combination of a robust and high order accurate integral representation and a fast direct solver permits the efficient simulation of scattering from fixed structures at multiple angles of incidence . we demonstrate the performance of the scheme with several numerical examples .
evidence supporting measure of similarity for reducing the complexity in information fusion . <eos> this paper presents a new method for reducing the number of sources of evidence to combine in order to reduce the complexity of the fusion processing . such a complexity reduction is often required in many applications where the real time constraint and limited computing resources are of prime importance . the basic idea consists in selecting . among all sources available . only a subset of sources of evidence to combine . the selection is based on an evidence supporting measure of similarity ( esms ) criterion which is an efficient generic tool for outlier sources identification and rejection . the esms between two sources of evidence can be defined using several measures of distance following different lattice structures . in this paper , we propose such four measures of distance for esms and we present in details the principle of generalized fusion machine ( gem ) . then we apply it experimentally to the real time perception of the environment with a mobile robot using sonar sensors . a comparative analysis of results is done and presented in the last part of this paper . ( c ) <digit> elsevier inc. all rights reserved .
a study of multimodal feedback to support collaborative manipulation tasks in virtual worlds . <eos> in the research community , developers of collaborative virtual environments ( cves ) usually refer to the terms awareness and feedback as something necessary to maintain a fluent collaboration when highly interactive tasks have to be performed . however , it is remarkable that few studies address the effect that including special kinds of feedback has on user awareness and task performance . this work follows a preliminary experiment where we already studied awareness in cves , evaluating the effect of visual cues in the performance of collaborative tasks and showing that users tend to make more mistakes when such feedback is not provided , that is , they are less aware . these early results were promising and encouraged us to continue investigating the benefit of improving awareness in tasks that require close collaboration between users , but this time analyzing more types of awareness and experimenting with visual , audio and vibrotactile feedback cues .
a type system for data flow integrity on windows vista . <eos> the windows vista operating system implements an interesting model of multi level integrity . we observe that in this model , trusted code must participate in any information flow attack . thus , it is possible to eliminate such attacks by statically restricting trusted code . we formalize this model by designing a type system that can efficiently enforce data flow integrity on windows vista . typechecking guarantees that objects whose contents are statically trusted never contain untrusted values , regardless of what untrusted code runs in the environment . some of windows vista 's runtime access checks are necessary for soundness others are redundant and can be optimized away .
two dimensional , unstructured mesh generation for tidal models . <eos> the successful implementation of a finite element model for computing shallow water flow requires the identification and spatial discretization of a surface water region . since no robust criterion or node spacing routine exists , which incorporates physical characteristics and subsequent responses into the mesh generation process , modelers are left to rely on crude gridding criteria as well as their knowledge of particular domains and their intuition . two separate methods to generate a finite element mesh are compared for the gulf of mexico . a wavelength based criterion and an alternative approach , which employs a localized truncation error analysis ( ltea ) , are presented . both meshes have roughly the same number of nodes , although the distribution of these nodes is very different . two dimensional depth averaged simulations of now using a linearized form of the generalized wave continuity equation and momentum equations are performed with the ltea based mesh and the wavelength to gridsize ratio mesh . all simulations are forced with a single tidal constituent , m <digit> . use of the ltea based procedure is shown to produce a superior ( i.e. , less error ) two dimensional grid because the physics of shallow water flow , as represented by discrete equations , are incorporated into the mesh generation process . copyright ( c ) <digit> john wiley sons , ltd .
new preconditioners for systems of linear equations with toeplitz structure . <eos> in this paper , we consider applying the preconditioned conjugate gradient ( pcg ) method to solve system of linear equations ( t x mathbf b ) where ( t ) is a block toeplitz matrix with toeplitz blocks ( bttb ) . we first consider level <digit> circulant preconditioners based on generalized jackson kernels . then , bttb preconditioners based on a splitting of bttb matrices are proposed . we show that the bttb preconditioners based on splitting are special cases of embedding based bttb preconditioners , which are also good bttb preconditioners . as an application , we apply the proposed preconditioners to solve bttb least squares problems . our preconditioners work for bttb systems with nonnegative generating functions . the implementations of the construction of the preconditioners and the relevant matrix vector multiplications are also presented . finally , numerical examples , including image restoration problems , are presented to demonstrate the efficiency of our proposed preconditioners .
beyond the digital natives debate towards a more nuanced understanding of students ' technology experiences . <eos> the idea of the digital natives , a generation of tech savvy young people immersed in digital technologies for which current education systems can not cater , has gained widespread popularity on the basis of claims rather than evidence . recent research has shown flaws in the argument that there is an identifiable generation or even a single type of highly adept technology user . for educators , the diversity revealed by these studies provides valuable insights into students ' experiences of technology inside and outside formal education . while this body of work provides a preliminary understanding , it also highlights subtleties and complexities that require further investigation . it suggests , for example , that we must go beyond simple dichotomies evident in the digital natives debate to develop a more sophisticated understanding of our students ' experiences of technology . using a review of recent research findings as a starting point , this paper identifies some key issues for educational researchers , offers new ways of conceptualizing key ideas using theoretical constructs from castells , bourdieu and bernstein , and makes a case for how we need to develop the debate in order to advance our understanding .
standards competition in the presence of digital conversion technology an empirical analysis of the flash memory card market . <eos> both theoretical and empirical evidence suggest that , in many markets with standards competition , network effects make the strong grow stronger and can tip the market toward a single , winner take all standard . we hypothesize , however , that low cost digital conversion technologies , which facilitate easy compatibility across competing standards , may reduce the strength of these network effects . we empirically test our hypotheses in the context of the digital flash memory card market . we first test for the presence of network effects in this market and find that network effects , as measured here , are associated with a significant positive price premium for leading flash memory card formats . we then find that the availability of digital converters reduces the price premium of the leading flash card formats and reduces the overall concentration in the flash memory market . thus , our results suggest that , in the presence of low cost conversion technologies and digital content , the probability of market dominance can be lessened to the point where multiple , otherwise incompatible , standards are viable . our conclusion that the presence of converters weakens network effects implies that producers of non dominant digital goods standards benefit from the provision of conversion technology . our analysis thus aids managers seeking to understand the impact of converters on market outcomes , and contributes to the existing literature on network effects by providing new insights into how conversion technologies can affect pricing strategies in these increasingly important digital settings .
inexact pattern matching using genetic algorithm . <eos> a genetic algorithm for graphical pattern matching based on angle matching had been proposed . it has proven quite effective in matching simple patterns . however , the algorithm needs some modifications to enhance its accuracy on pattern matching when there are some differences between two patterns in terms of numbers of nodes , shapes and rotations . this paper presents the modifications , such as the introduction of node exemption , inexact matching between straight lines and curves in the patterns , and consideration of rotational degrees of the patterns . each angle is also given with a weight to indicate the significant degree of the angle . a multi objective function is used to reflect the similarity between two patterns . the experiments designed to evaluate the algorithm have shown very promising results . it is highly accurate on patterns matching with dissimilarities in shapes , numbers of nodes and rotational degrees .
vector routing protocols for delay tolerant networks . <eos> in delay tolerant networks ( dtns ) with frequent network partitioning , routing packets is a challenge . because the successful establishment of an end to end path between source and destination nodes is not guaranteed . typical routing protocols for dtns depend on data replications over multiple paths for reliable data delivery since they invoke a lot of replicated packets . we propose three efficient vector routing protocols based on the vector of node movements in order to reduce the number of such replications flooding based , history based and location aware vector routing protocols using ns <digit> simulation with two different mobility models , we verify that vector routing protocols show better performance than other existing protocols in terms of less amount of traffic incurred without loss of packet delivery ratio .
debugging of globally optimized programs using data flow analysis . <eos> advanced processor and machine architectures need optimizing compilers to be efficiently programmed in high level languages . therefore the need for source level debuggers that can handle optimized programs is rising . one difficulty in debugging optimized code arises from the problem to determine the values of source code variables . to ensure correct debugger behaviour with optimized programs , the debugger not only has to determine the variable 's storage location or associated register . it must also verify that the variable is current , i.e. the value determined from that location is really the value that the variable would have in unoptimized code . we will deduce requirements on algorithms for currentness determination and present an algorithm meeting this requirements that is more general than previous work . we will also give first experiences with an implementation . to our knowledge this is the first implementation of a currentness determination algorithm for globally optimized code .
speed accuracy tradeoff in fitts ' law tasks on the equivalency of actual and nominal pointing precision . <eos> pointing tasks in human computer interaction obey certain speed accuracy tradeoff rules . in general , the more accurate the task to be accomplished , the longer it takes and vice versa . fitts ' law models the speed accuracy tradeoff effect in pointing as imposed by the task parameters , through fitts ' index of difficulty ( i d ) based on the ratio of the nominal movement distance and the size of the target . operating with different speed or accuracy biases , performers may utilize more or less area than the target specifies , introducing another subjective layer of speed accuracy tradeoff relative to the task specification . a conventional approach to overcome the impact of the subjective layer of speed accuracy tradeoff is to use the a posteriori effective pointing precision w , in lieu of the nominal target width w. such an approach has lacked a theoretical or empirical foundation . this study investigates the nature and the relationship of the two layers of speed accuracy tradeoff by systematically controlling both i d and the index of target utilization i u in a set of four experiments . their results show that the impacts of the two layers of speed accuracy tradeoff are not fundamentally equivalent . the use of w , could indeed compensate for the difference in target utilization , but not completely . more logical fitts ' law parameter estimates can be obtained by the w , adjustment , although its use also lowers the correlation between pointing time and the index of difficulty . the study also shows the complex interaction effect between i d and i u , suggesting that a simple and complete model accommodating both layers of speed accuracy tradeoff may not exist . ( c ) <digit> elsevier ltd. all rights reserved .
programming the flexram parallel intelligent memory system . <eos> in an intelligent memory architecture , the main memory of a computer is enhanced with many simple processors . the result is a highly parallel , heterogeneous machine that is able to exploit computation in the main memory . while several instantiations of this architecture have been proposed , the question of how to effectively program them with little effort has remained a major challenge . in this paper , we show how to effectively hand program an intelligent memory architecture at a high level and with very modest effort . we use flexram as a prototype architecture . to program it , we propose a family of high level compiler directives inspired by openmp called cflex . such directives enable the processors in memory to execute the program in cooperation with the main processor . in addition , we propose libraries of highly optimized functions called intelligent memory operations ( imos ) . these functions program the processors in memory through cflex , but make them completely transparent to the programmer . simulation results show that , with cflex and imos , a server with <digit> simple processors in memory runs on average <digit> times faster than a conventional server . moreover , a set of conventional programs with <digit> lines on average are transformed into cflex parallel form with only <digit> cflex directives and <digit> additional statements on average .
differentiated bandwidth allocation and wavelength assignment method in wdm epon . <eos> in a society based on the internet , demands for high capacity multimedia services are always increasing . however , because of lower transmission rates and the limitation of the distance through access networks , it is not possible to offer users satisfactory services . in order to solve the issue of limited transmission rates through access networks and to guarantee quality of service , we propose a differentiated bandwidth allocation method and a wavelength assignment method in wavelength division multiplexing ethernet passive optical networks . the proposed methods are implemented using opnet , and we have estimated the performance of the proposed methods . the results show that the proposed methods have outstanding performance and they can provide great parameters to model real access networks .
optimum beam design via stochastic programming . <eos> the purpose of the paper is to discuss the applicability of stochastic programming models and methods to civil engineering design problems . in cooperation with experts in civil engineering , the problem concerning an optimal design of beam dimensions has been chosen . the corresponding mathematical model involves an ode type constraint , uncertain parameter related to the material characteristics and multiple criteria . as a result , a multi criteria stochastic nonlinear optimization model is obtained . it has been shown that two stage stochastic programming offers a promising approach to solving similar problems . a computational scheme for this type of problems is proposed , including discretization methods for random elements and ode constraint . an approximation is derived to implement the mathematical model and solve it in gams . the solution quality is determined by an interval estimate of the optimality gap computed by a monte carlo bounding technique . the parametric analysis of a multi criteria model results in efficient frontier computation . furthermore , a progressive hedging algorithm is implemented and tested for the selected problem in view of the future possibilities of parallel computing of large engineering problems . finally , two discretization methods are compared by using gams and ansys .
ensuring the quality of conceptual representations . <eos> high quality data and process representations are critical to the success of system development efforts . despite this importance , quantitative methods for evaluating the quality of a representation are virtually nonexistent . this is a major shortcoming . however , there is another approach . instead of evaluating the quality of the final representation , the representation process itself can be evaluated . this paper views the modeling process as a communication channel . in a good communication channel , sufficient error prevention , error detection , and error correction mechanisms exist to ensure that the output message matches the input message . a good modeling process will also have mechanisms for preventing , detecting , and correcting errors at each step from observation to elicitation to analysis to final representation . this paper describes a theoretically based set of best practices for ensuring that each step of the process is performed correctly , followed by a proof of concept experiment demonstrating the utility of the method for producing a representation that closely reflects the real world .
clustering student skill set profiles in a unit hypercube using mixtures of multivariate betas . <eos> this paper presents a finite mixture of multivariate betas as a new model based clustering method tailored to applications where the feature space is constrained to the unit hypercube . the mixture component densities are taken to be conditionally independent , univariate unimodal beta densities ( from the subclass of reparameterized beta densities given by bagnato and punzo in comput stat <digit> ( <digit> ) <digit> . <digit> s00180 <digit> <digit> <digit> , <digit> ) . the em algorithm used to fit this mixture is discussed in detail , and results from both this beta mixture model and the more standard gaussian model based clustering are presented for simulated skill mastery data from a common cognitive diagnosis model and for real data from the assistment system online mathematics tutor ( feng et al. in j user model user adap inter <digit> ( <digit> ) <digit> , <digit> ) . the multivariate beta mixture appears to outperform the standard gaussian model based clustering approach , as would be expected on the constrained space . fewer components are selected ( by bic icl ) in the beta mixture than in the gaussian mixture , and the resulting clusters seem more reasonable and interpretable .
static approximation of dynamically generated web pages . <eos> server side programming is one of the key technologies that support today 's www environment . it makes it possible to generate web pages dynamically according to a user 's request and to customize pages for each user . however , the flexibility obtained by server side programming makes it much harder to guarantee validity and security of dynamically generated pages.to check statically the properties of web pages generated dynamically by a server side program , we develop a static program analysis that approximates the string output of a program with a context free grammar . the approximation obtained by the analyzer can be used to check various properties of a server side program and the pages it generates.to demonstrate the effectiveness of the analysis , we have implemented a string analyzer for the server side scripting language php . the analyzer is successfully applied to publicly available php programs to detect cross site scripting vulnerabilities and to validate pages they generate dynamically .
a simple geometric model for elastic deformations . <eos> we advocate a simple geometric model for elasticity distance between the differential of a deformation and the rotation group . it comes with rigorous differential geometric underpinnings , both smooth and discrete , and is computationally almost as simple and efficient as linear elasticity . owing to its geometric non linearity , though , it does not suffer from the usual linearization artifacts . a material model with standard elastic moduli ( lame parameters ) falls out naturally , and a minimizer for static problems is easily augmented to construct a fully variational <digit> ( nd ) order time integrator . it has excellent conservation properties even for very coarse simulations , making it very robust . our analysis was motivated by a number of heuristic , physics like algorithms from geometry processing ( editing , morphing , parameterization , and simulation ) . starting with a continuous energy formulation and taking the underlying geometry into account , we simplify and accelerate these algorithms while avoiding common pitfalls . through the connection with the biot strain of mechanics , the intuition of previous work that these ideas are like elasticity is shown to be spot on .
surfing the web backwards . <eos> life can only be understood backwards , but it must be lived forwards . '' soren kierkegaard from a user 's perspective , hypertext links on the web form a directed graph between distinct information sources . we investigate the effects of discovering backlinks ' from web resources , namely links pointing to the resource . we describe tools for backlink navigation on both the client and server side , using an applet for the client and a module for the apache web server . we also discuss possible extensions to the http protocol to facilitate the collection and navigation of backlink information in the world wide web .
speculative parallelization of sequential loops on multicores . <eos> the advent of multicores presents a promising opportunity for speeding up the execution of sequential programs through their parallelization . in this paper we present a novel solution for efficiently supporting software based speculative parallelization of sequential loops on multicore processors . the execution model we employ is based upon state separation , an approach for separately maintaining the speculative state of parallel threads and non speculative state of the computation . if speculation is successful , the results produced by parallel threads in speculative state are committed by copying them into the computation 's non speculative state . if misspeculation is detected , no costly state recovery mechanisms are needed as the speculative state can be simply discarded . techniques are proposed to reduce the cost of data copying between non speculative and speculative state and efficiently carrying out misspeculation detection . we apply the above approach to speculative parallelization of loops in several sequential programs which results in significant speedups on a dell poweredge <digit> server with two intel xeon quad core processors .
a variant of the jensen mercer operator inequality for superquadratic functions . <eos> a variant of the jensen mercer operator inequality for superquadratic functions , which is a refinement of the jensen mercer operator inequality for convex functions , is proved . the result obtained is used to refine some comparison inequalities between operator power and quasi arithmetic means of mercer 's type . ( c ) <digit> elsevier ltd. all rights reserved .
solid reconstruction from orthographic views using <digit> stage extrusion . <eos> in this paper , a method called <digit> stage extrusion is proposed to reconstruct a solid by modifying the incremental extrusion shum ssp . solid reconstruction from orthographic opaque views using incremental extrusion . computers graphics <digit> <digit> ( <digit> ) <digit> shum ssp et al. a low cost wireframe to solid implementation for reverse engineering . autofact '97 conference , <digit> november <digit> , ms98 ( <digit> ) , <digit> for translucent object domain . the algorithm has two extrusion stages . in each stage , geometric entities from only three orthogonal views ( viz. top , front and right ) are used . the entities involve both solid and dashed lines in each view . in the first stage , an exterior contour region in each view is swept along its normal direction according to the corresponding object dimension . as a result , three extrusion solids are produced . intersection of the three extrusion solids forms a basic solid . next , all interior entities of each view are treated by a filtering process . if all interior entities are discarded in the filtering , the second stage will be skipped and the basic solid becomes a solution solid . on the other hand , if any interior entity remains after filtering , they are processed in the second stage to generate an excess solid . in this case , the basic solid subtracts the excess solid to form the final three dimensional solution solid .
a method for topological entity matching in the integration of heterogeneous cad systems . <eos> topological entities are always used as reference or operation objects in the feature modeling procedure . hence , to achieve the integration of heterogeneous cad systems , the corresponding topological entities must be found in the target cad system to match the ones used in the source cad system . in this paper , the topological entities ' matching relations are first analyzed . based on the analysis , a method for topological entity matching in the integration of heterogeneous cad systems is proposed . the method consists of three key steps information retrieval , information combination and topological entities ' matching . first , the information of the topological entities used in the source cad system is retrieved . then , the retrieved information of those topological entities which satisfy the combination conditions is combined using the combination algorithm . finally , for each topological entity obtained after the combination , the topological entities that satisfy the matching conditions are found using the matching algorithm in the target cad system . the proposed method is evaluated by applying to a cad data exchange system and a replicated heterogeneous co design system the results demonstrate that our method works well for topological entity matching in the integration of heterogeneous cad systems and improves the previous methods in both topological entities ' combination and less constrained concurrency control .
efficient agent based models for non genomic evolution . <eos> modeling dynamical systems composed of aggregations of primitive proteins is critical to the field of astrobiological science , which studies early evolutionary structures dealing with the origins of life . current theories on the emergence of early life focus either on rna world models <digit> , <digit> or protein world models <digit> , <digit> . traditional modeling based on either model are generally either too slow to converge or too simplified to provide good tools for exploring the trade offs in the early stages of the emergence of life . this paper focuses on protein world models and discusses how to model protein aggregations through a utility based multi agent system . we define agents to control specific properties of a given set of proteins . these properties determine the dynamics of the system , such as the ability for proteins to join or split apart , while additional properties determine the aggregation 's fitness as a viable primitive cell . we show that over a wide range of starting conditions , there are mechanisms that allow protein aggregations to achieve high values of overall fitness . in addition through the use of agent specific utilities that remain aligned with the overall global utility , we are able to reach these conclusions with <digit> times fewer learning steps .
tracking objects with generic calibrated sensors an algorithm based on color and 3d shape features . <eos> we present a color and shape based 3d tracking system suited to a large class of vision sensors . the method is applicable , in principle , to any known calibrated projection model . the tracking architecture is based on particle filtering methods where each particle represents the 3d state of the object , rather than its state in the image , therefore overcoming the nonlinearity caused by the projection model . this allows the use of realistic 3d motion models and easy incorporation of self motion measurements . all nonlinearities are concentrated in the observation model so that each particle projects a few tens of special points onto the image , on ( and around ) the 3d object 's surface . the likelihood of each state is then evaluated by comparing the color distributions inside and outside the object 's occluding contour . since only pixel access operations are required , the method does not require the use of image processing routines like edge feature extraction , color segmentation or 3d reconstruction , which can be sensitive to motion blur and optical distortions typical in applications of omnidirectional sensors to robotics . we show tracking applications considering different objects ( balls , boxes ) , several projection models ( catadioptric , dioptric , perspective ) and several challenging scenarios ( clutter , occlusion , illumination changes , motion and optical blur ) . we compare our methodology against a state of the art alternative , both in realistic tracking sequences and with ground truth generated data . ( c ) <digit> elsevier b.v. all rights reserved .
mining templates from search result records of search engines . <eos> metasearch engine , comparison shopping and deep web crawling applications need to extract search result records enwrapped in result pages returned from search engines in response to user queries . the search result records from a given search engine are usually formatted based on a template . precisely identifying this template can greatly help extract and annotate the data units within each record correctly . in this paper , we propose a graph model to represent record template and develop a domain independent statistical method to automatically mine the record template for any search engine using sample search result records . our approach can identify both template tags ( html tags ) and template texts ( non tag texts ) , and it also explicitly addresses the mismatches between the tag structures and the data structures of search result records . our experimental results indicate that this approach is very effective .
processing knowledge to support knowledge based engineering systems specification . <eos> during design phase , engineering activities typically involve large groups of people from different domains and disciplines . these differences often generate important information flows that are difficult to manage . to face these difficulties , a knowledge engineering process is necessary to structure the information and its use this article , presents a deployment of a knowledge capitalization process based on the enrichment of methodology and tools oriented to knowledge based engineering applications methodology to support the integration of process planning knowledge in a cad system . our goal is to help different actors to work collaboratively by proposing one referential view of the domain , the context and the objectives assuming that it will help them in better decision making .
a heuristic for rationing inventory in two demand classes with backlog costs and a service constraint . <eos> we study the rationing policy in an inventory system with two demand classes and different service criteria for backorders . due to the difference of customer values , system performance sometimes has to be measured with a mixture of penalty cost and service level in managing inventory . with a continuous review ( r , q ) system , we develop a critical level rationing policy in which a threshold mechanism is adopted to allocate backorders when multiple outstanding orders exist . due to the complexity of the problem , a heuristic is developed based on the principle that both demand classes are served with respective target service levels . we also introduce bounds so that the search ranges of decision variables become restrictive . the numerical examples indicate an excellent performance of our heuristic . in addition , when ordering cost is medium or high , the threshold clearing mechanism has the same results as the optimal one . when ordering cost is small ( set to zero ) , different clearing mechanisms should be used depending on the priorities of demand classes . further analysis indicates that transforming the service constraint into a cost parameter and then applying the existing algorithm will not be a good approach for this problem with mixed performance criteria . it either increases the costs or violates the service constraint . this study also shows the importance of applying rationing policy when high priority class has a low demand volume , target service levels between two classes have a large gap , or replenishment lead time is long . the results of this study should enhance our understanding of how to implement rationing policies in practice .
ears of the robot three simultaneous speech segregation and recognition using robot mounted microphones . <eos> a new type of sound source segregation method using robot mounted microphones , which are free from strict head related transfer function ( hrtf ) estimation , has been proposed and successfully applied to three simultaneous speech recognition systems . the proposed segregation method is executed with sound intensity differences that are due to the particular arrangement of the four directivity microphones and the existence of a robot head acting as a sound barrier . the proposed method consists of three layered signal processing two line safia ( binary masking based on the narrow band sound intensity comparison ) , two line spectral subtraction and their integration . we performed 20k vocabulary continuous speech recognition test in the presence of three speakers ' simultaneous talk , and achieved more than <digit> % word error reduction compared with the case without any segregation processing .
fabrication of multi tiered structures on step and flash imprint lithography templates . <eos> step and flash imprint lithography ( sfil ) replicates patterns by using a transparent template with relief images etched into its surface . recent work has examined alternative methods for template fabrication . one scheme incorporates a conductive and transparent layer of indium tin oxide ( ito ) on the surface of the substrate . features are defined on the templates by patterning a thin layer of pecvd oxide that is deposited on the ito layer . a second method bypasses the oxide etch process by imaging a thin layer of hydrogen silsesquioxane ( hsq ) . by combining or iterating the two methods , it is possible to form multi tiered structures on a template . two and three tier structures were fabricated on silicon wafers and templates . a two layer structure was fabricated on a quartz photoplate by patterning pecvd oxide and subsequently patterning a second tier using hsq . the resulting relief structures were successfully replicated on wafers using sfil .
an evolutionary algorithm for the discovery of rare class association rules in learning management systems . <eos> association rule mining , an important data mining technique , has been widely focused on the extraction of frequent patterns . nevertheless , in some application domains it is interesting to discover patterns that do not frequently occur , even when they are strongly related . more specifically , this type of relation can be very appropriate in e learning domains due to its intrinsic imbalanced nature . in these domains , the aim is to discover a small but interesting and useful set of rules that could barely be extracted by traditional algorithms founded in exhaustive search based techniques . in this paper , we propose an evolutionary algorithm for mining rare class association rules when gathering student usage data from a moodle system . we analyse how the use of different parameters of the algorithm determine the rule characteristics , and provides some illustrative examples of them to show their interpretability and usefulness in e learning environments . we also compare our approach to other existing algorithms for mining both rare and frequent association rules . finally , an analysis of the rules mined is presented , which allows information about students unusual behaviour regarding the achievement of bad or good marks to be discovered .
minimizing the cost of fault location when testing from a finite state machine . <eos> if a test does not produce the expected output , the incorrect output may have been caused by an earlier state transfer failure . ghedamsi and coworkers generate a set of candidates and then produce further tests to locate the failures within this set . we consider a special case where then is a state identification process that is known to be correct . a number of preset and adaptive approaches to fault location are described and the problem of minimizing the cost is explored . some of the approaches lead to np hard optimization problems for which possible heuristics are suggested . ( c ) <digit> elsevier science b.v. all rights reserved .
accelerating calculations of rna secondary structure partition functions using gpus . <eos> rna performs many diverse functions in the cell in addition to its role as a messenger of genetic information . these functions depend on its ability to fold to a unique three dimensional structure determined by the sequence . the conformation of rna is in part determined by its secondary structure , or the particular set of contacts between pairs of complementary bases . prediction of the secondary structure of rna from its sequence is therefore of great interest , but can be computationally expensive . in this work we accelerate computations of base pair probababilities using parallel graphics processing units ( gpus ) .
the bayesian image retrieval system , pichunter theory , implementation , and psychophysical experiments . <eos> this paper presents the theory , design principles , implementation , and performance results of pichunter , a prototype content based image retrieval ( cbir ) system that has been developed over the past three years . in addition , this document presents the rationale , design , and results of psychophysical experiments that were conducted to address some key issues that arose during pichunter 's development , the pichunter project makes four primary contributions to research on content based image retrieval . first , pichunter represents a simple instance of a general bayesian framework we describe for using relevance feedback to direct a search . with an explicit model of what users would do , given what target image they want , pichunter uses bayes 's rule to predict what is the target they want , given their actions . this is done via a probability distribution over possible image targets , rather than by refining a query . second , an entropy minimizing display algorithm is described that attempts to maximize the information obtained from a user at each iteration of the search . third , pichunter makes use of bidden annotation rather than a possibly inaccurate inconsistent annotation structure that the user must learn and make queries in . finally , pichunter introduces two experimental paradigms to quantitatively evaluate the performance of the system , and psychophysical experiments are presented that support the theoretical claims .
a discrete crack joint model for nonlinear dynamic analysis of concrete arch dam . <eos> concrete arch dams are generally constructed of massive plain concrete with almost no tensile resistance . to control tensile forces due to concrete shrinkage , temperature variations and for construction facilitation , arch dams are built in cantilever monoliths separated by vertical contraction joints . earlier studies show that the modeling of such joints has significant influence on the seismic safety evaluation of arch dams . this fact is due to the tensile and shear failures of joints causing a redistribution of internal forces during and after a big earthquake . in the present study , a nonlinear joint element model with a coupled sheartensile behavior for realistic finite element analysis of damreservoir system is presented . reservoir upstream radiation , and bottom partial absorption of acoustic waves , as well as water compressibility are considered . the model when applied to simpler cases solved by other workers shows good performances . however , it is much more useful to solve problems not considered so far , e.g. , shear keys behavior , joint damages , etc. the model could be employed effectively and conveniently for earthquake safety evaluation of arch dams in highly active seismic regions .
expressions and iterative methods for the weighted group inverses of linear operators on banach space . <eos> in this note , some expressions and characterizations for the weighted group inverses a ( w ) ( parallel to ) of operator a by using the technique of block operator matrix are given , three iterative methods for computing a ( w ) ( parallel to ) are established , and the necessary and sufficient conditions for iterative convergence to a ( w ) ( parallel to ) are discussed .
building connected neighborhood graphs for isometric data embedding . <eos> neighborhood graph construction is usually the first step in algorithms for isometric data embedding and manifold learning that cope with the problem of projecting high dimensional data to a low space . this paper begins by explaining the algorithmic fundamentals of techniques for isometric data embedding and derives a general classification of these techniques . we will see that the nearest neighbor approaches commonly used to construct neighborhood graphs do not guarantee connectedness of the constructed neighborhood graphs and , consequently , may cause an algorithm fail to project data to a single low dimensional coordinate system . in this paper , we review three existing methods to construct k edge connected neighborhood graphs and propose a new method to construct k connected neighborhood graphs . these methods are applicable to a wide range of data including data distributed among clusters . their features are discussed and compared through experiments .
fuzzy evolutionary optimization modeling and its applications to unsupervised categorization and extractive summarization . <eos> modern information retrieval ( ir ) systems consist of many challenging components , e.g. clustering , summarization , etc. nowadays , without browsing the whole volume of datasets , ir systems present users with clusters of documents they are interested in , and summarize each document briefly which facilitates the task of finding the desired documents . this paper proposes a fuzzy evolutionary optimization modeling ( feom ) and its applications to unsupervised categorization and extractive summarization . in view of the nature of biological evolution , we take advantage of several fuzzy control parameters to adaptively regulate the behaviors of the evolutionary optimization , which can effectively prevent premature convergence to a local optimal solution . as a portable , modular and extensively executable model , feom is firstly implemented for clustering text documents . the searching capability of feom is exploited to explore appropriate partitions of documents such that the similarity metric of the resulting clusters is optimized . in order to further investigate its effectiveness as a generic data clustering model , feom is then applied to sentence clustering based extractive document summarization . it selects the most important sentence from each cluster to represent the overall meaning of document . we demonstrate the improved performance by a series of experiments using standard test sets , e.g. reuter document collection , <digit> newsgroup corpus , duc01 and duc02 , as evaluated by some commonly used metrics , i.e. f measure and rouge . the experimental results show that feom achieves performance as good as or better than state of arts of clustering and summarizing systems .
examining possible antecedents of it impact on the supply chain and its effect on firm performance . <eos> our study examined the impact of information technology ( it ) on the supply chain through a survey of <digit> large for profit us firms . specifically , it involved the determination of it antecedents to it impact on the supply chain and the effect that these relationships had on overall firm performance . the respondents were primarily chief information officers and other top it executives . the factors in the study were validated using confirmatory factor analysis . a model featuring the it antecedents , it impact on the supply chain , and firm performance was evaluated using a structural equation analysis . the data used fit well with the hypothesized relationships in the model , as all links were significant . the findings here suggested that the antecedents , it department technical quality , it plan utilization , and top management of it positively affected it impact on the supply chain . the results also revealed a positive relationship between it impact and firm performance .
on liveness of time poc nets with the static fair condition . <eos> petri net is a graphical and mathematical modeling tool for discrete event systems . this paper treats analysis problems of time petri nets . in this model , a minimal and a maximal firing delays are assigned to each transition . if a transition is ' enabled ' it can fire after minimal delay has passed and must fire before maximal delay has elapsed . since time petri net can simulate register machines , it has equivalent modeling power to that of turing machine . it means , however , that most of the analysis problems of time petri nets with general net structures are undecidable . in this paper , net structures are restricted to a subclass called partially ordered condition ( poc ) nets and dissynchronous choice ( dc ) nets . firing delays are also restricted to satisfy ' static fair condition ' which assures chance to fire for all transitions enabled simultaneously . first , a sufficient condition of liveness of time poc net with the static fair condition is derived . then it is shown that liveness of time dc net with static fair condition is equivalent to liveness of the underlying nontime net . this means that liveness problem of this class is decidable . lastly , liveness problem of extended free choice ( efc ) net is shown to be decidable .
light region based techniques for process discovery . <eos> a central problem in the area of process mining is to obtain a formal model that represents selected behavior of a system . the theory of regions has been applied to address this problem , enabling the derivation of a petri net whose language includes a set of traces . however , when dealing with real life systems , the available tool support for performing such a task is unsatisfactory , due to the complex algorithms that are required . in this paper , the theory of regions is revisited to devise a novel technique that explores the space of regions by combining the elements of a region basis . due to its light space requirements , the approach can represent an important step for bridging the gap between the theory of regions and its industrial application . experimental results show that there is improvement in orders of magnitude in comparison with state of the art tools for the same task .
monitoring and state transparency of distributed systems . <eos> this paper presents the system status suite of applications . these applications are used to provide a simple , uniform , and low developer cost system for exporting and tracking the state of otp applications and services over a distributed server farm network architecture . the terms , simple , and low developer cost , will be elaborated on later in the paper . the system is intended to provide no formalized management framework it is specifically a state status export and monitoring infrastructure .
mobile and fixed substitution for telephone service in china . <eos> a panel of province by year data for china from <digit> to <digit> is used to estimate own and cross price elasticities for fixed and mobile telephone service . identification is achieved through the use of measures of the level of competition and privatization of the industry . moreover , using neighboring province values ensures instrumental variable exogeneity . estimates suggest that fixed and mobile subscriptions are fairly strong substitutes . this provides evidence for the development of competition between these platforms .
a comparative survey of business process similarity measures . <eos> similarity measures for business process models have been suggested for different purposes such as measuring compliance between reference and actual models , searching for related models in a repository , or locating services that adhere to a specification given by a process model . the aim of our article is to provide a comprehensive survey on techniques to define and calculate such similarity measures . as the measures differ in many aspects , it is an interesting question how different measures rank similarity within the same set of models . we investigated , how different kinds of changes in a model influence the values of different similarity measures that have been published in academic literature . furthermore , we identified eight properties that a similarity measure should have from a theoretical point of view and analysed how these properties are fulfilled by the different measures . our results show that there are remarkable differences among existing measures . we give some recommendations which type of measure is useful for which kind of application . ( c ) <digit> elsevier b.v. all rights reserved .
extracting 3d model feature lines based on conditional random fields . <eos> we propose a 3d model feature line extraction method using templates for guidance . the 3d model is first projected into a depth map , and a set of candidate feature points are extracted . then , a conditional random fields ( crf ) model is established to match the sketch points and the candidate feature points . using sketch strokes , the candidate feature points can then be connected to obtain the feature lines , and using a crf matching model , the 2d image shape similarity features and 3d model geometric features can be effectively integrated . finally , a relational metric based on shape and topological similarity is proposed to evaluate the matching results , and an iterative matching process is applied to obtain the globally optimized model feature lines . experimental results showed that the proposed method can extract sound 3d model feature lines which correspond to the initial sketch template .
a novel acoustic method of partial discharge allocation considering structure borne waves . <eos> a novel pd allocation method based on acoustic sensors is proposed for power transformers . the structure borne waves are considered for higher precision . utilizing heuristic methods the location of pd is estimated with high precision .
efficient reversible data hiding for color filter array images . <eos> a reversible data hiding algorithm which uses prediction errors in the color difference domain for mosaic images with the bayer color filter array ( cfa ) is proposed . furthermore , the proposed algorithm can be extended to deal with the digital time delay and integration ( dtdi ) mosaic images and lukac and plataniotis ( lp ) mosaic images . experimental results on cfa , dtdi , and lp mosaic images demonstrate that the proposed algorithm can achieve high embedding capacity while maintaining good image quality .
a prototype computer network service for occupational therapists . <eos> due to recent reforms , the demands on the people working in community oriented health care service are increasing . the individual providers need professional knowledge and skills to perform their tasks quickly and safely . the individuals are also confronted with new tasks and situations of which they lack experience . at the same time , the resources for education and development are decreasing . the aim of this paper is to describe the implementation of a prototype computer network service to support occupational therapists in their daily work . a customized quality function deployment ( qfd ) model , including participatory design elements , was used for ( a ) identification of the occupational therapists needs and ( b ) for the transformation of these needs to prioritized design attributes . the main purpose of the prototype was to improve the visualization of the design attributes that were found to support the occupational therapists . an additional purpose was to be able to evaluate the design attributes and further improve them . the specific aim of this article is to describe the initial prototype with respect both to the tools and the information content .
a new approach for time series prediction using ensembles of anfis models . <eos> this paper describes an architecture for ensembles of anfis ( adaptive network based fuzzy inference system ) , with emphasis on its application to the prediction of chaotic time series , where the goal is to minimize the prediction error . the time series that we are considered are the mackey glass , dow jones and mexican stock exchange . the methods used for the integration of the ensembles of anfis are integrator by average and the integrator by weighted average . the performance obtained with this architecture overcomes several standard statistical approaches and neural network models reported in the literature by various researchers . in the experiments we changed the type of membership functions and the desired goal error , thereby increasing the complexity of the training . ( c ) <digit> elsevier ltd. all rights reserved .
ieee 802.21 media independence beyond handover . <eos> the ieee 802.21 standard facilitates media independent handovers by providing higher layer mobility management functions with common service primitives for all technologies . right after the base specification was published , several voices rose up in the working group advocating to broaden the scope of ieee 802.21 beyond handovers . this paper aims at updating the reader with the main challenges and functionalities required to create a media independence service layer , through the analysis of scenarios which are being discussed within the working group <digit> ) wireless coexistence , and <digit> ) heterogeneous wireless multihop backhaul networks .
evolving better population distribution and exploration in evolutionary multi objective optimization . <eos> the aim of multi objective evolutionary optimization is to minimize the distance between the solution set and the true pareto front , to distribute the solutions evenly and to maximize the spread of solution set . this paper addresses these issues by presenting two features that enhance the optimization ability of multi objective evolutionary algorithms . the first feature is a variant of the mutation operator that adapts the mutation rate along the evolution process to maintain a balance between the introduction of diversity and local fine tuning . in addition , this adaptive mutation operator adopts a new approach to strike a compromise between the preservation and disruption of genetic information . the second feature is an enhanced exploration strategy that encourages the exploration towards less populated areas and hence achieves better discovery of gaps in the generated front . the strategy also preserves non dominated solutions in the evolving population to achieve a good convergence for the optimization . comparative studies of some well known diversity operators , mutation operators and multi objective evolutionary algorithms are performed on different benchmark problems , which illustrate the effectiveness and efficiency of the proposed features .
solving corrective risk based security constrained optimal power flow with lagrangian relaxation and benders decomposition . <eos> an efficient decomposition based algorithm is developed to solve the corrective risk based scopf problem . the risk is a probabilistic index that is a function of conditions under normal and all contingencies . a combined use of lagrangian relaxation and benders decomposition can solve the problem efficiently . the proposed approach is tested on the ieee <digit> bus system and on the iso new england bulk system .
compressive acquisition cmos image sensor from the algorithm to hardware implementation . <eos> in this paper , a new design paradigm referred to as compressive acquisition cmos image sensors is introduced . the idea consists of compressing the data within each pixel prior to storage , and hence , reducing the size of the memory required for digital pixel sensor . the proposed compression algorithm uses a block based differential coding scheme in which differential values are captured and quantized online . a time domain encoding scheme is used in our cmos image sensor in which the brightest pixel within each block fires first and is selected as the reference pixel . the differential values between subsequent pixels and the reference within each block are calculated and quantized , using a reduced number of bits as their dynamic range is compressed . the proposed scheme enables reduced error accumulation as full precision is used at the start of each block , while also enabling reduced memory requirement , and hence , enabling significant silicon area saving . a mathematical model is derived to analyze the performance of the algorithm . experimental results on a field programmable gate array ( fpga ) platform illustrate that the proposed algorithm enables more than <digit> % memory saving at a peak signal to noise ratio level of <digit> db with 1.5 bit per pixel .
process variation aware performance analysis of asynchronous circuits . <eos> current technology trends have led to the growing impact of process variations on performance of asynchronous circuits . as it is imperative to model process parameter variations for sub 100nm technologies to produce a more real performance metric , it is equally important to consider the correlation of these variations to increase the accuracy of the performance computation . in this paper , we present an efficient method for performance evaluation of asynchronous circuits considering inter and intra die process variation . the proposed method includes both statistical static timing analysis ( ssta ) and statistical timed petri net based simulation . template based asynchronous circuit has been modeled using variant timed petri net . based on this model , the proposed ssta calculates the probability density function of the delay of global critical cycle . the efficiency for the proposed ssta is obtained from a technique that is derived from the principal component analysis ( pca ) method . this technique simplifies the computation of mean , variance and covariance values of a set of correlated random variables . in order to consider spatial correlation in the petri net based simulation , we also include a correlation coefficient to the proposed variant timed petri net which is obtained from partitioning the circuit . we also present a simulation tool of variant timed petri net and the results of the experiments are compared with monte carlo simulation based method .
application optimization in mobile cloud computing motivation , taxonomies , and open challenges . <eos> in mobile cloud computing ( mcc ) , migrating an application processing to the cloud data centers enables the execution of resource intensive applications on the mobile devices . however , the resource intensive migration approaches and the intrinsic limitations of the wireless medium impede the applications from attaining optimal performance in the cloud . hence , executing the application with low cost , minimal overhead , and non obtrusive migration is a challenging research area . this paper presents the state of the art mobile application execution frameworks and provides the readers a discussion on the optimization strategies that facilitate attaining the effective design , efficient deployment , and application migration with optimal performance in mcc . we highlight the significance of optimizing the application performance by providing real life scenarios requiring the effective design , efficient deployment , and optimal application execution in mcc . the paper also presents cloud based mobile application related taxonomies . moreover , we compare the application execution frameworks on the basis of significant optimization parameters that affect performance of the applications and mobile devices in mcc . we also discuss the future research directions for optimizing the application in mcc . finally , we conclude the paper by highlighting the key contributions and possible research directions in cloud based mobile application optimization .
using an adaptive self tuning approach to forecast power loads . <eos> to overcome the difficulty in forecasting power loads precisely , an adaptive self tuning approach is proposed . using an rbf neural network as a predictor and building an evaluator to evaluate the output of the predictor , then , according to the evaluator 's judgment , the predictor adjusts its structure and weight by using critical self learning mechanics . the predictor can keep the same pattern as the current power loads state and power loads can be forecasted precisely . the simulation of practical date indicated that this method is effective .
does tutoring really have to be intelligent . <eos> this experiment was designed to determine whether or not tutoring is more effective if it is relevant to the user 's current problems . the experimental design presented identical tutoring advice to pairs of subjects advice was directly relevant to one subject ( as determined by a human researcher monitoring a pre specified task ) and effectively random to the other.the quantitative and qualitative results were strikingly different . on one hand , subjects learned almost all tutored commands , regardless of their relevance to their immediate activities , and rarely learned commands that had not been tutored . on the other hand , subjects were very enthusiastic about relevant tutoring and were frustrated when it seemed random they felt it was an irritating interruption . an intelligent rule based tutor may be unnecessary for effective learning if users can control the tutoring environment .
online social networking behaviors among chinese younger and older adolescent the influences of age , gender , personality , and attachment styles . <eos> we conducted survey on social networking behaviors in <digit> chinese adolescents . we found many chinese adolescents displayed personal information online . participants privacy disclosure was negatively correlated with sns behaviors . sns behaviors were influenced by developmental factors . attachment styles moderated association between disclosure and sns behaviors .
user perceptions of search enhancements in web search . <eos> commercial web search engines are increasingly supplementing ordered lists of text results with various search enhancements . these enhancements include query suggestions , local business results , news results , related searches , and spelling suggestions . this study explores the relationships between enhancement utilization , perceptions of search effectiveness , and task complexity . analysis of <digit> search tasks conducted by <digit> study participants finds that perceptions of search effectiveness are negatively associated with task complexity . we also found that perceptions of search effectiveness increased when enhancements were utilized . however , perceptions of search effectiveness decreased when more than one enhancement was used in complex search tasks . these findings have important implications for practice and research .
eliminating spammers and ranking annotators for crowdsourced labeling tasks . <eos> with the advent of crowdsourcing services it has become quite cheap and reasonably effective to get a data set labeled by multiple annotators in a short amount of time . various methods have been proposed to estimate the consensus labels by correcting for the bias of annotators with different kinds of expertise . since we do not have control over the quality of the annotators , very often the annotations can be dominated by spammers , defined as annotators who assign labels randomly without actually looking at the instance . spammers can make the cost of acquiring labels very expensive and can potentially degrade the quality of the final consensus labels . in this paper we propose an empirical bayesian algorithm called spem that iteratively eliminates the spammers and estimates the consensus labels based only on the good annotators . the algorithm is motivated by defining a spammer score that can be used to rank the annotators . experiments on simulated and real data show that the proposed approach is better than ( or as good as ) the earlier approaches in terms of the accuracy and uses a significantly smaller number of annotators .
interface engineered euv multilayer mirrors . <eos> most applications of mo si multilayer optics in euv lithographic systems require a high normal incidence reflectivity . using dc magnetron sputtering we achieved r 68.8 % at the wavelength of 13.5 nm . different interface engineered mo x si x multilayers with a maximum reflectivity of 69.6 % were developed . these new multilayer mirrors consist of molybdenum and silicon layers separated by different interdiffusion barriers ( x c and sic ) . the mo c si c interface engineered mirrors were optimized in terms of high peak reflectivity at a wavelength near 13.5 nm ( rp 60.0 % ) and broad operating temperature range ( t 20500c ) . the best results were obtained with 0.8 nm thicknesses of carbon interlayers on both interfaces . the combination of good optical properties and high thermal stability of interface engineered mo c si c multilayer mirrors underlines their potential for their use in euv optics .
fuzzy multi attribute selection among transportation companies using axiomatic design and analytic hierarchy process . <eos> the basic concept in axiomatic design ( ad ) is the existence of design axioms . first of these axioms is the independence axiom and the second one is the information axiom . information axiom proposes the selection of the best alternative that has minimum information . analytic hierarchy process ( ahp ) is another multi attribute method which is a decision making method for selecting the best among a set of alternatives , given some criteria . the method has been extensively applied , especially in large scale problems where many criteria must be considered and where the evaluation of alternatives is mostly subjective . multi attribute transportation company selection is a very important activity for effective supply chain . selection of the best company under determined criteria ( such as cost , time , damage loss , flexibility and documentation ability ) using both multi attribute ad and ahp will be realized in this study . the fuzzy multi attribute ad approach is also developed and it is compared by one of fuzzy ahp methods in the literature . the selection process has been accomplished by aiding a software that includes crisp ad and fuzzy ad .
timing analysis considering spatial power ground level variation . <eos> spatial power ground level variation causes power ground level mismatch between driver and receiver , and the mismatch affects gate propagation delay . this paper proposes a timing analysis method based on a concept called pg level equalization which is compatible with conventional sta frameworks . we equalize the power ground levels of driver and receiver . the charging discharging current variation due to equalization is compensated by replacing output load . we present an implementation method of the proposed concept , and demonstrate that the proposed method works well for multiple input gates and rc load model .
on the sum of all distances in bipartite graphs . <eos> the transmission of a connected graph g g is the sum of all distances between all pairs of vertices in g g , it is also called the wiener index of g g . in this paper , sharp bounds on the transmission are determined for several classes of connected bipartite graphs . for example , in the class of all connected n n vertex bipartite graphs with a given matching number q q , the minimum transmission is realized only by the graph kq , n q k q , n q in the class of all connected n n vertex bipartite graphs of diameter d d , the extremal graphs with the minimal transmission are characterized . moreover , all the extremal graphs having the minimal transmission in the class of all connected n n vertex bipartite graphs with a given vertex connectivity ( resp.edge connectivity ) are also identified .
extending knowledge management to mobile workplaces . <eos> knowledge and knowledge management ( km ) are evolving into an increasingly eminent source of competitive advantage . however , for the time being , the potential of km is usually limited to stationary workplaces . this excludes a multiplicity of mobile workers , many of them in charge of knowledge intensive activities . this paper examines the capabilities and limitations of mobile technology usage in order to support km . after a general overview of km , the relevant mobile technology is introduced . subsequently , the theory of mobile added values is employed to analyze the contributions of mobile technology for supporting km in the different phases of the km process . especially the process of knowledge distribution is qualified to be supported through mobile technology .
lifetime planning for tdma based proactive wsn with structured deployment . <eos> it is commonly accepted that the primary design goal in wireless sensor networks is the reduction of energy costs . thus , the concept of lifetime planning is prioritized over capacity planning when dealing with such networks instead of the traditional ones . this paper offers a systematized treatment of the lifetime of tdma based proactive sensor networks with structured deployment . this is the case of many environmental networks , which are devoted to the regular monitoring of some physical or even chemical variables , for scientific study and or for taking appropriate control actions . on the other hand , sensor nodes in structured deployments are located at strategic positions that may not be as close to each other as in random and massive deployments . thus , in the second part of this paper , a cost effective mechanism that consists of inserting pure switching nodes in some links is proposed , as a way of enhancing lifetime by alleviating the severe effects of the path loss exponent in the radio propagation model .
methodology for reconstructing early zebrafish development from in vivo multiphoton microscopy . <eos> investigating cell dynamics during early zebrafish embryogenesis requires specific image acquisition and analysis strategies . multiharmonic microscopy , i.e. , second and third harmonic generations , allows imaging cell divisions and cell membranes in unstained zebrafish embryos from <digit> to <digit> cell stage . this paper presents the design and implementation of a dedicated image processing pipeline ( tracking and segmentation ) for the reconstruction of cell dynamics during these developmental stages . this methodology allows the reconstruction of the cell lineage tree including division timings , spatial coordinates , and cell shape until the <digit> cell stage with minute temporal accuracy and micrometer spatial resolution . data analysis of the digital embryos provides an extensive quantitative description of early zebrafish embryogenesis .
excel workbook for automatic parameter calculation of epg data . <eos> the electrical penetration graph ( epg ) technique is a powerful tool for studying feeding behavior of pierce sucking insects , because it allows quantification of complex insectplant interactions occurring inside the plant . however , this technique has an important limitation related to the time consuming analysis of the acquired data , mainly due to the length of the epg recordings and the complexity of the parameters used to explore insect behavior . this paper presents a microsoft excel workbook that simplifies the analyses of epg data and automatically calculates a large number of epg parameters that characterize insect probing and ingestion behavior . these parameters arise from a wide review of epg papers related to different aspects of insectplant and virusinsect vector interactions . another advantage of this application is that data input can be entered from both ac and dc based amplifiers and from different software packages used to acquire epg data ( e.g. macstylet , windaq and probe ) . the workbook summarizes the results and generates an output sheet for further statistical analysis . in this report , we explain how the workbook can be used to analyze the probing and ingestion behavior of two hemipteran species , demonstrating its flexibility and potentiality for efficient and rapid analysis of epg data .
lda based document models for ad hoc retrieval . <eos> search algorithms incorporating some form of topic model have a long history in information retrieval . for example , cluster based retrieval has been studied since the 60s and has recently produced good results in the language model framework . an approach to building topic models based on a formal generative model of documents , latent dirichlet allocation ( lda ) , is heavily cited in the machine learning literature , but its feasibility and effectiveness in information retrieval is mostly unknown . in this paper , we study how to efficiently use lda to improve ad hoc retrieval . we propose an lda based document model within the language modeling framework , and evaluate it on several trec collections . gibbs sampling is employed to conduct approximate inference in lda and the computational complexity is analyzed . we show that improvements over retrieval using cluster based models can be obtained with reasonable efficiency .
exploring the molecular mechanism of action between drug and rna polymerase based on partially resolved spatial structures . <eos> the rna polymerase of influenza a virus ( iav ) , which is comprised of three units pa , pb1 and pb2 , is involved in transcription and replication of the influenza virus . in order to develop effective treatment for iav , researchers have focused on designing drugs targeting iav polymerase . currently , crystal structures of the iav polymerase pa pb1 , pb1 pb2 complexes and the pa subunit have been obtained by several groups , providing useful information regarding potential binding sites in drug design . however , to gain full understanding of the molecular mechanism of iav polymerase in viral transcription and replication , thereby aiding drug development , a complete atomistic structure of the rna polymerase is required . in this paper , we employed computer aided drug design tools to describe the complete structure of the rna polymerase and proposed a putative mechanism . we predict that the combination of vancomycin and oseltamivir will be an effective drug to universally treat iavs with no resultant drug resistance if this putative mechanism is true .
non invasive analysis of sleep patterns via multimodal sensor input . <eos> the monitoring of sleep patterns is of major importance for various reasons such as the detection and treatment of sleep disorders , the assessment of the effect of different medical conditions or medications on the sleep quality , and the assessment of mortality risks associated with sleeping patterns in adults and children . sleep monitoring by itself is a difficult problem due to both privacy and technical considerations . the proposed system uses a combination of non invasive sensors to assess and report sleep patterns a contact based pressure mattress and a non contact 3d image acquisition device , which can complement each other . to evaluate our system , we used real data collected in heracleia labs assistive living apartment . our system uses machine learning techniques to automatically analyze the collected data and recognize sleep patterns . it is non invasive , as it does not disrupt the users usual sleeping behavior and it can be used both at the clinic and at home with minimal cost .
escience today and tomorrow . <eos> escience requires simultaneous advances in infrastructure and applications . such advances are happening rapidly , and have been presented at the <digit> ieee international conference on escience . this special issue highlights some of these advances , and the science they enable .
numerical computation of periodic responses of nonlinear large scale systems by shooting method . <eos> geometrically nonlinear vibrations of three dimensional elastic structures , due to harmonic external excitations , are investigated in the frequency domain . the material of the structure is assumed to be linearly elastic . the equation of motion is derived by the conservation of linear momentum in lagrangian coordinate system and it is discretized into a system of ordinary differential equations by the finite element method . the shooting method is used , to obtain the periodic solutions . a procedure which transforms the initial value problem into a two point boundary value problem , for the periodicity condition , and then it finds the initial conditions which lead to periodic response , is developed and presented , for systems of second order ordinary differential equations . the elmer software is used for computing the local and global mass and stiffness matrices and the force vector , as well for computing the correction of the initial conditions by the shooting method . stability of the solutions is studied by the floquet theory . sequential continuation method is used to define the prediction for the next point from the frequency response diagram . the main goal of the current work is to investigate and present the potential of the proposed numerical methods for the efficient computation of the frequency response functions of large scale nonlinear systems , which often result from space discretization of real life engineering applications .
learning discrete hidden markov models . <eos> this paper describes a learning program for discrete hidden markov models ( hmm ) . the learning of the theory of hidden markov models is a hard task and many students have been disappointed after the first equations . this program attempts to facilitate the learning of the theory by means of a spread sheet interface for interactive user simulations . the spread sheet of observation probabilities simplify the understanding of several discrete hmm equations . the program allows the student to observe several convergence properties of the training algorithm . ( c ) <digit> john wiley sons , inc .
fpga based multiple channel vibration analyzer for industrial applications with reconfigurable post processing capabilities for automatic failure detection on machinery . <eos> machine monitoring is one of the major concerns in modern industry in order to guarantee the overall efficiency during the production process . several monitoring techniques for machinery failure detection have been developed , being vibration analysis one of the most important techniques . the typical equipment used for vibration analysis is a general purpose single channel spectrum analyzer that most of the cases is not well suited for the specific task and lacks from the capability of simultaneous multiple channel analysis and it is not specifically designed for vibration analysis . the contribution of this work is to present the development of a low cost fpga based <digit> axis simultaneous vibration analyzer for embedded machinery monitoring with the novelty of a post processing stage that can be designed and implemented into the same fpga for automatic on line detection of specific machinery failures thanks to its reconfigurability . the vibration analyzer has three stages vibration monitoring with a mems accelerometer as sensor , three parallel <digit> point fft cores and one post processor for the analysis of the specific vibration related failure , which can be reconfigured to attend the specific task . one of the most important failures in induction motors is the broken bar condition and the developed vibration analyzer was tested to detect this condition on several motors for different failure severities , giving good detection results . other vibration analysis can be performed by the three channel fft cores with the reconfiguration of the post processing unit to detect or enhance a specific characteristic under study
modelling of pulp flow rate with variable consistency . <eos> pulp and paper mills can be seen as big pumping plants , where mass is pumped front one step to another . the proper operation of the process in its different stages does not allow large deviations from given operation conditions , which makes it essential to monitor and control the flow rate and the consistency of the pulp . a structure is suggested for the modelling of pulp flow rate , and possibilities for using the pump valve system as a flow meter are examined . the overall model structure consists of a wiener model for pressure difference , a non linear dynamic valve model , and a static mapping for flow rate . a full scale pilot plant of pulp flow through a valve in a pipeline is used for experimentation . linear dynamics and non linearities due to the pump , valve and the pulp consistency are identified based on online measurements obtained from calibration tests . the results show that a simple model could be identified which , together with a valve model , call be further applied for the purposes of accurate flow control .
a hierarchical optimization technique for the strategic design of distribution networks . <eos> the paper addresses the optimal design of the last supply chain branch , i.e. , the distribution network ( dn ) , starting from manufacturers till the retailers . it considers a distributed system composed of different stages connected by material links labeled with suitable performance indices . a hierarchical procedure employing direct graph ( digraph ) modeling , mixed integer linear programming , and the analytic hierarchy process ( ahp ) is presented to select the optimal dn configuration . more in detail , a first level dn optimization problem taking into account the definition and evaluation of the distribution chain performance provides a set of pareto optimal solutions defined by digraph modeling . a second level dn optimization using the ahp method selects , on the basis of further criteria , the dn configuration from the pareto face alternatives . to show the method effectiveness , the optimization model is applied to a case study describing an italian regional healthcare drug dn . the problem solution by the proposed design method allows improving the dn flexibility and performance . ( c ) <digit> elsevier ltd. all rights reserved .
parallel execution of object oriented programs message handling strategies . <eos> parallel execution of object oriented ( o o ) programs requires adoption of parallel and concurrent programming solutions . however , classification and inheritance restrict the constructions which would otherwise be applicable among the objects treated as pure distributed processes . in message handling the main problem is that of explicit controlling the set of acceptable messages according to the current status of the object because it may result in very complex conditional expressions and it hinders inheritance as well . in this paper , controlling the corresponding operations , i.e. methods is suggested instead . most of the tools for intra object synchronization suffer from the same problem as controlling the set of messages , e.g. using semaphores and monitors , therefore , the conditional critical regions ( ccrs ) concept is to be used which promotes inheritance too , contrasted to the object based parallel programs today , using the constructions suggested here could result in real object oriented ones .
increased autoantibodies to sox13 in swedish patients with type <digit> diabetes . <eos> this article aims to estimate the prevalence of sox13 antibodies in swedish patients with type <digit> diabetes and healthy controls . the patients ( n <digit> median age <digit> years range , <digit> <digit> ) were newly diagnosed with type <digit> diabetes in a defined area in southern sweden during <digit> <digit> . islet cell antibodies ( ica ) were analyzed with immunofluorescence , while glutamic acid decarboxylase antibodies ( gada ) , tyrosine phosphatase antibodies ( ia 2a ) , and antibodies against the transcription factor sox13 ( sox13ab ) were analyzed with radioimmunoprecipitating assays . sox13ab were found in 9.8 % ( <digit> <digit> ) of type <digit> patients compared to 2.0 % ( <digit> <digit> ) in healthy controls ( p 0.033 ) . at least one of the four autoantibodies ( ica , gada , ia 2a or sox13ab ) were identified in <digit> % ( <digit> <digit> ) of the patients . samples positive for ia 2a were only in one case positive also for sox13ab . ia 2a positive patients were often positive also for ica and gada ( <digit> <digit> ) , and the same combination was also common for sox13ab positive patients ( <digit> <digit> ) . only 2.0 % ( <digit> <digit> ) were positive for sox13ab alone . ica , gada and ia 2a were more frequent in younger patients ( <digit> years of age ) than in older patients , while sox13ab showed similar frequency in both groups . we concluded that the frequency of sox13ab was significantly increased in swedish patients with type <digit> diabetes , but that the addition of sox13ab to the combination of gada and ia2 a only increased the sensitivity by <digit> % for autoimmune diabetes . therefore , sox13 could be a minor autoantigen involved in the pathogenesis of type <digit> diabetes .
using the simultaneous generalized schur decomposition as a candecomp parafac alogrithm for ill conditioned data . <eos> the candecomp parafac ( cp ) method decomposes a three way array into a prespecified number r of outer product arrays , by minimizing the sum of squares of the residual array . the practical use of cp is sometimes complicated by the occurrence of so called ' degenerate ' sequences of solutions , in which several outer product arrays become highly correlated in all three modes and some elements of the outer product arrays become very large in magnitude . it is known that for i x j x <digit> arrays , fitting a simultaneous generalized schur decomposition ( sgsd ) avoids the problems of ' degeneracy ' due to the non existence of an optimal cp solution . in this paper , we consider the application of the sgsd method also for other array formats , when the array has a best fitting cp decomposition with ill conditioned component matrices , in particular such that it resembles the pattern of a ' degeneracy ' . for these cases , we compare the performance of two sgsd algorithms and the alternating least squares ( als ) cp algorithm in a series of numerical experiments . copyright ( c ) <digit> john wiley sons , ltd .
a p300 based braincomputer interface aimed at operating electronic devices at home for severely disabled people . <eos> the present study aims at developing and assessing an assistive tool for operating electronic devices at home by means of a p300 based braincomputer interface ( bci ) . fifteen severely impaired subjects participated in the study . the developed tool allows users to interact with their usual environment fulfilling their main needs . it allows for navigation through ten menus and to manage up to <digit> control commands from eight electronic devices . ten out of the fifteen subjects were able to operate the proposed tool with accuracy above <digit> % . eight out of them reached accuracies higher than <digit> % . moreover , bitrates up to 20.1 bit min were achieved . the novelty of this study lies in the use of an environment control application in a real scenario real devices managed by potential bci end users . although impaired users might not be able to set up this system without aid of others , this study takes a significant step to evaluate the degree to which such populations could eventually operate a stand alone system . our results suggest that neither the type nor the degree of disability is a relevant issue to suitably operate a p300 based bci . hence , it could be useful to assist disabled people at home improving their personal autonomy .
three dimensional numerical simulations on wind and tide induced currents the case of augusta harbour ( italy ) . <eos> we model the hydrodynamic flow field induced by wind and tide . we examine the water volume exchange between open sea and lagoon . the importance of three dimensional numerical model is investigated . we validate the numerical model in complex environmental domain . we applied an open source mpi parallel numerical models .
computational methods for the fast boundary stabilization of flexible structures . part <digit> the case of beams . <eos> an efficient active control strategy for flexible systems v. komornik , rapid boundary stabilization of linear distributed systems , siam j. control optim . <digit> ( <digit> ) ( <digit> ) <digit> is thoroughly investigated from the numerical point of view . a non standard computational framework is proved to be relevant both for simulation and control synthesis . the proposed formulation proves necessary in so far as a standard numerical approach is shown to fail . the observed properties of the state feedback law confirm the theory as far as beams are concerned . in particular , one can achieve an arbitrarily large decay rate of some weak norm of the system . moreover , the control law compares favourably with the integral force feedback in terms of efficiency . finally , smoothing procedures are introduced to decrease the control spill over associated with the possible lack of compatibility between boundary control and initial conditions . this purely artificial spill over is proved to result from oversimplification of the control process modelling .
locally adaptive template sizes for matching repeat images of earth surface mass movements . <eos> this paper presents an algorithm for locally adaptive template sizes in normalized cross correlation ( ncc ) based image matching for measuring horizontal surface displacements of mass movements . after adaptively identifying candidate templates based on the image signal to noise ratio ( snr ) , the algorithm iteratively looks for the size at which the maximum cross correlation coefficient attains a local peak and the matching position gets fixed . the algorithm is tested on modeled ( deformed ) images and applied to real bi temporal images of different earth surface mass movements . it is evaluated in comparison with globally ( image wide ) fixed template sizes ranging from <digit> to 101pixels based on the improvement in the accuracy of displacement estimation and the snr of image reconstruction . the results show that the algorithm could reduce the error of displacement estimation by up to over <digit> % ( in the modeled case ) and improve the snr of the matching by up to over four times compared to the globally fixed template sizes highly reducing the effects of geometric distortion and noise . the algorithm pushes terrain displacement measurement from repeat images one step forward towards full automation .
region based glrt detection of oil spills in sar images . <eos> in the study , we propose a fast region based method for the detection of oil spills in sar images . the proposed method combines the image segmentation technique and conventional detection theory to improve the accuracy of oil spills detection . from the image statistical characteristics , we first segment the image into regions by using moment preserving method . then , to get a more integrated segmentation result , we adopt n nearest neighbor rule to merge the image regions according to their spatial correlation . performing the split and merge procedure , we can partition the image into oil polluted and sea reflection regions , respectively . based oil the segmentation results , we build data models of oil spills and approximate them by using normal distributions . employing the built oil spills model and the generalized likelihood ratio test ( glrt ) detection theory , we derive a closed form solution for oil spills detection . our proposed method possesses a smaller variance and can reduce the confusion interval in decision . moreover , we adopt the sample average of image region to reduce the computation complexity . the false alarm rate and oil spills detection probability of the proposed method are derived rheoretically . under the criterion of constant false alarm ratio ( cfar ) , we determine the threshold of the decision rule automatically . simulation results performed on ers2 sar images have demonstrated the efficiency of the proposed approach . ( c ) <digit> elsevier b.v. all rights reserved .
are turing machines platonists inferentialism and the computational theory of mind . <eos> we first discuss michael dummetts philosophy of mathematics and robert brandoms philosophy of language to demonstrate that inferentialism entails the falsity of churchs thesis and , as a consequence , the computational theory of mind . this amounts to an entirely novel critique of mechanism in the philosophy of mind , one we show to have tremendous advantages over the traditional lucas penrose argument .
graph multidimensional scaling with self organizing maps . <eos> self organizing maps ( som ) are unsupervised , competitive neural networks used to project high dimensional data onto a low dimensional space . in this paper it is shown that som can be used to perform multidimensional scaling ( mds ) on graphs . the som based approach is applied to two families of random graphs and three real world networks .
nearest neighbor decoding for additive non gaussian noise channels . <eos> we study the performance of a transmission scheme employing random gaussian codebooks and nearest neighbor decoding over a power limited additive non gaussian noise channel . we show that the achievable rates depend on the noise distribution only via its power and thus coincide with the capacity region of a white gaussian noise channel with signal and noise power equal to those of the original channel . the results are presented for single user channels as well as multiple access channels , and are extended to lading channels with side information at the receiver .
a linear algebra framework for automatic determination of optimal data layouts . <eos> this paper presents a data layout optimization technique for sequential and parallel programs based on the theory of hyperplanes from linear algebra . given a program , our framework automatically determines suitable memory layouts that can be expressed by hyperplanes for each array that is referenced . we discuss the cases where data transformations are preferable to loop transformations and show that under certain conditions a loop nest can be optimized for perfect spatial locality by using data transformations . we argue that data transformations can also optimize spatial locality for some arrays without distorting temporal spatial locality exhibited by others . we divide the problem of optimizing data layout into two independent subproblems <digit> ) determining optimal static data layouts , and <digit> ) determining data transformation matrices to implement the optimal layouts . by postponing the determination of the transformation matrix to the last stage , our method can be adapted to compilers with different default layouts . we then present an algorithm that considers optimizing parallelism and spatial locality simultaneously . our results on eight programs on two distributed shared memory multiprocessors , the convex exemplar spp <digit> and the sgi origin <digit> , show that the layout optimizations are effective in optimizing spatial locality and parallelism .
an approximate method for numerically solving fractional order optimal control problems of general form . <eos> in this article , we discuss fractional order optimal control problems ( focps ) and their solutions by means of rational approximation . the methodology developed here allows us to solve a very large class of focps ( linear nonlinear , time invariant time variant , siso mimo , state input constrained , free terminal conditions etc. ) by converting them into a general , rational form of optimal control problem ( ocp ) . the fractional differentiation operator used in the focp is approximated using oustaloups approximation into a state space realization form . the original problem is then reformulated to fit the definition used in general purpose optimal control problem ( ocp ) solvers such as riots_95 , a solver created as a matlab toolbox . illustrative examples from the literature are reproduced to demonstrate the effectiveness of the proposed methodology and a free final time ocp is also solved .
a grid enabled distributed branch and bound algorithm with application on the steiner problem in graphs . <eos> this work introduces a distributed branch and bound algorithm to be run on computational grids . grids are often organized in a hierarchical fashion clusters of processors connected via high speed links , while the clusters themselves are geographically distant and connected through slower links . our algorithm does not employ the usual masterworker paradigm and it considers the hierarchical structure of grids in its load balance and fault tolerance procedures . this algorithm was applied over an existing code for the steiner problem in graphs . experiments on real grid conditions have demonstrated its efficiency and scalability .
a portfolio selection model using fuzzy returns . <eos> we study a static portfolio selection problem , in which future returns of securities are given as fuzzy sets . in contrast to traditional analysis , we assume that investment decisions are not based on statistical expectation values , but rather on maximal and minimal potential returns resulting from the so called alpha cuts of these fuzzy sets . by aggregating over all alpha cuts and assigning weights for both best and worst possible cases we get a new objective function to derive an optimal portfolio . allowing for short sales and modelling alpha cuts in ellipsoidal shape , we obtain the optimal portfolio as the unique solution of a simple optimization problem . since our model does not include any stochastic assumptions , we present a procedure , which turns the data of observable returns as well as experts ' expectations into fuzzy sets in order to quantify the potential future returns and the investment risk .
system reliability prediction model based on evidential reasoning algorithm with nonlinear optimization . <eos> in this paper , a novel reliability prediction technique based on the evidential reasoning ( er ) algorithm is developed and applied to forecast reliability in turbocharger engine systems . the focus of this study is to examine the feasibility and validity of the er algorithm in systems reliability prediction by comparing it with some existing approaches . to determine the parameters of the proposed model accurately , some nonlinear optimization models are investigated to search for the optimal parameters of forecasting model by minimizing the mean square error ( mse ) criterion . finally , a numerical example is provided to demonstrate the detailed implementation procedures . the experimental results show that the prediction performance of the er based prediction model outperforms several existing methods in terms of prediction accuracy or speed .
decision support tools for environmentally benign process design under uncertainty . <eos> this paper presents a new systematic framework for the synthesis of an environmentally benign process under uncertainty . the uncertainty is classified depending on its sources and mathematical model structure as deterministic or stochastic . the proposed methodology is a two layer algorithm . in the outer layer , the synthesis problem is represented by a multi objective optimization problem considering the performances associated with design parameters . in the inner layer , the problem is expressed as a single objective optimization problem taking in to account the operating performances in the presence of uncertainty . the proposed hybrid approach consisting of multi period and stochastic optimization formulations is then employed . additionally , the effect of variability on decisions related to process performance and quality is also discussed . applicability of the developed methodology is illustrated in a case study . four configurations of membrane based toluene recovery process are investigated to quantitatively compare economic , environmental and robustness performance of each configuration . the developed methodology can select solutions with minimal environmental impacts and adequate robustness at a desired economic performance . ( c ) <digit> elsevier ltd. all rights reserved .
liner shipping network design with deadlines . <eos> it is crucial for a liner shipping company to design its container shipping network . given a set of port to port container shipment demands with delivery deadlines , the liner shipping company aims to design itineraries of portcalls , deploy ships on these itineraries and determine how to transport containers with the deployed ships in order to maximize its total profit . in this paper we first demonstrate np hardness of this problem and subsequently formulate it as a mixed integer non linear non convex programming model . a column generation based heuristic method is proposed for solving this problem . numerical experiments for container shipping on the asiaeurope trade lane show that the proposed solution algorithm is efficient to find good quality solutions .
inter cluster connectivity analysis for technology opportunity discovery . <eos> in todays competitive business environment , the timely identification of potential technology opportunities is becoming increasingly important for the strategic management of technology and innovation . existing studies in the field of technology opportunity discovery ( tod ) focus exclusively on patent textual information . in this article , we introduce a new method that tackles tod via technology convergence , using both patent textual data and patent citation networks . we identify technology groups with high convergence potential by measuring connectivity between clusters of patents . from such technology groups we select pairs of core patents based on their technological relatedness , on their past involvement in convergence , and on the impact of their new potential convergence . we finally carry out tod by extracting representative keywords from the text of the selected patent pairs and organizing them into the basic description of a new invention , which the potential convergence of the patent pair might produce . we illustrate our proposed method using a data set of u.s. patents in the field of digital information and security .
evaluation of compiler generated parallel programs on three multicomputers . <eos> distributed memory parallel processors ( dmpps ) have no hardware support for a global address space . however , conventional programs written in a sequential imperative language such as fortran typically manipulate few , large arrays . the oxygen compiler , developed as part of the k2 project , translates conventional fortran code , augmented with code and data distribution directives , into c programs including send receive communication primitives . the compiler directives , which are either supplied by the user , or for simple programs generated automatically , support a global name space through a run time mechanism called data consistency analysis . we report in this paper the performance of seven parallel programs generated by oxygen for three dmpps , namely for a parsytec supercluster , an iwarp , and for the fujitsu ap1000 . all machines were configured as <digit> x <digit> tori .
online reconfiguration of channel assignment in multi channel multi radio wireless mesh networks . <eos> efficient utilization of multi channel multi radio ( mc mr ) wireless mesh networks ( wmns ) can be achieved only by intelligent channel assignment ( ca ) and link scheduling ( ls ) . due to the dynamic nature of traffic demand in wmns , the ca has to be reconfigured whenever traffic demand changes , in order to achieve maximum throughput in the network the reconfiguration of ca requires channel switching at radios which leads to disruption of ongoing traffic in the network . so , we have to consider this traffic disruption overhead while reconfiguring the network for traffic adaptation . the existing ca algorithms for mc mr wmns in the literature do not consider the reconfiguration overhead caused by the channel switching . in this paper , we propose a novel reconfiguration model that considers both network throughput and reconfiguration overhead to quantitatively evaluate a reconfiguration algorithm . based on the reconfiguration model , we formulate the problem of reconfiguration of ca as a mixed integer linear program ( milp ) . we also propose an on line heuristic algorithm for ca called demand based state aware channel reconfiguration algorithm ( desara ) that finds the ca for the current traffic demand by considering the existing ca of the network to minimize the reconfiguration overhead . through extensive simulations , we show the importance of considering the overhead in reconfiguration of ca , by comparing the performance of desara with a static ca and a fully dynamic ca that does not consider the reconfiguration overhead . we also study the performance of the proposed algorithm with real network traces collected in a campus network to show its practicality . ( c ) <digit> elsevier b.v. all rights reserved .
weee treatment strategies ' evaluation using fuzzy linmap method . <eos> electrical and electronic equipments ( eee ) have already begun to be accumulated at the garbage dumps . this garbage accumulation brings big danger to the environment and human health . that 's why one should look for exploring the ways to dispose of these wastes and emphasize the waste treatment strategies . waste treatment strategies also contribute either to local or global economies by creating a new sector and employment opportunities , and by reducing use of scarce resources . in this paper , a linear programming technique for multidimensional analysis of preference ( linmap ) method for solving multi attribute group decision making ( magdm ) problems with preference information on alternatives in fuzzy environment is developed . the aim is to develop a fuzzy linmap model for evaluation and select of a waste treatment strategy for eee . thus , three treatment strategy alternatives and eight criteria are determined . the best strategy is selected and the key criterion is found accordingly . the best alternative is found to be treating waste electrical and electronic equipments ( weee ) by reuse and recycling methods . ( c ) <digit> elsevier ltd. all rights reserved .
distributed routing for signal detection in wireless sensor networks . <eos> we study the problem of energy efficient routing for signal detection in wireless sensor networks . generic routing protocols use networking centric measures such as minimum hop or minimum energy to establish routes . these schemes do not take into account the performance of application specific algorithms that is achievable from the data collected by the nodes along the routes . routing protocols for signal detection have recently been proposed to facilitate joint optimization of detection performance and energy efficiency by developing metrics that connect detection performance with energy consumption of each link along the routes . in existing routing for signal detection ( rsd ) schemes , however , the routes are computed centrally requiring complex optimization algorithms and global information such as locations and observation coefficients of all nodes in the network . clearly , for large scale networks , or networks with dynamically changing topologies , distributed routing schemes are more practical due to their better flexibility and scalability . we present a distributed rsd protocol where each node , based on locally available information , selects its next hop with the goal of maximizing the detection performance associated with unit energy expenditure . we show that the proposed protocol is readily implementable in zigbee networks , and present simulation results that reveal its significant improvements in detection performance and energy efficiency over generic routing protocols .
approximation of class regions by quasi convex hulls1 . <eos> a discrimination method is proposed in which a class region is taken as a set of quasi convex hulls . a subclass method approximates a class region by a set of hyper rectangles , while the proposed method uses a set of quasi convex hulls . the experimental results show the effectiveness of the proposed method .
a matlab toolbox for calculating spring indices from daily meteorological data . <eos> spring indices can be used to characterize spring onset across a wide range of spatial scales . these indices are based on phenological models , but can be computed from meteorological data . we present software and documentation for calculating spring indices . some basic examples of the potential uses and extensions of the spring indices are also shown .
online signature verification based on signatures turning angle representation using longest common subsequence matching . <eos> online signature verification has been intensively investigated in several directions , such as the selected feature ( s ) , similarity estimation and classification method . local feature approaches combined with elastic distance metrics have the most successful performance so far . the turning angle sequence ( tas ) feature has not been extensively explored for signature verification , while the fusion of tass of different scales , the turning angle scale space ( tass ) is a new approach in this field . in this paper , we study the signatures tas and tass representations and their application to online signature verification . in the matching stage , a variation of the longest common sub sequence matching technique has been employed . experimental results using varying tas ( s ) representation parameters on two publicly available signature databases , the svc2004 and susig , show the improved performance of the selected feature along with the chosen elastic distance measure on the equal error rate results of the online signature verification task .
curve skeleton extraction using iterative least squares optimization . <eos> a curve skeleton is a compact representation of 3d objects and has numerous applications . it can be used to describe an object 's geometry and topology . in this paper , we introduce a novel approach for computing curve skeletons for volumetric representations of the input models . our algorithm consists of three major steps <digit> ) using iterative least squares optimization to shrink models and , at the same time , preserving their geometries and topologies , <digit> ) extracting curve skeletons through the thinning algorithm , and <digit> ) pruning unnecessary branches based on shrinking ratios . the proposed method is less sensitive to noise on the surface of models and can generate smoother skeletons . in addition , our shrinking algorithm requires little computation , since the optimization system can be factorized and stored in the precomputational step . we demonstrate several extracted skeletons that help evaluate our algorithm . we also experimentally compare the proposed method with other well known methods . experimental results show advantages when using our method over other techniques .
on mobile sensor assisted field coverage . <eos> providing field coverage is a key task in many sensor network applications . with unevenly distributed static sensors , quality coverage with acceptable network lifetime is often difficult to achieve . fortunately , recent advances on embedded and robotic systems make mobile sensors possible , and we suggest that a small set of mobile sensors can be leveraged toward a cost effective solution for field coverage . there are , however , a series of fundamental questions to be answered in such a hybrid network of static and mobile sensors ( <digit> ) given the expected coverage quality and system lifetime , how many mobile sensors should be deployed ( <digit> ) what are the necessary coverage contributions from each type of sensors ( <digit> ) what working and moving patterns should the sensors adopt to achieve the desired coverage contributions in this article , we offer an analytical study on these problems , and the results lead to a practical system design . specifically , we present an optimal algorithm for calculating the contributions from different types of sensors , which fully exploits the potentials of the mobile sensors and maximizes the network lifetime . we then present a random walk model for the mobile sensors . the model is distributed with very low control overhead . its parameters can be fine tuned to match the moving capability of different mobile sensors and the demands from a broad spectrum of applications . a node collaboration scheme is then introduced to further enhance the system performance . we demonstrate through analysis and simulation that , in our mobile assisted design , a small set of mobile sensors can effectively address the uneven distribution of the static sensors and significantly improve the coverage quality .
what is electrical overstress analysis and conclusions . <eos> high level investigation of publications of the past <digit> years . analysis of definitions and perceptions of eos . historical evolution of the meaning and characteristics of eos . result there is no common understanding on eos . criteria for a useful definition of eos and proposal of a corresponding definition .
intuitive multi touch gestures for mobile web browsers . <eos> this study attempted to find intuitive gestures for mobile web browsers and identify the basic elements that constitute multi touch gestures for mobile devices . thirty six mobile phone users participated in an experiment in which they were asked to provide hand gestures appropriate for web browsing commands and explain the reasons . a total of <digit> gestures were collected for <digit> web browsing commands . identical gestures were classified into the same group , and the degree to which the participants agreed on each gesture ( agreement level ) was evaluated . the average agreement level was 0.16 . six important gesture elements were also identified from the collected gestures . these elements are expected to be very useful in describing complex gestures formally and in providing designers with helpful guidelines , such as identifying dominant elements . this study successfully collected gestures that are intuitive to users , but further research is necessary to design the final gesture set .
morphological appearance manifolds for group wise morphometric analysis . <eos> computational anatomy quantifies anatomical shape based on diffeomorphic transformations of a template . however , different templates warping algorithms , regularization parameters , or templates , lead to different representations of the same exact anatomy , raising a uniqueness issue variations of these parameters are confounding factors as they give rise to non unique representations . recently , it has been shown that learning the equivalence class derived from the multitude of representations of a given anatomy can lead to improved and more stable morphological descriptors . herein , we follow that approach , by approximating this equivalence class of morphological descriptors by a ( nonlinear ) morphological appearance manifold fitting to the data via a locally linear model . our approach parallels work in the computer vision field , in which variations lighting , pose and other parameters lead to image appearance manifolds representing the exact same figure in different ways . the proposed framework is then used for group wise registration and statistical analysis of biomedical images , by employing a minimum variance criterion to perform manifold constrained optimization , i.e. to traverse each individuals morphological appearance manifold until group variance is minimal . the hypothesis is that this process is likely to reduce aforementioned confounding effects and potentially lead to morphological representations reflecting purely biological variations , instead of variations introduced by modeling assumptions and parameter settings .
integration by communication knowledge exchange in global outsourcing of product software development . <eos> global outsourcing is a growing trend among independent software vendors . in these projects like other distributed work , distances have negative effects on communication and coordination , directly impacting performance . we present a normative model designed to address this issue by improving communication and knowledge exchange . the model consists of six distinct practices and a tool blueprint , each coming with practical guidelines . it is based in part on two case studies of dutch software vendors who have successfully outsourced part of their activities to an eastern european outsourcing vendor , and validated by a panel of six experts from industry and the scientific community . it is concluded that knowledge exchange in global software outsourcing is a by product of efforts to enhance communication and coordination , rather than specific technical solutions . by committing to sharing knowledge , emphasizing transparency and integrating the outsourcing team into their organizations , customers from the product software business can realize the benefits of global outsourcing .
a transcoding system for audio standards . <eos> we describe an efficient implementation for converting mpeg <digit> mpeg <digit> advanced audio coding ( aac ) encoded data to dolby digital ac <digit> . we describe many techniques to exploit the information in the aac bitstream to simplify the ac <digit> encoder . these techniques can be straightforwardly used in other transcoding schemes between different multimedia standards .
a brief review of the international breast cancer intervention study ( ibis ) , the other current breast cancer prevention trials , and proposals for future trialsa . <eos> the available results from breast cancer chemoprevention trials are reviewed . four trials using tamoxifen have been performed , of which three have reported efficacy results . a fifth trial using raloxifene has also been published . the largest tamoxifen trial shows approximately a <digit> % reduction in breast cancer incidence in the short term , but the two smaller trials have not found any incidence reduction . greater agreement exists for side effects
tracking control of mobile robots localized via chained fusion of discrete and continuous epipolar geometry , imu and odometry . <eos> this paper presents a novel navigation and control system for autonomous mobile robots that includes path planning , localization , and control . a unique vision based pose and velocity estimation scheme utilizing both the continuous and discrete forms of the euclidean homography matrix is fused with inertial and optical encoder measurements to estimate the pose , orientation , and velocity of the robot and ensure accurate localization and control signals . a depth estimation system is integrated in order to overcome the loss of scale inherent in vision based estimation . a path following control system is introduced that is capable of guiding the robot along a designated curve . stability analysis is provided for the control system and experimental results are presented that prove the combined localization and control system performs with high accuracy .
the educational consequences of bateson 's economy of flexibility . <eos> purpose the education system worldwide is regulated through the dominant paradigm of planning and enactment , but the representations of curricula and lesson plans underpinning the paradigm are poorly correlated to classroom practice . this paper aims to understand how despite this the paradigm remains dominant and explores the implications of its continued success for the current educational practice . design methodology approach bateson 's concept of the economy of flexibility is applied to the education . genetic control is mapped onto the formal specification of learning activities , while somatic control is mapped onto teachers ' improvisatory practice . the conflicting regulatory messages generated within the dominant paradigm are discussed in terms of bateson 's double bind theory . findings the success of the dominant paradigm is comprehensible when conceived of as an economy of flexibility . however , the analysis indicates that this success is dependent on two conditions that sufficient flexibility is maintained in classroom practice , and that there should be a weak but reliable channel whereby innovations in classroom can filter through to the level of planning . current developments in educational technology and management practice threaten both these conditions , by increasing the ability of managers to monitor educational activities , and by providing technocratic solutions to pedagogic questions . flexibility is squeezed out of the system , and the contradictions of the dominant paradigm are increasingly enforced to place teachers in a double bind . originality value the analysis provides a model for relating the problems experienced by teachers in their practice to changes in technology , policy and institutional organisation .
on endocytoscopy and posttherapy pathologic staging in esophageal cancers , and on evidence based methodology . <eos> the following , from the 12th oeso world conference cancers of the esophagus , includes commentaries on the value of endocytoscopy to replace biopsy histology for squamous cell carcinoma and the clinical significance of posttherapy pathologic stage in patients with esophageal adenocarcinoma following preoperative chemoradiation a short discussion of evidence based methodology is also included .
solving a multiobjective location routing problem with a metaheuristic based on tabu search . application to a real case in andalusia . <eos> in this work we present a multiobjective location routing problem and solve it with a multiobjective metaheuristic procedure . in this type of problem , we have to locate some plants within a set of possible locations to meet the demands of a number of clients with multiple objectives . this type of model is used to solve a problem with real data in the region of andalusia ( spain ) . thus , we study the location of two incineration plants for the disposal of solid animal waste from some preestablished locations in andalusia , and design the routes to serve the different slaughterhouses in this region . this must be done while taking into account certain economic objectives ( start up , maintenance , and transport costs ) and social objectives ( social rejection by towns on the truck routes , maximum risk as an equity criterion , and the negative implications for towns close to the plant ) .
centralized star architecture of web service node as integration solution in complex organization . <eos> nowadays , non integrated system is one of the most common problems in enterprise or organization . most departments in the enterprise have information systems developed based on their non standardized requirement processes which often propose different platforms , non standardized business processes , and non easy data transfer protocols . unfortunately , the departments realize that they have big problem when they have to span business processes across enterprise and have to transfer and integrate data smoothly . as one possible solution , web service is proposed as a data exchange technology used to integrate data and business processes . in this paper , we describe the most common architecture of web service nodes used in integration processes . then we analyze the weakness of this architecture , especially if it is used in integration of complex enterprises or organizations , and then propose new architecture called centralized star architecture as a proposed solution . this new architecture was designed and implemented in institute technology of bandung ( itb ) under i mhere project .
a review of segmentation methods in short axis cardiac mr images . <eos> for the last <digit> years , magnetic resonance imaging ( mri ) has become a reference examination for cardiac morphology , function and perfusion in humans . yet , due to the characteristics of cardiac mr images and to the great variability of the images among patients , the problem of heart cavities segmentation in mri is still open . this paper is a review of fully and semi automated methods performing segmentation in short axis images using a cardiac cine mri sequence . medical background and specific segmentation difficulties associated to these images are presented . for this particularly complex segmentation task , prior knowledge is required . we thus propose an original categorization for cardiac segmentation methods , with a special emphasis on what level of external information is required ( weak or strong ) and how it is used to constrain segmentation . after reviewing method principles and analyzing segmentation results , we conclude with a discussion and future trends in this field regarding methodological and medical issues .
broadband availability and economic growth . <eos> purpose the purpose of this paper is to provide a model to measure the effect of broadband availability on economic growth in developed oecd countries . design methodology approach the effect of the broadband availability related variables on economic growth is analyzed by using cross country panel data for <digit> oecd countries over the years <digit> <digit> . the robustness of the results by the six econometric estimation approaches is compared . the preferable dynamic panel model with the system of generalized method of moments is selected . findings the access channels per inhabitant and total broadband per inhabitant have improved over time , but vary across the analysed oecd countries . the improved access channels per inhabitant and gross capital growth ( investment ) play a positive and significant role in the per capita gross domestic product ( gdp ) growth . labour productivity growth has encouraged economic growth positively . these results are robust independently of the estimation procedure . research limitations implications the authors do not find a positive and significant role of the total broadband per inhabitant on the per capita gdp growth . these findings and the results for control variables pertaining to trade openness and inward foreign direct investment ( fdi ) in the growth equation are biased to the estimation procedure . originality value the conceptual empirical value to the research of new connections made using the key elements of economic growth theory with focus on the effect of the broadband availability , main macroeconomic and economic openness variables on economic growth . this is one of the first studies that , in the growth equation , uses different broadband availability related variables , which in addition to gross capital growth , government consumption , and inflation in the adjusted augmented growth model are controlled for labour productivity growth , trade openness , and inward fdi .
controlling dynamic simulation with kinematic constraints . <eos> theoretical and numerical aspects of the implementation of a dynamic motion system , dubbed dynamo , for the dynamic simulation of linked figures is presented . the system introduces three means for achieving , control of the resulting motion which have not been present in previous dynamic simulation systems for computer animation . ( <digit> ) kinematic constraints permit traditional keyframe animation systems to be embedded within a dynamic analysis . joint limit constraints are also handled correctly through kinematic constraints . ( <digit> ) behavior functions relate the momentary state of the dynamic system to desired forces and accelerations within the figure . ( <digit> ) inverse dynamics provides a means of determining the forces required to perform a specified motion.the combination of kinematic and dynamic specifications allows the animator to think about each part of the animation in the way that is most suitable for the task . successful experimental results are presented which demonstate the ability to provide control without disrupting the dynamic integrity of the resulting motion .
dead beat estimation problems for 2d behaviors . <eos> in this paper we address the dead beat estimation problem for the class of two dimensional ( 2d ) behaviors which are described by means of a 2d difference equation , and whose trajectories evolutions pertain the nonnegative half plane . for this class of behaviors , the concept of nilpotency is defined and fully characterized . after this preliminary step , the estimation problem is formally addressed . indeed , the concept of ( consistent or not ) 2d dead beat observer ( dbo ) of the system relevant variables from the system measured variables is introduced , and a set of necessary and sufficient conditions for the existence of such a dbo is given . finally , a complete parametrization of the family of all consistent dbos is given , and some partial results , together with an interesting counterexample , regarding the class of ( not necessarily consistent ) dbos are presented . to conclude the paper , the general dead beat estimation theory developed in this paper is adjusted to address and solve several problems , like state estimation , the design of unknown input observers or the design of fault detectors and identifiers ( possibly in the presence of disturbances ) , for 2d systems described by quarter plane causal 2d state space models .
an alternative multiple attribute decision making methodology for solving optimal facility layout design selection problems . <eos> in the present work , a systematic and an alternative multiple attribute decision making methodology is presented for selection of facility layout design selection problems . the proposed methodology is based on preference selection index ( psi ) method . in the proposed methodology appropriate facility layout design is selected for a given application without considering relative importance between facility layout design selection attributes . two different types of facility layout design selection problems are examined to demonstrate , validate , and to check the reliability of proposed methodology . in addition , subjective cost benefit analysis is performed to study the benefits to cost to the company . finally , the study has concluded that the facility layout design selection methodology based on psi method is simple , logical , and more appropriate for solving the facility layout design selection problems . ( c ) <digit> elsevier ltd. all rights reserved .
deadlock checking by a behavioral effect system for lock handling . <eos> deadlocks are a common error in programs with lock based concurrency and are hard to avoid or even to detect . one way for deadlock prevention is to statically analyze the program code to spot sources of potential deadlocks . often static approaches try to confirm that the lock taking adheres to a given order , or , better , to infer that such an order exists . such an order precludes situations of cyclic waiting for each other 's resources , which constitute a deadlock . in contrast , we do not enforce or infer an explicit order on locks . instead we use a behavioral type and effect system that , in a first stage , checks the behavior of each thread or process against the declared behavior , which captures potential interaction of the thread with the locks . in a second step on a global level , the state space of the behavior is explored to detect potential deadlocks . we define a notion of deadlock sensitive simulation to prove the soundness of the abstraction inherent in the behavioral description . soundness of the effect system is proven by subject reduction , formulated such that it captures deadlock sensitive simulation . to render the state space finite , we show two further abstractions of the behavior sound , namely restricting the upper bound on re entrant lock counters , and similarly by abstracting the ( in general context free ) behavioral effect into a coarser , tail recursive description . we prove our analysis sound using a simple , concurrent calculus with re entrant locks . ( c ) <digit> elsevier inc. all rights reserved .
two sided inequalities for the extended hurwitzlerch zeta function . <eos> recently , srivastava etal . ( <digit> ) <digit> unified and extended several interesting generalizations of the familiar hurwitzlerch zeta function ( z , s , a ) ( z , s , a ) by introducing a foxwright type generalized hypergeometric function in the kernel . for this newly introduced special function , two integral representations of different kinds are investigated here by means of a known result involving a foxwright generalized hypergeometric function kernel , which was given earlier by srivastava etal . ( <digit> ) <digit> , and by applying some mathieu ( a , ) series techniques . finally , by appealing to each of these two integral representations , two sets of two sided bounding inequalities are proved , thereby extending and generalizing the recent work by jankov etal . ( <digit> ) <digit> .
stochastic programming model for oversaturated intersection signal timing . <eos> we propose a stochastic programming model for oversaturated intersection . we show this new model gives better performance in total vehicle delay and total throughput . we show that this stochastic programming model yield nice queue balancing property .
an insight into the system dynamics method a case study in the dynamics of international minerals investment . <eos> this paper presents an explanation of the system dynamics method . it is based on the development of a detailed simulation model designed to examine the effectiveness of various environmental , fiscal and corporate policies on the flow of investment funds and mineral resources among a number of simulated mining firms and competing countries . emphasis is placed on the development process and the reader is referred elsewhere for presentation and discussion of model output .
silhouette segmentation in multiple views . <eos> in this paper , we present a method for extracting consistent foreground regions when multiple views of a scene are available . we propose a framework that automatically identifies such regions in images under the assumption that , in each image , background and foreground regions present different color properties . to achieve this task , monocular color information is not sufficient and we exploit the spatial consistency constraint that several image projections of the same space region must satisfy . combining the monocular color consistency constraint with multiview spatial constraints allows us to automatically and simultaneously segment the foreground and background regions in multiview images . in contrast to standard background subtraction methods , the proposed approach does not require a priori knowledge of the background nor user interaction . experimental results under realistic scenarios demonstrate the effectiveness of the method for multiple camera set ups .
a tree height hierarchy of context free languages . <eos> we consider the minimal height of a derivation tree as a complexity measure for context free languages and show that this leads to a strict and dense hierarchy between logarithmic and linear ( arbitrary ) tree height . in doing so , we improve a result obtained by gabarro in <digit> . furthermore , we provide a counter example to disprove a conjecture of culik and maurer in <digit> who suggested that all languages with logarithmic tree height would be regular . as a new method , we use counter representations where the successor relation can be handled as the complement of context free languages . a similar hierarchy is obtained considering the ambiguity as a complexity measure .
path delay fault diagnosis in combinational circuits with implicit fault enumeration . <eos> a new methodology involving effect cause analysis has been demonstrated for the diagnosis of path delay faults . the paper illustrates a structural representation , called the suspect circuit , of all the possible path delay faults in a faulty circuit . this representation has been used to design efficient algorithms that enable us to manipulate the suspect faults without having to enumerate them explicitly . procedures for removing fault free paths from the list of suspect faults have been implemented to improve the diagnostic resolution . moreover , efficient data structures are used to complement the procedures and reduce the memory footprint of the algorithms . results indicate that the diagnostic resolution obtained is very high and includes all possible causes of the observed delay faults .
elias an efficient storage underlay for mobile peer to peer systems . <eos> our physical environment is a natural storage where we can store and share information . for instance , a stop sign is a piece of information implanted in a particular location . in this paper , we propose a storage underlay platform called elias ( every location is a storage ) for a mobile peer to peer system . elias enables user applications to transparently save retrieve data items to from a location without concerning with low level detailed implementation . the underlying platform chooses appropriate mobile nodes to store retrieve data items . we discuss an efficient implementation of elias , and present a detailed analytical model that is validated by simulation . finally , we show results of sensitivity analysis on a variety of factors that impact the performance of elias .
deconvolution by thresholding in mirror wavelet bases . <eos> the deconvolution of signals is studied with thresholding estimators that decompose signals in an orthonormal basis and threshold the resulting coefficients . a general criterion is established to choose the orthonormal basis in order to minimize the estimation risk . wavelet bases are highly sub optimal to restore signals and images blurred by a low pass filter whose transfer function vanishes at high frequencies . a new orthonormal basis called mirror wavelet basis is constructed to minimize the risk for such deconvolutions . an application to the restoration of satellite images is shown .
towards a generic power estimator . <eos> data centers play an important role on worldwide electrical energy consumption . understanding their power dissipation is a key aspect to achieve energy efficiency . some application specific models were proposed , while other generic ones lack accuracy . the contributions of this paper are threefold . first we expose the importance of modelling alternating to direct current conversion losses . second , a weakness of cpu proportional models is evidenced . finally , a methodology to estimate the power consumed by applications with machine learning techniques is proposed . since the results of such techniques are deeply data dependent , a study on devices power profiles was executed to generate a small set of synthetic benchmarks able to emulate generic applications behaviour . our approach is then compared with two other models , showing that the percentage error of energy estimation of an application can be less than <digit> % .
new perfect polyphase sequences and mutually orthogonal zcz polyphase sequence sets . <eos> in communication systems , zcz sequences and perfect sequences play important roles in removing multiple access interference ( mai ) and synchronization , respectively . based on an uncorrelated polyphase base sequence set , a novel construction method , which can produce mutually orthogonal ( mo ) zcz polyphase sequence ( ps ) sets and perfect pss , is presented . the obtained zcz pss of each set are of ideal periodic cross correlation functions ( pccfs ) , in other words , the pccfs between such two different sequences vanishes , and the sequences between different sets are orthogonal . on the other hand , the proposed perfect pss include frank perfect pss as a special case and the family size of the former is quite larger than that of the latter .
involvement of signal transduction cascade via dopamine d1 receptors in phencyclidine dependence . <eos> we investigated the molecular mechanisms of development to phencyclidine ( pcp ) induced rewarding effect by using tyrosine hydroxylase ( th ) heterozygous ( th ) mice . pcp ( <digit> mg kg ) induced the place preference in wild type mice pretreated with pcp ( <digit> mg kg day for <digit> days ) . the place preference induced by pcp is attenuated by <digit> hydroxydopamine , a dopaminergic neurotoxin , and ( ) sch <digit> , a dopamine d1 receptor antagonist , but not by dsp <digit> , a noradrenergic neurotoxin , and ( ) sulpiride , a dopamine d2 receptor antagonist . in th mice pretreated with pcp ( <digit> mg kg day for <digit> days ) , no pcp ( <digit> mg kg ) induced place preference was observed . in wild type mice pretreated with pcp , the levels of camp , camp response element binding protein ( creb ) , and c fos mrna in the nucleus accumbens were increased . the levels of camp , creb , and c fos mrna in the nucleus accumbens were not increased by the same treatment schedule of pcp in th mice . these findings suggest that changes in dopaminergic and or camp signal cascades induced by repeated pcp treatment play an important role in the development of pcp induced rewarding effect .
conversing with customers . <eos> in the challenging business of customer service , it can be difficult to gauge how well you and your employees are doing . since you do n't usually create a physical product , measuring the somewhat intangible results of your work can be a challenge particularly when you provide support in other than face to face interactions . the bottom line is often how happy or satisfied your customers are . to assist in measuring customer satisfaction , we developed a real time survey that is sent to each person who contacts the solution center at iowa state university . each customer then gets an opportunity to rate us on how we did and whether we were able to help them . in addition , the customer can ask that a manager contact them to follow up on the situation . this effort to reach out creates a channel of communication that lets the customer know we care about their experience and the quality of help they receive . additionally , it also allows us to see where there are opportunities to improve our processes and helps us to assess potential problem areas . the real time survey has been beneficial in the development of our student employees as future it support professionals . the survey yields real life responses to employees ' performance in an environment where they can learn from their experience . this session is intended for all levels of user support staff .
time aware analysis and ranking of lurkers in social networks . <eos> mining the silent members of an online community , also called lurkers , has been recognized as an important problem that accompanies the extensive use of online social networks ( osns ) . existing solutions to the ranking of lurkers can aid understanding the lurking behaviors in an osn . however , they are limited to use only structural properties of the static network graph , thus ignoring any relevant information concerning the time dimension . our goal in this work is to push forward research in lurker mining in a twofold manner ( <digit> ) to provide an in depth analysis of temporal aspects that aims to unveil the behavior of lurkers and their relations with other users , and ( <digit> ) to enhance existing methods for ranking lurkers by integrating different time aware properties concerning information production and information consumption actions . network analysis and ranking evaluation performed on flickr , friendfeed and instagram networks allowed us to draw interesting remarks on both the understanding of lurking dynamics and on transient and cumulative scenarios of time aware ranking .
robust train to wayside video communications in tunnels using h. <digit> error resilient video encoding combined with multiple antenna systems . <eos> with the development of driverless metro systems , the demand for high data rate train to wayside wireless transmission is increasing drastically in order to satisfy operational needs such as maintenance , video surveillance of the inside of the trains and passenger information . thus , the association of new transmission techniques such as new video coding techniques , multi antennas at transmission and reception sides and recent precoders provides technically and economically efficient solutions to improve existing systems . this paper presents and evaluates two novel strategies to enhance train to wayside wireless video transmissions in tunnels using realistic channel models obtained with ray tracing previously experimentally validated . multiple description coding ( mdc ) or region of interest ( roi ) coding , using the new flexible macroblock ordering ( fmo ) technique , is combined with appropriate multiple input multiple output ( mimo ) schemes , namely , spatial multiplexing ( sm ) , orthogonal spatial multiplexing ( osm ) and precoded orthogonal spatial multiplexing ( p osm ) depending if full channel state information at transmitter side ( csi t ) is available or not . for each strategy , both video encoding process and mimo algorithm are combined in an efficient way to provide the best video quality at the receiver with no increase of the number of radio access points along the infrastructure .
improving multivariate horner schemes with monte carlo tree search . <eos> optimizing the cost of evaluating a polynomial is a classic problem in computer science . for polynomials in one variable , horners method provides a scheme for producing a computationally efficient form . for multivariate polynomials it is possible to generalize horners method , but this leaves freedom in the order of the variables . traditionally , greedy schemes like most occurring variable first are used . this simple textbook algorithm has given remarkably efficient results . finding better algorithms has proved difficult . in trying to improve upon the greedy scheme we have implemented monte carlo tree search , a recent search method from the field of artificial intelligence . this results in better horner schemes and reduces the cost of evaluating polynomials , sometimes by factors up to two .
h infinity fuzzy control design for nonlinear stochastic fuzzy systems . <eos> this paper describes the robust output feedback h infinity fuzzy control design for a class of nonlinear stochastic systems . the system dynamic is modelled by ito type stochastic differential equations . for general nonlinear stochastic systems , the h infinity ontrol can be obtained by solving a second order nonlinear hamilton jacobi inequality . in general , it is difficult to solve the second order nonlinear hamilton jacobi inequality . in this paper , using fuzzy approach ( t s fuzzy model ) , the h infinity fuzzy control design for the nonlinear stochastic systems can be given via solving linear matrix inequalities ( lmis ) instead of a second order hamilton jacobi inequality . simulation example is provided to illustrate the design procedure and expected performance .
population dynamics in presence of state dependent fluctuations . <eos> we discuss a model of a system of interacting populations for the case when ( i ) the growth rates and the coefficients of interaction among the populations depend on the populations densities and ( ii ) the environment influences the growth rates and this influence can be modelled by a gaussian white noise . the system of model equations for this case is a system of stochastic differential equations with ( i ) deterministic part in the form of polynomial nonlinearities and ( ii ) state dependent stochastic part in the form of multiplicative gaussian white noise . we discuss both the cases when the formal integration of the stochastic differential equations leads ( i ) to integrals of it kind or ( ii ) to integrals of stratonovich kind . the systems of stochastic differential equations are reduced to the corresponding fokkerplanck equations . for the it case and for the case of <digit> population analytic results are obtained for the stationary p.d.f. of the population density . for the case of more than one population and for both the it case and stratonovich case the detailed balance conditions are not satisfied . as a result the exact analytic solutions of the corresponding fokkerplanck equations for the stationary p.d.f.s for the population densities are not known . we obtain approximate solutions for this case by the method of adiabatic elimination .

evaluation of stress integration algorithms for elasticplastic constitutive models based on associated and non associated flow rules . <eos> four stress integration algorithms are formulated for afr and non afr models . the formulations are general and include plastic anisotropy and hardening asymmetry . backward eulers in implicit solver was <digit> times faster than same in explicit solver . a recent explicit algorithm resulted in the fastest simulation in explicit solver . backward eulers in implicit solver was faster than any explicit in explicit solver .
a multimodal approach to coding discourse collaboration , distributed cognition , and geometric reasoning . <eos> our research aims to identify childrens communicative strategies when faced with the task of solving a geometric puzzle in cscl contexts . we investigated how to identify and trace distributed cognition in problem solving interactions based on discursive cohesion to objects , participants , and prior discursive content , and geometric and cooperative concepts . we report on the development of a method of coding and representation of verbal and gestural content for multimodal interactional data and initial application of this framework to a microethnographic case study of two small groups of <digit> and <digit> year old learners solving tangram manipulatives in physical and virtual desktop settings . we characterize the establishment of shared reference points as coreferences which cohere on object , para , and meta levels through both gesture and speech . our analysis foregrounds how participants establish common referential ground to facilitate collaborative problem solving with either computer supported or physical puzzles . using multimodal analysis and a theoretical framework we developed to study interactional dynamics , we identified patterns of focus , dominance , and coalition formation as they relate to coreferentiality on multiple levels . initial findings indicate increased communication and cohesion to higher level principles in the virtual tangram puzzle solving setting . this work contributes to available models of multimodal analysis of distributed cognition using current manipulative technologies for early childhood mathematics education .
sample clock offset detection and correction in the lte downlink . <eos> the narrow subcarrier spacing and wide bandwidth arrangement in the lte downlink produce a vulnerability to sample clock mismatch between the transmitting and receiving data converters . without high precision sampling clock frequencies , a high level of inter carrier interference ( ici ) is introduced , yielding undesirable performance . in this article , a method to jointly estimate and correct sampling frequency mismatch is proposed . the proposed method uses information already known to the receiver , operates strictly in the time domain and does not require the aid of pilot symbols or other frequency domain information . the method allows clocks with lower precision to be used with minimal performance degradation . results are presented using matlab simulation as well as an fpga hardware implementation .
a hierarchical modeling and analysis for grid service reliability . <eos> grid computing is a recently developed technology . although the developmental tools and techniques for the grid have been extensively studied , grid reliability analysis is not easy because of its complexity . this paper is the first one that presents a hierarchical model for the grid service reliability analysis and evaluation . the hierarchical modeling is mapped to the physical and logical architecture of the grid service system and makes the evaluation and calculation tractable by identifying the independence among layers . various types of failures are interleaved in the grid computing environment , such as blocking failures , time out failures , matchmaking failures , network failures , program failures , and resource failures . this paper investigates all of them to achieve a complete picture about grid service reliability . markov models , queuing theory , and graph theory are mainly used here to model , evaluate , and analyze the grid service reliability . numerical examples are illustrated .
provably secure electronic cash based on certificateless partially blind signatures . <eos> we extend the partially blind signature approach into certificateless public key cryptography to eliminate the key escrow problem that occurs with identities in public key cryptography . we also formalize conditions for security for certificateless partially blind signature schemes . we also present a practical certificateless partially blind signature scheme to make electronic cash untraceable . we prove the scheme to be unforgeable in the face of message attacks under the computational diffie hellman assumption . ( c ) <digit> elsevier b. v. all rights reserved .
classifications and conventions structure the handling of models within the digital factory . <eos> today , the realization of the digital factory is the strategic goal of many manufacturing enterprises for the coming years . up to now , the work has focused on the technical linkage of various planning tools . now . ( lie goal is it ) integrate aspects of the digital factory into the planning processes , therefore , it is necessary to define a semantic correlation between the distributed models as well as the associated databases . furthermore , a suitable presentation method has it ) be selected which is appropriate for the application within a specific task and for a specific target group , this article presents an approach , which introduces modelling conventions based on a common world view of its users by applying the metaphor of the electronic catalogue as well as a well defined workflow in order to simplify the work with digital factory models as a substantial step towards the application of the digital factory to meet practical requirements . ( c ) <digit> elsevier b.v. all rights reserved .
new chosen ciphertext secure identity based encryption with tight security reduction to the bilinear diffiehellman problem . <eos> we propose a new identity based encryption ( ibe ) system that achieves a tight security reduction to the bilinear diffiehellman ( bdh ) problem in the random oracle model . tightness indicates that some level of ibe system security can be straightforwardly based on the hardness of the bdh problem at the same security level . achieving such tightness requires two strategies ( <digit> ) a key generation technique for all identities , and ( <digit> ) a searching method for the solution to the bdh problem . to implement the first strategy , our system relies on a key generation paradigm recently introduced with the parklee ibe system . to implement the second strategy , we base our system on the strong twin bdh problem that includes access to a decision oracle . we compare the efficiency of our system with that of the previous nishioka ibe system ( based on the katzwang key generation paradigm ) combined with another tight variant of the fujisakiokamoto transform .
development of digital watermarking technology to protect cadastral map information . <eos> this research aimed to prevent , by means of digital watermarking technology , illegal distribution and reproduction of digital cadastral map information . to this end , a digital watermarking technique was developed in consideration of the properties of cadastral maps and based on watermarking methods , after which its performance was evaluated . a watermark key and a one way function was used to compensate for the algorithm and , therefore , watermarking security , based on the existing research results . in these ways , the present research meets the requirements for fidelity , robustness , false positive rate and the maintenance of consistent topology . the advanced techniques suggested in this paper were devised so as to be suitable for vector data such as gis and navigation data as well as cadastral maps . moreover , if the existing methodology is further improved , it could be expected to be used even more widely .
network formation games in non cooperative service overlay networks . <eos> in this paper , we develop a rational model of a market oriented service overlay network in which peers trade resources and services through a common currency called energy . in our model , an overlay network is created by a set of non cooperative resource providing peers , called platforms , that modify local connectivities to other platforms to maximize their own energy gains . resource consuming peers , called agents , are simply designed to migrate platform to platform to find least expensive resources in the network . computer simulations are conducted to study network structures that emerge from local interactions among a group of platforms and agents .
infant joint attention , neural networks and social cognition . <eos> neural network models of attention can provide a unifying approach to the study of human cognitive and emotional development ( posner rothbart , <digit> ) . in this paper we argue that a neural network approach to the infant development of joint attention can inform our understanding of the nature of human social learning , symbolic thought process and social cognition . at its most basic , joint attention involves the capacity to coordinate ones own visual attention with that of another person . we propose that joint attention development involves increments in the capacity to engage in simultaneous or parallel processing of information about ones own attention and the attention of other people . infant practice with joint attention is both a consequence and an organizer of the development of a distributed and integrated brain network involving frontal and parietal cortical systems . this executive distributed network first serves to regulate the capacity of infants to respond to and direct the overt behavior of other people in order to share experience with others through the social coordination of visual attention . in this paper we describe this parallel and distributed neural network model of joint attention development and discuss two hypotheses that stem from this model . one is that activation of this distributed network during coordinated attention enhances the depth of information processing and encoding beginning in the first year of life . we also propose that with development , joint attention becomes internalized as the capacity to socially coordinate mental attention to internal representations . as this occurs the executive joint attention network makes vital contributions to the development of human symbolic thinking and social cognition .
the federica infrastructure and experience . <eos> the european commission co funded project federica started in <digit> with the objective to support future internet research and experimentation . the project created a europe wide infrastructure based on virtualization in wired networks and computing elements , offering fully configurable and controllable virtual testbeds as a service to researchers . this article reviews the architecture , its deployment and current active status , usage experience , including virtual resource reproducibility and elaborates on challenges for future internet testbed support facilities .
a complete deductive system for probability logic . <eos> in this article , we provide a complete deductive system sigma ( ) for probability logic that is different from the systems by fagin and halpern and by heifetz and mongin in the literature . the most important principle of the axiomatization is an infinitary archimedean rule ( arch ) . our proof of the completeness of sigma ( ) is in keeping with the kripke style proof of completeness in modal logic . with the fourier motzkin elimination method , we show both the decidability and moss 's conjecture that the rule ( arch ) is essentially finitary . the perspective of this article is mainly logical . at the end , we point to some further research continuing this piece of work from a coalgebraic perspective .
linear subcodes of turbo codes with improved distance spectra . <eos> in this correspondence , we present a technique for generation of linear subcodes of a given turbo code with better distance spectrum than the original mother turbo code , via an iterative process of trace bit injection which minimally reduces code rate , followed by selective puncturing that allows recovery of the rate loss incurred during the trace bit injection . the technique allows for asymptotic performance improvement of any linear turbo code . in effect , we trim the distance spectrum of a turbo code via elimination of the low distance and or high multiplicity codewords from the output space of the code . to this end , we perform a greedy minimization of a cost function closely related to the asymptotic bit error probability ( or frame error probability ) of the code . this improves the performance of the code everywhere , but its main impact is a reduction in the error floor of the turbo code which is important for delay constrained applications employing short interleavers .
a method for overcoming the surface tension time step constraint in multiphase flows ii . <eos> a method for overcoming the surface tension time step constraint is presented . the algorithm presented in this work is an improvement on the work presented by sussman and ohta ( siam j sci comput <digit> ) . in this work , the method of sussman and ohta is extended in order to treat problems with contact angle dynamics . furthermore , this work presents a more efficient method for computing volume preserving motion by mean curvature than the method presented previously . the new method is tested on the following four 2d problems ( <digit> ) 3d axisymmetric ( r z ) surface tension driven zero gravity droplet oscillation , ( <digit> ) measurement of the magnitude of parasitic currents for a droplet on a substrate initialized in static equilibrium , ( <digit> ) relaxation of a 2d droplet on a substrate to static shape , and ( <digit> ) relaxation of a 2d bubble on a substrate to static shape . copyright ( c ) <digit> john wiley sons , ltd .
modeling spanish anxiolytic consumption economic , demographic and behavioral influences . <eos> anxiolytics ( ax ) are the psychotropic drugs prescribed for the treatment of anxiety and insomnia for <digit> <digit> weeks , for longer periods of consumption ( > <digit> month ) may lead to the development of tolerance or addiction . in fact , its prescription was <digit> % of the total pharmaceutical expenditure in spain in <digit> . this paper deals with the development of a mathematical model describing the dynamic of the addiction to ax for the case study of the spanish region of castellon . the reasons believed to cause the development of addicts to ax are the economic situation , the marriage termination and the social contact . the simulations performed to forecast the addicts rate for the period <digit> <digit> showed an increase from <digit> % in <digit> to <digit> % in <digit> with a fluctuation of about <digit> % between the possible economic scenarios . finally , the analysis of sensitivity of the rate of addicts to the fluctuation of the social contact parameters was performed , letting us estimate its impact on the pharmaceutical expenditure . ( c ) <digit> elsevier ltd. all rights reserved .
texture overlay onto deformable surface for virtual clothing . <eos> in this article , we describe a method for overlaying arbitrary texture image onto surface of t shirt worn by a user . in this method , the texture image is previously divided into a number of patches . on the t shirt , markers are printed at the positions corresponding to the vertices of the patches . the markers on the surface of the t shirt are tracked in the motion image taken by a camera . the texture image is warped according to the tracked positions of the markers , which is overlaid onto the captured image . this article presents experimental results with the pilot system of virtual clothing implemented based on the proposed method .
apart <digit> a generalized mathematica apart function . <eos> the mathematica function apart has been generalized to any dimension in apart package <digit> , we upgrade this package to the 2nd version , in which the core part has been completely rewritten . the new version is more efficient than the 1st version of apart , and the output is now more compact and more suitable for the input for the fire <digit> or fiesta <digit> . program title apart 2.0 catalogue identifier aemk_v2_0 program summary url http cpc.cs.qub.ac.uk summaries aemk_v_0.html program obtainable from cpc program library , queens university , belfast , n. ireland licensing provisions standard cpc licence , http cpc.cs.qub.ac.uk licence licence.html no . of lines in distributed program , including test data , etc. <digit> no . of bytes in distributed program , including test data , etc. <digit> distribution format tar.gz programming language mathematica . computer any computer with mathematica installed . operating system any capable of running mathematica . classification 11.1 . catalogue identifier of previous version aemk_v1_0 journal reference of previous version comput . phys . comm . <digit> ( <digit> ) <digit> does the new version supersede the previous version yes nature of problem as discussed in <digit> , the general procedure to compute a cross section for a physical process in perturbative quantum field theory involves generating the corresponding amplitude via feynman diagram and performing the loop integrals in dimensional regularization <digit> . the essential part in the computation is to reduce these loop integrals to a small number of standard integrals , which are called master integrals ( mi ) , via the systematic methods of integration by parts ( ibp ) identities <digit> , <digit> and lorentz invariance ( li ) identities <digit> . the basic reduction algorithm is introduced by laporta <digit> , which defines an ordering for feynman integrals , generates ibp identities and solves the corresponding linear equations . alternative methods to exploit ibp and li identities for reductions can be found in <digit> . there are many public computer programs for implementations of different reduction algorithms air <digit> , fire <digit> and reduze <digit> . to facilitate the input for fire <digit> , reduze <digit> , etc. we need to decompose the linear independent propagators to independent ones , this procedure can be done by the apart package <digit> which generalizes the mathematica function apart from one dimension to any n n dimensions . solution method we have proven that all linear independent propagators can be decomposed into the summation of linear independent ones in <digit> , apart is such a mathematica package that implements such a reduction method and generalizes the mathematica apart function from <digit> to any n n dimensions . reasons for new version the mathematica pattern matching in the last version may become very slow when the number of variables becomes large , this calls for a revised version with a more efficient reduction . the feature with all positive or negative sign of some variables is favored in combined usage of fire <digit> and fiesta <digit> . summary of revisions we introduce an abstract and compact representation for the linear composition of the independent variables , this results in a more efficient and fast reduction during the apart partial fraction , we also introduce an extra feature to make the sign of some variables always positive or negative during the reduction . running time a few seconds or less . references f. feng , apart a generalized mathematica apart function , comput . phys . commun . <digit> ( <digit> ) <digit> arxiv 1204.2314 hep ph . a. v. smirnov , fire5 a c implementation of feynman integral reduction , comput . phys . commun . <digit> ( <digit> ) <digit> arxiv 1408.2372 hep ph . a. v. smirnov , fiesta <digit> cluster parallelizable multiloop numerical calculations in physical regions , comput . phys . commun . <digit> ( <digit> ) <digit> arxiv 1312.3186 hep ph . t. hahn , generating feynman diagrams and amplitudes with feynarts <digit> , comput . phys . commun . <digit> ( <digit> ) <digit> hep ph <digit> . r. mertig , m. bohm and a. denner , feyncalc computer algebraic calculation of feynman amplitudes , comput . phys . commun . <digit> ( <digit> ) <digit> . f. feng and r. mertig , formlink feyncalcformlink embedding form in mathematica and feyncalc , arxiv 1212.3522 . g.t hooft and m. j. g. veltman , regularization and renormalization of gauge fields , nucl . phys . b <digit> ( <digit> ) <digit> . f. v. tkachov , a theorem on analytical calculability of four loop renormalization group functions , phys . lett . b <digit> ( <digit> ) <digit> . k. g. chetyrkin and f. v. tkachov , integration by parts the algorithm to calculate beta functions in <digit> loops , nucl . phys . b <digit> ( <digit> ) <digit> . t. gehrmann and e. remiddi , differential equations for two loop four point functions , nucl . phys . b <digit> ( <digit> ) <digit> arxiv hep ph <digit> . s. laporta , high precision calculation of multi loop feynman integrals by difference equations , int . j. mod . phys . a <digit> ( <digit> ) <digit> arxiv hep ph <digit> . a. v. smirnov and v. a. smirnov , applying grbner bases to solve reduction problems for feynman integrals , jhep <digit> ( <digit> ) <digit> arxiv hep lat <digit> . a. v. smirnov , an algorithm to construct grbner bases for solving integration by parts relations , jhep <digit> ( <digit> ) <digit> arxiv hep ph <digit> . j. gluza , k. kajda and d. a. kosower , towards a basis for planar two loop integrals , phys . rev. d <digit> ( <digit> ) <digit> arxiv 1009.0472 hep th . r. m. schabinger , a new algorithm for the generation of unitarity compatible integration by parts relations , arxiv 1111.4220 hep ph . r. n. lee , group structure of the integration by part identities and its application to the reduction of multiloop integrals , jhep <digit> ( <digit> ) <digit> arxiv 0804.3008 hep ph . a. g. grozin , integration by parts an introduction , int . j. mod . phys . a <digit> ( <digit> ) <digit> arxiv 1104.3993 hep ph . c. anastasiou and a. lazopoulos , automatic integral reduction for higher order perturbative calculations , jhep <digit> ( <digit> ) <digit> arxiv hep ph <digit> a. v. smirnov , algorithm firefeynman integral reduction , jhep <digit> , <digit> ( <digit> ) arxiv 0807.3243 hep ph c. studerus , reduzefeynman integral reduction in c , comput . phys . commun . <digit> , <digit> ( <digit> ) arxiv 0912.2546 physics.comp ph .
a quantitative analysis of the state of the art in quality provisioning for multimedia services over next generation networks . <eos> in this paper , we perform meta analysis of leading publications in the communication area to dig out the exhausted and least investigated areas within the umbrella of multimedia quality provisioning . we aggregate the result trends according to an original subject taxonomy , as well as several other relevant classifications ( scenario , approach followed , and so on ) . the main motivation behind this work is to gather information and possibly identify the future research trends on this very important area .
semantic analysis and biological modelling in selected classes of cognitive information systems . <eos> cognitive categorisation systems are used for in depth analyses of data which contains significant layers of information . these layers consist of the semantic information found in the data sets , whose information allows the system executing data analysis processes to understand the data to a certain extent and to reason based on this analysed information . such processes are executed by semantic data analysis systems which are called cognitive categorisation systems in the introduced classification of cognitive systems dedicated to analyses in various fields of application . cognitive data analysis systems are also expanded by adding processes of learning new solutions hitherto unknown to the system because it had no appropriate pattern defined or because it had no data allowing the analysed data to be unambiguously assigned to its corresponding pattern . the ability to train the system so that it would correctly interpret the analysed data marks the beginning of the development of a new class of systems analysing data individual features in the course of biological modelling , personalisation and personal identification processes . identification systems are enhanced by adding elements of cognitive categorisation systems in order to execute an in depth , more detailed personal analysis using the information collected in the system , whose information concerns not only the anatomical and physical features , but also , or maybe primarily , lesions found in various human organs . such systems could be used in personal identification cases in which there are doubts and a risk arises due to reasoning from incomplete data sets . adding semantic analysis modules to personal identification systems represents a novel scientific proposition which marks the beginning of the use of semantic analysis processes for biological modelling and personalisation tasks . the solutions proposed are illustrated with the example of selected e ubias systems which analyse medical image data in combination with the identity analysis . the use of dna cryptography and dna code to analyse personal data makes it possible to unanimously assign analysed data to an individual at the personal identification stage . this publication presents also the system with semantic analysis processes conducted based on semantic interpretation and cognitive processes allows the possible lesions that the person suffers from to be identified and authorised . ( c ) <digit> elsevier ltd. all rights reserved .
on irreversibility of von neumann additive cellular automata on grids . <eos> the von neumann cellular automaton appears in many different settings in operations research varying from applications in formal languages to biology . one of the major questions related to it is to find a general condition for irreversibility of a class of two dimensional cellular automata on square grids ( automata ) . this question is partially answered here with the proposal of a sufficient condition for the irreversibility of automata .
a clustering based strategy to identify coincidental correctness in fault localization . <eos> coverage based fault localization techniques leverage the coverage information to identify the faulty elements of a program . however , these techniques can be adversely affected by coincidental correctness , which occurs when the defect is executed but no failure is revealed . in this paper , we propose a clustering based strategy to identify coincidental correctness in fault localization . the insight behind this strategy is that tests in the same cluster have similar behaviors . thus a passed test in a cluster with many failed tests is highly possible to be coincidentally correct because it has the potential to execute the faulty elements as those failed ones do . we evaluated this technique from two aspects the ability to identify coincidental correctness and the er effectiveness to improve fault localization . the experimental results show that our strategy can alleviate the coincidental correctness problem and improve the er effectiveness of fault localization .
evaluating visual table data understanding . <eos> in this paper , we focus on evaluating how information visualization supports exploration for visual table data . we present a controlled experiment designed to evaluate how the layout of table data affects the user understanding and his exploration process . this experiment raised interesting problems from the design phase to the data analysis . we present our task taxonomy , the experiment procedure and give clues about data collection and analysis . we conclude with lessons learnt from this experiment and discuss the format of future evaluation .
elucidating concurrent algorithms via layers of abstraction and reification . <eos> arguing that intricate concurrent programs satisfy their specifications can be difficult recording understandable explanations is important for subsequent readers . abstraction is a key tool even for sequential programs . the purpose here is to explore some abstractions that help readers ( and writers ) understand the design of concurrent programs . as an illustration , the paper presents a formal development of a non trivial parallel program simpson 's implementation of asynchronous communication mechanisms . although the correctness of this <digit> slot algorithm has been shown elsewhere , earlier proofs fail to offer much insight into the design . from an understandable ( yet formal ) design history of this one algorithm , the techniques employed in the explanation are teased out for wider application . among these techniques is using a fiction of atomicity as an aid to understanding the initial steps of development . the rely guarantee approach is , here , combined with notions of read write frames and phased specifications furthermore , the atomicity assumptions implied by the rely guarantee conditions are achieved by clever choice of data representations .
cultural based particle swarm for dynamic optimisation problems . <eos> many practical optimisation problems are with the existence of uncertainties , among which a significant number belong to the dynamic optimisation problem ( dop ) category in which the fitness function changes through time . in this study , we propose the cultural based particle swarm optimisation ( pso ) to solve dop problems . a cultural framework is adopted incorporating the required information from the pso into five sections of the belief space , namely situational , temporal , domain , normative and spatial knowledge . the stored information will be adopted to detect the changes in the environment and assists response to the change through a diversity based repulsion among particles and migration among swarms in the population space , and also helps in selecting the leading particles in three different levels , personal , swarm and global levels . comparison of the proposed heuristics over several difficult dynamic benchmark problems demonstrates the better or equal performance with respect to most of other selected state of the art dynamic pso heuristics .
distribution of eigenvalues in gaps of the essential spectrum of sturm liouville operators a numerical approach . <eos> this paper reports on a new numerical procedure to count eigenvalues in spectral gaps for a class of perturbed periodic sturm liouville operators . it is motivated by the desire to analyse the distribution of eigenvalues in the dense point spectrum of d dimensional radially periodic schrodinger operators . our numerical results indicate that the well known asymptotic formula for the large coupling limit gives a good description already for moderate values of the coupling constant .
almost sure stability of networked control systems under exponentially bounded bursts of dropouts . <eos> a wireless networked control systems ( ncs ) is a control system whose feedback path is realized over a wireless communication network . the stability of such systems can be problematic given the random way in which wireless channels drop feedback messages . this paper establishes sufficient conditions for the almost sure stability of ncs under random dropouts . these conditions relate the burstiness in the dropout process to the nominal response of the controlled system . in particular , this means that the burstiness of the dropout process provides a convenient quality of service ( qos ) constraint on the wireless channel that can be used to adaptively reconfigure the control system in a manner that guarantees the almost sure stability of the ncs . we also show how a probabilistic extension of the network calculus can be used to reconfigure multi hop communication networks so this paper 's sufficient stability condition is not violated .
boundary element methods for boundary condition inverse problems in elasticity using pcgm and cgm regularization . <eos> for an isotropic linear elastic body , only displacement or traction boundary conditions are given on a part of its boundary , whilst all of displacement and traction vectors are unknown on the rest of the boundary . the inverse problem is different from the cauchy problems . all the unknown boundary conditions on the whole boundary must be determined with some interior points ' information . the preconditioned conjugate gradient method ( pcgm ) in combination with the boundary element method ( bem ) is developed for reconstructing the boundary conditions , and the pcgm is compared with the conjugate gradient method ( cgm ) . morozov 's discrepancy principle is employed to select the iteration step . the analytical integral algorithm is proposed to treat the nearly singular integrals when the interior points are very close to the boundary . the numerical solutions of the boundary conditions are not sensitive to the locations of the interior points if these points are distributed along the entire boundary of the considered domain . the numerical results confirm that the pcgm and cgm produce convergent and stable numerical solutions with respect to increasing the number of interior points and decreasing the amount of noise added into the input data .
schedule extended synchronous dataflow graphs . <eos> synchronous dataflow graphs ( sdfgs ) are used extensively to model streaming applications . an sdfg can be extended with scheduling decisions , allowing sdfg analysis to obtain properties , such as throughput or buffer sizes for the scheduled graphs . analysis times depend strongly on the size of the sdfg . sdfgs can be statically scheduled using static order schedules . the only generally applicable technique to model a static order schedule in an sdfg is to convert it to a homogeneous sdfg ( hsdfg ) . this may lead to an exponential increase in the size of the graph and to suboptimal analysis results ( e. g. , for buffer sizes in multiprocessors ) . we present techniques to model two types of static order schedules , i.e. , periodic schedules and periodic single appearance schedules , directly in an sdfg . experiments show that both techniques produce more compact graphs compared to the technique that relies on a conversion to an hsdfg . this results in reduced analysis times for performance properties and tighter resource requirements .
some observations on modelling case based reasoning with formal argument models . <eos> in this paper i shall explore the modelling of case based reasoning using a formal model of argument , taking the approach of prakken and sartor as my starting point . i first consider their method of representing cases , and describe how if we restrict ourselves to independent boolean factors we can fruitfully model the domain as a partial order on rules . i then consider the issues relating to quantifiable factors , as used in hypo , and factor hierarchies , as used in cato . the former presents some difficulties for modelling as a partial order , and , coupled with the latter , forces us to recognise two different kinds of reasoning used in concept application which have different implications for representing the domain . i then present some conclusions arising from the discussion .
galprop webrun an internet based service for calculating galactic cosmic ray propagation and associated photon emissions . <eos> galprop is a numerical code for calculating the galactic propagation of relativistic charged particles and the diffuse emissions produced during their propagation . the code incorporates as much realistic astrophysical input as possible together with latest theoretical developments and has become a de facto standard in astrophysics of cosmic rays . we present galprop webrun , a service to the scientific community enabling easy use of the freely available galprop code via web browsers . in addition , we introduce the latest galprop version <digit> , available through this service .
scaling up partial evaluation for optimizing the sun commercial rpc protocol . <eos> we report here a successful experiment in using partial evaluation on a realistic program , namely the sun commercial rpc ( remote procedure call ) protocol . the sun rpc is implemented in a highly generic way that offers multiple opportunities of specialization.our study also shows the incapacity of traditional binding time analyses to treat real system programs . our experiment has been made with tempo , a partial evaluator for c programs targeted towards system software . tempo 's binding time analysis had to be improved to integrate partially static data structures ( interprocedurally ) , context sensitivity , use sensitivity and return sensitivity.the sun rpc experiment files , including the specialized implementation , are publicly available upon request to the authors .
detection of traps induced and activated by high field stress in an n channel vdmosfet transistor using current deep level transient spectroscopy ( cdlts ) . <eos> commercial vdmosfets transistors were subjected to positive and negative high field stress . a new model of current deep level transient spectroscopy ( cdlts ) characterization is adopted in a research of defects induced and activated by electrical stress . this model is based on pulse width scan instead of classical temperature scan . the band gap is scanned by varying the pulse base level . positive and negative high field stresses were applied for different periods ranging from <digit> to <digit> min . after each stress period , activation energies and capture cross sections of detected traps were estimated . different defects were detected and we have distinguished the doping levels and interface states from deep levels located in the forbidden band gap .
homotopy perturbation method for fractional fornberg whitham equation . <eos> this article presents the approximate analytical solutions to solve the nonlinear fornberg whitham equation with fractional time derivative . by using initial values , the explicit solutions of the equations are solved by using a reliable algorithm like homotopy perturbation method . the fractional derivatives are taken in the caputo sense . numerical results show that the hpm is easy to implement and accurate when applied to time fractional pdes . ( c ) <digit> elsevier ltd. all rights reserved .
evaluating google queries based on language preferences . <eos> this paper evaluates the assumption that users expect search engines to retrieve the same results for queries regardless of the language or the location of the originator . the dependency of the google search engine on the language and location from which the query is submitted has been evaluated . the most popular queries in arabic language were selected and translated into english for comparison using the google translator . when studying keyword traffic on both google search based keyword tool and google insights for search , results showed that <digit> % of the arab internet users prefer to use english queries instead of their arabic counterpart . when studying google responses to some popular queries we have found that google ranking algorithm depends on the language of the query more than on the keyword popularity . although results justify search engines ' favouritism of giving documents in english priority over those of other languages , nonetheless , future search engine indexers should separate the document language from its content in a structure that makes the language a pluggable attribute for those indexed documents .
some experiences with the numerical simulation of newtonian and bingham fluids in dip coating . <eos> the dip coating process is simulated numerically for newtonian and bingham fluids with a particular emphasis on finding the free surface location under different sets of conditions . the main focus of the simulations is the evaluation of the finite volume method ( fvm ) combined with the volume of fluid ( vof ) technique embedded in the commercial code ( fluent ) , while some results are also obtained with the finite element method ( fem ) using another commercial code ( polyflow ) . the objective is to check how well the results compare with previous results regarding the free surface location for both axisymmetric and planar geometries , and also provide new results . the numerical results were first validated against previous experimental data for the newtonian limit case . then numerical results were compared favourably with previous simulations available in the literature for both newtonian and bingham fluids . the influence of the coating fluid properties as well as surface withdrawal speed for newtonian fluids and of the yield stress for bingham fluids are discussed . the effect of inertia for the planar case is investigated , and the formation of the cusp and wavy shape of the free surface is analyzed . ( c ) <digit> elsevier ltd. all rights reserved .
an ensemble of transliteration models for information retrieval . <eos> transliteration is used to phonetically translate proper names and technical terms especially from languages in roman alphabets to languages in non roman alphabets such as from english to korean , japanese , and chinese . because transliterations are usually representative index terms for documents , proper handling of the transliterations is important for an effective information retrieval system . however , there are limitations on handling transliterations depending on dictionary lookup , because transliterations are usually not registered in the dictionary . for this reason , many researchers have been trying to overcome the problem using machine transliteration . in this paper , we propose a method for improving machine transliteration using an ensemble of three different transliteration models . because one transliteration model alone has limitation on reflecting all possible transliteration behaviors , several transliteration models should be complementary used in order to achieve a high performance machine transliteration system . this paper describes a method about transliteration production using the several machine transliteration models and transliteration ranking with web data and relevance scores given by each transliteration model . we report evaluation results for our ensemble transliteration model and experimental results for its impact on ir effectiveness . machine transliteration tests on english to korean transliteration and english to japanese transliteration show that our proposed method achieves <digit> % word accuracy . information retrieval tests on ktset and ntcir <digit> test collection show that our transliteration model can improve the performance of an information retrieval system about <digit> % .
updating topographic mapping in great britain using imagery from high resolution satellite sensors . <eos> topographic mapping from remotely sensed imagery is carried out all over the world , using data from an ever growing number of sensors . traditional film cameras are gradually being replaced by digital cameras and scanners , but most topographic mapping still relies on sensors based on airborne platforms . this paper examines the potential of high resolution satellite sensor imagery for the updating of topographic mapping , from the perspective of a national mapping agency . after a review of satellites capable of being used for this purpose , several examples of mapping projects are presented . the paper ends with a look to the future , and asks whether satellite imagery can ever replace airborne ( digital or analogue ) photography for the makers of maps . it is concluded that high resolution satellite sensor imagery does have a role to play in the update of topographic mapping , especially in the detection of change .
watch and comment as a paradigm toward ubiquitous interactive video editing . <eos> the literature reports research efforts allowing the editing of interactive tv multimedia documents by end users . in this article we propose complementary contributions relative to end user generated interactive video , video tagging , and collaboration . in earlier work we proposed the watch and comment ( wac ) paradigm as the seamless capture of an individual 's comments so that corresponding annotated interactive videos be automatically generated . as a proof of concept , we implemented a prototype application , the wactool , that supports the capture of digital ink and voice comments over individual frames and segments of the video , producing a declarative document that specifies both different media stream structure and synchronization . in this article , we extend the wac paradigm in two ways . first , user video interactions are associated with edit commands and digital ink operations . second , focusing on collaboration and distribution issues , we employ annotations as simple containers for context information by using them as tags in order to organize , store and distribute information in a p2p based multimedia capture platform . we highlight the design principles of the watch and comment paradigm , and demonstrate related results including the current version of the wactool and its architecture . we also illustrate how an interactive video produced by the wactool can be rendered in an interactive video environment , the ginga ncl player , and include results from a preliminary evaluation .
comparison of persistent homologies for vector functions from continuous to discrete and back . <eos> the theory of multidimensional persistent homology was initially developed in the discrete setting , and involved the study of simplicial complexes filtered through an ordering of the simplices . later , stability properties of multidimensional persistence have been proved to hold when topological spaces are filtered by continuous functions , i.e. for continuous data . this paper aims to provide a bridge between the continuous setting , where stability properties hold , and the discrete setting , where actual computations are carried out . more precisely , a stability preserving method is developed to compare the rank invariants of vector functions obtained from discrete data . these advances confirm that multidimensional persistent homology is an appropriate tool for shape comparison in computer vision and computer graphics applications . the results are supported by numerical tests .
conservative unsteady aerodynamic simulation of arbitrary boundary motion using structured and unstructured meshes in time . <eos> simulation of unsteady fluid behaviour with arbitrary boundary motion or topological change remains restricted owing to mesh deformation limitations , and usually requires the application of special techniques using overlapping meshes , sliding planes , remeshing or immersed boundaries . this work presents the application of a spacetime interpretation of the fluid conservation laws that unifies meshes in space and time . this effectively replaces the problem of mesh deformation with the problem of mesh generation and , because connectivity is no longer restricted to being constant in time , any motion or topological change may be simulated . examples are given applying the method to a piston , a pitching naca0012 aerofoil , an appearing disappearing object , a two dimensional store separation and a rotation case to validate and then demonstrate the capabilities of the method . results for the pitching aerofoil case are compared with a conventional moving mesh unsteady computation and shown to be consistent , whereas the demonstration cases show qualitatively correct behaviour and illustrate the general nature of the technique . copyright ( c ) <digit> john wiley sons , ltd .
long cycles in hypercubes with distant faulty vertices . <eos> in this paper , we study long cycles in induced subgraphs of hypercubes obtained by removing a given set of faulty vertices such that every two faults are distant . first , we show that every induced subgraph of q ( n ) with minimum degree n <digit> contains a cycle of length at least <digit> ( n ) <digit> ( f ) where f is the number of removed vertices . this length is the best possible when all removed vertices are from the same bipartite class of q ( n ) . next , we prove that every induced subgraph of q ( n ) obtained by removing vertices of some given set m of edges of q ( n ) contains a hamiltonian cycle if every two edges of m are at distance at least <digit> . the last result shows that the shell of every linear code with odd minimum distance at least <digit> contains a hamiltonian cycle . in all these results we obtain significantly more tolerable faulty vertices than in the previously known results . we also conjecture that every induced subgraph of q ( n ) obtained by removing a balanced set of vertices with minimum distance at least <digit> contains a hamiltonian cycle .
a generic framework for efficient <digit> d and <digit> d facial expression analogy . <eos> facial expression analogy provides computer animation professionals with a tool to map expressions of an arbitrary source face onto an arbitrary target face . in the recent past , several algorithms have been presented in the literature that aim at putting the expression analogy paradigm into practice . some of these methods exclusively handle expression mapping between <digit> d face models , while others enable the transfer of expressions between images of faces only . none of them , however , represents a more general framework that can be applied to either of these two face representations . in this paper , we describe a novel generic method for analogy based facial animation that employs the same efficient framework to transfer facial expressions between arbitrary <digit> d face models , as well as between images of performer 's faces . we propose a novel geometry encoding for triangle meshes , vertex tent coordinates , that enables us to formulate expression transfer in the <digit> d and the <digit> d case as a solution to a simple system of linear equations . our experiments show that our method outperforms many previous analogy based animation approaches in terms of achieved animation quality , computation time and generality .
asynchronous byzantine agreement with optimal resilience . <eos> we present an efficient , optimally resilient asynchronous byzantine agreement ( aba ) protocol involving ( n 3t <digit> ) parties over a completely asynchronous network , tolerating a computationally unbounded byzantine adversary , capable of corrupting at most ( t ) out of the ( n ) parties . in comparison with the best known optimally resilient aba protocols of canetti and rabin ( stoc <digit> ) and abraham et al. ( podc <digit> ) , our protocol is significantly more efficient in terms of the communication complexity . our aba protocol is built on a new statistical asynchronous verifiable secret sharing ( avss ) protocol with optimal resilience . our avss protocol significantly improves the communication complexity of the only known statistical and optimally resilient avss protocol of canetti et al. our avss protocol is further built on an asynchronous primitive called asynchronous weak commitment ( awc ) , while the avss of canetti et al. is built on the primitive called asynchronous weak secret sharing ( awss ) . we observe that awc has weaker requirements than awss and hence it can be designed more efficiently than awss . the common coin primitive is one of the most important building blocks for the construction of an aba protocol . in this paper , we extend the existing common coin protocol to make it compatible with our new avss protocol that shares multiple secrets simultaneously . as a byproduct , our new common coin protocol is more communication efficient than all the existing common coin protocols .
a niching genetic k means algorithm and its applications to gene expression data . <eos> partitional clustering is a common approach to cluster analysis . although many algorithms have been proposed , partitional clustering remains a challenging problem with respect to the reliability and efficiency of recovering high quality solutions in terms of its criterion functions . in this paper , we propose a niching genetic k means algorithm ( ngka ) for partitional clustering , which aims at reliably and efficiently identifying high quality solutions in terms of the sum of squared errors criterion . within the ngka , we design a niching method , which encourages mating among similar clustering solutions while allowing for some competitions among dissimilar solutions , and integrate it into a genetic algorithm to prevent premature convergence during the evolutionary clustering search . further , we incorporate one step of k means operation into the regeneration steps of the resulted niching genetic algorithm to improve its computational efficiency . the proposed algorithm was applied to cluster both simulated data and gene expression data and compared with previous work . experimental results clear show that the ngka is an effective clustering algorithm and outperforms two other genetic algorithm based clustering methods implemented for comparison .
adaptive support weight approach for correspondence search . <eos> we present a new window based method for correspondence search using varying support weights . we adjust the support weights of the pixels in a given support window based on color similarity and geometric proximity to reduce the image ambiguity . our method outperforms other local methods on standard stereo benchmarks .
the factors influencing members ' continuance intentions in professional virtual communities a longitudinal study . <eos> the advance of internet technology has stimulated the rise of professional virtual communities ( pvcs ) . the objective of pvcs is to encourage people to exploit or explore knowledge through websites . however , many virtual communities have failed due to the reluctance of members to continue their participation in these pvcs . motivated by such concerns , this study formulates and tests a theoretical model to explain the factors influencing individuals ' intention to continue participating in pvcs ' knowledge activities . drawing from the information system and knowledge management literatures , two academic perspectives related to pvc continuance are incorporated in the integrated model . this model posits that an individual 's intention to stay in a professional virtual community is influenced by a contextual factor and technological factors . specifically , the antecedents of pvc members ' intention to continue sharing knowledge include social interaction ties capital and satisfaction at post usage stage . these variables , in turn , are adjusted based on the confirmation of pre usage expectations . a longitudinal study is conducted with <digit> members of a professional virtual community . results indicate that the contextual factor and technological factors both exert significant impacts on pvc participants ' continuance intentions .
one step approach for heat exchanger network retrofitting using integrated differential evolution . <eos> heat exchanger network ( hen ) retrofitting is more important and challenging than hen synthesis since it involves modifying existing network for improved energy efficiency . additional factors to be considered include spatial constraints , relocation and re piping costs , reassignment and effective use of existing heat exchanger areas . the previous studies using stochastic global optimization algorithms are mainly focused on two level approach the first level uses a stochastic algorithm for optimizing structure , and the second level uses either a stochastic or a deterministic algorithm for optimizing continuous variables . in this study , we propose and test one step approach where a stochastic global optimization method , namely , integrated differential evolution ( ide ) , handles both discrete and continuous variables together . thus , hen structure and retrofitting model parameters are simultaneously optimized by ide , which avoids the algorithm trapping at a local optimum and also improves the computational efficiency . results on hen applications show that the proposed approach gives better solutions . ( c ) <digit> elsevier ltd. all rights reserved .
a design of rftog model for distributed real time applications . <eos> in this paper , we design the real time fault tolerant object group ( rftog ) model that supports the grouping of distributed objects that are required for distributed application . the proposed model basically provides two services . one is the group management service , which supports both consistency maintenance and transparency of the replicated objects with a variety of replication mechanisms . it also provides the load balancing of distributed applications . the other is real time service in an object group . when the clients request the service to the service object selected through the load balance , this service guarantees the service execution within deadline for the clients ' requests . we develop the naval air defense system ( nads ) simulator for verifying the effectiveness of the services proposed by the rftog model .
information retrieval based on collaborative filtering with latent interest semantic map . <eos> in this paper , we propose an information retrieval model called latent interest semantic map ( lism ) , which features retrieval composed of both collaborative filtering ( cf ) and probabilistic latent semantic analysis ( plsa ) . the motivation behind this study is that the relation between users and documents can be explained by the two different latent classes , where users belong probabilistically in one or more classes with the same interest groups , while documents also belong probabilistically in one or more class with the same topic groups . the novel aspect of lism is that it simultaneously provides a user model and latent semantic analysis in one map . this benefit of lism is to enable collaborative filtering in terms of user interest and document topic and thus solve the cold start problem .
intrusion detection aware component based systems a specification based framework . <eos> component based software engineering ( cbse ) increases the reusability of software and hence decreases software development time and cost . unfortunately , developing components for maximum reusability and acquiring third party components invite many security related concerns . the security related issues are more crucial for embedded and real time systems . currently , many approaches are proposed to aid the development and evaluation of secure components . however , it is well known among practitioners that , like any other software entities , components can not be completely secure . this fact leads us to incorporate intrusion detection facilities to equip components with mechanisms to discover intrusions against components . in this paper , we present a framework for developing components with intrusion detection capabilities . this framework uses umlintr , a uml profile for intrusion specifications . the profile allows developers to specify intrusion scenarios using uml diagrams . specifying intrusion scenarios using the same language that is used for specifying software behavior eliminates the need for separate languages for describing intrusions . other software specification languages can be easily adopted into this framework . the outcome of this framework are components equipped with intrusion detectors . based on umlintr , a prototype is built and used to generate signatures for some intrusions included in the benchmark darpa attack datasets . furthermore , we describe an intrusion detection system ( ids ) which uses these signatures to detect component intrusions .
optimal generation share based dynamic available transfer capability improvement in deregulated electricity market . <eos> a bi level programming approach to maximize the dynamic atc is proposed . atc is maximized by optimally injecting the generation power in bilateral contract . dynamic atc is calculated at the hopf bifurcation point . real code genetic algorithm is used as a tool to solve the proposed approach .
similarity transformation parameters recovery based on radon transform . application in image registration and object recognition . <eos> we define <digit> based radon transform ( rt ) and motivate its use . we propose an algorithm to recover rst transforms using only rt. the algorithm is tested on images in real world application ( e.g. ct , mri , etc. ) this rst parameter recovery method is newly applied in object recognition . a new confusion matrix is designed to identify also a mislabeled class element .
hyperspectral images segmentation a proposal . <eos> hyper spectral imaging ( his ) also known as chemical or spectroscopic imaging is an emerging technique that combines imaging and spectroscopy to capture both spectral and spatial information from an object . hyperspectral images are made up of contiguous wavebands in a given spectral band . these images provide information on the chemical make up profile of objects , thus allowing the differentiation of objects of the same colour but which possess make up profile . yet , whatever the application field , most of the methods devoted to his processing conduct data analysis without taking into account spatial information . pixels are processed individually , as an array of spectral data without any spatial structure . standard classification approaches are thus widely used ( k means , fuzzy c means hierarchical classification ... ) . linear modelling methods such as partial least square analysis ( pls ) or non linear approaches like support vector machine ( svm ) are also used at different scales ( remote sensing or laboratory applications ) . however , with the development of high resolution sensors , coupled exploitation of spectral and spatial information to process complex images , would appear to be a very relevant approach . however , few methods are proposed in the litterature . the most recent approaches can be broadly classified in two main categories . the first ones are related to a direct extension of individual pixel classification methods using just the spectral dimension ( k means , fuzzy c means or fcm , support vector machine or svm ) . spatial dimension is integrated as an additionnal classification parameter ( markov fields with local homogeneity constrainst <digit> , support vector machine or svm with spectral and spatial kernels combination <digit> , geometrically guided fuzzy c means <digit> ... ) . the second ones combine the two fields related to each dimension ( spectral and spatial ) , namely chemometric and image analysis . various strategies have been attempted . the first one is to rely on chemometrics methods ( principal component analysis or pca , independant component analysis or ica , curvilinear component analysis ... ) to reduce the spectral dimension and then to apply standard images processing technics on the resulting score images i.e. data projection on a subspace . another approach is to extend the definition of basic image processing operators to this new dimensionality ( morphological operators for example <digit> , <digit> ) . however , the approaches mentioned above tend to favour only one description either directly or indirectly ( spectral or spatial ) . the purpose of this paper is to propose a hyperspectral processing approach that strikes a better balance in the treatment of both kinds of information . to achieve this , a generic scheme is proposed to associate more closely the spectral and spatial aspects symmetrically and conjunctively . this method , called butterfly , aims to perform an iterative and a cross analysis of data in the spectral and the spatial domains lead to the segmentation of the hyperspectral image . the strategy is based on two steps extraction of a spatial structure ( topology ) incorporating a spectral structure , extraction of a spectral structure ( latent variables ) incorporating a spatial structure , these steps are processed in a successive , iterative and inter dependent way . in this article , we will focus solely on specific notions of topology i.e. the notions of connectivity and adjacency . thus , the first stage deals with the use of commonly used image processing tools ( region segmentation algorithms ) on a limited number of score images . this makes it relatively easy to process . to carry out the second step , we use chemometric tools to reveal a subspace ( latent variables ) enabling the characterization of heterogeneity of the obtained image partitions . however , the scheme can be applied on two different ways depending on the region segmentation strategy used i.e. top down approaches ( splitting ) or bottom up approaches ( merging ) . we have implemented this scheme by using a split and merge strategy based on the quadtree approach . each phase is done over successive steps ( iterations ) . at each iteration of the split phase , the data are projected on k ( <digit> ) suitable latent variables . the split of each existing region ( partition ) into four disjoints quadrants is tested and the one maximising the wilks lambda parameter is chosen . at each iteration of the merge phase , the data are projected on k ( <digit> ) suitable latent variables and all the merging of two adjacent regions are tested . the one maximising the wilks lambda parameter is chosen . lastly , we tested our approach on a simple synthetic image to show more precisely its characteristics and also on two real images at different scales ( in field acquisition on crop , remote sensing image of urban zone ) . the results obtained on real images underline the benefit of the butterfly approach . however , further work will be undertaken to investigate the influence of various parameters . moreover , other topology notions and image analysis algorithm could be also investigated .
optimization of site exploration program for improved prediction of tunneling induced ground settlement in clays . <eos> excessive settlement caused by tunneling during construction often damages adjacent infrastructures and utilities . such excessive settlement can also present a challenge in the maintenance of subways during their operation . thus , it is important to be able to accurately predict tunneling induced settlement . the uncertainties in geotechnical parameters , however , can lead to either an overestimation or an underestimation of the tunneling induced settlement . such uncertainties can arise from many sources such as spatial variability , measurement error , and model error in this paper , the focus is on the geotechnical parameters characterization from site exploration . the goal here is to determine an optimal level of site exploration effort so that effective predictions of the tunneling induced settlement in clays can be achieved . to this end , a monte carlo simulation based numerical model of site exploration is first established to generate artificial test data . then , a series of parametric analyses are performed to investigate the relationship between the level of site exploration effort and the accuracy of the tunneling induced ground settlement prediction . through the assumed different levels of site exploration effort , statistics of soil parameters are estimated using the maximum likelihood method and the tunneling induced ground settlement is then analyzed using the probabilistic method , and finally the effect of site exploration effort is assessed . the knowledge generated from this series of analyses is then used to develop the proposed framework for selecting an optimal site exploration program for improved prediction of the tunneling induced ground settlement in clays . examples are presented to illustrate the proposed framework and demonstrate its effectiveness and significance .
a study of using an out of box commercial mt system for query translation in clir . <eos> recent availability of commercial online machine translation ( mt ) systems makes it possible for layman web users to utilize the mt capability for cross language information retrieval ( clir ) . to study the effectiveness of using mt for query translation , we conducted a set of experiments using google translate , an online mt system provided by google , for translating queries in clir . the experiments show that mt is an excellent tool for the query translation task , and with the help of relevance feedback , it can achieve significant improvement over the monolingual baseline . the mt based query translation not only works for long queries , but is also effective for the short web queries .
portfolio optimization using a credibility mean absolute semi deviation model . <eos> we present a cardinality constrained credibility mean absolute semi deviation model . we prove relationships for possibility and credibility moments for lr fuzzy variables . the return on a given portfolio is modeled by means of lr type fuzzy variables . we solve the portfolio selection problem using an evolutionary procedure with a dss . we select best portfolio from pareto front with a ranking strategy based on fuzzy var .
analysis on feelings for designs . <eos> the objects of data processing in this research are adjectives such as innovative , bright , elegant or cheerful expressed by people who are just selecting goods to purchase . the subjective data obtained by a product rating usually has a big variance reflecting vagueness , nonlinearity or non additivity of the feelings of people . such data should be considered as fuzzy data as well as statistical data . in this paper , a method of constructing a fuzzy factor space is suggested . one of the problems here is how to define the second moment of fuzzy data . after defining a fuzzy metric space , a covering problem is formulated to detect representative adjectives that are used with relatively small differences between people . finally , the problem of identifying a relation between selected adjectives and pre selected designs is considered by using a multi channel adaptive bidirectional associative memory .
risk assessment in social lending via random forests . <eos> social lending has emerged as a viable platform alternative to banks . widespread adoption depends on better risk attribution to borrowers . a random forest ( rf ) based method is proposed for identifying good borrowers . our results indicate rf outperforms traditional credit scoring methods .
wireless ad hoc networks strategies and scaling laws for the fixed snr regime . <eos> this paper deals with throughput scaling laws for random ad hoc wireless networks in a rich scattering environment . ' we develop schemes to optimize the ratio lambda ( n ) of achievable network sum capacity to the sum of the point to point capacities of source destinations ( s d ) pairs operating in isolation . our focus in this paper is on fixed signal to noise ratio ( snr ) networks , i.e. , networks where the worst case snr over the s d pairs is fixed independent of n. for such fixed snr networks , which include fixed area networks as a special case , we show that collaborative strategies yield a scaling law of lambda ( n ) omega ( <digit> n ( <digit> <digit> ) ) in contrast to multihop strategies which yield a scaling law of a ( n ) while networks where worst case snr goes to zero do not preclude the possibility of collaboration , multihop strategies achieve optimal throughput . the plausible reason is that the gains due to collaboration can not offset the effect of vanishing receive snr . this suggests that for fixed snr networks , a network designer should look for network protocols that exploit collaboration .
lattice qcd on gpu clusters , using the quda library and the chroma software system . <eos> the quda library for optimized lattice quantum chromodynamics using gpus , combined with a high level application framework such as the chroma software system , provides a powerful tool for computing quark propagators , a key step in current calculations of hadron spectroscopy , nuclear structure , and nuclear forces . in this contribution we discuss our experiences , including performance and strong scaling of the quda library and chroma on the edge cluster at lawrence livermore national laboratory and on various clusters at jefferson lab . we highlight some scientific successes and consider future directions for graphics processing units in lattice quantum chromodynamics calculations .
3d qsar studies of 1,3,4 benzotriazepine derivatives as cck2 receptor antagonists . <eos> a number of cck2 antagonists have been reported to play an important role in controlling gastric acid related conditions , nervous system related disorders and certain types of cancer . to obtain the helpful information for designing potent antagonists with novel structures and to investigate the quantitative structureactivity relationship of a group of <digit> different cck2 receptor antagonists with varying structures and potencies , comfa , comsia , and hqsar studies were carried out on a series of 1,3,4 benzotriazepine based cck2 receptor antagonists . qsar models were derived from a training set of <digit> compounds . by applying leave one out ( loo ) cross validation study , cross validated ( r cv <digit> ) values of 0.673 and 0.608 and non cross validated ( r ncv <digit> ) values of 0.966 and 0.969 were obtained for the comfa and comsia models , respectively . the predictive ability of the comfa and comsia models was determined using a test set of <digit> compounds , which gave predictive correlation coefficients ( r pred <digit> ) of 0.793 and 0.786 , respectively . hqsar was also carried out as a complementary study , and the best hqsar model was generated using atoms , bonds , hydrogen atoms , and chirality as fragment distinction with fragment size ( <digit> ) and six components showing r cv <digit> and r ncv <digit> values of 0.744 and 0.918 , respectively . comfa steric and electrostatic , comsia hydrophobic and hydrogen bond acceptor fields , and hqsar atomic contribution maps were used to analyze the structural features of the datasets that govern their antagonistic potency .
from crayons to computers the evolution of computer use in redistricting . <eos> following the most recent round of redistricting , observers across the political spectrum warned that computing technology had fundamentally changed redistricting , for the worse . they are concerned that computers enable the creation of finely crafted redistricting plans that promote partisan and career goals , to the detriment of electoral competition , and that , ultimately , thwart voters ' ability to express their will through the ballot box . in this article , we provide an overview of the use of computers in redistricting , from the earliest reports of their utilization , through today . we then report responses to our survey of state redistricting authorities ' computer use in <digit> and <digit> . with these data , we assess the use of computers in redistricting , and the fundamental capabilities of computer redistricting systems .
effects of cycling humidity on the performance of rfid tags with aca joints . <eos> radio frequency identification ( rfid ) tags with anisotropically conductive joints ( acas ) are used in different applications where the environmental conditions may impair their reliability . thus the effects of different environmental stresses on reliability need to be investigated . the effects of high temperature and humidity may change the performance of the tags . moreover , the effects of constantly varying temperature and humidity conditions may be even more harmful . in this study , the effects of changing humidity conditions on the performance of a passive ultra high frequency ( uhf ) rfid tag with aca joints were studied . the tags were tested in a humidity cycling test where humidity varied from <digit> % rh to <digit> % rh , and temperature from <digit> degrees c to <digit> degrees c. tags with four different sets of bonding parameters were tested . significant differences in the reliability between the tags with different bonding parameters were observed . the results were also compared with results from a corresponding constant humidity test where the humidity was <digit> % rh , and the temperature <digit> degrees c. the tags had different failure times , modes , and mechanisms in these two tests . furthermore , the effects of bonding parameters on the reliability were different in these tests . according to this study , it is important to investigate the effects of changing humidity , when the reliability in different environments is investigated , but the constant humidity test can not be replaced with the faster humidity cycling test .
pivoting approaches for bulk extraction of entity attribute value data . <eos> entity attribute value ( eav ) data , as present in repositories of clinical patient data , must be transformed ( pivoted ) into one column per parameter format before it can be used by a variety of analytical programs . pivoting approaches have not been described in depth in the literature , and existing descriptions are dated . we describe and benchmark three alternative algorithms to perform pivoting of clinical data in the context of a clinical study data management system . we conclude that when the number of attributes to be returned is not too large , it is feasible to use static sql as the basis for views on the data . an alternative but more complex approach that utilizes hash tables and the presence of abundant random access memory can achieve improved performance by reducing the load on the database server . ( c ) <digit> published by elsevier ireland ltd .
mobile video surveillance with low bandwidth low latency video streaming . <eos> this paper presents a system for remote live video surveillance . videos are acquired from a fixed camera at <digit> fps and qvga resolution , compressed at <digit> or <digit> kbit s with h. <digit> , and streamed to a remote site , where they get processed by an automatic video surveillance system . the target surveillance application performs moving object segmentation and tracking . both ends ( video acquisition and processing ) could be connected through a wireless network , specifically gprs.the whole system is studied and optimized to maintain low latency . the reported experiments demonstrate that the proposed system is able to send up to four video streams over gprs or e gprs network , without significantly affecting the performance of the automatic video surveillance system . comparative tests have been performed with other existing streaming solutions .
level of detail for terrain geometry images . <eos> we consider the rendering of geometry images obtained by parameterization of terrain geometry . this technique reduces texture warping in areas of steep gradient . the selected terrain representation allows terrain that can not be represented by a heightfield to be rendered . finally , we demonstrate that the slightly irregular sampling can be rendered efficiently using modern graphics hardware . we introduce an efficient level of detail algorithm that can be applied to terrain geometry images or regular terrain .
on spectrum sharing games . <eos> efficient spectrum sharing mechanisms are crucial to alleviate the bandwidth limitation in wireless networks . in this paper , we consider the following question can free spectrum be shared efficiently we study this problem in the context of 802.11 or wifi networks . each access point ( ap ) in a wifi network must be assigned a channel for it to service users . there are only finitely many possible channels that can be assigned . moreover , neighboring access points must use different channels so as to avoid interference . currently these channels are assigned by administrators who carefully consider channel conflicts and network loads . channel conflicts among aps operated by different entities are currently resolved in an ad hoc manner ( i.e. , not in a coordinated way ) or not resolved at all . we view the channel assignment problem as a game , where the players are the service providers and aps are acquired sequentially . we consider the price of anarchy of this game , which is the ratio between the total coverage of the aps in the worst nash equilibrium of the game and what the total coverage of the aps would be if the channel assignment were done optimally by a central authority . we provide bounds on the price of anarchy depending on assumptions on the underlying network and the type of bargaining allowed between service providers . the key tool in the analysis is the identification of the nash equilibria with the solutions to a maximal coloring problem in an appropriate graph . we relate the price of anarchy of these games to the approximation factor of local optimization algorithms for the maximum k colorable subgraph problem . we also study the speed of convergence in these games .
link based spam algorithms in adversarial information retrieval . <eos> web spam has become one of the most exciting challenges and threats to web search engines . the relationship between the search systems and those who try to manipulate them came up with the field of adversarial information retrieval . in this article , we set up several experiments to compare hostrank and trustrank to show how effective it is for trustrank to combat web spam , and we report a comparison on different link based web spam detection algorithms .
an autonomous intelligent gateway infrastructure for in field processing in precision viticulture . <eos> wireless sensor networks have found multiple applications in precision viticulture . despite the steady progress in sensing devices and wireless technologies , some of the crucial items needed to improve the usability and scalability of the networks , such as gateway infrastructures and in field processing , have been comparatively neglected . this paper describes the hardware , communication capabilities and software architecture of an intelligent autonomous gateway , designed to provide the necessary middleware between locally deployed sensor networks and a remote location within the whole farm concept . this solar powered infrastructure , denoted by ipagat ( intelligent precision agriculture gateway ) , runs an aggregation engine that fills a local database with environmental data gathered by a locally deployed zigbee wireless sensor network . aggregated data are then retrieved by external queries over the built in data integration system . in addition , embedded communication capabilities , including bluetooth , ieee 802.11 and gprs , allow local and remote users to access both gateway and remote data , as well as the internet , and run site specific management tools using authenticated smartphones . field experiments provide convincing evidence that ipagat represents an important step forward in the development of distributed service oriented information systems for precision viticulture applications .
implementation of bayesian theory on lrfd of axially loaded driven piles . <eos> in this paper , a framework based on bayesian theory and proof pile load test results was used to update resistance factors of axially loaded driven piles . prior to implementation of the framework , resistance factors were calibrated based on the distribution of the measured to predicted pile ultimate bearing capacity using the results of static pile load tests conducted to failure . these resistance factors and the distribution were considered to be prior . the prior distribution of the measured to predicted ultimate bearing capacity was updated based on bayesian theory to incorporate additional proof pile load test results . using the measured to predicted load distributions and the updated ( or posterior ) measured to predicted bearing capacity distributions , resistance factors were calibrated ( or updated ) from the first order reliability method ( form ) for two different target reliability indices , 2.33 and 3.0 . this research attempted to use the results of proof pile load tests , which are generally conducted to verify pile designs , to update resistance factors . the updated resistance factors varied substantially depending on the proof pile load test results . therefore , the bayesian implementation can contribute to economical pile designs .
statistical background modeling for non stationary camera . <eos> a new background subtraction method is proposed in this paper for the foreground detection from a non stationary background . usually , motion compensation is required when applying background subtraction to a non stationary background . in practice , it is difficult to realize this to sufficient pixel accuracy . the problem is further complicated when the moving objects to be detected tracked are small , since the pixel error in motion compensating the background will hide the small targets . a spatial distribution of gaussians model is proposed to deal with moving object detection where the motion compensation is not exact but approximated . the distribution of each background pixel is temporally and spatially modeled . based on this statistical model , a pixel in the current frame is classified as belonging to the foreground or background . in addition , a new background restoration and adaptation algorithm is developed for the non stationary background over an extended period of time . test cases involving a surveillance system to detect small moving objects ( human and car ) within a highly textured background and a pan tilt human tracking system are demonstrated successfully .
a note on tanner graphs for group block codes and lattices . <eos> in this letter , some more concrete trellis relations between a lattice and its dual lattice are firstly given . based on these relations , we generalize the main results of <digit> .
faster deterministic wakeup in multiple access channels . <eos> we consider the fundamental problem of waking up n processors sharing a multiple access channel . we assume the weakest model of synchronization , the locally synchronous model , in which no global clock is available processors have local clocks ticking at the same rate , but each clock starts counting the rounds in the round in which the correspondent processor wakes up . moreover , the number n of processors is not known to the processors . we propose a new deterministic algorithm for this problem in time o ( n3log3n ) o ( n <digit> log <digit> n ) , which improves on the currently best upper bound of o ( n4log5n ) o ( n <digit> log <digit> n ) .
automatic status updates in distributed software development . <eos> this study investigates how automatic , real time , user centered awareness information can help distributed software development teams . we created an eclipse plugin that automatically determines a user 's activity in their eclipse ide and publishes the activity information as the status of their instant messenger client . the status is updated in real time every time the user changes his or her activities in their ide . we evaluated this tool by demonstrating it to eighty one academics and industry workers in the field of computer science and interviewing them about the perceived benefits and usefulness of the tool . the results reveal various factors that can impact a participant 's desire for increased awareness information . despite these factors there was a general desire to improve awareness of users ' activities via the tool . there was also some indication that the tool might help with interruption management .
crafting the initial user experience to achieve community goals . <eos> recommender systems try to address the new user problem by quickly and painlessly learning user preferences so that users can begin receiving recommendations as soon as possible . we take an expanded perspective on the new user experience , seeing it as an opportunity to elicit valuable contributions to the community and shape subsequent user behavior . we conducted a field experiment in movielens where we imposed additional work on new users not only did they have to rate movies , they also had to enter varying numbers of tags . while requiring more work led to fewer users completing the entry process , the benefits were significant the remaining users produced a large volume of tags initially , and continued to enter tags at a much higher rate than a control group . further , their rating behavior was not depressed . our results suggest that careful design of the initial user experience can lead to significant benefits for an online community .
phoebus a system for high throughput data movement . <eos> phoebus is an infrastructure for improving end to end throughput in high bandwidth , long distance networks by using a session layer protocol and gateways in the network . phoebus has the ability to dynamically allocate network resources and to use segment specific transport protocols between gateways , as well as to apply other performance improving techniques on behalf of the user . we have developed interfaces to phoebus to allow its use in various real applications and data movement services . this paper extends our earlier work with tests of phoebus enabled applications on both real world networks as well as over configurable network testbeds that allow us to modify latency and loss rates . we demonstrate that phoebus improves the performance of bulk data transfer in a variety of network configurations and conditions .
a convenient technique for solving integral equations of the first kind by the adomian decomposition method . <eos> purpose in this paper , fredholm integral equations of the first kind , the schlomilch integral equation , and a class of related integral equations of the first kind with constant limits of integration are transformed in such a manner that the adomian decomposition method ( adm ) can be applied . some examples with closed form solutions are studied in detail to further illustrate the proposed technique , and the results obtained indicate this approach is indeed practical and efficient . the purpose of this paper is to develop a new iterative procedure where the integral equations of the first kind are recast into a canonical form suitable for the adm. hence it examines how this new procedure provides the exact solution . design methodology approach the new technique , as presented in this paper in extending the applicability of the adm , has been shown to be very efficient for solving fredholm integral equations of the first kind , the schlomilch integral equation and a related class of nonlinear integral equations with constant limits of integration . findings by using the new proposed technique , the adm can be easily used to solve the integral equations of the first kind , the schlomilch integral equation , and a class of related integral equations of the first kind with constant limits of integration . originality value the paper shows that this new technique is easy to implement and produces accurate results .
an empirical study of xml data management in business information systems . <eos> due the popularity of xml , an increasingly large amount of business transactions encoded in xml have been exchanged on line . currently there are two approaches to process and manage these xml data . one is to store them in relational databases , and the other is to store them in recently developed native xml databases . there is no conclusion yet as to which approach suits better for contemporary business information systems . also , the effectiveness of native xml databases used in daily operational systems has not been completely investigated . therefore , in this paper , we provide ( <digit> ) a complete and systematic survey of the current development and challenges of processing xml data in relational and native xml databases , ( <digit> ) a useful benchmark for it practitioners who need to process xml data effectively , ( <digit> ) experimental results and detailed analysis which reveal several interesting tips that can be helpful to xml document designers , and ( <digit> ) a conclusion , based on the findings of using native xml databases in edi processes , that it is practical to use native xml databases for daily operations although our experimental results showed that relational database systems outperform native xml databases in processing xml data . ( c ) <digit> elsevier inc. all rights reserved .
a novel content based image retrieval system using k means knn with feature extraction . <eos> image retrieval has been popular for several years . there are different system designs for content based image retrieval ( cbir ) system . this paper propose a novel system architecture for cbir system which combines techniques include content based image and color analysis , as well as data mining techniques . to our best knowledge , this is the first time to propose segmentation and grid module , feature extraction module , k means and k nearest neighbor clustering algorithms and bring in the neighborhood module to build the cbir system . concept of neighborhood color analysis module which also recognizes the side of every grids of image is first contributed in this paper . the results show the cbir systems performs well in the training and it also indicates there contains many interested issue to be optimized in the query stage of image retrieval .
inducing grammars from sparse data sets a survey of algorithms and results . <eos> this paper provides a comprehensive survey of the field of grammar induction applied to randomly generated languages using sparse example sets .
estimation of the partial volume effect in mri . <eos> the partial volume effect ( pve ) arises in volumetric images when more than one tissue type occurs in a voxel . in such cases , the voxel intensity depends not only on the imaging sequence and tissue properties , but also on the proportions of each tissue type present in the voxel . we have demonstrated in previous work that ignoring this effect by establishing binary voxel based segmentations introduces significant errors in quantitative measurements , such as estimations of the volumes of brain structures . in this paper , we provide a statistical estimation framework to quantify pve and to propagate voxel based estimates in order to compute global magnitudes , such as volume , with associated estimates of uncertainty . validation is performed on ground truth synthetic images and mri phantoms , and a clinical study is reported . results show that the method allows for robust morphometric studies and provides resolution unattainable to date .
fuzzy rough sets hybrid scheme for breast cancer detection . <eos> this paper introduces a hybrid scheme that combines the advantages of fuzzy sets and rough sets in conjunction with statistical feature extraction techniques . an application of breast cancer imaging has been chosen and hybridization scheme have been applied to see their ability and accuracy to classify the breast cancer images into two outcomes cancer or non cancer . the introduced scheme starts with fuzzy image processing as pre processing techniques to enhance the contrast of the whole image to extracts the region of interest and then to enhance the edges surrounding the region of interest . a subsequently extract features from the segmented regions of the interested regions using the gray level co occurrence matrix is presented . rough sets approach for generation of all reducts that contains minimal number of attributes and rules is introduced . finally , these rules can then be passed to a classifier for discrimination for different regions of interest to test whether they are cancer or non cancer . to measure the similarity , a new rough set distance function is presented . the experimental results show that the hybrid scheme applied in this study perform well reaching over <digit> % in overall accuracy with minimal number of generated rules . ( this paper was not presented at any ifac meeting ) .
using link gradients to predict the impact of network latency on multitier applications . <eos> managing geographically dispersed deployments of complex multitier applications involves dealing with the substantial effects of network latency . however , the effects of network latency on an application 's end to end performance can be far from obvious , thus making it difficult to predict the true impact of infrastructure changes such as network upgrades or server relocation on the users of an application . in this paper , we propose a new metric to quantify this impact called the link gradient . we develop a novel noise resistant , nonintrusive technique to measure the link gradients in running systems without requiring knowledge of the system structure by using a combination of run time delay injection and spectral analysis . we evaluate the intrusiveness and accuracy of our approach using micro benchmarks and a deployment of two benchmark multitier web applications on planetlab . using these results , we show that link gradients can be used to accurately predict the impact of network latency changes on the end to end responsiveness of individual application transactions , even in new application configurations and without requiring a dedicated test environment .
catchment modellinga resource managers perspective . <eos> models are invaluable tools for resource management . models help resource managers develop a shared conceptual understanding of complex natural systems , allow testing of management scenarios , predict outcomes of high risk and high cost environmental manipulations , and set priorities . catchment modelling is a specialist field , and different modelling approaches are specialist areas in themselves . there are a plethora of models available that apply to integrated catchment management , from micro to landscape scales , from deterministic models to broad brush models . different philosophies abound , with some experts advocating topdown systems approaches and others who dismiss these as being too uncertain and based on opinion rather than fact . even when the approach is agreed upon , experts may be at odds over which modelling product is superior and have a vested interest in their particular product . so , how does the resource manager obtain objective , independent technical advice on needs and applications , and then choose the best modelling approach model development can be onerous , expensive , time consuming , and often bewildering for the resource manager . it is also an iterative process where the true magnitude of the effort , time and data required is often not fully understood until well into the process . resourcing can become problematic . this paper explores the dilemmas faced by resource managers who dare to venture down the path of catchment modelling and proposes ways to minimize the pain and maximize the gain .
a chinese expert disambiguation method based on semi supervised graph clustering . <eos> in order to utilize the associated relationship in the expert page efficiently , wed like to introduce a chinese expert disambiguation method based on the semi supervised graph clustering with the integration of various associated relationships . firstly , extract the correlation characteristics of the expert attributes according to the correlation analysis on the expert page . secondly , construct a similarity matrix between the documents on different expert pages with the utilization of the attributes characteristics and the associated relationship of the expert pages . finally , with the adoption of the attribute correlation as the semi supervised constraint , construct an expert disambiguation model by applying the graph based clustering approach to get the solution of the model through the kernel based method for the purpose to achieve expert name disambiguation . through the contrast experiment in the chinese expert disambiguation , it turns out that the disambiguation effect is much better with the adoption of the semi supervised graph clustering method that has been integrated with the expert associated relationships .
brownian dynamics simulation of the aggregation of submicron particles in static gas . <eos> a brownian dynamics simulation was conducted to investigate the formation of aggregates that are composed of submicron particles such as soot . three models were considered for aggregation a diffusion limited aggregation model , in which an aggregate grows around a fixed particle a particle cluster aggregation model , in which a single aggregate grows by collisions between particles and the aggregate and a cluster cluster aggregation ( cca ) model , in which many particles and aggregates form multiple aggregates . a comparison of the three aggregation models showed that the cca model resulted in a soot like branching shape . the aggregation was investigated by employing the cca model it was determined that increase in gas temperature affected the shielding effect of the aggregate branch by changing the displacement and velocity of brownian particles . furthermore , these simulations demonstrated that the size and aspect ratio of the field and the particle density also affected aggregation shape . ( c ) <digit> elsevier ltd. all rights reserved .
estrogen and the brain beyond er , er , and <digit> estradiol . <eos> abstract the brain of both sexes is a major target of estradiol and a site of estrogen synthesis during development and in the adult . in addition to the classical intranuclear estrogen receptors ( ers ) er and er , we have recently identified a novel , plasma membrane associated er that is neither er nor er in the brain and uterus , which we have designated er x. er x is a developmentally regulated estrogen binding protein that is present in wild type , er gene disrupted ( erko ) and er null mice . er x is re expressed after ischemic brain injury and in adult transgenic mice with alzheimer 's disease . although er x shares some homology with the c terminal region of er , it is not an alternative splicing variant of er and may be a new gene . er x mediates <digit> estradiol and <digit> estradiol activation of mapk erk . in contrast , er does not elicit erk activation but , surprisingly , is inhibitory . the potential importance of <digit> estradiol , the preferred ligand of er x , for the brain is underscored by our findings by liquid chromatography tandem mass spectrometry that the endogenous levels of <digit> estradiol are significantly elevated in the postnatal day <digit> and adult mouse neocortex and hippocampus , as compared with <digit> estradiol . that there is so much more <digit> estradiol than <digit> estradiol in the brain suggests that this enantiomer would be readily available to the brain . in considering estrogens for postmenopausal treatment , one should consider all the ers present in the brain , not just er and er , but er x as well , and focus on ligands such as <digit> estradiol that may be more selective for this er .
relations between de facto criteria in the evaluation of a spoken dialogue system . <eos> evaluation of spoken dialogue systems has been traditionally carried out in terms of instrumentally or expert derived measures ( usually called objective evaluation ) and quality judgments of users who have previously interacted with the system ( also called subjective evaluation ) . different research efforts have been made to extract relationships between these evaluation criteria . in this paper we report empirical results obtained from statistical studies , which were carried out on interactions of real users with our spoken dialogue system . these studies have rarely been exploited in the literature . our results show that they can indicate important relationships between criteria , which can be used as guidelines for refinement of the systems under evaluation , as well as contributing to the state of the art knowledge about how quantitative aspects of the systems affect the users perceptions about them .
runtime verification of cryptographic protocols . <eos> there has been a significant amount of work devoted to the static verification of security protocol designs virtually all of these results , when applied to an actual implementation of a security protocol , rely on certain implicit assumptions on the implementation ( for example , that the cryptographic checks that according to the design have to be performed by the protocol participants are carried out correctly ) so far there seems to be no approach that would enforce these implicit assumptions for a given implementation of a security protocol ( in particular regarding legacy implementations which have not been developed with formal verification in mind ) in this paper , we use a code assurance technique called runtime verification to solve this open problem runtime verification determines whether or not the behaviour observed during the execution of a system matches a given formal specification of a reference behaviour . by applying runtime verification to an implementation of any of the participants of a security protocol , we can make sure during the execution of that implementation that the implicit assumptions that had to be made to ensure the security of the overall protocol will be fulfilled the overall assurance process then proceeds in two steps first , a design model of the security protocol in uml is verified against security properties such as secrecy of data second , the implicit assumptions on the protocol participants are derived from the design model , formalised in linear time temporal logic , and the validity of these formulae at runtime is monitored using runtime verification the aim is to increase one 's confidence that statically verified properties are satisfied not only by a model of the system , but also by the actual running system itself we demonstrate the approach at the hand of the open source implementation jessie of the de facto internet security protocol standard ssl we also briefly explain how to transfer the results to the ssl implementation within the java secure sockets extension ( jsse ) recently made open source by sun microsystems . ( c ) <digit> elsevier ltd all rights reserved
development of software for spectral imaging data acquisition using labview . <eos> developing data acquisition software is a major challenge in integrating a spectral imaging system . this paper presents the design and implementation of a data acquisition program using labview for a liquid crystal tunable filter based spectral imaging system ( 9001700nm ) . the module based program was designed in a three tier structure . the image acquisition process , modelled by a finite state machine , was implemented in labview to control the spectral imaging system to collect hyperspectral or multispectral images . the collected spectral images were encoded in general format and could be further processed by other common spectral image analysis tools . in addition , the program could be used to observe band ratio images of the test object in real time , collect spectral images after ensemble averaging , and select region of interest for spectral image acquisitions . this program is a useful data acquisition tool for the filter based spectral imaging system . the design and implementation techniques described in this article could also be used to develop similar spectral image acquisition programs .
an artificial intelligent algorithm for tumor detection in screening mammogram . <eos> cancerous tumor mass is one of the major types of breast cancer . when cancerous masses are embedded in and camouflaged by varying densities of parenchymal tissue structures , they are very difficult to be visually detected on mammograms . this paper presents an algorithm that combines several artificial intelligent techniques with the discrete wavelet transform ( dwt ) for detection of masses in mammograms . the ai techniques include fractal dimension analysis , multiresolution markov random field , dogs and rabbits algorithm , and others . the fractal dimension analysis serves as a preprocessor to determine the approximate locations of the regions suspicious for cancer in the mammogram , the dogs and rabbits clustering algorithm is used to initiate the segmentation at the ll subband of a three level dwt decomposition of the mammogram . a tree type classification strategy is applied at the end to determine whether a given region is suspicious for cancer . we have verified the algorithm with <digit> mammograms in the mammographic image analysis society database . the verification results show that the proposed algorithm has a sensitivity of 97.3 % and the number of false positive per image is 3.92 .
computational investigation of the adsorption of carbon dioxide onto zirconium oxide clusters . <eos> a theoretical investigation of the adsorption of co2 onto zro2 is presented . various cluster models were used to mimic different basic and acidic sites on the surface . the method used was the density functional theory with the generalized gradient approximation and including grimme 's empirical model in order to properly describe the weak interactions that may occur between the adsorbate and the surface . we found that the adsorption at sites exhibiting two adjacent unsaturated zirconium atoms led to either the exothermic dissociation of co2 or to a strongly physisorbed state . by contrast , on a single unsaturated zirconium , co2 was adsorbed in an apical manner . in this case , the molecule is highly polarized and the adsorption energy amounts to 64.6 kj mol ( <digit> ) . finally , the weakest adsorption of co2 occurred on the basic oh sites on the surface .

a new method for multi oriented graphics scene 3d text classification in video . <eos> we propose a novel method for classifying graphics scene and 2d3d texts in video . an iterative procedure to identify text candidates is presented . stroke width and medial axis are explored for classifying graphics and scene texts . gradient directions and medial axis are combined for classifying 2d and 3d texts .
on the <digit> local profiles of graphs . <eos> for a graph g , let be the probability that three distinct random vertices span exactly i edges . we call the <digit> local profile of g. we investigate the set of all vectors that are arbitrarily close to the <digit> local profiles of arbitrarily large graphs . we give a full description of the projection of to the plane . the upper envelope of this planar domain is obtained from cliques on a fraction of the vertex set and complements of such graphs . the lower envelope is goodman 's inequality . we also give a full description of the triangle free case , i.e. the intersection of with the hyperplane . this planar domain is characterized by an sdp constraint that is derived from razborov 's flag algebra theory .
enabling real time physics simulation in future interactive entertainment . <eos> interactive entertainment has long been one of the driving factors behind architectural innovation , pushing the boundaries of computing to achieve ever more realistic virtual experiences . future entertainment applications will feature robust physics modeling to enable on the fly content creation . however , application designers must provide at least <digit> graphical frames per second to provide the illusion of visual continuity . this constraint directly impacts the physics engine , which must deliver the results of physical interactions in the virtual world at a fraction of this frame rate . with more sophisticated applications combining massive numbers of complex entities , the cost of robust physics simulation will easily exceed the capability of today 's most power machines.this work explores the characteristics of real time physics simulation , and proposes a suite of future thinking benchmarks stressing different situations that represent the demands of future interactive entertainment . with this suite , we then explore techniques to help meet these demands , including parallel execution , a fast estimation approach that self regulates error , and a value prediction technique that is allowed to get close enough to the real value . we demonstrate that parallel execution together with the proposed fast estimation approach can satisfy the demands of nearly all of the physicsbench suite .
internal and external bitstream relocation for partial dynamic reconfiguration . <eos> the research described in this paper shows how the runtime relocation of a reconfigurable component can be obtained using a system component that is able to update the bitstream information , moving the reconfigurable module in the desired position . this scenario defines the so called partial bitstream relocation activity . this paper proposes a relocation filter that can be implemented both as a hardware and a software component . the former is hosted in the static part of the reconfigurable architecture , while the latter is made to be run on the processor placed on the field programmable gate array ( fpga ) . the proposed approach has also been validated over different fpgas , i.e. , virtex ii pro , virtex <digit> , and virtex <digit> , proposing a runtime relocation support that can be customized to meet all the different constraints associated with these different target architectures .
social network based service recommendation with trust enhancement . <eos> proposed a social network based service recommendation method . an extended random walk algorithm is proposed to recommend services . experiments with real world data indicate the good performance of the recommendation .
a hybrid heuristic algorithm for the open pit mining operational planning problem . <eos> this paper deals with the open pit mining operational planning problem with dynamic truck allocation . the objective is to optimize mineral extraction in the mines by minimizing the number of mining trucks used to meet production goals and quality requirements . according to the literature , this problem is np hard , so a heuristic strategy is justified . we present a hybrid algorithm that combines characteristics of two metaheuristics greedy randomized adaptive search procedures and general variable neighborhood search . the proposed algorithm was tested using a set of real data problems and the results were validated by running the cplex optimizer with the same data . this solver used a mixed integer programming model also developed in this work . the computational experiments show that the proposed algorithm is very competitive , finding near optimal solutions ( with a gap of less than <digit> % ) in most instances , demanding short computing times .
wcag formalization with w3c standards . <eos> web accessibility consists on a set of checkpoints which are rather expensive to evaluate or to spot . however , using w3c technologies , this cost can be clearly minimized . this article presents a w3c formalized rule set version for automatable checkpoints from wcag 1.0 .
3d free form object recognition in range images using local surface patches . <eos> this paper introduces an integrated local surface descriptor for surface representation and 3d object recognition . a local surface descriptor is characterized by its centroid , its local surface type and a 2d histogram . the 2d histogram shows the frequency of occurrence of shape index values vs. the angles between the normal of reference feature point and that of its neighbors . instead of calculating local surface descriptors for all the 3d surface points , they are calculated only for feature points that are in areas with large shape variation . in order to speed up the retrieval of surface descriptors and to deal with a large set of objects , the local surface patches of models are indexed into a hash table . given a set of test local surface patches , votes are cast for models containing similar surface descriptors . based on potential corresponding local surface patches candidate models are hypothesized . verification is performed by running the iterative closest point ( icp ) algorithm to align models with the test data for the most likely models occurring in a scene . experimental results with real range data are presented to demonstrate and compare the effectiveness and efficiency of the proposed approach with the spin image and the spherical spin image representations .
on the applicability of self consistent global model for the characterization of cl2 ar inductively coupled plasma . <eos> this work reports the results on the influence of gas mixing ratio , gas pressure ( 0.263.3 pa ) and input power ( 400900w ) on the cl2 ar plasma parameters in the planar inductively coupled plasma reactor . the investigation combined plasma diagnostics by langmuir probe and quadrupole mass spectroscopy with plasma modelling given by the self consistent global ( <digit> dimensional ) model with maxwellian approximation for the electron energy distribution function . it was shown that , for the given range of experimental conditions , the model showed an outstanding agreement with the experiments and provided the near to adequate data on kinetics of plasma active species , their densities and fluxes .
practical deadlock free fault tolerant routing in meshes based on the planar network fault model . <eos> the number of virtual channels required for deadlock free routing is important for cost effective and high performance system design . the planar adaptive routing scheme is an effective deadlock avoidance technique using only three virtual channels for each physical channel in 3d or higher dimensional mesh networks with a very simple deadlock avoidance scheme . however , there exist one idle virtual channel for all physical channels along the first dimension and two idle virtual channels for channels along the last dimension in a mesh network based on the planar adaptive routing algorithm . a new deadlock avoidance technique is proposed for 3d meshes using only two virtual channels by making full use of the idle channels . the deadlock free adaptive routing scheme is then modified to a deadlock free adaptive fault tolerant routing scheme based on a planar network ( pn ) fault model . the proposed deadlock free adaptive routing scheme is also extended to n dimensional meshes still using two virtual channels . sufficient simulation results are presented to demonstrate the effectiveness of the proposed algorithm .
optical solutions for system level interconnect . <eos> throughput , power consumption , signal integrity , pin count and routing complexity are all increasingly important interconnect issues that the system designer must deal with . recent advances in integrated optical devices may deliver alternative interconnect solutions enabling drastically enhanced performance . this paper begins by outlining some of the more pressing issues in interconnect design , and goes on to describe system level optical interconnect for inter and intra chip applications . inter chip optical interconnect , now a relatively mature technology , can enable greater connectivity for parallel computing for example through the use of optical i o pads and wavelength division multiplexing . intra chip optical interconnect , technologically challenging and requiring new design methods , is presented through a proposal for heterogeneous integration of a photonic above ic layer followed by a design methodology for on chip optical links . design technology issues are highlighted and the paper concludes with examples of the use of optical links in clock distribution ( with quantitative comparisons of dissipated power between electrical and optical clock distribution networks ) and for novel network on chip architectures .
high voltage mosfet gate bulk driver controller for a microbattery switch matrix in a 0.35 mu m microwave soi technology . <eos> integrated microbatteries are being currently developed to act as a micropower source in microsatellites . the current and voltage rating of the microbattery is fixed . certain highly miniaturized systems require higher voltages and currents . a switching matrix is designed to achieve the same . the switching matrix is designed using high voltage metal oxide semiconductor ( mos ) structures and bulk isolated h gate transistors . this paper presents a design approach to help attain any random grouping pattern between the microbatteries . in this case , the result is an ability to charge microbatteries in parallel and to discharge microbatteries in parallel or pairs of microbatteries in series . this is achieved by providing the appropriate gate bulk voltages to the matrix . high voltage mos structures are developed which can take higher drain to source voltages in a 3.3 v process . the designs are built using microwave silicon on insulator process .
lighting transfer functions using gradient aligned sampling . <eos> an important task in volume rendering is the visualization of boundaries between materials . this is typically accomplished using transfer functions that increase opacity based on a voxels value and gradient . lighting also plays a crucial role in illustrating surfaces . in this paper we present a multi dimensional transfer function method for enhancing surfaces , not through the variation of opacity , but through the modification of surface shading . the technique uses a lighting transfer function that takes into account the distribution of values along a material boundary and features a novel interface for visualizing and specifying these transfer functions . with our method , the user is given a means of visualizing boundaries without modifying opacity , allowing opacity to be used for illustrating the thickness of homogeneous materials through the absorption of light .
hands on interaction with virtual environments . <eos> in this paper we describe the evolution of a whole hand interface to our virtual environment graphical system . we present a set of abstractions that can be used to implement device independent interfaces for hand measurement devices . some of these abstractions correspond to known logical device abstractions , while others take further advantage of the richness of expression in the human hand . we describe these abstractions in the context of their use in our development of virtual environments .
schedulability and performance analysis of the similarity stack protocol . <eos> we propose a class of real time data access protocols called ssp ( similarity stack protocol ) . the correctness of ssp schedules is justified by the concept of similarity which allows different but sufficiently timely data to be used in a computation without adversely affecting the outcome . ssp schedules are deadlock free , subject to limited blocking , and do not use locks , we give a schedulability bound for ssp and also report simulation results which show that ssp is especially useful for scheduling real time data access on multiprocessor systems . finally , we present a variation of ssp which can be implemented in an autonomous fashion in the sense that scheduling decisions can be made with local information only .
on the access pricing and network scaling issues of wireless mesh networks . <eos> distributed wireless mesh network technology is ready for public deployment in the near future . however , without an incentive system , one should not assume that private self interested wireless nodes would participate in such a public network and cooperate in the packet forwarding service . this paper studies the use of pricing as an incentive mechanism for stimulating participation and collaboration in public wireless mesh networks . our focus is on the economic behavior of the network nodes the pricing and purchasing strategies of the access point , wireless relaying nodes , and clients . we use a game theoretic approach to analyze their interactions from one hop to multihop networks and when the network has an unlimited or limited channel capacity . the important results that we show are that the access point and relaying wireless nodes will adopt a simple yet optimal fixed rate pricing strategy in a multihop network with an unlimited capacity . however , the access price grows quickly with the hop distance between a client and the access point , which may limit the scalability of the wireless mesh network . in case where the network has limited capacity , the optimal strategy for the access point is to vary the access charge and even interrupt service to connecting clients . to this end , we focus on the access point adopting a non self enforcing but more practical fixed rate noninterrupted service model and propose an algorithm based on the markovian decision theory to devise the optimal pricing strategy . results show that the scalability of a network with limited capacity is upper bounded by one with an unlimited capacity . we believe that this work will shed light on the deployment and pricing issues of distributed public wireless mesh networks .
fast algorithms for robust classification with bayesian nets . <eos> we focus on a well known classification task with expert systems based on bayesian networks predicting the state of a target variable given an incomplete observation of the other variables in the network , i.e. , an observation of a subset of all the possible variables . to provide conclusions robust to near ignorance about the process that prevents some of the variables from being observed , it has recently been derived a new rule , called conservative updating . with this paper we address the problem to efficiently compute the conservative updating rule for robust classification with bayesian networks . we show first that the general problem is np hard , thus establishing a fundamental limit to the possibility to do robust classification efficiently . then we define a wide subclass of bayesian networks that does admit efficient computation . we show this by developing a new classification algorithm for such a class , which extends substantially the limits of efficient computation with respect to the previously existing algorithm . the algorithm is formulated as a variable elimination procedure , whose computation time is linear in the input size . ( c ) <digit> elsevier inc. all rights reserved .
a theoretical investigation of cytotoxic activity of celastroid triterpenoids . <eos> quantum chemical calculations at the b3lyp <digit> 31g level of theory have been carried out on <digit> celastroid triterpenoids to obtain a set of molecular electronic properties and to correlate these with cytotoxic activities . the cytotoxic activities of these compounds can be roughly correlated with electronic effects related to nucleophilic addition to c ( <digit> ) of the compounds the energies of the frontier molecular orbitals ( e ( homo ) and e ( lumo ) ) , the homo lumo energy gap , the dipole moment , the charge on c ( <digit> ) , and the electrophilicity on c ( <digit> ) .
reconstruction of inaccessible boundary value in a sideways parabolic problem with variable coefficientsforward collocation with finite integration method . <eos> we investigate a sideways problem of reconstructing an inaccessible boundary value for parabolic equation with variable coefficients . formulating the sideways problem into a sequence of well posed direct problems ( dp ) and a system of ordinary differential equations ( ode ) , we combine the recently developed finite integration method ( fim ) with radial basis functions ( rbf ) to iteratively obtain the solution of each dp by solving an ill posed linear system . the use of numerical integration instead of finite quotient formula in fim completely avoids the well known roundoff discretization errors problem in finite difference method and the use of rbf as forward collocation method ( fcm ) gives a truly meshless computational scheme . for tackling the ill posedness of the sideways problem , we adapt the traditional tikhonov regularization technique to obtain stable solution to the system of odes . convergence analysis is then derived and error estimate shows that the error tends to zero when perturbation <digit> <digit> . we can then obtain highly accurate and stable solution under some assumptions . numerical results validate the feasibility and effectiveness of the proposed numerical algorithms .
parallel algorithms for normalization . <eos> given a reduced affine algebra a over a perfect field k , we present parallel algorithms to compute the normalization ( a ) over bar of a. our starting point is the algorithm of greuel et al. ( <digit> ) , which is an improvement of de jong 's algorithm ( de jong , <digit> decker et al. , <digit> ) . first , we propose to stratify the singular locus sing ( a ) in a way which is compatible with normalization , apply a local version of the normalization algorithm at each stratum , and find ( a ) over bar by putting the local results together . second , in the case where k q is the field of rationals , we propose modular versions of the global and local to global algorithms . we have implemented our algorithms in the computer algebra system singular and compare their performance with that of the algorithm of greuel et al. ( <digit> ) . in the case where k q , we also discuss the use of modular computations of grobner bases , radicals , and primary decompositions . we point out that in most examples , the new algorithms outperform the algorithm of greuel et al. ( <digit> ) by far , even if we do not run them in parallel . ( c ) <digit> elsevier b.v. all rights reserved .
two stage decoding algorithm for unmodulated parallel combinatory high compaction multicarrier modulation signals . <eos> a new decoding algorithm that consists of two decoding stages to reduce the computational complexity of maximum likelihood decoding for parallel combinatory high compaction multicarrier modulation is proposed . the first decoding stage is responsible for a preliminary decision that serves to roughly find candidate messages using the qrd m algorithm , and the second decoding stage is responsible for the final decision that reduces the error contained in the candidate and determines the message using the minimum value of the hamming distances between the candidate and the replicas of the message . the complexity is considerably reduced by the proposed two stage decoding algorithm at a cost of approximately 1.5 db or less in e b n <digit> with respect to the bit error rate of <digit> <digit> under the given parameter settings .
design of an adaptive fuzzy model based controller for chaotic dynamics in lorenz systems with uncertainty . <eos> this paper presents the control methodology for uncertain chaotic dynamics of lorenz systems . an adaptive fuzzy control ( afc ) scheme based on well known takagisugeno ( ts ) fuzzy models for the mimo plants is constructed . the developed control law and adaptive law guarantee the boundedness of all signals in the closed loop system . in addition , the chaotic state tracks the state of the stable reference model ( srm ) asymptotically with time for any bounded reference input signal . the proposed control is applied to control of an uncertain lorenz system such as stabilization , synchronization and chaotic model following control ( cmfc ) .
meshless collocation method by delta shaped basis functions for default barrier model . <eos> in this paper we first approximate a nearly singular function , which tends to be the dirac delta function , to high degree of accuracy by using a recently developed delta shaped basis function . the hermite based meshless collocation method based on radial basis functions is then applied to solve a default barrier model , which is a time dependent boundary value problem with a singularity at the initial condition . for numerical verification on the accuracy and efficiency of the newly proposed method , we compare the results with an analytical solution of the default barrier model under an assumption on the affine boundary . numerical results indicate that the proposed method has potential advantage to solve problems with dirac type singularities .
video block device for user friendly delivery in iaas clouds . <eos> peer to peer ( p2p ) streaming technology based on various service requirements often remains on the elusive benefits of user friendly video streaming in cloud computing environments . cloud assisted p2p media streaming gives an opportunity to enhance on demand , dynamic and easily accessible videos . this paper outlines the fundamental video block device ( vbd ) method for user friendly viewing patterns that inherits from user access patterns and provides efficient delivery using an enhanced bittorrent protocol .
operational research and ethics a literature review . <eos> any human activity raises ethical questions , questions about good and right ways to act and to live or to put it differently , questions of values and responsibility . from its inception operational research ( or ) has engaged with such questions in terms of professional behavior , the handling of preferences in or , the societal role of or , the process of or intervention and the content of or analysis . as a result , analytical methods and processes have been developed to help clients explore the ethical dimension of their decisions . the paper reviews the literature published in selected or journals ( management science operations research interfaces the european journal of operational research the journal of the operations research society omega international transactions in operational research the journal of multi criteria decision analysis ) , organizing it along the lines of ors core competences . the review identifies a number of significant research programmes that are well established and are being energetically pursued the research findings are being applied to a wide range of important issues . ethical questions lie at the heart of the great governmental and commercial issues of the day economic growth and instability inequality and injustice environmental degradation and sustainability . they also lie at the heart of the more mundane decisions of day to day or . ethics therefore provides a useful focus for or both in terms of raising the awareness of all concerned and in providing a theme for research . as a result of the review some research questions are suggested . there is much of interest , much to do .
enhancing non task sociability of asynchronous cscl environments . <eos> while from a technological perspective computer supported collaborative learning ( cscl ) systems have been improved considerably , previous studies have shown that the social aspect of the cscl is often neglected or assumed to happen automatically by simply creating such virtual learning environments . by distinguishing between students ' non task social interactions from on task interactions , and through a content analysis , this paper demonstrates that non task interactions do occur frequently in cscl environments . furthermore , by conducting a self reported survey , the present study operationalizes non task sociability of cscl environments and determines factors that affect them . the findings from the survey revealed that the sense of cohesion and awareness about others significantly impact the non task sociability of cscl . furthermore , the study demonstrates that the perception of self representation and perception of compatibility affect the sense of cohesion and awareness about others and indirectly contribute to the perceived non pedagogical sociability of the environment . the findings of this paper can be used in future research for investigating the relationship between the non task sociability of cscl and other cscl factors . the study also provides the cscl lecturers and facilitators with a conceptual model by which sociability can be explicitly addressed in their course planning and delivery processes . and finally , this study develops and validates an instrument that guides required changes in current cscl systems to improve the non task social functionality of the environment . ( c ) <digit> elsevier ltd. all rights reserved .
langmuir probe rf plasma compensation using a simulation method . <eos> the problem of langmuir probe data deformation due to rf pickup by the probe is treated through a computer simulation method . it is pointed out that proper rf compensations can be obtained by treatment of the langmuir probe raw data through the use of computer software . it is demonstrated that correct , rf unaffected probe iv i v characteristics can be accurately reproduced from the rf contaminated data . this eliminates the need for the use of any filters or other hardware procedures . user friendly matlab based software is presented . the software automatically retrieves the correct rf iv i v characteristics for single langmuir probe data which consequently allows for proper evaluation of plasma parameters such as the plasma electron temperature , electron number density and the electron energy distribution function ( eedf ) program title rf compensation catalogue identifier aeqr_v1_0 program summary url http cpc.cs.qub.ac.uk summaries aeqr_v1_0.html program obtainable from cpc program library , queens university , belfast , n. ireland licensing provisions copyright ( c ) <digit> , aasim azooz all rights reserved . redistribution and use in source and binary forms , with or without modification , are permitted provided that the following conditions are met redistributions of source code must retain the above copyright notice , this list of conditions and the following disclaimer . redistributions in binary form must reproduce the above copyright notice , this list of conditions and the following disclaimer in the documentation and or other materials provided with the distribution . no . of lines in distributed program , including test data , etc. <digit> no . of bytes in distributed program , including test data , etc. <digit> distribution format tar.gz programming language matlab 6.5 or higher . computer any laptop or desktop . operating system windows xp . ram bytes <digit> k classification <digit> . nature of problem in rf plasma langmuir probe diagnostics , the probe iv i v characteristics obtained experimentally does not represent the true iv i v . this is because the probe picks up some rf plasma voltage which modulates the applied bias voltage and causes a current flow that corresponds to the rf affected bias rather that the actual dc bias . this if untreated can lead to false results for the values of plasma parameters derived from the obtained iv i v . several hardware based methods are used to perform such correction ( compensation ) . solution method the suggested method is based on filtration of raw uncompensated iv i v data through software operation rather than hardware based filtrations which have their limitations . running time a few milliseconds . references <digit> a.a. azooz , review of scientific instruments <digit> ( <digit> ) <digit> .
evaluation of amd 's advanced synchronization facility within a complete transactional memory stack . <eos> amd 's advanced synchronization facility ( asf ) is an x86 instruction set extension proposal intended to simplify and speed up the synchronization of concurrent programs . in this paper , we report our experiences using asf for implementing transactional memory . we have extended a c c compiler to support language level transactions and generate code that takes advantage of asf . we use a software fallback mechanism for transactions that can not be committed within asf ( e.g. , because of hardware capacity limitations ) . our evaluation uses a cycle accurate x86 simulator that we have extended with asf support . building a complete asf based software stack allows us to evaluate the performance gains that a user level program can obtain from asf . our measurements on a wide range of benchmarks indicate that the overheads traditionally associated with software transactional memories can be significantly reduced with the help of asf .
full duplex relay based cognitive radio system with physical layer network coding . <eos> in this paper , a full duplex relay ( fdr ) based cognitive radio ( cr ) system is proposed with two secondary source nodes and two secondary destination nodes . the concept of physical layer network coding ( plnc ) is applied on decode and forward ( df ) fdr to ensure that both source nodes can transmit data to both destination nodes . further , the availability of multiple spectrum bands enables cr system to decode source symbols at destination nodes . the major advantage of the proposed system with fdr and plnc is that it requires only one time slot to send the data from both the source nodes to both the destination nodes . if fdr is replaced with half duplex relay ( hdr ) , <digit> time slots are required for the cr system . if plnc is not applied at the df relay , the cr system would require <digit> or <digit> time slots when fdr or hdr is employed , respectively . in nakagami ( m ) fading channels environment , analytical expressions are derived for end to end outage performance of the proposed plnc based system in the presence of echo interference at the fdr and with hdr in the place of fdr . the outage performance of the proposed cr system is analyzed for various scaling and shape parameters of nakagami ( m ) fading channels .
user control and direction of a more efficient simplifier in acl2 . <eos> we present an efficient term simplifier written in acl2 and interfaced with acl2 as an untrusted clause processor . we also demonstrate how an advanced user can extend this simplifier in a sound manner by proving rewrite rules with special annotations and programmed constraints on their application . for problems requiring extensive case analysis , the simplifier is more efficient than acl2 built in simplification and we demonstrate this on some relevant examples . in addition , we discuss the issue of user control over predictable simplification and conclude the paper with the proposed implementation of invariant discovery using the simplifier .
optimal allocation of boundary singularities for stokes flows about pairs of particles . <eos> methods of allocation of singularities for the method of fundamental solutions are proposed , implemented and applied to a stokes flow about pairs of particles . new local normal and combined stokeslets allocation methods are proposed to solve stokes flows using a moderate number of singularities . in the proposed methods the singularities are located at surfaces inside the particles but dissimilar to the particles shapes . optimization of location of stokeslets is performed for peanut shaped and barrel shaped particles . convergence of numerical solution as a function of numbers of stokeslets is evaluated and show substantial reduction in the needed number of stokeslets compared to the prior methods in which stokeslets are located at surfaces created similar to the particle shape . using proposed methods of allocation of stokeslets , patterns of pressure and velocity vector field near particles are obtained and discussed . the stokes force exerted by a stokes flow on the pair of particles is computed at different stages of their collective behavior including separate location of particles in proximity to each other , merging of particles , and re orientation of cluster along the flow . these results help to determine the least stable location of particles pair for the purpose of their separation off the flow .
using stochastic analysis to capture unstable equilibrium in natural convection . <eos> a stabilized stochastic finite element implementation for the natural convection system of equations under boussinesq assumptions with uncertainty in inputs is considered . the stabilized formulations are derived using the variational multiscale framework assuming a one step trapezoidal time integration rule . the stabilization parameters are shown to be functions of the time step size . provision is made for explicit tracking of the subgrid scale solution through time . a support space stochastic galerkin approach and the generalized polynomial chaos expansion ( gpce ) approach are considered for inputoutput uncertainty representation . stochastic versions of standard rayleighbnard convection problems are used to evaluate the approach . it is shown that for simulations around critical points , the gpce approach fails to capture the highly non linear input uncertainty propagation whereas the support space approach gives fairly accurate results . a summary of the results and findings is provided .
intelligent garbage can decision making model evolution algorithm for optimization of structural topology of plane trusses . <eos> the optimum design of structural topology of trusses is widely acknowledged as the most difficult and challenging problem in the area of structural optimization . based on differential evolution algorithms and using the framework of a garbage can decision making model , we proposed an intelligent garbage can decision making model evolution algorithm ( igcmea ) to simulate the decision making process in human social organizations . in a decision making process , when faced with issues such as unclear goals and methods , employee turnover and so forth , representatives of all participating parties will communicate , argue , compromise and adapt with each other in order to find a solution to the problems . group meetings are conducted to choose the best solution in a more objective , reasonable and efficient manner . by applying the differential evolution ( de ) algorithm and igcmea to perform an optimization test on the <digit> dimensional schwefel function , we showed that igcmea can achieve an efficient and satisfactory result . we also optimized the truss topology using igcmea and obtained a better result than when using the genetic algorithm as in the literature , thus illustrating the superior power of igcmea .
development of smart sensors system for machine fault diagnosis . <eos> machine fault diagnosis is a traditional maintenance problem . in the past , the maintenance using tradition sensors is money cost , which limits wide application in industry . to develop a cost effective maintenance technique , this paper presents a novel research using smart sensor systems for machine fault diagnosis . in this paper , a smart sensors system is developed which acquires three types of signals involving vibration , current , and flux from induction motors . and then , support vector machine , linear discriminant analysis , k nearest neighbors , and random forests algorithm are employed as classifiers for fault diagnosis . the parameters of these classifiers are optimized by using cross validation method . the experimental results show that smart sensor system has the similar performance for applying in intelligent machine fault diagnosis with reduced product cost . developed smart sensors have feasibility to apply for intelligent fault diagnosis .
structural tolerance and delaunay triangulation . <eos> in this paper we consider the tolerance of a geometric or combinatorial structure associated to a set of points as a measure of how much the set of points can be perturbed while leaving the ( topological or combinatorial ) structure essentially unchanged . we concentrate on studying the delaunay triangulation and show that its tolerance can be computed in o ( n ) time if the triangulation is given as part of the input , while the problem has complexity theta ( n log n ) if the triangulation is not known . we also study the problem of computing the tolerance of the edges of the triangulation , and show that the tolerance of all the edges can be computed in o ( n log n ) time . finally , we extend our study to some subgraphs of the delaunay triangulation . ( c ) <digit> published by elsevier science b.v. all rights reserved .
analysis of impersonation attacks on systems using rf fingerprinting and low end receivers . <eos> the vulnerability to an impersonation attack is assessed for a modulation based rf fingerprinting system employing low end commodity hardware ( by legitimate and malicious users alike ) . all the transceivers ( including two impersonators ) have the same manufacturer and their transceiver front ends are equipped with similar components . a receiver s front end also contributes to its rf fingerprinting of a specific transmitter . the success rate of an impersonation attack is receiver dependent and receiver impairment effectively decreases this rate .
differences between informational and transactional tasks in information seeking on the web . <eos> we examine the influence of task types on information seeking behaviors on the web by using screen capture logs and eye movement data . eleven participants performed two different types of web search , an informational task and a transactional task , and their think aloud protocols and behaviors were recorded . analyses of the screen capture logs showed that the task type affected the participants ' informationseeking behaviors . in the transactional task , participants visited more web pages than for the informational task , but their reading time for each page was shorter than in the informational task . a preliminary analysis of eye movement data for nine participants revealed characteristics of the scanpaths followed in search result pages as well as the distribution of lookzones for each task .
climate change and the ecology and evolution of arctic vertebrates . <eos> climate change is taking place more rapidly and severely in the arctic than anywhere on the globe , exposing arctic vertebrates to a host of impacts . changes in the cryosphere dominate the physical changes that already affect these animals , but increasing air temperatures , changes in precipitation , and ocean acidification will also affect arctic ecosystems in the future . adaptation via natural selection is problematic in such a rapidly changing environment . adjustment via phenotypic plasticity is therefore likely to dominate arctic vertebrate responses in the short term , and many such adjustments have already been documented . changes in phenology and range will occur for most species but will only partly mitigate climate change impacts , which are particularly difficult to forecast due to the many interactions within and between trophic levels . even though arctic species richness is increasing via immigration from the south , many arctic vertebrates are expected to become increasingly threatened during this century .
design of ioim for vme bus based cpu using cpld for nuclear power plants . <eos> in a nuclear reactor or process control systems it is required to continuously measure the signals from sensors and transmitters placed at various locations in the field . accurate measurement of these signal parameters is essential to carry out the control operations effectively . the measured parameters have to be sent to the processor for necessary action . information about the signals is transmitted to the processor using an input output interface module ( ioim ) . this paper describes the design of ioim using cpld ( complex programmable logic device ) . the use of cpld replaces the ttl ic 's present in the system to reduce the board components and improve the system reliability . the reduction in components reduces component density on board , lowers heat dissipation , emi levels and increases the speed of operation . the reprogramming capability of cpld enables to change the design instantly for no cost as many times as possible and thereby help in building reconfigurable systems and upgrade system functions as and when required .
a multi core parallelization strategy for statistical significance testing in learning classifier systems . <eos> permutation based statistics for evaluating the significance of class prediction , predictive attributes , and patterns of association have only appeared within the learning classifier system ( lcs ) literature since <digit> . while still not widely utilized by the lcs research community , formal evaluations of statistical confidence are imperative to large and complex real world applications such as genetic epidemiology where it is standard practice to quantify the likelihood that a seemingly meaningful statistic could have been obtained purely by chance . learning classifier system algorithms are relatively computationally expensive on their own . the compounding requirements for generating permutation based statistics may be a limiting factor for some researchers interested in applying lcs algorithms to real world problems . technology has made lcs parallelization strategies more accessible and thus more popular in recent years . in the present study we examine the benefits of externally parallelizing a series of independent lcs runs such that permutation testing with cross validation becomes more feasible to complete on a single multi core workstation . we test our python implementation of this strategy in the context of a simulated complex genetic epidemiological data mining problem . our evaluations indicate that as long as the number of concurrent processes does not exceed the number of cpu cores , the speedup achieved is approximately linear .
an energy efficient hole healing mechanism for wireless sensor networks with obstacles . <eos> in wireless sensor networks ( wsns ) , coverage of the monitoring area represents the surveillance quality . since sensor nodes are battery powered and placed outdoor , there will be failures due to energy exhaustion or environmental influence , resulting in coverage loss . in literature , a number of studies developed robot repairing algorithms that aim at maintaining full coverage . however , they did not consider the time constraint for network maintenance . furthermore , they did not consider the existence of obstacles and the constraint of limited energy of the robot . this paper presents a novel tracking mechanism and robot repairing algorithm for maintaining the coverage quality of the given wsn . without support of location information , the tracking mechanism leaves robot 's footmark on sensors so that they can learn better routes for sending repairing requests to the robot . upon receiving several repairing request messages , the robot applies the proposed repairing algorithm to establish an efficient route that passes through all failure regions with low overhead in terms of the required time and the power consumption . in addition , the proposed repairing algorithm also considers the remaining energy of the robot so that the robot can move back to home for recharging energy and overcome the unpredicted obstacles . performance results reveal that the developed protocol can efficiently maintain the coverage quality while the required time and energy consumption are significantly reduced . copyright ( c ) <digit> john wiley sons , ltd .
two dimensional analysis of the surface state effects in 4h sic mesfets . <eos> two dimensional small signal ac and transient analysis of surface trap effects in 4h sic mesfets have been performed in this paper . the mechanism by which acceptor type traps effect the transconductance and drain current changes has been discussed . the simulation results show that transconductance exhibits negative frequency dispersion behavior , which is caused by the charge exchange via the surface states existing between the gate source and gate drain terminals . the current degradation behavior is also observed due to acceptor type traps , acting as electron traps , in mesfet devices . a detailed study involving the density , ionization and energy level of traps reveals conclusive results in the devices analyzed .
an exploratory study on joint analysis of visual classification in narrow domains and the discriminative power of tags . <eos> the popularity of social media sharing sites such as flickr has driven a significant amount of research on the analysis of information contained in the tags used to annotate images . many of such tags are not useful to describe the contents of an image and are often labeled as not descriptive or even noisy . in this work we focus on the descriptiveness of a tag in an exploratory way , within a relatively narrow domain , and with the help of a visual classifier . preliminary experimental results demonstrate the possibility to infer descriptiveness of tags from a joint analysis of tag entropy calculations and the results of an automated visual classifier with a limited number of classes without taking tag content or tag co occurrence into account . we postulate that these experiments can be extended and improved toward a working solution that might answer the question given a semantic category , which tags would you use for searching an image from that category
performance evaluation of multiple channel slotted ring networks with tunable transmitters and fixed receivers . <eos> this study presents a non preemptive priority queue model to approximate the cell delay of a multi channel slotted ring network with a single tunable transmitter and fixed receiver , and one queue for storing cells for each channel at each node . to analyze network performance , this network is translated to the proposed non preemptive priority queue model . with this model , the analytical cell delay approximations can be obtained by close form formulas . the analysis considers two network environments where the number of nodes equals the number of channels and the number of nodes is a multiple of the number of channels . the accuracy of the model is assessed using the simulation results , and the two are found to be very close . in addition to its appropriateness for the multi channel slotted ring network , the proposed analytical model can also derive the cell delay results of a single channel slotted network with the destination removal policy . ( c ) <digit> elsevier science b.v. all rights reserved .
simulation , shared spaces , and information management to support discovery and learning an application in emergency management . <eos> in the academic enterprise , research and education are closely linked , and often highly complementary . yet guidelines for developing internet based systems to support this linkage are few , and many of the promising tools have costs that put them out of reach of students . this paper reports on the development and evaluation of an open source based online workspace for supporting integrated research and education activities in the domain of emergency management . motivating the workspace design is the notion that experience based education programs create opportunities to generate and acquire knowledge individually and in groups . accordingly , the workspace is organized to enable close integration between processes of discovery and learning . of particular interest is the use of computer simulation , shared workspaces , and information management tools to support learning . this work contributes to our understanding of how research and education activities may be integrated and undertaken for mutual benefit within an online environment . ( c ) <digit> wiley periodicals , inc. comput appl eng educ 20 232 238 , <digit> view this article online at wileyonlinelibrary.com journal cae doi 10.1002 cae .20389
interactive relighting with dynamic brdfs . <eos> we present a technique for interactive relighting in which source radiance , viewing direction , and brdfs can all be changed on the fly . in handling dynamic brdfs , our method efficiently accounts for the effects of brdf modification on the reflectance and incident radiance at a surface point . for reflectance , we develop a brdf tensor representation that can be factorized into adjustable terms for lighting , viewing , and brdf parameters . for incident radiance , there exists a non linear relationship between indirect lighting and brdfs in a scene , which makes linear light transport frameworks such as prt unsuitable . to overcome this problem , we introduce precomputed transfer tensors ( ptts ) which decompose indirect lighting into precomputable components that are each a function of brdfs in the scene , and can be rapidly combined at run time to correctly determine incident radiance . we additionally describe a method for efficient handling of high frequency specular reflections by separating them from the brdf tensor representation and processing them using precomputed visibility information . with relighting based on ptts , interactive performance with indirect lighting is demonstrated in applications to brdf animation and material tuning .
software retrieval by samples using concept analysis . <eos> finding and retrieving software components is one of the tasks of the building block approach to software reuse . one interesting property of code components unlike other types of software artifacts is that they can be executed . the execution based retrieval process tends to be too long to be incorporated in practice and faces the problem of non termination and very long execution time . this paper describes a software component retrieval method using sample inputoutput behavior of the components ( but without actual execution ) based on concept analysis . the retrieval uses samples chosen by the developers of the components ( rather than generated randomly or provided by the users ) . based on the validity relation between components and samples , a concept lattice is constructed for the library by applying formal concept analysis . the user retrieves components by selecting valid samples incrementally for a desired component from a dynamically created menu of samples available in the library . our method avoids the problems associated with actual execution based retrieval such as non termination and very long execution time , and also improves the retrieval time . our approach can be directly applied to other levels of software components than code components as long as the components can be described in terms of some inputoutput relation .
a stable high order finite difference scheme for the compressible navierstokes equations , far field boundary conditions . <eos> we construct a stable high order finite difference scheme for the compressible navierstokes equations , that satisfy an energy estimate . the equations are discretized with high order accurate finite difference methods that satisfy a summation by parts rule . the boundary conditions are imposed with penalty terms known as the simultaneous approximation term technique . the main result is a stability proof for the full three dimensional navierstokes equations , including the boundary conditions . we show the theoretical third , fourth , and fifth order convergence rate , for a viscous shock , where the analytic solution is known . we demonstrate the stability and discuss the non reflecting properties of the outflow conditions for a vortex in free space . furthermore , we compute the three dimensional vortex shedding behind a circular cylinder in an oblique free stream for mach number 0.5 and reynolds number <digit> .
compact explosion diagrams . <eos> this paper presents a system to automatically generate compact explosion diagrams . inspired by handmade illustrations , our approach reduces the complexity of an explosion diagram by rendering an exploded view only for a subset of the assemblies of an object . however , the exploded views are chosen so that they allow inference of the remaining unexploded assemblies of the entire 3d model . in particular , our approach demonstrates the assembly of a set of identical groups of parts , by presenting an exploded view only for a single representative . in order to identify the representatives , our system automatically searches for recurring subassemblies . it selects representatives depending on a quality evaluation of their potential exploded view . our system takes into account visibility information of both the exploded view of a potential representative as well as visibility information of the remaining unexploded assemblies . this allows rendering a balanced compact explosion diagram , consisting of a clear presentation of the exploded representatives as well as the unexploded remaining assemblies . since representatives may interfere with one another , our system furthermore optimizes combinations of representatives . throughout this paper we show a number of examples , which have all been rendered from unmodified 3d cad models .
delay analysis of cmos gates using modified logical effort model . <eos> in this paper , modified logical effort ( mle ) technique is proposed to provide delay estimation for cmos gates . the model accounts for the behavior of series connected mosfet structure ( scms ) , the input transition time , and internodal charges . also , the model takes into account deep submicron effects , such as mobility degradation and velocity saturation . this model exhibits good accuracy when compared with spectre simulations based on bsim3v3 model . using umc 's 0.13 mu m and tsmc 's 0.18 mu m technologies , the model has an average error of 4.5 % and a maximum error of <digit> % .
conflict cause identification in web based concurrent engineering design system . <eos> modern engineering design commonly involves the , cooperative effort of design experts , who often come from different engineering backgrounds , have different geological locations , and communicate primarily via the web . conflicts , which commonly exist in the design process , are exacerbated by this situation . this paper first presents a well organized structure of conflict classes and causes . based on an analysis of conflict nature and conflict related human behavior in the concurrent engineering design , a new computational model for autonomously identifying the conflict cause in the web based concurrent engineering design environment is developed and tested . the plane world design simulation system is used as a test bed to demonstrate and verify the conflict cause identification methods developed in this work .
online hodgerank on random graphs for crowdsourceable qoe evaluation . <eos> hodgerank on random graphs is proposed recently as an effective framework for multimedia quality assessment problem based on paired comparison methods . with a random design on graphs , it is particularly suitable for large scale crowd sourcing experiments on the internet . however , there still lacks a systematic study about online schemes to deal with the rising streaming and massive data in crowdsourceable scenarios . to fill in this gap , we propose in this paper an online ranking rating scheme based on stochastic approximation of hodgerank on random graphs for quality of experience ( qoe ) evaluation , where assessors and rating pairs enter the system in a sequential or streaming way . the scheme is shown in both theory and experiments to be efficient in obtaining global ranking by exhibiting the same asymptotic performance as batch hodgerank under a general edge independent sampling process . moreover , the proposed framework enables us to monitor topological changement and triangular inconsistency in real time . among a wide spectrum of choices , two particular types of random graphs are studied in detail , i.e. , erdos renyi random graph and preferential attachment random graph . the former is the simplest i.i.d. ( independent and identically distributed ) sampling and the latter may achieve more efficient performance in ranking the top items due to its rich get richer property . we demonstrate the effectiveness of the proposed framework on live and ivc databases .
italic detection and rectification . <eos> in this paper , a novel italic detection and rectification method without the prerequisite of character recognition is proposed . an italic style character can be obtained by performing shear transformation on its corresponding non italic style character . traditional italic detection methods have to be operated at least on the word , sentence or even the whole paragraph . the merit of the proposed method is that it can be operated directly on a single character so that more accurate statistical information can be obtained . the rationale of our proposed method is that the difference of certain features derived from italic style characters after shear transformation will be canceled , whereas the difference will be more obvious for non italic style ( normal style ) characters . in our proposed approach , the virtual strokes embedded in the considered character image are extracted first . then , reverse transformation is operated on the considered character image . the <digit> upper and <digit> lower alphabets are classified into three classes based on the structural information of the extracted virtual strokes . the italic and non italic style characters can then be distinguished based on the classification rule devised for each class of characters . last , the exact shear angle of the identified italic character is calculated to perform more accurate reverse shear transformation to rectify the italic style character into normal ( non italic ) style character to facilitate the later ocr task . experiments were conducted on <digit> document images with mixed italic and normal style characters . satisfactory accuracy rate 99.59 % for italic style characters and 99.85 % for normal style characters are achieved . experimental results verify the validity of our proposed method in distinguishing italic and non italic style characters .
towards context aware wireless spectrum agility . <eos> spectrum agility ( sa ) is a novel way of improving spectrum utilization efficiency and making greater bandwidth available to network applications . despite its advantages , the fundamental operations ( e.g. , periodic spectrum sensing ) involved in realizing sa can also adversely affect application performance . we propose context aware spectrum agility ( casa ) , which uses application hints together with the current channel condition to adapt key sa parameters . our preliminary evaluation shows that casa improves throughput by <digit> % , and is also more effective in meeting application demands than conventional sa .
dynamic object viewers for data structures . <eos> the jgrasp lightweight ide has been extended to provide object viewers that automatically generate dynamic , state based visualizations of data structures in java . these viewers provide multiple synchronized visualizations of data structures as the user steps through the source code in either debug or workbench mode . this tight integration in a lightweight ide provides a unique and promising environment for learning data structures . initial classroom use has demonstrated the object viewers ' potential as an aid to students who are learning to write and modify classes representing data structures . recently completed controlled experiments with cs2 students indicate that these viewers can have a significant positive impact on student performance .
reactive provisioning of backend databases in shared dynamic content server clusters . <eos> this paper introduces a self configuring architecture for on demand resource allocation to applications in a shared database cluster . we use a unified approach to load and fault management based on data replication and reactive replica provisioning . while data replication provides scaling and high availability , reactive provisioning dynamically allocates additional replicas to applications in response to peak loads or failure conditions , thus providing per application performance . we design an efficient method for data migration when joining a new replica to a running application that allows for the quick addition of replicas with minimal disruption of transaction processing . furthermore , by augmenting the adaptation feedback loop with awareness of the delay introduced by the data migration process in our replicated system , we avoid oscillations in resource allocation . we investigate our transparent database provisioning mechanisms in the context of multitier dynamic content web servers . we dynamically expand contract the respective allocations within the database tier for two different applications , the tpc w e commerce benchmark and the rubis online auction benchmark . we demonstrate that our techniques provide quality of service under different load and failure scenarios .
defending rfid authentication protocols against dos attacks . <eos> in this paper , we present a security weakness of a forward secure authentication protocol proposed by tri van le et al. called o frap which stands for optimistic forward secure rfid authentication protocol . in particular , we point out that in the o frap protocol , the server can be subject to a denial of service attack due to a flaw in the database querying procedure . our attack also applies to a simplified version of o frap called o rap ( optimistic rfid authentication protocol ) which is essentially o frap but without a secret key updating procedure ( and thus forward security ) . we then propose two improved protocols called o frap . and o rap . which prevent the said denial of service attack . in addition , the o frap protocol also addresses two security weaknesses of o frap pointed out earlier by khaled and raphael . in terms of performance , comparing to o frap , o frap ( ) requires a few more computational steps but much less storage at the back end server . ( c ) <digit> elsevier b.v. all rights reserved .
dsp based real time platform for remote control of internet connected systems . <eos> this work presents the development of an experimental platform for remote control of internet connected systems . the aim is to describe and to evaluate this platform , which exhibits some interesting peculiarities w.r.t. the mainstream literature real time support over the internet , capacity to handle loss disorder of packets , and varying delays . practical tests are given to show its capabilities to telecontrol different systems . the evaluation is carried out comparing the performance of some controllers for position coordination of a pair of localremote internet connected systems . besides , an assessment of platform usage and learning effectiveness was developed to conclude on its pedagogical viability in engineering education . <digit> wiley periodicals , inc. comput appl eng educ 21 203213 , <digit>
scalable similarity search of timeseries with variable dimensionality . <eos> timeseries can be similar in shape but differ in length . for example , the sound waves produced by the same word spoken twice have roughly the same shape , but one may be shorter in duration . stream data mining , approximate querying of image and video databases , data compression , and near duplicate detection are applications that need to be able to classify or cluster such timeseries , and to search for and rank timeseries that are similar to a chosen timeseries . we demonstrate software for clustering and performing similarity search in databases of timeseries data , where the timeseries have high and variable dimensionality . our demonstration uses timeseries sensitive hashing ( tsh ) <digit> to index the timeseries . tsh adapts locality sensitive hashing ( lsh ) , which is an approximate algorithm to index data points in a d dimensional space under some ( e.g. , euclidean ) distance function . tsh , unlike lsh , can index points that do not have the same dimensionality . as examples of the potential of tsh , the demonstration will index and classify timeseries from an image database and timeseries describing human motion extracted from a video stream and a motion capture system .
the measurement of membership by subjective ratio estimation . <eos> when we perform algebraic operations on membership degrees , we must be sure that they are compatible with the kind of scale on which these degrees are measured . for example , we will not add ( in a weighted average ) membership degrees that are measured on an ordinal scale . an important question is therefore how to determine on which kind of scale we measure the membership . there are several techniques for measuring membership . in this paper , we present some of them , based on stevens ' technique of ratio estimation , and we characterize them , thereby providing a sound way to determine the level of measurement . we also discuss the problem of the representation of the union or intersection by a t conorm or t norm . ( c ) <digit> elsevier b.v. all rights reserved .
a molecular field based similarity study of non nucleoside hiv <digit> reverse transcriptase inhibitors . <eos> this article describes a molecular field based similarity method fur aligning molecules by matching their steric and electrostatic fields and an application of the method to the alignment of three structurally diverse non nucleoside hiv <digit> reverse transcriptase inhibitors . a brief description of the method , as implemented in the program mimic , is presented , including a discussion of pairwise and multi molecule similarity based matching . the application provides an example that illustrates how relative binding orientations of molecules can be determined in the absence of detailed structural information on their target protein . in the particular system studied here , availability of the x ray crystal structures of the respective ligand protein complexes provides a means for constructing an ' experimental model ' of the relative binding orientations of the three inhibitors . the experimental model is derived by using mimic to align the steric fields of the three protein p66 subunit main chains , producing an overlay with a 1.41 angstrom average rms distance between the corresponding c alpha 's in the three chains . the inter chain residue similarities for the backbone structures show that the main chain conformations are conserved in the region of the inhibitor binding site , with the major deviations located primarily in the ' finger ' and rnase h regions . the resulting inhibitor structure overlay provides an experimental based model that can be used to evaluate the quality of the direct a priori inhibitor alignment obtained using mimic . it is found that the ' best ' pairwise alignments do not always correspond to the experimental model alignments . therefore , simply combining the best pairwise alignments will nor necessarily produce the optimal multi molecule alignment . however , the best simultaneous three molecule alignment was found to reproduce the experimental inhibitor alignment model . a pairwise consistency index has been derived which gauges the quality of combining the pairwise alignments and aids in efficiently forming the optimal multi molecule alignment analysis . two post alignment procedures are described that provide information on feature based and field based pharmacophoric patterns . the former corresponds to traditional pharmacophore models and is derived from the contribution of individual atoms to the total similarity . the latter is based on molecular regions rather than atoms and is constructed by computing the percent contribution to the similarity of individual points in a regular lattice surrounding the molecules , which when contoured and colored visually depict regions of highly conserved similarity . a discussion of how the information provided by each of the procedures is useful in drug design is also presented .
preserving avatar genuineness in different display media . <eos> our research focuses on enabling users to interact with others using 3d avatars with the same appearance and personality in different media such as the internet , sms or television and using different devices such as pcs , macs , pdas , mobile phones and televisions . this works main contribution is its use of a unique architecture , tested with a contact application , which is compatible with different media . the avatar appearance editor , the animation engine and the 3d models are the same for the different media tv , sms and internet chat .
a logic based computational method for the automated induction of fuzzy ontology axioms . <eos> fuzzy description logics ( dls ) are logics that allow to deal with structured vague knowledge . although a relatively important amount of work has been carried out in the last years concerning the use of fuzzy dls as ontology languages , the problem of automatically managing the evolution of fuzzy ontologies has received very little attention so far . we describe here a logic based computational method for the automated induction of fuzzy ontology axioms which follows the machine learning approach of inductive logic programming . the potential usefulness of the method is illustrated by means of an example taken from the tourism application domain .
automatic extraction of moving objects for head shoulder video sequence . <eos> recently , video object extraction has received great attention because it is a critical technique in object based video processing . this paper presents a temporal to spatial segmentation technique to extract object from a video sequence . the temporal phase employs a simple blockwise temporal activity measure to approximately locate the object boundary . and then a block based maximum a posteriori ( map ) scheme , which exploits spatial features of image blocks around the approximated boundary , is adopted to refine the temporal segmentation result . the proposed technique achieves good segmentation quality with very low computational cost for head and shoulder sequences with static background . ( c ) <digit> elsevier inc. all rights reserved .
adding dynamic reconfiguration support to jboss aop . <eos> the majority of aspect oriented middlewares supporting dynamic aspect weaving fail to preserve important safety properties while weaving or unweaving a distributed aspect at runtime . this position paper looks in particular at the safety properties of structural integrity and global state consistency . preserving these two safety properties in the presence of dynamic change has already been extensively addressed in the space of dynamic reconfiguration of component based distributed systems . as will be argued in this position paper , existing coordination protocols developed in this space can be largely reused for distributed aspect weaving provided that some small adaptations are made to account for the aspect oriented composition mechanisms . to demonstrate results and as a proof of concept , we describe how we have ported the necoman dynamic reconfiguration support on top of the jboss aop framework . as a result , system wide consistency can be preserved in jboss when weaving or unweaving a distributed aspect at runtime .
search based procedural generation of maze like levels . <eos> a correctly designed dynamic programming algorithm can be used as a fitness function to permit the evolution of maze like levels for use in games . this study compares multiple representations for evolvable mazes including direct , as well as positive and negative indirect representations . the first direct representation simply specifies , with a binary gene , which squares of a grid are obstructed . the second paints the maze grid and passage is allowed only between colors that are the same or adjacent in a rainbow . the positive and negative representations are developmental and evolve directions for adding barriers or digging tunnels . these representations are tested with a design space of fitness functions that automatically generate levels with controllable properties . fitness function design is the most difficult part of automatic level generation and this study gives a simple framework for designing fitness functions that permits substantial control over the character of the mazes that evolve . this technique relies on using checkpoints within the maze to characterize the connectivity and path lengths within the level . called checkpoint based fitness , these fitness functions are built on a menu of properties that can be rewarded . the choice of which qualities are rewarded , in turn , specifies within broad limits the characteristics of the mazes to be evolved . three of the representations are found to benefit from a technique called sparse initialization in which a maze starts mostly empty and variation operators fill in details while increasing fitness . different representations are found to produce mazes with very different appearances , even when the same fitness function is used . the example fitness functions designed around dynamic programming with checkpoints are found to permit substantial control over the properties of the evolved mazes .
idea discovery a scenario based systematic approach for decision making in market innovation . <eos> a new trend of researches on knowledge discovery and chance discovery is to identify human insights through data synthesis rather than to discover facts through data analysis . in this paper , we propose a systematic approach named idea discovery which is committed to turning data into effective human insights . idea discovery focuses on dynamic and sustainable process for high quality ideas cultivation , construction , integration and evaluation through humancomputer and humanhuman interaction . it mainly relies on latent information and its dynamic changes to drive ideas creation , integration and evaluation during sustainable creativity process . the process of idea discovery is in accordance with a dynamic model which contains two key components ( <digit> ) mining algorithms to turn data into scenario maps for eliciting human insights ( <digit> ) scenario based creativity support activities towards actionable ideas generation . an intelligence system called galaxy integrated with ideagraph algorithm has been developed to support the dynamic process of idea discovery . a case study in an automobile company has validated the effectiveness of proposed method and system .
decoupled access execute computer architectures . <eos> an architecture for improving computer performance is presented and discussed . the main feature of the architecture is a high degree of decoupling between operand access and execution . this results in an implementation which has two separate instruction streams that communicate via queues . a similar architecture has been previously proposed for array processors , but in that context the software is called on to do most of the coordination and synchronization between the instruction streams . this paper emphasizes implementation features that remove this burden from the programmer . performance comparisons with a conventional scalar architecture are given , and these show that considerable performance gains are possible . single instruction stream versions , both physical and conceptual , are discussed with the primary goal of minimizing the differences with conventional architectures . this would allow known compilation and programming techniques to be used . finally , the problem of deadlock in such a system is discussed , and one possible solution is given .
fault model independent , maximal compaction of test responses in the presence of unknown response bits . <eos> test response compaction offers test time and data volume reduction on the output side . an on chip circuitry , denoted as the response compactor , is utilized , compressing the responses of the circuit , and thus enabling the storage of compacted responses on the tester memory . while the test cost is thus reduced , such a circuitry may result in error masking , and hence the degradation of the fault error coverage level . response bits that are unknown during simulation time pose additional challenges on the design of a response compactor , as they contribute to error masking also . assumptions regarding a particular fault model and or distribution of unknown response bits ease the design of a response compactor however , the coverage loss of unmodeled faults is inevitable in the presence of such a compactor . furthermore , modeled faults also may become unobserved if the distribution of unknown bits deviates from the assumed one . in this paper , we propose a response compaction technique that is independent of any fault model . we design the response compactor based on the expected responses of the circuit under test . as a result , any originally detectable unmodeled defect or modeled fault is still detectable with the proposed compactor , regardless of the number and the distribution of the unknown response bits . the output bit width of the proposed response compactor is also the minimum that can be achieved when original defect and fault coverage levels are delivered . we also present an analysis that can be utilized for a quick computation of parameters , such as the lower and upper bounds and the expected value for the optimal output bandwidth , which the proposed compaction methodology is capable of attaining . as the proposed technique is a test set dependent approach , it is more suitable for application in the domain of core based system on chips ( socs ) , wherein a set of test vectors and expected responses is delivered along with a core . parallelism among core tests is increased by narrowing down the bit width for each core , delivering test time reduction for the socs .
vrml in cancer research local molecular properties of the p53 tumor suppressor protein dna interface . <eos> the three dimensional structure information of the p53 core domain dna complex is presented . the virtual reality modeling language ( vrml ) , a new concept of information transfer is used for this biochemical application . vrml provides an object oriented method for the three dimensional description of molecular models <digit> , <digit> . this vrml www pages contain the 3d structure of the p53 core domain dna complex crystallized by pavletich and coworkers <digit> , now available from the brookhaven protein databank , and the p53 dna binding region with related biochemical information like hydrophilicity lipophilicity and local electrostatic partial charges . these investigations are done in a close cooperation with h. bartsch and his group at the division of toxicology cancer risk factor prevention of the german cancer research center in heidelberg , germany .
real time simulation of electric machine drives with hardware in the loop . <eos> purpose this paper seeks to present a fully digital , real time ( rt ) hardware in the loop ( hil ) simulator on pc cluster , of electric systems and drives for research and education purposes to use the developed system to conduct several motor drives implementation and to evaluate the motor and the control algorithm performance in rt. design methodology approach this simulator was developed with the aim of meeting the simulation needs of electromechanical drives and power electronics systems while solving the limitations of traditional rt simulators . this simulator has two main subsystems , software and hardware . the two subsystems were coordinated together to achieve the rt simulation . the software subsystem includes matlab simulink environment , a c compiler and rt shell . the hardware subsystem includes fpga data acquisition card , the control board , the sensors , and the controlled motor . findings the complexity of rt implementation of motor drives is greatly reduced by utilizing this simulator . the detailed operation and implementation of this simulator are presented , together with test results and comparisons with simulated virtual environment for a permanent magnet dc and induction motors ( im ) . the simulator performance is adequate for both open and closed loops motor drives . the simulation time step is limited by the system master target cpu 's speed , the communication network type , and the complexity of the control algorithm . practical implications a typical application for this system is to select and evaluate the performance of electric motors for a hybrid electric vehicle in a real vehicle environment without actually installing that component in the real vehicle . originality value the use of the developed rt simulator to achieve hil simulation allows rapid prototyping , converter inverter topologies testing , motors testing , and control strategies evaluation . the transition from simulated virtual environment to the hil mode can be performed by replacing the model of the physical system ( e.g. motor ) with the daq blocks to represent the channels connected to the physical system sensors . the use of a single environment for both simulation and hil control provides a quick experimentation and performance comparison between the real and simulated systems .
performance evaluation of cai and rai transmission modes in a gi g <digit> queue . <eos> in this paper , a discrete time single server queue is considered with an uncorrelated message arrival process and an uncorrelated server interruption process . the length of the messages is modeled as a series of positive i.i.d. random variables and two operation modes are considered . in continue after interruption mode ( cai ) the processing of an interrupted message resumes with the next packet of this message . in repeat after interruption mode ( rai ) , the complete message is reprocessed after a server interruption . using a generating functions approach , explicit expressions for the mean and variance of buffer occupancy and message delay are derived for both operation modes . for illustration purposes , a discrete time pois geo <digit> system is then evaluated . discrete time queueing theory has gained a lot of interest recently to describe queueing related phenomena in an environment where time is slotted , such as various digital computer and communication systems including mobile and b isdn networks based on asynchronous transfer mode technology . this work focuses on a discrete time queueing system with server interruptions , which can occur when several users have to share a common resource , or when this resource is temporarily unavailable due to external causes such as maintenance or system failures . the contribution of this paper mainly lies in the model that is considered , the solution method that is applied , as well as the results that are generated . first , we want to introduce a model with an interrupted service process that allows us to compare an environment where interrupted messages must be retransmitted with the situation where this is not the case . second , we will show that a generating functions approach is extremely suitable for solving such a queueing model . and finally , we want to establish closed form formulas in terms of the system parameters that enable us to assess the system performance for the two transmission modes that are considered .
optimal backup policy for a database system with incremental and full backups . <eos> the fundamental recovery technique for some medium failures in a database system is regularly carried out by executing a full backup that takes all copies of updated files . however , the overhead of such full backup becomes sometimes very large in a massive database system . to lessen the overhead of backups , an incremental backup with small overhead , which takes only copies of newly updated files , is usually adopted in most database systems . however , the overhead of an incremental backup increases in proportion to the total amount of updated files . it would be necessary to determine when to make full backups . this paper proposes a stochastic model with incremental and full backups the expected costs incurred for two backups are obtained , and an optimal full backup interval which minimizes them is discussed . it is shown that an optimal interval is given by a finite and unique solution of an equation under suitable conditions . finally , a numerical example is given and some useful discussions are made . ( c ) <digit> elsevier ltd. all rights reserved .
efficiency criteria for environmental model quality assessment areview and its application to wastewater treatment . <eos> <digit> efficiency criteria of model quality in environmental sciences are reviewed . after studying equivalence of functional form <digit> efficiency criteria remain . a methodology is proposed to quantify dissimilarity between remaining criteria . the methodology is applied to a wastewater treatment plant case study . dissimilarity is assessed as a function of target variables and operating conditions .
energy based correlation method for location of partial discharge in transformer winding . <eos> partial discharge ( pd ) is the major source of insulation failure in power transformer . when transformers are subjected to electrical stress during operation , pd can occur . pd identification is an important diagnostic tool for the reliable operation of transformers . the pd signal detection and location is one of the main challenges for system utilities and equipment manufacturers . in this paper energy based correlation method is proposed for locating the source of pd for different pulse durations . simulation and experiment are performed on lumped physical layer winding to prove the feasibility of the method and also verified with distributed model of 22kv prototype interleaved winding .
a randomized pricing decision support system in electronic commerce . <eos> the internet has provided great convenience for online shoppers and has presented unprecedented opportunities for online retailers to understand their customers . getting the pricing right has emerged as one of the ultimate keys to the success of electronic commerce . although some online retailers have tried some personalized pricing strategies for perishable capacity or inventory in some industries , consumers ' resistance to price discrimination is still a great concern . can we develop other price discrimination strategies for online sellers to sell standard durable products without giving the impression that they are treating their customers unfairly randomized pricing , which is proposed in this paper , belongs to this kind of strategy . in this paper , we present a framework that can be used to study the randomized pricing strategy by incorporating some new features into electronic commerce . for example , information asymmetry about the prices of products does not exist across internet users because of easy access to price information and very low searching cost . consumers ' reneging behavior is also considered . online consumers usually wait up to a certain period of time for deals . specifically , we model online retailers ' price variation as a markov process in which the price randomly switches between high level and low level . strategic consumers make a tradeoff between buying immediately at a high price with instant utility or buying later at a low price with a probability and discounted utility . we show in this paper that randomized pricing strategy can always generate more profit than flat pricing strategy . the effects of consumers ' patience and discount factor on optimal prices and promotion probability are studied . finally , we show that the optimal benefit that the retailer can obtain from hiding promotion probability depends on the value of the discount factor .
cafes a framework for intrachip application modeling and communication architecture design . <eos> this paper describes cafes , an extensible , open source framework supporting several tasks related to high level modeling and design of applications employing complex intrachip communication infrastructures . cafes comprises several built in models , including application , communication architecture , energy consumption and timing models . it also includes a set of generic and specific algorithms and additional supporting tools , which jointly with the cited models allow the designer to describe and evaluate applications requirements and constraints on specified communication architectures . several examples of the use of cafes underline the usefulness of the framework . some of these are approached in this paper ( i ) a realistic application captured at high level that has its computation time estimated after mapping at the clock cycle level ( ii ) a multi application system that is automatically mapped to a large intrachip network with related tasks occupying contiguous areas in the chip layout ( iii ) a set of mapping algorithms explored to define trade offs between run time and energy savings for small to large intrachip communication architectures .
test case comparison and clustering using program profiles and static execution . <eos> selection of diverse test cases and elimination of duplicates are two major problems in product testing life cycle , especially in sustained engineering environment . in order to solve these , we introduce a framework of test case comparison metrics which will quantitatively describe the distance between any arbitrary test case pair of an existing test suite , allowing various test case analysis applications . we combine program profiles from test execution , static analysis and statistical techniques to capture various aspects of test execution and compute a specialized test case distance measurement . using these distance metrics , we drive a customized hierarchical test suite clustering algorithm that groups similar test cases together . we present an industrial strength framework called spirit that works at binary level , implementing different metrics in the form of coverage , control , data , def use , temporal variances and does test case clustering . this is step towards integrating runtime analysis , static analysis , statistical techniques and machine learning to drive new generation of test suite analysis algorithms .
effects of project size and resource constraints on project duration through priority rule base heuristics . <eos> priority rules are one of the frequently used methods in project programming with resource constraints . in this paper , the effects of project size and number of resource constraints on project duration are compared to the performances of pre selected priority rules . ten projects in different sizes have been programmed with <digit> , <digit> , <digit> , <digit> , and <digit> limited resource conditions by means of mrpl ( maximum remaining path length ) , lft ( latest finish time ) , mnslck ( minimum slack time ) , eft ( earliest finish time ) , and lst ( latest start time ) priority rules . when the number of resource constraints is low , the performance of mrpl is generally observed to be higher . as the number of resource constraints increases , a decrease in the performance of mrpl is observed in contrast with an increase in the performance of lft .
implicit solution of a two dimensional parabolic inverse problem with temperature overspecification . <eos> three different implicit finite difference schemes for solving the two dimensional parabolic inverse problem with temperature overspecification are considered . these schemes are developed for indentifying the control parameter which produces , at any given time , a desired temperature distribution at a given point in the spatial domain . the numerical methods discussed , are based on the second order ( <digit> , <digit> ) backward time centered space ( btcs ) implicit formula , and the second order ( 5,5 ) crank nicolson implicit finite difference formula and the fourth order ( 9,9 ) implicit scheme . these finite difference schemes are unconditionally stable . the ( 9,9 ) implicit formula takes a huge amount of cpu time , but its fourth order accuracy is significant . the results of a numerical experiment are presented , and the accuracy and central processor ( cpu ) times needed for each of the methods are discussed and compared . the implicit finite difference schemes use more central processor times than the explicit finite difference techniques , but they are stable for every diffusion number .
managing information technology investment risk a real options perspective . <eos> past information systems research on real options has focused mainly on evaluating information technology ( it ) investments that embed a single , a priori known option ( such as , deferral option , prototype option ) . in other words , only once a specific isolated option is identified as being embedded in a target it investment , does this research call upon using real options analysis to evaluate the option . in effect , however , because real options are not inherent in any it investment , they usually must be planned and intentionally embedded in a target it investment in order to control various investment specific risks , just like financial risk management uses carefully chosen options to actively manage investment risks . moreover , when an it investment involves multiple risks , there could be numerous ways to reconfigure the investment using different series of cascading ( compound ) options . in this light , we present an approach for managing it investment risk that helps to rationally choose which options to deliberately embed in an investment so as to optimally control the balance between risk and reward . we also illustrate how the approach is applied to an it investment entailing the establishment of an internet sales channel .
multi spectral texture segmentation based on the spectral cooccurrence matrix . <eos> multi spectral images are becoming more common in industrial inspection ia ks where the colour is used as a quality measure . in this paper we propose a spectral cooccurrence matrix based method tu analyse multi spectral texture images , in which every pixel contains a measured colour spectrum . we first quantise the spectral domain of the multi spectral images using the self organising mao ( som ) . next we label the spectral domain according to the quantised spectra . in the spatial domain , we represent a multi spectral texture using thf spectral cooccurrence matrix , which we calculate from the labelled image . in the experimental part of this paper , we present the results of segmenting natural multi spectral textures . we compared . the k nearest neighbour ( k nn ) classifier and the multilayer perceptron ( mlp ) neural network based segmentation results of the multi spectral and rgb colour textures .
evaluation of analytical models for thermal analysis and design of electronic packages . <eos> the objective of this study is to evaluate the use of several analytical compact heat transfer models for thermal design , optimization , and performance evaluation in electronic packaging . a model for heat spreading in orthotropic materials is developed . the developed model is used in conjunction with the other available heat transfer models in a resistance network for calculation of heat transfer rate and junction temperatures in a multi chip module ( mcm ) . refrigeration cooled mcm of an ibm server is used to illustrate the methodology . results of the analytical model and resistance network analysis are compared with a numerical solution . capability of the analytical model in predicting the thermal field is discussed and effectiveness of using the analytical models in thermal design and optimization of electronic packages is demonstrated .
image interpolation by adaptive <digit> d autoregressive modeling and soft decision estimation . <eos> the challenge of image interpolation is to preserve spatial details . we propose a soft decision interpolation technique that estimates missing pixels in groups rather than one at a time . the new technique learns and adapts to varying scene structures using a <digit> d piecewise autoregressive model . the model parameters are estimated in a moving window in the input low resolution image . the pixel structure dictated by the learnt model is enforced by the soft decision estimation process onto a block of pixels , including both observed and estimated . the result is equivalent to that of a high order adaptive nonseparable <digit> d interpolation filter . this new image interpolation approach preserves spatial coherence of interpolated images better than the existing methods , and it produces the best results so far over a wide range of scenes in both psnr measure and subjective visual quality . edges and textures are well preserved , and common interpolation artifacts ( blurring , ringing , jaggies , zippering , etc. ) are greatly reduced .
symbolic integration the stormy decade . <eos> three approaches to symbolic integration in the <digit> 's are described . the first , from artificial intelligence , led to slagle 's saint and to a large degree to moses ' sin . the second , from algebraic manipulation , led to manove 's implementation and to horowitz ' and tobey 's re examination of the hermite algorithm for integrating rational functions . the third , from mathematics , led to richardson 's proof of the unsolvability of the problem for a class of functions and for risch 's decision procedure for the elementary functions . generalizations of risch 's algorithm to a class of special functions and programs for solving differential equations and for finding the definite integral are also described .
highlighting data clusters by graph embedding . <eos> we propose a novel method , modularity embedding , to embed high dimensional data or graphs in a low dimensional space . central to our work is a model that quantifies the relationship of two data points by their pairwise modular value . a larger value indicates a higher chance that they should be placed near to each other , and vice versa . the objective function of the model has a simple formulation of minimizing the sum of squared distances between data points weighted by pairwise modular values . it is naturally relaxed as a semi definite program that learns a low rank kernel matrix with only one linear constraint , which can be solved efficiently by modern mathematical optimization solvers . compared with traditional graph embedding algorithms , the proposed method is shown to be able to highlight cluster structures inherent in high dimensional data and graphs , which provides a promising tool in data analysis applications .
a new control strategy for the optimization of distributed mppt in pv applications . <eos> in dmppt pv applications , the energetic efficiency depends on the several factors . in the mismatched pv systems , the joint adoption of the cmppt and dmppt techniques is necessary . a new control technique for the optimization of dmppt is proposed . the first advantage of the proposed technique is the high speed of tracking . the second advantage of the proposed technique is the high energetic efficiency .
modeling and verifying composite dynamic evolution of software architectures using hypergraph grammars . <eos> as software systems become more and more complex , there is need to consider not only data structures and algorithms but also the general structure or architecture of the system . many researchers have presently focused on dynamic evolution of software architectures . most of them usually emphasized on describing and analyzing the dynamic evolution process of software architectures , while lacking formally modeling and verifying composite dynamic evolution of software architectures . in this paper , we propose a formal method of modeling and verifying composite dynamic evolution of software architectures using hypergraph grammars . we represent software architectures with hypergraphs , give out corresponding composite evolution rules of software architectures , and then model composite dynamic evolution of software architectures according to those rules . at last we verify the liveness property of composite dynamic evolution of software architectures using model checking , and give out corresponding verification algorithms . our approach provides a graphical representation for composite dynamic evolution of software architectures , and displays a formal theoretical basis on grammars .
multi agent physical a with large pheromones . <eos> physical a ( pha ) and its multi agent version mapha are algorithms that find the shortest path between two points in an unknown real physical environment with one or many mobile agents a. felner et al. journal of artificial intelligence research , 21 631 679 , <digit> a. felner et al. proceedings of the first international joint conference on autonomous agents and multi agent systems , bologna , italy , 2002 240 247 . previous work assumed a complete sharing of knowledge between agents . here we apply this algorithm to a more restricted model of communication which we call large pheromones , where agents communicate by writing and reading data at nodes of the graph that constitutes their environment . previous works on pheromones usually assumed that only a limited amount of data can be written at each node . the large pheromones model assumes no limitation on the size of the pheromones and thus each agent can write its entire knowledge at a node . we show that with this model of communication the behavior of a multi agent system is almost as good as with complete knowledge sharing . under this model we also introduce a new type of agent , a communication agent , that is responsible for spreading the knowledge among other agents by moving around the graph and copying pheromones . experimental results show that the contribution of communication agents is rather limited as data is already spread to other agents very well with large pheromones .
personal media exploration with semantic regions . <eos> computer users deal with large amount of personal media data and they often face problems in managing and exploring them . the paper presents an innovative approach , semantic regions that are rectangular regions directly drawn on 2d space with semantics so that their layout can form users ' various mental models toward the personal media data . a prototype personal media exploring application , mediafinder , based on the concept of semantic regions is presented . usability tests will be conducted to evaluate the semantic regions as a personal media management model including organization , search , navigation , indexing , meaning extraction , and distribution .
new cfoa based sinusoidal oscillators retaining independent control of oscillation frequency even under the influence of parasitic impedances . <eos> there have been two efforts earlier on evolving cfoa based fully uncoupled oscillators i.e. circuits in which none of the resistors controlling the frequency of oscillation ( fo ) appear in the condition of oscillation and vice versa . however , a non ideal analysis of the earlier known circuits reveals that due to the effect of the parasitic impedances of the cfoas , the independent controllability of fo is completely destroyed . the main objective of this paper is to present two new fully uncoupled oscillators in which the independent controllability of the fo remains intact even under the influence of the non ideal parameters parasitics of the cfoas employed . the workability of the proposed circuits has been confirmed by experimental results using ad844 type cfoas .
transformation from committed progenitor to leukemia stem cells . <eos> leukemias are composed of a hierarchy of cells , only a fraction of which have stem celllike properties and are capable of self renewal . mixed lineage leukemia ( mll ) fusion proteins produced by translocations involving the mll gene on chromosome 11q23 confer stem celllike properties on committed hematopoietic progenitors . this provides an opportunity to assess changes in immunophenotype , gene expression , and epigenetic programs during the transition from a hematopoietic cell with minimal inherent self renewal capability to cells capable of leukemic self renewal .
opportunistic routing with in network aggregation for duty cycled wsns with delay requirements . <eos> this paper proposes an opportunistic routing protocol for wireless sensor networks that works on top of an asynchronous duty cycling medium access control ( mac ) protocol . the proposed protocol is designed for applications that are not real time but still have some requirements on packet delay . the main idea is that if a packet has time to spare , it can wait on a node hoping that it can be aggregated with other packets , resulting in reduced number of transmissions . the forwarders and the packet hold time depend on the energy status of nodes in the network . the simulation results show that the proposed protocol achieves longer network lifetime compared to the other state of the art protocols , while satisfying application delay requirements .
retrieval and constraint based human posture reconstruction from a single image . <eos> in this study , we present a novel model based approach to reconstruct the 3d human posture from a single image . the approach is guided by a posture library and a set of constraints . given a 2d human figure , i.e. , a set of labeled body segments and estimated root orientation in the image , a 3d pivotal posture whose 2d projection is similar to the human figure is first retrieved from the posture library . to facilitate the retrieval process , a table lookup technique is proposed to index postures according to their 2d projections with respect to designated view directions . next physical and environmental constraints , including segment length ratios , joint angle limits , pivotal posture reference , and feet floor contact , are automatically applied to reconstruct the 3d posture . experimental results show the effectiveness of the proposed approach . ( c ) <digit> elsevier inc. all rights reserved .

' obsessive compulsive font disorder ' the challenge of supporting pupils writing with the computer . <eos> writing with the computer provokes and enables pupils to engage with aspects of multimodal design multiliteracies literacy learning and the design of social futures , routledge , london , <digit> . at the same time the traditional stages of the writing process become much more fluid and integrated aust . j. language literacy <digit> ( <digit> ) ( <digit> ) <digit> . these consequences of technology are not recognised within the curriculum , the assessment system or current models of teaching the writing process in the uk . using examples from current classroom research this paper argues that the significance of pupils ' uses of the , available designs ' of digital experience multiliteracies literacy learning and the design of social futures , routledge , london , <digit> is undervalued . furthermore it suggests that this undervaluing leaves teachers without well developed pedagogic models of literacy when computers are involved . ( c ) <digit> elsevier ltd. all rights reserved .
fault tolerant evaluation of continuous selection queries over sensor data . <eos> we consider the problem of evaluating continuous selection queries over sensor generated values in the presence of faults . small sensors are fragile , have finite energy and memory , and communicate over a lossy medium hence , tuples produced by them may not reach the querying node , resulting in an incomplete and ambiguous answer , as any of the non reporting sensors may have produced a tuple which was lost . we develop a protocol , fault tolerant evaluation of continuous selection queries ( fate csq ) , which guarantees a user requested level of quality in an efficient manner . when many faults occur , this may not be achievable in that case , we aim for the best possible answer , under the query 's time constraints . fate csq is designed to be resilient to different kinds of failures . our design decisions are based on an analytical model of different fault tolerance strategies based on feedback and retransmission . additionally , we evaluate fate csq and competing protocols with realistic simulation parameters under a variety of conditions , demonstrating its good performance .
low power architectures for programmable multimedia processors . <eos> this paper describes low power architecture methodologies for programmable multimedia processors , which will become major functional units in system on a chip . after brief review on multimedia processing and low power considerations , recent programmable chips , including mpus and dsps , are investigated in terms of low power implementation . in order to show the difference of the low power approaches between programmable processors and asic processors , a single chip mpeg <digit> encoder is also included as an example of asic design .
modelling semantics across multiple time series and its applications . <eos> analysis based on the holistic multiple time series system has been a practical and crucial topic . in this paper , we mainly study a new problem that how the data is produced underneath the multiple time series system , which means how to model time series data generating and evolving rules ( here denoted as semantics ) . we assume that there exist a set of latent states , which are the system basis and make the system run data generating and evolving . thus , there are several challenges on the problem ( <digit> ) how to detect the latent states ( <digit> ) how to learn the rules based on the states ( <digit> ) what the semantics can be used for . hence , a novel correlation field based semantics learning method is proposed to learn the semantics . in the method , we first detect latent state assignment by comprehensively considering kinds of multiple time series characteristics , which contain tick by tick data , temporal ordering , relationship among multiple time series and so on . then , the semantics are learnt by bayesian markov characteristic . actually , the learned semantics could be applied into various applications , such as prediction or anomaly detection for further analysis . thus , we propose two algorithms based on the semantics knowledge , which are applied to make next n step prediction and detect anomalies respectively . some experiments on real world data sets were conducted to show the efficiency of our proposed method .
a calculus and logic of bunched resources and processes . <eos> mathematical modelling and simulation modelling are fundamental tools of engineering , science , and social sciences such as economics , and provide decision support tools in management . mathematical models are essentially deployed at all scales , all levels of complexity , and all levels of abstraction . models are often required to be executable , as a simulation , on a computer . we present some contributions to the process theoretic and logical foundations of discrete event modelling with resources and processes . building on previous work in resource semantics , process calculus , and modal logic , we describe a process calculus with an explicit representation of resources in which processes and resources co evolve . the calculus is closely connected to a substructural modal logic that may be used as a specification language for properties of models . in contrast to earlier work , we formulate the resource semantics , and its relationship with process calculus , in such a way that we obtain soundness and completeness of bisimulation with respect to logical equivalence for the naturally full range of logical connectives and modalities . we give a range of examples of the use of the process combinators and logical structure to describe system structure and behaviour .
fixed parameter complexity of minimum profile problems . <eos> the profile of a graph is an integer valued parameter defined via vertex orderings it is known that the profile of a graph equals the smallest number of edges of an interval supergraph . since computing the profile of a graph is an np hard problem , we consider parameterized versions of the problem . namely , we study the problem of deciding whether the profile of a connected graph of order n is at most n <digit> k , considering k as the parameter this is a parameterization above guaranteed value , since n <digit> is a tight lower bound for the profile . we present two fixed parameter algorithms for this problem . the first algorithm is based on a forbidden subgraph characterization of interval graphs . the second algorithm is based on two simple kernelization rules which allow us to produce a kernel with linear number of vertices and edges . for showing the correctness of the second algorithm we need to establish structural properties of graphs with small profile which are of independent interest .
synaptic background activity affects the dynamics of dendritic integration in model neocortical pyramidal neurons . <eos> neocortical pyramidal neurons in vivo are subject to an intense synaptic background activity which may significantly impact on dendritic integration , but this aspect is largely unexplored . here we use computational models of morphologically reconstructed pyramidal neurons , in which synaptic background activity was simulated according to recent measurements in cat parietal cortex . we show that background activity markedly enhances voltage attenuation , which results in a relative electrotonic isolation of different dendritic segments . on the other hand , the active propagation of action potentials in dendrites is minimally affected . the consequence is that inputs are integrated locally and their impact on the soma is independent on their position in the dendritic tree . we conclude that background activity sets up a dynamics of dendritic integration which is radically different compared to quiescent states .
shallow water model on cubed sphere by multi moment finite volume method . <eos> a global numerical model for shallow water flows on the cubed sphere grid is proposed in this paper . the model is constructed by using the constrained interpolation profile multi moment finite volume method ( cip mm fvm ) . two kinds of moments , i.e. the point value ( pv ) and the volume integrated average ( via ) are defined and independently updated in the present model by different numerical formulations . the laxfriedrichs upwind splitting is used to update the pv moment in terms of a derivative riemann problem , and a finite volume formulation derived by integrating the governing equations over each mesh element is used to predict the via moment . the cubed sphere grid is applied to get around the polar singularity and to obtain uniform grid spacing for a spherical geometry . highly localized reconstruction in cip mm fvm is well suited for the cubed sphere grid , especially in dealing with the discontinuity in the coordinates between different patches . the mass conservation is completely achieved over the whole globe . the numerical model has been verified by williamsons standard test set for shallow water equation model on sphere . the results reveal that the present model is competitive to most existing ones .
object oriented programming in boundary element methods using c . <eos> a new method of writing boundary element programmes using the programming paradigms known as object oriented programming ( oop ) is presented in this paper . among oop paradigms , c is more suited to numerical programming than a pure oop language , and the fact that c is chosen in this paper intends to illustrate the efficiency of object orient boundary element programming . the advantage of object orient boundary element programmingi.e . being superior to other paradigms such as fortranis shown by discussion of a sample c code of boundary element methods .
parallel computation of the topology of level sets . <eos> this paper introduces two efficient algorithms that compute the contourtree of a three dimensional scalar field f and its augmented version with the betti numbers of each isosurface . the contour tree is a fundamental data structure in scientific visualization that is used to pre process the domain mesh to allow optimal computation of isosurfaces with minimal overhead storage . the contour tree can also be used to build user interfaces reporting the complete topological characterization of a scalar field , as shown in figure <digit> . data exploration time is reduced since the user understands the evolution of level set components with changing isovalue . the augmented contour tree provides even more accurate information segmenting the range space of the scalar field into regions of invariant topology . the exploration time for a single isosurface is also improved since its genus is known in advance . our first new algorithm augments any given contour tree with the betti numbers of all possible corresponding isocontours in linear time with the size of the tree . moreover , we show how to extend the scheme introduced in <digit> with the betti number computation without increasing its complexity . thus , we improve on the time complexity in our previous approach <digit> from o ( m log m ) to o ( n log n m ) , where m is the number of cells and n is the number of vertices in the domain of f. our second contribution is a new divide and conquer algorithm that computes the augmented contour tree with improved efficiency . the approach computes the output contour tree by merging two intermediate contour trees and is independent of the interpolant . in this way we confine any knowledge regarding a specific interpolant to an independent function that computes the tree for a single cell . we have implemented this function for the trilinear interpolant and plan to replace it with higher order interpolants when needed . the time complexity is o ( n t log n ) , where t is the number of critical points of t. for the first time we can compute the contour tree in linear time in many practical cases where t o ( n ( <digit> epsilon ) ) . we report the running times for a parallel implementation , showing good scalability with the number of processors .
an overview of lessons learnt from ertms implementation in european railways . <eos> safety of european railways can be significantly enhanced through integration of modernised technology . ertms is a progressive way forward enabling railways to achieve this aim through continuous monitoring and data communication . the role of human factors , specifically actions provided by the train driver should always be included when considering the safety of the railway . the railway environment can not be devoid of humans so their role must be included to fully understand system operation .
two wheeled piezoelectric system . <eos> two wheeled piezoelectric system is proposed for applications in micro stepping displacement devices . the system includes a beam and two displacement members which are respectively pivoted on the beam . two displacement members are not rotatable . in addition , each displacement member includes a wheel sheet and a piezoelectricity element embedded on its surface . when the piezoelectricity element generates and transmits power to the wheel sheet , the wheel induces vibration and deformation . therefore , due to the wheel sheets and the touched ground involving their relative motion , the displacement device can move and orient its motion direction in a micro manner . the wheel piezoelectric system is direct movement , no rotor requirement . in this research , a <digit> d mechanical element with an extra electrical degree of freedom is employed to simulate the dynamic vibration modes of the linear piezoelectric , mechanical and piezoelectric mechanic behaviours of the wheel piezoelectric system .
efficiently supporting ad hoc queries in large datasets of time sequences . <eos> ad hoc querying is difficult on very large datasets , since it is usually not possible to have the entire dataset on disk . while compression can be used to decrease the size of the dataset , compressed data is notoriously difficult to index or access . in this paper we consider a very large dataset comprising multiple distinct time sequences . each point in the sequence is a numerical value . we show how to compress such a dataset into a format that supports ad hoc querying , provided that a small error can be tolerated when the data is uncompressed . experiments on large , real world datasets ( at t customer calling patterns ) show that the proposed method achieves an average of less than <digit> % error in any data value after compressing to a mere 2.5 % of the original space ( i.e. , a <digit> <digit> compression ratio ) , with these numbers not very sensitive to dataset size . experiments on aggregate queries achieved a 0.5 % reconstruction error with a space requirement under <digit> % .
convexity analysis of active contour problems . <eos> a general active contour formulation is considered and a convexity analysis of its energy function is presented . conditions under which this formulation has a unique solution are derived these conditions involve both the active contour energy potential and the regularization parameters . this analysis is then applied to four particular active contour formulations , revealing important characteristics about their convexity , and suggesting that external potentials involving center of mass computations may be better behaved than the usual potentials based on image gradients . our analysis also provides an explanation for the poor convergence behavior at concave boundaries and suggests an alternate algorithm for approaching these types of boundaries .
large network analysis for fisheries management using coevolutionary genetic algorithms . <eos> traditionally , a genetic algorithm is used to analyze networks by maximizing the modularity ( q ) measure to create a favorable community . a coevolutionary algorithm is used here to not only find the appropriate community division for a network , but to find interesting networks containing substantial changes in data within a very large network space . the network is one of the largest , if not the largest , analyzed by evolutionary computation techniques to date and is created using a real world data set consisting of fisheries catch data in the north atlantic ocean off the coast of canada . this work examines the quantitative performance of two types of coevolutionary algorithms against both a standard ga that uses a natural ( but not necessarily optimal ) division of the data set into communities , and simulated annealing . the goal for all search algorithms was to automatically find anomalies ( differences in catch ) within the data . to measure practical usefulness of the system , a fisheries expert analyzed the best networks located by the search algorithms using an existing visualization software prototype . the expert indicated that a refined version of coevolutionary ga known as pamdga was found to most reliably locate subnetworks containing catch differences of biological relevance .
tourists intention to visit a destination the role of augmented reality ( ar ) application for a heritage site . <eos> we investigated augmented reality ( ar ) application users intention to visit destination . ar application is developed as a part of smart tourism to provide information about destination . we conduct surveys and apply structural equation model to find the causal relationship . personal , stimulus , and situational factors are significantly affect the ar usage intention and destination visit intention . the findings explain why tourist use ar application and its influence to the intention to visit destination .
constrained constrained multi dimensional aggregation . <eos> the sql <digit> standard introduced window functions to enhance the analytical processing capabilities of sql . the key concept of window functions is to sort the input relation and to compute the aggregate results during a scan of the sorted relation . for multi dimensional olap queries with aggregation groups defined by a general condition an appropriate ordering does not exist , though , and hence expensive join based solutions are required . in this paper we introduce constrained constrained multi dimensional aggregation ( mda mda ) , which supports multi dimensional olap queries with aggregation groups defined by inequalities . mda mda is not based on an ordering of the data relation . instead , the tuples that shall be considered for computing an aggregate value can be determined by a general condition . this facilitates the formulation of complex queries , such as multi dimensional cumulative aggregates , which are difficult to express in sql because no appropriate ordering exists . we present algebraic transformation rules that demonstrate how the mda mda interacts with other operators of a multi set algebra . various techniques for achieving an efficient evaluation of the mda mda are investigated , and we integrate them into concrete evaluation algorithms and provide cost formulas . an empirical evaluation with data from the tpc h benchmark confirms the scalability of the mda mda operator and shows performance improvements of up to one order of magnitude over equivalent sql implementations .
quantitative business decision making for the investment of preventing safety accidents in chemical plants . <eos> this paper proposes a new quantitative method of supporting business decision making while investing safety related facility and service . this method suggests the priority of investment relevant to safety within limited budget , so most possible hazards can be removed or the company may not invest money for the acceptable hazards depending on the budget . the typical theory that risk is equivalent to the multiplication of frequency and severity , given by ccps ( center for chemical process safety ) in american institute of chemical engineers ( aiche ) , is modified to consider more detailed classifications . the criteria of decision are determined by the manager 's survey . computation of r matrix results the priority of investment and the investment is finally chosen by the decision criteria . this method is proved to be effective in reducing safety accidents by proper management through the analysis of real accident data of a korean petrochemical company for using yonsei safety information management system ( ysims ) . we collected <digit> accident data for one and a half year by using the ' detailed classification sheets ' including more than <digit> accident cause types . as a result of applying this system to the company , the number of accidents of class a and b was significantly reduced in <digit> months after the systematically chosen investment by <digit> % and <digit> % that was from <digit> to <digit> and from <digit> to <digit> , respectively . the aim of this study is to manage safety accidents properly and to prevent major accidents from petrochemical industry . ( c ) <digit> elsevier science ltd. all rights reserved .
from stress sensor towards back end of line embedded thermo mechanical sensor . <eos> a novel approach is proposed to investigate thermo mechanical properties of beol . design of a metallic stress sensor is modified to address electrical polarization . experiment is performed in situ sem using electrical nano probing . results are compared to analytical modeling and finite element method ( fem ) . stress , coefficient of thermal expansion and thermal conductivity are identified .
experimental characterization of veering crossing and lock in in simple mechanical systems . <eos> crossing , veering and lock in , are fully characterized in a simple <digit> dof system . . parameters influencing the behavioral are found in closed form the experiments proved that highlighted phenomena occur in real environment . experiments confirm the influence of the damping in determining the overall behaviour . systems with non uniform damping distribution may exhibit unusual dynamics .
a conditional probability approach to m m g g <digit> like queues . <eos> following up on a recently renewed interest in computational methods for m m g g <digit> type processes , this paper considers an m m g g <digit> like system in which the service time distribution is represented by a coxian series of memoryless stages . we presenta novel approach to the solution of such systems . our method is based on conditional probabilities , and provides a simple , computationally efficient and stable approach to the evaluation of the steady state queue length distribution . we provide a proof of the numerical stability of our method . without explicit use of matrix geometric techniques or stochastic complementation , we are able to handle systems with state dependent service and arrival rates . the proposed approach can be used to compute the queue length distribution for both finite and infinite m m g g <digit> like queues . in the case of an infinite , state independent queue , our method allows us to show using elementary tools that the queue length distribution is asymptotically geometric . the parameter of the asymptotic geometric can be expressed through a simple set of equations , easily solved using fixed point iteration . our approach is very thrifty in terms of memory requirements , easy to implement , andgenerally fast . numerical examples illustrate the performance of the proposed method .
comparative performance analysis of directed flow control for real time sci . <eos> the distributed nature of routing and flow control in a register insertion ring topology complicates priority enforcement for real time systems . two divergent approaches for priority enforcement for ring based networks are reviewed a node oriented scheme called preemptive priority queue and a ring wide arbitration approach dubbed train . this paper introduces a hybrid protocol named directed flow control that combines node and ring oriented flow control to yield greater performance . a functional comparison of the three protocols as implemented on the scalable coherent interface is presented , followed by performance results obtained through high fidelity modeling and simulation .
activity based matching in distributed camera networks . <eos> in this paper , we consider the problem of finding correspondences between distributed cameras that have partially overlapping field of views . when multiple cameras with adaptable orientations and zooms are deployed , as in many wide area surveillance applications , identifying correspondence between different activities becomes a fundamental issue . we propose a correspondence method based upon activity features that , unlike photometric features , have certain geometry independence properties . the proposed method is robust to pose , illumination and geometric effects , unsupervised ( does not require any calibration objects ) . in addition , these features are amenable to low communication bandwidth and distributed network applications . we present quantitative and qualitative results with synthetic and real life examples , and compare the proposed method with scale invariant feature transform ( sift ) based method . we show that our method significantly outperforms the sift method when cameras have significantly different orientations . we then describe extensions of our method in a number of directions including topology reconstruction , camera calibration , and distributed anomaly detection .
comparison of hybrid arq schemes and optimization key parameters for high speed packet transmission in w cdma forward link . <eos> this paper elucidates the most appropriate hybrid automatic repeat request ( arq ) scheme . i.e. , which can achieve the highest throughput , for high speed packet transmission ill the w cdma forward link by comparing the throughput performance of three types of hybrid arq schemes type i hy brid arq with packet combining ( pc ) , type ii hybrid arq , and basic type i hybrid arq as a reference . moreover , from the view point of maximum throughput , the respective optimum roles of arq and channel coding in hybrid arq are also clarified . such as the optimum coding rate and the packet length related to the interleaving effect . the simulation results reveal that the type ii scheme exhibits the best throughput performance , and the required received signal energy per chip to background noise spectral density ratio ( e c n <digit> ) at the throughput efficiency of 0.2 0.4 0.6 is improved by 0.7 0.3 0.1 db and 3.9 1.8 0.5 sdb . respectively , compared to the type i scheme with and without pc in a <digit> path rayleigh fading channel with the average equal power at the maximum doppler frequency of <digit> hz and the packet length of <digit> slots ( 0.667 x <digit> 2.667 msec ) . however . the improvement of the type ii scheme compared to the type i scheme with pc is small or the achievable throughput is almost identical in the high received e c n <digit> region . on the other hand , the type i scheme with pc is much less complex and thus preferable . while maintaining almost the same throughput performance allowing very minor degradation compared to that with type ii . the results also elucidate that , while the optimum coding rate depends on the required throughput in the basic type i and type ii with pc schemes it is around between <digit> <digit> and <digit> <digit> in type ii . resulting in a higher throughput efficiency . in addition , for highspeed packer transmission employing a hybrid arq scheme , a shorter retransmission unit size is preferable such as <digit> slot , and the fast transmit power control is effective only under conditions such as a tory maximum doppler frequency and a high transmit e c n <digit> region .
characterizations of maximal consistent theories in the formal deductive system l ( nm logic ) and cantor space . <eos> a maximal consistent theory is a maximal theory with respect to its consistency . the present paper is divided into two parts . the first one is devoted to characterize the maximality of a consistent theory in the formal deductive system l ( which is a logic system equivalent to the nilpotent minimum logic ) . it is proved that each maximal consistent theory in this logic must be the deductive closure of a collection of simple compound formulas . hence , it follows that there is a one to one correspondence between the set of all maximal consistent theories and the set of evaluations a assigning to each propositional variable pits truth degree e ( p ) is an element of <digit> , <digit> <digit> , <digit> . the satisfiability theorem and compactness theorem of l are obtained . the second part is to investigate the topological structure of the set of all maximal consistent theories over l , and the results show that this topological space is a cantor space . ( c ) <digit> elsevier b.v all rights reserved .
design and implementation of a java based meeting space over internet . <eos> this paper describes the implementation of java meeting space ( jms ) , a generic , extensible framework and environment for developing synchronous collaborative applications . the jms framework is based on a fully object oriented replicated architecture where the application instances and management services are all replicated at each site . jms provides basic cscw coordination services session management and dynamic floor control services . as a framework , it provides a set of programming interfaces that allow an application developer to take advantage of coordination services in the runtime environment .
the effect of provider characteristics on the responses to medication related decision support alerts . <eos> provider characteristics have effects on how providers respond to medication alerts . six physician characteristics were significantly related to the override rates . the combined six characteristics explained <digit> % of the prescription overrides variability . the combined six characteristics explained half of the alert overrides variability . consideration of specific physician characteristics may help to improve prescriber decision making .
can active tracking of inroamer location optimise a live gsm network . <eos> the mobile communication industry has experienced huge growth in recent years , which amplified the competition among telecommunication companies . this inspired the search for new means of gsm network optimization that would bring better signal coverage and thus competitive advantage . one of the possibilities is active tracking of inroamer location in order to find where they disappear to a rival network . data collected in this way can then be analyzed to detect weak points of network or traffic anomalies . this paper presents our implementation of active tracking of inroamer location within a live gsm network and the subsequent analysis of collected data , together with future plans for our platform called ss7tracker .
estimating germinability of plasmopara viticola oospores by means of neural networks . <eos> neural networks are trained to estimate the germination percentages of plasmopara viticola oospores , overwintered in natural conditions in two viticultural areas in northern italy , by using climatic ( temperature and rainfall ) data , as well as the previous germination measurement , as input variables . the <digit> available patterns consist of a set of selected independent variables associated with the corresponding germination percentage . all <digit> networks investigated converge to a non linear relationship between the selected independent variables and oospore germination . the highest correlation coefficient ( equal to 0.83 ) between the real and estimated germination percentages is obtained by considering , as input to the network , the climatic data ( both temperature and rainfall ) recorded during the <digit> days before sampling and the germination percentage assessed in the germination assay carried out immediately before the present sampling .
incremental mining of the schema of semistructured data . <eos> semistructured data are specified in lack of any fixed and rigid schema , even though typically some implicit structure appears in the data . the huge amounts of on line applications make it important and imperative to mine the schema of semistructured data , both for the users ( e.g. , to gather useful information and facilitate querying ) and for the systems ( e.g. , to optimize access ) . the critical problem is to discover the hidden structure in the semistructured data . current methods in extracting web data structure are either in a general way independent of application background , or bound in some concrete environment such as html , xml etc. but both face the burden of expensive cost and difficulty in keeping along with the frequent and complicated variances of web data . in this paper , the problem of incremental mining of schema for semistructured data after the update of the raw data is discussed . an algorithm for incrementally mining the schema of semistructured data is provided , and some experimental results are also given , which show that incremental mining for semistructured data is more efficient than non incremental mining .
early stopping heuristics in pool based incremental active learning for least squares probabilistic classifier . <eos> the objective of pool based incremental active learning is to choose a sample to label from a pool of unlabeled samples in an incremental manner so that the generalization error is minimized . in this scenario , the generalization error often hits a minimum in the middle of the incremental active learning procedure and then it starts to increase . in this paper , we address the problem of early labeling stopping in probabilistic classification for minimizing the generalization error and the labeling cost . among several possible strategies , we propose to stop labeling when the empirical class posterior approximation error is maximized . experiments on benchmark datasets demonstrate the usefulness of the proposed strategy .
effect of evaluators ' cognitive style on heuristic evaluation field dependent and field independent evaluators . <eos> heuristic evaluation is a widely used usability evaluation method rosenbaum et al. , <digit> . a toolkit for strategic usability results from workshops , panels , and surveys . in little , r. , nigay , l. ( eds . ) in proceedings of acm chi <digit> conference , new york , pp. <digit> <digit> . but it suffers from large variability in the evaluation results due to differences among evaluators nielsen , <digit> . usability engineering . academic press , boston , ma . the evaluation performance of evaluators with two types of cognitive styles ten field independent ( fi ) subjects and ten field dependent ( fd ) subjects were compared . the results indicated that the fi subjects produced evaluation results with significantly higher thoroughness ( t ( <digit> ) 3.49 p 0.0026 ) , validity ( t ( <digit> ) 4.263 p 0.0005 ) , effectiveness ( t ( <digit> ) 5.14 , p 0.0001 ) , and sensitivity ( t ( <digit> ) 3.16 p 0.005 ) than the fd subjects . when assessing their own evaluation experiences , the fi subjects felt it was easier to find usability problems than the fd subjects ( t ( <digit> ) 2.10 , p 0.049 ) , but the fd subjects felt more guided during the evaluation than the fi subjects ( t ( <digit> ) 2.281 p 0.035 ) . ( c ) <digit> elsevier ltd. all rights reserved .
a linear space algorithm for computing a longest common increasing subsequence . <eos> let x and y be sequences of integers . a common increasing subsequence of x and y is an increasing subsequence common to x and y. in this note , we propose an o ( vertical bar x vertical bar center dot vertical bar y vertical bar ) time and o ( vertical bar x vertical bar vertical bar y vertical bar ) space algorithm for finding one of the longest common increasing subsequences of x and y , which improves the space complexity of yang et al. i.h. yang , c.r huang , k.m. chao , a fast algorithm for computing a longest common increasing subsequence , inform . process . lett . <digit> ( <digit> ) <digit> <digit> o ( vertical bar x vertical bar center dot vertical bar y vertical bar ) time and o ( vertical bar x vertical bar center dot vertical bar y vertical bar ) space algorithm , where vertical bar x vertical bar and vertical bar y vertical bar denote the lengths of x and y , respectively . ( c ) <digit> elsevier b.v. all rights reserved .
fuzzy order statistics and their application to fuzzy clustering . <eos> the median and the median absolute deviation ( mad ) are robust statistics based on order statistics . order statistics are extended to fuzzy sets to define a fuzzy median and a fuzzy mad . the fuzzy c means ( fcm ) clustering algorithm is defined for any p norm ( ppcm ) , including the l ( <digit> ) norm ( 1fcm ) . the 1fcm clustering algorithm is implemented via the alternating optimization ( ao ) method and the clustering centers are shown to be the fuzzy median . the resulting ao 1fcm clustering algorithm is called the fuzzy c medians ( fcmed ) clustering algorithm . an example illustrates the robustness of the fcmed .
setup and open stacks minimization in one dimensional stock cutting . <eos> the primary objective in cutting and packing problems is trim loss or material input minimization ( in stock cutting ) or value maximization ( in knapsack type problems ) . however , in real life production we usually have many other objectives ( costs ) and constraints . probably the most complex auxiliary criteria in one dimensional stock cutting are the number of different cutting patterns ( setups ) and the maximum number of open stacks during the cutting process . there are applications where the number of stacks is restricted to two . we design a sequential heuristic to minimize material input and show its high effectiveness for this purpose . then we extend it to restrict the number of open stacks to any given limit . then , the heuristic is simplified and integrated into a setup minimization approach in order to combine setup and open stacks minimization . to get a smaller number of open stacks , we may split up the problem into several parts of smaller sizes . different solutions are evaluated in relation to the multiple objectives using the pareto criterion .
on optimizing csma for wide area ad hoc networks . <eos> the recent deployment of data rich smart phones has led to a fresh impetus for understanding the performance of wide area ad hoc networks . the most popular medium access mechanism for such ad hoc networks is csma ca with rts cts . in csma like mechanisms , spatial reuse is achieved by implementing energy based guard zones . we consider the problem of simultaneously scheduling the maximum number of links that can achieve a given signal to interference ratio ( sir ) . in this paper , using tools from stochastic geometry , we study and maximize the medium access probability of a typical link . our contributions are two fold ( i ) we show that a simple modification to the rts cts mechanism , viz. , changing the receiver yield decision from an energy level guard zone to an sir guard zone , leads to performance gains and ( ii ) we show that this combined with a simple modification to the transmit power levelsetting it inversely proportional to the square root of the link gainleads to significant improvements in network throughput . further , this simple power level choice is no worse than a factor of two away from optimal over the class of all local power level selection strategies for fading channels , and further is optimal in the non fading case . the analysis relies on an extension of the matrn hard core point process which allows us to quantify both these sir guard zones and this power control mechanism .
efficient clientserver based implementations of mobile speech recognition services . <eos> the purpose of this paper is to demonstrate the efficiencies that can be achieved when automatic speech recognition ( asr ) applications are provided to large user populations using clientserver implementations of interactive voice services . it is shown that , through proper design of a clientserver framework , excellent overall system performance can be obtained with minimal demands on the computing resources that are allocated to asr . system performance is considered in the paper in terms of both asr speed and accuracy in multi user scenarios . an asr resource allocation strategy is presented that maintains sub second average speech recognition response latencies observed by users even as the number of concurrent users exceeds the available number of asr servers by more than an order of magnitude . an architecture for unsupervised estimation of user specific feature space adaptation and normalization algorithms is also described and evaluated . significant reductions in asr word error rate were obtained by applying these techniques to utterances collected from users of hand held mobile devices . these results are important because , while there is a large body of work addressing the speed and accuracy of individual asr decoders , there has been very little effort applied to dealing with the same issues when a large number of asr decoders are used in multi user scenarios .
enabling improved ir based feature location . <eos> recent solutions to software engineering problems have incorporated tools and techniques from information retrieval ( ir ) . the use of ir requires choosing an appropriate retrieval model and deciding on a query that best captures a particular information need . taking feature location as a representative example , three research questions are investigated ( <digit> ) the impact of query preprocessing , ( <digit> ) the impact that different scraping techniques for queries have on retrieval performance , ( <digit> ) the performance impact that the underlying retrieval model has on identifying the correct source code functions ( the correct documents ) . these research questions are addressed using the five open source projects released as part of the semeru dataset . in the experiments , five methods of scraping queries from modification requests and seven retrieval model instances are considered . using the standard evaluation metric mean reciprocal rank ( mrr ) , the experimental analysis reveals that better retrieval models are not the ones commonly used by software engineering researchers . results find that models based on query likelihood perform about twice as well as models in common use in software engineering such as lsi and thus deserve greater attention . furthermore , corpus preprocessing has a significant impact as the top performing setting is over <digit> % better than the average .
input sensitive scalable continuous join query processing . <eos> this article considers the problem of scalably processing a large number of continuous queries . our approach , consisting of novel data structures and algorithms and a flexible processing framework , advances the state of the art in several ways . first , our approach is query sensitive in the sense that it exploits potential overlaps in query predicates for efficient group processing . we partition the collection of continuous queries into groups based on the clustering patterns of the query predicates , and apply specialized processing strategies to heavily clustered groups ( or hotspots ) . we show how to maintain the hotspots efficiently , and use them to scalably process continuous select join , band join , and window join queries . second , our approach is also data sensitive , in the sense that it makes cost based decisions on how to process each incoming tuple based on its characteristics . experiments demonstrate that our approach can improve the processing throughput by orders of magnitude .
automated plan assessment in cognitive manufacturing . <eos> in a cognitive factory setting , product manufacturing is automatically planned and scheduled , exploiting a knowledge base that describes component capabilities and behaviors of the factory . however , because planning and scheduling are computationally hard , they must typically be done offline using a simplified system model , and are thus unaware of online observations and potential component faults . this leads to a problem given behavior models and online observations of possibly faulty behavior , how likely is each manufacturing process plan to still succeed in this work , we first formalize this problem in the context of probabilistic reasoning as plan assessment . then we contribute a solution which computes plan success probabilities based on most likely system behaviors retrieved from solving a constraint optimization problem . the constraint optimization problem is solved using well optimized off the shelf solvers . results obtained with a prototype show that our method can guide systems away from plans which rely on suspect components .
autotutor and family a review of <digit> years of natural language tutoring . <eos> autotutor is a natural language tutoring system that has produced learning gains across multiple domains ( e.g. , computer literacy , physics , critical thinking ) . in this paper , we review the development , key research findings , and systems that have evolved from autotutor . first , the rationale for developing autotutor is outlined and the advantages of natural language tutoring are presented . next , we review three central themes in autotutors development human inspired tutoring strategies , pedagogical agents , and technologies that support natural language tutoring . research on early versions of autotutor documented the impact on deep learning by co constructed explanations , feedback , conversational scaffolding , and subject matter content . systems that evolved from autotutor added additional components that have been evaluated with respect to learning and motivation . the latter findings include the effectiveness of deep reasoning questions for tutoring multiple domains , of adapting to the affect of low knowledge learners , of content over surface features such as voices and persona of animated agents , and of alternative tutoring strategies such as collaborative lecturing and vicarious tutoring demonstrations . the paper also considers advances in pedagogical agent roles ( such as trialogs ) and in tutoring technologies , such semantic processing and tutoring delivery platforms . this paper summarizes and integrates significant findings produced by studies using autotutor and related systems .
a thd reduction high efficiency audio amplifier using inverter based otas with filter output feedback . <eos> this paper presents an integrated low voltage thd reduction high efficiency class d audio amplifier using inverter based operational transconductance amplifiers ( otas ) . we propose a negative feedback loop which can compensate for external perturbations and improving output precision . the compensator increases the audio frequency loop gain , and leads to a better rejection of audio frequency disturbances . the use of inverter based ota and comparator provides low voltage operation and low power dissipation . the audio amplifier operates with a 1.5 v supply voltage with maximum power efficiency of <digit> % . the proposed class d amplifier was implemented using a tsmc 0.18 m 1p6m cmos process , and the active chip area is 1.87 mm2 .
tangent hyperplane kernel principal component analysis for denoising . <eos> kernel principal component analysis ( kpca ) is a method widely used for denoising multivariate data . using geometric arguments , we investigate why a projection operation inherent to all existing kpca denoising algorithms can sometimes cause very poor denoising . based on this , we propose a modification to the projection operation that remedies this problem and can be incorporated into any of the existing kpca algorithms . using toy examples and real datasets , we show that the proposed algorithm can substantially improve denoising performance and is more robust to misspecification of an important tuning parameter .
a case study on value based requirements tracing . <eos> project managers aim at keeping track of interdependencies between various artifacts of the software development lifecycle , to find out potential requirements conflicts , to better understand the impact of change requests , and to fulfill process quality standards , such as cmmi requirements . while there are many methods and techniques on how to technically store requirements traces , the economic issues of dealing with requirements tracing complexity remain open . in practice tracing is typically not an explicit systematic process , but occurs rather ad hoc with considerable hidden tracing related quality costs . this paper reports a case study on value based requirements tracing ( vbrt ) that systematically supports project managers in tailoring requirements tracing precision and effort based on the parameters stakeholder value , requirements risk volatility , and tracing costs . main results of the case study were ( a ) vbrt took around <digit> % effort of full requirements tracing ( b ) more risky or volatile requirements warranted more detailed tracing because of their higher change probability .
noise based deterministic logic and computing a brief survey . <eos> a short survey is provided about our recent explorations of the young topic of noise based logic . after outlining the motivation behind noise based computation schemes , we present a short summary of our ongoing efforts in the introduction , development and design of several noise based deterministic multivalued logic schemes and elements . in particular , we describe classical , instantaneous , continuum , spike and random telegraph signal based schemes with applications such as circuits that emulate the brain 's functioning and string verification via a slow communication channel .
on using deterministic fea software to solve problems in stochastic structural mechanics . <eos> over the last three decades there has been an outstanding growth in the development of deterministic finite element codes with extensive analysis capabilities . extension of such deterministic codes to solve problems in stochastic mechanics is of much interest to the academic research community and industry . in this paper we discuss some of the issues involved in integrating fully grown third party deterministic finite element codes with stochastic projection schemes . the objective of this study is to lay the foundation for development of an easy to use general purpose stochastic finite element software for carrying out probabilistic analysis of large scale engineering systems . we present a brief introduction to stochastic reduced basis projection schemes and the steps involved in coupling them with a typical deterministic finite element software . we demonstrate with the help of a number of case studies how a coupled framework can be used for solving problems in probabilistic mechanics .
gene selection with guided regularized random forest . <eos> derive an upper bound for the number of distinct gini information gain values in a tree node , and discuss the node sparsity issue at a node with a small number of instances and a large number of features . propose the guided rrf method . conduct extensive experiments and analysis . demonstrate the advantages of grrf over well known methods .
using eigencolor normalization for illumination invariant color object recognition . <eos> color is one of salient features for color object recognition , however , the colors of object images sensitively depend on scene illumination . to overcome the lighting dependency problem , a color constancy or color normalization method has to be used . this paper presents a color image normalization method , called eigencolor normalization , which consists of two phases as follows . first , the compacting method , which was originally used for compensating the adverse effect due to shape distortion for <digit> d planar objects , is exploited for <digit> d color space to make the color distribution less correlated and more compact . second , the compact color image is further normalized by rotating the histogram to align with the reference axis computed . consequently , the object colors are transformed into a new color space , called eigencolor space , which reflects the inherent colors of the object and is more invariant to illumination changes . experimental results show that our eigencolor normalization method is superior to other existing color constancy or color normalization schemes on achieving more accurate color object recognition .
adaptive watermark mechanism for rightful ownership protection . <eos> watermarking is used to protect the integrity and copyright of images . conventional copyright protection mechanisms however , are not robust enough or require complex computations to embed the watermark into the host image . in this article , we propose an adaptive copyright protection scheme without the use of discrete cosine transformation ( dct ) and discrete wavelet transformation ( dwt ) . this novel approach allows image owners to adjust the strength of watermarks through a threshold , so that the robustness of the watermark can be enhanced . moreover , our scheme can resist various signal processing operations ( such as blurring , jpeg compression , and noising ) and geometric transformations ( such as cropping , rotation , and scaling ) . the experimental results show that our scheme outperforms related works in most cases . specifically , our scheme preserves the data lossless requirement , so it is suitable for medical and artistic images .
social networking and digital gaming media convergence classification and its consequences for appropriation . <eos> within the field of information systems , a good proportion of research is concerned with the work organisation and this has , to some extent , restricted the kind of application areas given consideration . yet , it is clear that information and communication technology deployments beyond the work organisation are acquiring increased importance in our lives . with this in mind , we offer a field study of the appropriation of an online play space known as habbo hotel . habbo hotel , as a site of media convergence , incorporates social networking and digital gaming functionality . our research highlights the ethical problems such a dual classification of technology may bring . we focus upon a particular set of activities undertaken within and facilitated by the spacescamming . scammers dupe members with respect to their furni , virtual objects that have online and offline economic value . through our analysis we show that sometimes , online activities are bracketed off from those defined as offline and that this can be related to how the technology is classified by membersas a social networking site and or a digital game . in turn , this may affect members beliefs about rights and wrongs . we conclude that given increasing media convergence , the way forward is to continue the project of educating people regarding the difficulties of determining rights and wrongs , and how rights and wrongs may be acted out with respect to new technologies of play online and offline .
the financing of innovative smes a multicriteria credit rating model . <eos> innovative smes face many obstacles to access the credit market . we propose a multicriteria credit rating model to the aim of financing innovative smes . the multicriteria method is electre tri , outperformed within a smaa tri analysis . soft information is emphasized in the model proposed . we carry out a real case study to illustrate the multicriteria model proposed .
virtual communities in cyberspace . <eos> reintroduces alienation theory and research . considers alienation in a cybernetic sense as a generic term for different types of information processing problems in individuals . considers these , more often than not , to be ultimately caused by developments in their social environment and reflecting back on that environment . discusses the emergence of virtual communities , giving as an example the thematic group on sociocybernetics and social systems . describes its function and composition in a virtual community context . considers whether norbert wiener foresaw the emergence of virtual communities and examines the link with the society envisaged by niklas luhmann .
wholesale price rebate vs. capacity expansion the optimal strategy for seasonal products in a supply chain . <eos> we study a manufacturer sells a seasonal product through a retailer to the market . the manufacturer uses two strategies wholesale price rebate or capacity expansion . the manufacturer can offer the retailer a subsidy for taking away the inventories . the manufacturer can raise the capacity to aggregate the production . the supply chain can achieve a winwin situation and both parties are better off .
decomposition integrals . <eos> decomposition integrals recently proposed by even and lehrer are deeply studied and discussed . characterization of integrals recognizing and distinguishing the underlying measures is given . as a by product , a graded class of integrals varying from shilkret integral to choquet integral is proposed . ( c ) <digit> elsevier inc. all rights reserved .
analysis , design , and performance evaluation of ms rtcp more scalable scheme for the real time control protocol . <eos> recently , some problems related to the use of the real time control protocol ( rtcp ) in very large dynamic groups have arisen . some of these problems are feedback delay , increasing storage state at every member , and ineffective rtcp bandwidth usage , especially for the receivers that obtain incoming rtcp reports through low bandwidth links . more schemes are proposed to alleviate the rtcp problems . the famous and recent one , which was introduced by el marakby and was named scalable rtcp ( s rtcp ) , still has several drawbacks . this paper will evaluate the previous model by introducing all its drawbacks . consequently , we will demonstrate a design of more scalable rtcp ( ms rtcp ) scheme based on hierarchical structure , distributed management , and el marakby scheme . also , we will show how our scheme will alleviate all the drawbacks found in the s rtcp . finally , we will introduce our scheme implementation to analyze and evaluate its performance .
chat mining automatically determination of chat conversations ' topic in turkish text based chat mediums . <eos> mostly , the conversations taking place in chat mediums bear important information concerning the speakers . this information can vary in many fields such as tendencies , habits , attitudes , guilt situations , and intentions of the speakers . therefore , analysis and processing of these conversations are of much importance . many social and semantic inferences can be made from these conversations . in determining characteristics of conversations and analysis of conversations , subject designation can be grounded on . in this study , chat mining is chosen as an application of text mining , and a study concerning determination of subject in the turkish text based chat conversations is conducted . in sorting the conversations , supervised learning methods are used in this study . as for classifiers , naive bayes , k nearest neighbor and support vector machine are used . ninety one percent success is achieved in determination of subject . ( c ) <digit> elsevier ltd. all rights reserved .
